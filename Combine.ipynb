{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15a8e01f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93b59c98",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "#dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, [0,1,2]]\n",
    "#y_train = dataset[:train_split, 3]\n",
    "#X_test = dataset[train_split:, [0,1,2]]\n",
    "#y_test = dataset[train_split:, 3]\n",
    "\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "#prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "#prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "#X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16000, 1999])\n",
      "torch.Size([16000])\n",
      "torch.Size([4000, 1999])\n",
      "torch.Size([4000])\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized.txt\", sep=' ', header=None, nrows=20000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8dd85410",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1017f7dd",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/250000], Loss: 1.00000000\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/250000], Loss: 0.99998397\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/250000], Loss: 0.99996618\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/250000], Loss: 0.99994839\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/250000], Loss: 0.99993062\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/250000], Loss: 0.99991285\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/250000], Loss: 0.99989510\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/250000], Loss: 0.99987735\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/250000], Loss: 0.99985962\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/250000], Loss: 0.99984190\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/250000], Loss: 0.99982418\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/250000], Loss: 0.99980647\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/250000], Loss: 0.99978878\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/250000], Loss: 0.99977109\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/250000], Loss: 0.99975341\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/250000], Loss: 0.99973574\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/250000], Loss: 0.99971808\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/250000], Loss: 0.99970042\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/250000], Loss: 0.99968278\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/250000], Loss: 0.99966514\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/250000], Loss: 0.99964751\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/250000], Loss: 0.99962989\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/250000], Loss: 0.99961227\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/250000], Loss: 0.99959466\n",
      "tensor(-0.3854, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/250000], Loss: 0.99957706\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb Cell 6\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m learning_rate \u001b[39m=\u001b[39m \u001b[39m0.001\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m num_iterations \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m w, b, cost_history \u001b[39m=\u001b[39m gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m \u001b[39m# Print the learned parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLearned parameters:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m cost_history \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_iterations):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m# Calculate predictions\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(X, w) \u001b[39m+\u001b[39m b\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Calculate the difference between predictions and actual values\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#W6sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     error \u001b[39m=\u001b[39m y_pred \u001b[39m-\u001b[39m y\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bbf7d08d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb Cell 7\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m batch_size \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m w, b, cost_history \u001b[39m=\u001b[39m stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m \u001b[39m# Print the learned parameters\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLearned parameters:\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb Cell 7\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     \u001b[39m# Print the loss every 100 epochs\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m (epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39m10\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mor\u001b[39;00m epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         \u001b[39mprint\u001b[39m(outputs[\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mcost\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.8f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ronaldng/Documents/FYP/FTEC4998-main/Combine.ipynb#X10sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mreturn\u001b[39;00m w, b, cost_history\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradien Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cfbbe",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD Test (This is done by Chris for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9bc595f3",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10000], Loss: 0.49668750, Error: 0.49668750166893005\n",
      "Epoch [1000/10000], Loss: 0.19400567, Error: 0.1912499964237213\n",
      "Epoch [2000/10000], Loss: 0.16407800, Error: 0.148375004529953\n",
      "Epoch [3000/10000], Loss: 0.14538425, Error: 0.1301874965429306\n",
      "Epoch [4000/10000], Loss: 0.13329563, Error: 0.12162499874830246\n",
      "Epoch [5000/10000], Loss: 0.12525328, Error: 0.11625000089406967\n",
      "Epoch [6000/10000], Loss: 0.11973827, Error: 0.11243750154972076\n",
      "Epoch [7000/10000], Loss: 0.11583292, Error: 0.10925000160932541\n",
      "Epoch [8000/10000], Loss: 0.11297543, Error: 0.1068124994635582\n",
      "Epoch [9000/10000], Loss: 0.11081700, Error: 0.10499999672174454\n",
      "Epoch [10000/10000], Loss: 0.10913745, Error: 0.10275000333786011\n",
      "Trained weights: tensor([ 0.0911,  0.0423,  0.3707,  ...,  0.0096,  0.0364, -0.0435],\n",
      "       requires_grad=True)\n",
      "Trained bias: tensor([0.4960], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.1028)\n",
      "w detach tensor([ 0.0911,  0.0423,  0.3707,  ...,  0.0096,  0.0364, -0.0435])\n",
      "b detach tensor([0.4960])\n",
      "Test error rate: tensor(0.1268)\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.1\n",
    "num_epochs = 10000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0 or epoch == 0:\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "print(\"w detach\", w.detach())\n",
    "print(\"b detach\", b.detach())\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6390f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=1999, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(-0.1951, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/5000], Loss: 1.06646037\n",
      "tensor(-0.0071, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/5000], Loss: 1.01015317\n",
      "tensor(-0.0336, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/5000], Loss: 1.00486779\n",
      "tensor(-0.0493, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/5000], Loss: 1.00089991\n",
      "tensor(-0.0586, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/5000], Loss: 0.99740100\n",
      "tensor(-0.0643, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/5000], Loss: 0.99407750\n",
      "tensor(-0.0678, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/5000], Loss: 0.99082839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_4576\\304550788.py:18: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  param.data.add_(-lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0699, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/5000], Loss: 0.98761863\n",
      "tensor(-0.0714, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/5000], Loss: 0.98443586\n",
      "tensor(-0.0724, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/5000], Loss: 0.98127580\n",
      "tensor(-0.0731, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/5000], Loss: 0.97813642\n",
      "tensor(-0.0737, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/5000], Loss: 0.97501755\n",
      "tensor(-0.0741, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/5000], Loss: 0.97191846\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/5000], Loss: 0.96883893\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/5000], Loss: 0.96577883\n",
      "tensor(-0.0753, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/5000], Loss: 0.96273780\n",
      "tensor(-0.0756, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/5000], Loss: 0.95971566\n",
      "tensor(-0.0760, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/5000], Loss: 0.95671219\n",
      "tensor(-0.0763, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/5000], Loss: 0.95372725\n",
      "tensor(-0.0767, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/5000], Loss: 0.95076066\n",
      "tensor(-0.0770, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/5000], Loss: 0.94781202\n",
      "tensor(-0.0773, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/5000], Loss: 0.94488144\n",
      "tensor(-0.0776, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/5000], Loss: 0.94196856\n",
      "tensor(-0.0779, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/5000], Loss: 0.93907303\n",
      "tensor(-0.0782, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/5000], Loss: 0.93619502\n",
      "tensor(-0.0786, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/5000], Loss: 0.93333417\n",
      "tensor(-0.0789, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/5000], Loss: 0.93049020\n",
      "tensor(-0.0792, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/5000], Loss: 0.92766309\n",
      "tensor(-0.0795, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/5000], Loss: 0.92485261\n",
      "tensor(-0.0798, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/5000], Loss: 0.92205864\n",
      "tensor(-0.0800, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/5000], Loss: 0.91928083\n",
      "tensor(-0.0803, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/5000], Loss: 0.91651922\n",
      "tensor(-0.0806, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/5000], Loss: 0.91377366\n",
      "tensor(-0.0809, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/5000], Loss: 0.91104376\n",
      "tensor(-0.0812, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/5000], Loss: 0.90832973\n",
      "tensor(-0.0815, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/5000], Loss: 0.90563095\n",
      "tensor(-0.0818, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/5000], Loss: 0.90294749\n",
      "tensor(-0.0820, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/5000], Loss: 0.90027940\n",
      "tensor(-0.0823, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/5000], Loss: 0.89762634\n",
      "tensor(-0.0826, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/5000], Loss: 0.89498800\n",
      "tensor(-0.0828, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/5000], Loss: 0.89236444\n",
      "tensor(-0.0831, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/5000], Loss: 0.88975561\n",
      "tensor(-0.0834, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/5000], Loss: 0.88716108\n",
      "tensor(-0.0836, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/5000], Loss: 0.88458109\n",
      "tensor(-0.0839, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/5000], Loss: 0.88201505\n",
      "tensor(-0.0841, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/5000], Loss: 0.87946326\n",
      "tensor(-0.0844, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/5000], Loss: 0.87692535\n",
      "tensor(-0.0847, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/5000], Loss: 0.87440115\n",
      "tensor(-0.0849, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/5000], Loss: 0.87189072\n",
      "tensor(-0.0851, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/5000], Loss: 0.86939377\n",
      "tensor(-0.0854, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/5000], Loss: 0.86691010\n",
      "tensor(-0.0856, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/5000], Loss: 0.86443990\n",
      "tensor(-0.0859, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/5000], Loss: 0.86198282\n",
      "tensor(-0.0861, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/5000], Loss: 0.85953873\n",
      "tensor(-0.0863, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/5000], Loss: 0.85710770\n",
      "tensor(-0.0866, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/5000], Loss: 0.85468936\n",
      "tensor(-0.0868, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/5000], Loss: 0.85228378\n",
      "tensor(-0.0870, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/5000], Loss: 0.84989077\n",
      "tensor(-0.0873, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/5000], Loss: 0.84751028\n",
      "tensor(-0.0875, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/5000], Loss: 0.84514207\n",
      "tensor(-0.0877, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/5000], Loss: 0.84278607\n",
      "tensor(-0.0879, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/5000], Loss: 0.84044236\n",
      "tensor(-0.0881, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/5000], Loss: 0.83811069\n",
      "tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/5000], Loss: 0.83579087\n",
      "tensor(-0.0886, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/5000], Loss: 0.83348298\n",
      "tensor(-0.0888, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/5000], Loss: 0.83118677\n",
      "tensor(-0.0890, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/5000], Loss: 0.82890213\n",
      "tensor(-0.0892, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/5000], Loss: 0.82662910\n",
      "tensor(-0.0894, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/5000], Loss: 0.82436752\n",
      "tensor(-0.0896, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/5000], Loss: 0.82211733\n",
      "tensor(-0.0898, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/5000], Loss: 0.81987840\n",
      "tensor(-0.0900, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/5000], Loss: 0.81765044\n",
      "tensor(-0.0902, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/5000], Loss: 0.81543374\n",
      "tensor(-0.0904, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/5000], Loss: 0.81322801\n",
      "tensor(-0.0906, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/5000], Loss: 0.81103307\n",
      "tensor(-0.0908, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/5000], Loss: 0.80884898\n",
      "tensor(-0.0910, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/5000], Loss: 0.80667555\n",
      "tensor(-0.0912, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/5000], Loss: 0.80451274\n",
      "tensor(-0.0913, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/5000], Loss: 0.80236048\n",
      "tensor(-0.0915, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/5000], Loss: 0.80021882\n",
      "tensor(-0.0917, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/5000], Loss: 0.79808730\n",
      "tensor(-0.0919, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/5000], Loss: 0.79596621\n",
      "tensor(-0.0921, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/5000], Loss: 0.79385519\n",
      "tensor(-0.0922, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/5000], Loss: 0.79175454\n",
      "tensor(-0.0924, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/5000], Loss: 0.78966367\n",
      "tensor(-0.0926, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/5000], Loss: 0.78758299\n",
      "tensor(-0.0928, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/5000], Loss: 0.78551209\n",
      "tensor(-0.0929, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/5000], Loss: 0.78345102\n",
      "tensor(-0.0931, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/5000], Loss: 0.78139979\n",
      "tensor(-0.0933, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/5000], Loss: 0.77935809\n",
      "tensor(-0.0934, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/5000], Loss: 0.77732599\n",
      "tensor(-0.0936, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/5000], Loss: 0.77530348\n",
      "tensor(-0.0938, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/5000], Loss: 0.77329040\n",
      "tensor(-0.0939, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/5000], Loss: 0.77128673\n",
      "tensor(-0.0941, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/5000], Loss: 0.76929235\n",
      "tensor(-0.0942, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/5000], Loss: 0.76730722\n",
      "tensor(-0.0944, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/5000], Loss: 0.76533121\n",
      "tensor(-0.0945, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/5000], Loss: 0.76336443\n",
      "tensor(-0.0947, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/5000], Loss: 0.76140666\n",
      "tensor(-0.0948, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/5000], Loss: 0.75945771\n",
      "tensor(-0.0950, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/5000], Loss: 0.75751787\n",
      "tensor(-0.0951, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/5000], Loss: 0.75558668\n",
      "tensor(-0.0953, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/5000], Loss: 0.75366449\n",
      "tensor(-0.0954, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/5000], Loss: 0.75175089\n",
      "tensor(-0.0956, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/5000], Loss: 0.74984586\n",
      "tensor(-0.0957, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/5000], Loss: 0.74794966\n",
      "tensor(-0.0958, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/5000], Loss: 0.74606168\n",
      "tensor(-0.0960, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/5000], Loss: 0.74418253\n",
      "tensor(-0.0961, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/5000], Loss: 0.74231154\n",
      "tensor(-0.0963, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/5000], Loss: 0.74044907\n",
      "tensor(-0.0964, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/5000], Loss: 0.73859483\n",
      "tensor(-0.0965, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/5000], Loss: 0.73674881\n",
      "tensor(-0.0967, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/5000], Loss: 0.73491096\n",
      "tensor(-0.0968, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/5000], Loss: 0.73308128\n",
      "tensor(-0.0969, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/5000], Loss: 0.73125958\n",
      "tensor(-0.0970, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/5000], Loss: 0.72944611\n",
      "tensor(-0.0972, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/5000], Loss: 0.72764051\n",
      "tensor(-0.0973, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/5000], Loss: 0.72584283\n",
      "tensor(-0.0974, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/5000], Loss: 0.72405303\n",
      "tensor(-0.0975, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/5000], Loss: 0.72227079\n",
      "tensor(-0.0977, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/5000], Loss: 0.72049665\n",
      "tensor(-0.0978, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/5000], Loss: 0.71873009\n",
      "tensor(-0.0979, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/5000], Loss: 0.71697098\n",
      "tensor(-0.0980, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/5000], Loss: 0.71521974\n",
      "tensor(-0.0981, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/5000], Loss: 0.71347594\n",
      "tensor(-0.0982, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/5000], Loss: 0.71173966\n",
      "tensor(-0.0983, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/5000], Loss: 0.71001089\n",
      "tensor(-0.0985, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/5000], Loss: 0.70828938\n",
      "tensor(-0.0986, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/5000], Loss: 0.70657545\n",
      "tensor(-0.0987, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/5000], Loss: 0.70486856\n",
      "tensor(-0.0988, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/5000], Loss: 0.70316917\n",
      "tensor(-0.0989, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/5000], Loss: 0.70147681\n",
      "tensor(-0.0990, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/5000], Loss: 0.69979179\n",
      "tensor(-0.0991, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/5000], Loss: 0.69811386\n",
      "tensor(-0.0992, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/5000], Loss: 0.69644296\n",
      "tensor(-0.0993, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/5000], Loss: 0.69477904\n",
      "tensor(-0.0994, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/5000], Loss: 0.69312227\n",
      "tensor(-0.0995, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/5000], Loss: 0.69147241\n",
      "tensor(-0.0996, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/5000], Loss: 0.68982935\n",
      "tensor(-0.0997, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/5000], Loss: 0.68819326\n",
      "tensor(-0.0998, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/5000], Loss: 0.68656403\n",
      "tensor(-0.0999, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/5000], Loss: 0.68494135\n",
      "tensor(-0.1000, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/5000], Loss: 0.68332571\n",
      "tensor(-0.1001, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/5000], Loss: 0.68171662\n",
      "tensor(-0.1002, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/5000], Loss: 0.68011421\n",
      "tensor(-0.1002, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/5000], Loss: 0.67851835\n",
      "tensor(-0.1003, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/5000], Loss: 0.67692912\n",
      "tensor(-0.1004, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/5000], Loss: 0.67534643\n",
      "tensor(-0.1005, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/5000], Loss: 0.67377019\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/5000], Loss: 0.67220056\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/5000], Loss: 0.67063719\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/5000], Loss: 0.66908026\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/5000], Loss: 0.66752970\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/5000], Loss: 0.66598541\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/5000], Loss: 0.66444749\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/5000], Loss: 0.66291565\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/5000], Loss: 0.66139007\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/5000], Loss: 0.65987074\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/5000], Loss: 0.65835738\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/5000], Loss: 0.65685022\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/5000], Loss: 0.65534914\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/5000], Loss: 0.65385407\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/5000], Loss: 0.65236491\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/5000], Loss: 0.65088171\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/5000], Loss: 0.64940447\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/5000], Loss: 0.64793307\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/5000], Loss: 0.64646769\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/5000], Loss: 0.64500791\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/5000], Loss: 0.64355397\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/5000], Loss: 0.64210588\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/5000], Loss: 0.64066344\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/5000], Loss: 0.63922673\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/5000], Loss: 0.63779563\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/5000], Loss: 0.63637018\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/5000], Loss: 0.63495034\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/5000], Loss: 0.63353610\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/5000], Loss: 0.63212734\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/5000], Loss: 0.63072413\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/5000], Loss: 0.62932634\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/5000], Loss: 0.62793404\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/5000], Loss: 0.62654710\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/5000], Loss: 0.62516558\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/5000], Loss: 0.62378943\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/5000], Loss: 0.62241858\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/5000], Loss: 0.62105310\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/5000], Loss: 0.61969286\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/5000], Loss: 0.61833781\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/5000], Loss: 0.61698806\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/5000], Loss: 0.61564344\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/5000], Loss: 0.61430401\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/5000], Loss: 0.61296964\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/5000], Loss: 0.61164045\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/5000], Loss: 0.61031634\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/5000], Loss: 0.60899729\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/5000], Loss: 0.60768324\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/5000], Loss: 0.60637414\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/5000], Loss: 0.60507005\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/5000], Loss: 0.60377097\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/5000], Loss: 0.60247678\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/5000], Loss: 0.60118747\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/5000], Loss: 0.59990299\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/5000], Loss: 0.59862351\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/5000], Loss: 0.59734863\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/5000], Loss: 0.59607869\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/5000], Loss: 0.59481359\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/5000], Loss: 0.59355319\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/5000], Loss: 0.59229743\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/5000], Loss: 0.59104651\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/5000], Loss: 0.58980006\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/5000], Loss: 0.58855838\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/5000], Loss: 0.58732146\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/5000], Loss: 0.58608902\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/5000], Loss: 0.58486104\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/5000], Loss: 0.58363783\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/5000], Loss: 0.58241904\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/5000], Loss: 0.58120477\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/5000], Loss: 0.57999504\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/5000], Loss: 0.57878983\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/5000], Loss: 0.57758898\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/5000], Loss: 0.57639265\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/5000], Loss: 0.57520056\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/5000], Loss: 0.57401299\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/5000], Loss: 0.57282972\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/5000], Loss: 0.57165086\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/5000], Loss: 0.57047629\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/5000], Loss: 0.56930602\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/5000], Loss: 0.56814003\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/5000], Loss: 0.56697828\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/5000], Loss: 0.56582075\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/5000], Loss: 0.56466746\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/5000], Loss: 0.56351835\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/5000], Loss: 0.56237340\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/5000], Loss: 0.56123263\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/5000], Loss: 0.56009603\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/5000], Loss: 0.55896354\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/5000], Loss: 0.55783510\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/5000], Loss: 0.55671072\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/5000], Loss: 0.55559045\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/5000], Loss: 0.55447423\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/5000], Loss: 0.55336195\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/5000], Loss: 0.55225372\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/5000], Loss: 0.55114949\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/5000], Loss: 0.55004919\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/5000], Loss: 0.54895288\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/5000], Loss: 0.54786038\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/5000], Loss: 0.54677194\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/5000], Loss: 0.54568732\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/5000], Loss: 0.54460651\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/5000], Loss: 0.54352957\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/5000], Loss: 0.54245657\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/5000], Loss: 0.54138732\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2510/5000], Loss: 0.54032183\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2520/5000], Loss: 0.53926015\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2530/5000], Loss: 0.53820223\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2540/5000], Loss: 0.53714806\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2550/5000], Loss: 0.53609765\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2560/5000], Loss: 0.53505093\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2570/5000], Loss: 0.53400785\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2580/5000], Loss: 0.53296846\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2590/5000], Loss: 0.53193283\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2600/5000], Loss: 0.53090072\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2610/5000], Loss: 0.52987230\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2620/5000], Loss: 0.52884740\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2630/5000], Loss: 0.52782625\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2640/5000], Loss: 0.52680856\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2650/5000], Loss: 0.52579451\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2660/5000], Loss: 0.52478385\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2670/5000], Loss: 0.52377695\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2680/5000], Loss: 0.52277339\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2690/5000], Loss: 0.52177334\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2700/5000], Loss: 0.52077681\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2710/5000], Loss: 0.51978374\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2720/5000], Loss: 0.51879412\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2730/5000], Loss: 0.51780796\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2740/5000], Loss: 0.51682520\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2750/5000], Loss: 0.51584584\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2760/5000], Loss: 0.51486987\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2770/5000], Loss: 0.51389718\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2780/5000], Loss: 0.51292807\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2790/5000], Loss: 0.51196212\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2800/5000], Loss: 0.51099944\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2810/5000], Loss: 0.51004022\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2820/5000], Loss: 0.50908428\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2830/5000], Loss: 0.50813156\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2840/5000], Loss: 0.50718218\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2850/5000], Loss: 0.50623602\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2860/5000], Loss: 0.50529307\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2870/5000], Loss: 0.50435334\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2880/5000], Loss: 0.50341684\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2890/5000], Loss: 0.50248355\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2900/5000], Loss: 0.50155342\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2910/5000], Loss: 0.50062656\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2920/5000], Loss: 0.49970275\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2930/5000], Loss: 0.49878213\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2940/5000], Loss: 0.49786460\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2950/5000], Loss: 0.49695024\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2960/5000], Loss: 0.49603888\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2970/5000], Loss: 0.49513069\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2980/5000], Loss: 0.49422553\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2990/5000], Loss: 0.49332348\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3000/5000], Loss: 0.49242452\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3010/5000], Loss: 0.49152842\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3020/5000], Loss: 0.49063545\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3030/5000], Loss: 0.48974550\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3040/5000], Loss: 0.48885849\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3050/5000], Loss: 0.48797452\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3060/5000], Loss: 0.48709351\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3070/5000], Loss: 0.48621535\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3080/5000], Loss: 0.48534027\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3090/5000], Loss: 0.48446804\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3100/5000], Loss: 0.48359871\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3110/5000], Loss: 0.48273239\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3120/5000], Loss: 0.48186886\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3130/5000], Loss: 0.48100823\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3140/5000], Loss: 0.48015052\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3150/5000], Loss: 0.47929561\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3160/5000], Loss: 0.47844359\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3170/5000], Loss: 0.47759438\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3180/5000], Loss: 0.47674796\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3190/5000], Loss: 0.47590438\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3200/5000], Loss: 0.47506359\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3210/5000], Loss: 0.47422549\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3220/5000], Loss: 0.47339028\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3230/5000], Loss: 0.47255778\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3240/5000], Loss: 0.47172809\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3250/5000], Loss: 0.47090110\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3260/5000], Loss: 0.47007692\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3270/5000], Loss: 0.46925527\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3280/5000], Loss: 0.46843639\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3290/5000], Loss: 0.46762025\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3300/5000], Loss: 0.46680677\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3310/5000], Loss: 0.46599588\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3320/5000], Loss: 0.46518776\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3330/5000], Loss: 0.46438220\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3340/5000], Loss: 0.46357933\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3350/5000], Loss: 0.46277913\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3360/5000], Loss: 0.46198142\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3370/5000], Loss: 0.46118632\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3380/5000], Loss: 0.46039382\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3390/5000], Loss: 0.45960400\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3400/5000], Loss: 0.45881662\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3410/5000], Loss: 0.45803192\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3420/5000], Loss: 0.45724973\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3430/5000], Loss: 0.45646998\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3440/5000], Loss: 0.45569283\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3450/5000], Loss: 0.45491821\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3460/5000], Loss: 0.45414609\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3470/5000], Loss: 0.45337647\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3480/5000], Loss: 0.45260924\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3490/5000], Loss: 0.45184457\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3500/5000], Loss: 0.45108241\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3510/5000], Loss: 0.45032257\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3520/5000], Loss: 0.44956532\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3530/5000], Loss: 0.44881034\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3540/5000], Loss: 0.44805795\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3550/5000], Loss: 0.44730785\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3560/5000], Loss: 0.44656026\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3570/5000], Loss: 0.44581497\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3580/5000], Loss: 0.44507208\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3590/5000], Loss: 0.44433156\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3600/5000], Loss: 0.44359344\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3610/5000], Loss: 0.44285762\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3620/5000], Loss: 0.44212419\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3630/5000], Loss: 0.44139308\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3640/5000], Loss: 0.44066438\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3650/5000], Loss: 0.43993777\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3660/5000], Loss: 0.43921372\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3670/5000], Loss: 0.43849179\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3680/5000], Loss: 0.43777224\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3690/5000], Loss: 0.43705499\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3700/5000], Loss: 0.43633991\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3710/5000], Loss: 0.43562713\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3720/5000], Loss: 0.43491662\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3730/5000], Loss: 0.43420836\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3740/5000], Loss: 0.43350229\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3750/5000], Loss: 0.43279848\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3760/5000], Loss: 0.43209690\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3770/5000], Loss: 0.43139747\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3780/5000], Loss: 0.43070027\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3790/5000], Loss: 0.43000522\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3800/5000], Loss: 0.42931241\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3810/5000], Loss: 0.42862171\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3820/5000], Loss: 0.42793319\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3830/5000], Loss: 0.42724687\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3840/5000], Loss: 0.42656258\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3850/5000], Loss: 0.42588055\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3860/5000], Loss: 0.42520049\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3870/5000], Loss: 0.42452276\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3880/5000], Loss: 0.42384702\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3890/5000], Loss: 0.42317340\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3900/5000], Loss: 0.42250180\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3910/5000], Loss: 0.42183235\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3920/5000], Loss: 0.42116502\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3930/5000], Loss: 0.42049962\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3940/5000], Loss: 0.41983643\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3950/5000], Loss: 0.41917518\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3960/5000], Loss: 0.41851604\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3970/5000], Loss: 0.41785896\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3980/5000], Loss: 0.41720381\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3990/5000], Loss: 0.41655076\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4000/5000], Loss: 0.41589966\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4010/5000], Loss: 0.41525063\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4020/5000], Loss: 0.41460347\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4030/5000], Loss: 0.41395843\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4040/5000], Loss: 0.41331527\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4050/5000], Loss: 0.41267413\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4060/5000], Loss: 0.41203499\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4070/5000], Loss: 0.41139776\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4080/5000], Loss: 0.41076243\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4090/5000], Loss: 0.41012916\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4100/5000], Loss: 0.40949768\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4110/5000], Loss: 0.40886819\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4120/5000], Loss: 0.40824068\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4130/5000], Loss: 0.40761501\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4140/5000], Loss: 0.40699124\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4150/5000], Loss: 0.40636939\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4160/5000], Loss: 0.40574935\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4170/5000], Loss: 0.40513125\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4180/5000], Loss: 0.40451503\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4190/5000], Loss: 0.40390065\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4200/5000], Loss: 0.40328813\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4210/5000], Loss: 0.40267745\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4220/5000], Loss: 0.40206864\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4230/5000], Loss: 0.40146172\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4240/5000], Loss: 0.40085652\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4250/5000], Loss: 0.40025318\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4260/5000], Loss: 0.39965162\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4270/5000], Loss: 0.39905190\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4280/5000], Loss: 0.39845398\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4290/5000], Loss: 0.39785793\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4300/5000], Loss: 0.39726353\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4310/5000], Loss: 0.39667100\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4320/5000], Loss: 0.39608026\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4330/5000], Loss: 0.39549115\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4340/5000], Loss: 0.39490387\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4350/5000], Loss: 0.39431840\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4360/5000], Loss: 0.39373457\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4370/5000], Loss: 0.39315253\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4380/5000], Loss: 0.39257225\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4390/5000], Loss: 0.39199361\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4400/5000], Loss: 0.39141673\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4410/5000], Loss: 0.39084160\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4420/5000], Loss: 0.39026821\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4430/5000], Loss: 0.38969639\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4440/5000], Loss: 0.38912627\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4450/5000], Loss: 0.38855794\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4460/5000], Loss: 0.38799119\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4470/5000], Loss: 0.38742614\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4480/5000], Loss: 0.38686278\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4490/5000], Loss: 0.38630107\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4500/5000], Loss: 0.38574100\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4510/5000], Loss: 0.38518262\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4520/5000], Loss: 0.38462579\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4530/5000], Loss: 0.38407063\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4540/5000], Loss: 0.38351712\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4550/5000], Loss: 0.38296524\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4560/5000], Loss: 0.38241500\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4570/5000], Loss: 0.38186622\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4580/5000], Loss: 0.38131917\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4590/5000], Loss: 0.38077369\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4600/5000], Loss: 0.38022977\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4610/5000], Loss: 0.37968743\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4620/5000], Loss: 0.37914675\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4630/5000], Loss: 0.37860757\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4640/5000], Loss: 0.37806997\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4650/5000], Loss: 0.37753388\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4660/5000], Loss: 0.37699944\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4670/5000], Loss: 0.37646651\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4680/5000], Loss: 0.37593505\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4690/5000], Loss: 0.37540522\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4700/5000], Loss: 0.37487683\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4710/5000], Loss: 0.37435010\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4720/5000], Loss: 0.37382472\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4730/5000], Loss: 0.37330094\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4740/5000], Loss: 0.37277871\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4750/5000], Loss: 0.37225792\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4760/5000], Loss: 0.37173867\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4770/5000], Loss: 0.37122077\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4780/5000], Loss: 0.37070450\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4790/5000], Loss: 0.37018967\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4800/5000], Loss: 0.36967629\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4810/5000], Loss: 0.36916438\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4820/5000], Loss: 0.36865398\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4830/5000], Loss: 0.36814499\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4840/5000], Loss: 0.36763749\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4850/5000], Loss: 0.36713135\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4860/5000], Loss: 0.36662671\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4870/5000], Loss: 0.36612350\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4880/5000], Loss: 0.36562172\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4890/5000], Loss: 0.36512133\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4900/5000], Loss: 0.36462238\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4910/5000], Loss: 0.36412483\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4920/5000], Loss: 0.36362875\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4930/5000], Loss: 0.36313400\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4940/5000], Loss: 0.36264074\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4950/5000], Loss: 0.36214870\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4960/5000], Loss: 0.36165822\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4970/5000], Loss: 0.36116898\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4980/5000], Loss: 0.36068124\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [4990/5000], Loss: 0.36019477\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [5000/5000], Loss: 0.35970971\n",
      "activation_stack.0.weight: tensor([[-0.0149,  0.0335,  0.0247,  ..., -0.0062,  0.0082, -0.0144]])\n",
      "activation_stack.0.bias: tensor([-0.0090])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(1999, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)\n",
    "    \n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork()\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d2153",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset preprocessing for Federated Learning\n",
    "def split_data_for_client():\n",
    "    dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\School Documents\\CUHK Notes\\FTEC4998 notes\\Testing Enviroment\\Combine.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     send_client_weights(global_weights, client_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Aggregate client weights on the server\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m aggregated_weights \u001b[39m=\u001b[39m aggregate_weights_on_server(global_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Update global weights with aggregated weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(aggregated_weights)\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\School Documents\\CUHK Notes\\FTEC4998 notes\\Testing Enviroment\\Combine.ipynb Cell 17\u001b[0m line \u001b[0;36maggregate_weights_on_server\u001b[1;34m(client_weights_list)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m aggregated_weights \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Aggregate the client weights\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m client_weights_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     aggregated_weights[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([weights[key] \u001b[39mfor\u001b[39;00m weights \u001b[39min\u001b[39;00m client_weights_list])\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientUpdate:\n",
    "    def __init__(self, model, criterion, optimizer, train_data):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in self.train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self.model.state_dict()\n",
    "\n",
    "def send_client_weights(server, weights):\n",
    "    server.append(weights)\n",
    "    \n",
    "def aggregate_weights_on_server(client_weights_list):\n",
    "    aggregated_weights = {}\n",
    "    # Aggregate the client weights\n",
    "    for key in client_weights_list[0].keys():\n",
    "        aggregated_weights[key] = torch.stack([weights[key] for weights in client_weights_list]).mean(dim=0)\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5000\n",
    "batch_size = 1\n",
    "num_clients = 5\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Add the client data to the train_data list\n",
    "data\n",
    "\n",
    "# Perform training\n",
    "num_clients = len(train_data)\n",
    "global_weights = model.state_dict()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    m = max(num_clients, 1)\n",
    "    selected_clients = torch.randperm(num_clients)[:m]\n",
    "    loss = 0.00\n",
    "\n",
    "    for client in selected_clients:\n",
    "        client_data = train_data[client]\n",
    "        client_update = ClientUpdate(model, criterion, optimizer, client_data[client])\n",
    "        client_weights = client_update.update_weights(num_epochs)\n",
    "\n",
    "        # Send client weights to the server\n",
    "        send_client_weights(global_weights, client_weights)\n",
    "\n",
    "    # Aggregate client weights on the server\n",
    "    aggregated_weights = aggregate_weights_on_server(global_weights)\n",
    "\n",
    "    # Update global weights with aggregated weights\n",
    "    model.load_state_dict(aggregated_weights)\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fb72",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.3",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "julia",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
