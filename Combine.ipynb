{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install math\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "#dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, [0,1,2]]\n",
    "#y_train = dataset[:train_split, 3]\n",
    "#X_test = dataset[train_split:, [0,1,2]]\n",
    "#y_test = dataset[train_split:, 3]\n",
    "\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "#prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "#prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "#X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized\", sep=' ', header=None, nrows=20000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom Vanilia Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(y_pred[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_iterations}], Loss: {cost.item():.8f}')\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.001\n",
    "num_iterations = 100\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradient Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(y_pred[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.1\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch + 1) % 1000 == 0 or epoch == 0:\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "print(\"w detach\", w.detach())\n",
    "print(\"b detach\", b.detach())\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features=1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            outputs = self.model(self.X_train.float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(outputs, self.y_train.float())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Define server wise functions\n",
    "def send_client_weights(server_weights, local_weights):\n",
    "    server_weights.append(local_weights)\n",
    "\n",
    "# Total weight processing functions\n",
    "def Federated_Averaging(client_weights_total):\n",
    "    aggregate_weights = {}\n",
    "    num_clients = len(client_weights_total)\n",
    "    \n",
    "    # Iterate over the parameters of the model\n",
    "    for param_name in client_weights_total[0].keys():\n",
    "        # Initialize the aggregated parameter tensor\n",
    "        aggregated_param = client_weights_total[0][param_name].clone()\n",
    "\n",
    "        # Sum the parameter tensors from all clients\n",
    "        for client_weights in client_weights_total[1:]:\n",
    "            aggregated_param += client_weights[param_name]\n",
    "\n",
    "        # Calculate the average parameter value\n",
    "        aggregated_param /= num_clients\n",
    "\n",
    "        # Assign the averaged parameter to the aggregate_weights dictionary\n",
    "        aggregate_weights[param_name] = aggregated_param\n",
    "\n",
    "    return aggregate_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2500\n",
    "batch_size = 1\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [1]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        print(f'Client_X_train[{client}]: {X_train_client[client]}')\n",
    "        print(f'Client_y_train[{client}]: {y_train_client[client]}')\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "        print(client_device[client].model)\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = client_weights[client]['activation_stack.0.weight']\n",
    "            send_cost = send_cost + client_weights_size.numel()\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record the loss\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot the cost history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot send cost history\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Record the history of loss and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Loss Error Rate\")\n",
    "plt.title(\"Loss Error Rate vs Send Cost\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2500\n",
    "batch_size = 1\n",
    "num_clients_list = [5]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        print(f'Client_X_train[{client}]: {X_train_client[client]}')\n",
    "        print(f'Client_y_train[{client}]: {y_train_client[client]}')\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "        print(client_device[client].model)\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = client_weights[client]['activation_stack.0.weight']\n",
    "            send_cost = send_cost + client_weights_size.numel()\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record the loss\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot the cost history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot send cost history\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Record the history of loss and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "\n",
    "# Plot the error rate between cost history with local_update_epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Loss Error Rate\")\n",
    "plt.title(\"Loss Error Rate vs Send Cost\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
