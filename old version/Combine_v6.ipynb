{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 161  89]\n",
      " [  1 179 127]\n",
      " [  1 172 139]\n",
      " [  1 153 104]\n",
      " [  1 165  68]\n",
      " [  1 172  92]\n",
      " [  1 182 108]\n",
      " [  1 179 130]\n",
      " [  1 142  71]\n",
      " [  0 158 153]\n",
      " [  1 194 108]\n",
      " [  0 178 107]\n",
      " [  1 155  57]\n",
      " [  0 151  64]\n",
      " [  0 181  80]\n",
      " [  0 147 126]\n",
      " [  0 142 159]\n",
      " [  1 165 155]\n",
      " [  0 146 104]\n",
      " [  1 157  56]\n",
      " [  0 173  82]\n",
      " [  0 170 102]\n",
      " [  0 190 118]\n",
      " [  0 168 140]\n",
      " [  0 153  78]\n",
      " [  1 188 123]\n",
      " [  0 162  64]\n",
      " [  1 182 104]\n",
      " [  1 194 115]\n",
      " [  0 185 102]\n",
      " [  1 178  52]\n",
      " [  0 192  90]\n",
      " [  1 147 142]\n",
      " [  1 152 103]\n",
      " [  0 169  54]\n",
      " [  1 178 127]\n",
      " [  1 165 105]\n",
      " [  0 172  67]\n",
      " [  1 140 129]\n",
      " [  1 141  86]\n",
      " [  0 172  96]\n",
      " [  1 191  96]\n",
      " [  0 147 107]\n",
      " [  1 197 119]\n",
      " [  0 162 159]\n",
      " [  1 173 139]\n",
      " [  1 185 139]\n",
      " [  0 152  90]\n",
      " [  1 163 131]\n",
      " [  1 188 141]\n",
      " [  1 189 104]\n",
      " [  0 165  57]\n",
      " [  1 185  76]\n",
      " [  0 157 110]\n",
      " [  1 179  56]\n",
      " [  0 186 143]\n",
      " [  0 195 104]\n",
      " [  0 141 126]\n",
      " [  1 148 141]\n",
      " [  0 191  54]\n",
      " [  0 150  70]\n",
      " [  1 170  95]\n",
      " [  0 153  77]\n",
      " [  1 179 152]\n",
      " [  1 184 157]\n",
      " [  1 154 112]\n",
      " [  1 190  95]\n",
      " [  1 154 111]\n",
      " [  1 182  50]\n",
      " [  0 181 106]\n",
      " [  1 144  80]\n",
      " [  1 168 160]\n",
      " [  0 179 150]\n",
      " [  1 144 108]\n",
      " [  0 177  96]\n",
      " [  0 185 119]\n",
      " [  0 163  63]\n",
      " [  1 177 117]\n",
      " [  1 140  79]\n",
      " [  0 147  67]\n",
      " [  0 167  58]\n",
      " [  1 145 108]\n",
      " [  1 193 130]\n",
      " [  0 187  94]\n",
      " [  0 183  79]\n",
      " [  0 167  85]\n",
      " [  1 141  80]\n",
      " [  0 168  59]\n",
      " [  0 150  84]\n",
      " [  0 151  67]\n",
      " [  0 170  53]\n",
      " [  0 150 144]\n",
      " [  0 195 104]\n",
      " [  0 187 121]\n",
      " [  1 149  66]\n",
      " [  0 188  90]\n",
      " [  1 197 125]\n",
      " [  0 194 111]\n",
      " [  0 150  60]\n",
      " [  0 186 127]\n",
      " [  1 145 142]\n",
      " [  1 186 118]\n",
      " [  1 151 114]\n",
      " [  1 181  78]\n",
      " [  0 182 143]\n",
      " [  1 193 151]\n",
      " [  0 171 120]\n",
      " [  0 185 100]\n",
      " [  0 151  55]\n",
      " [  0 180  58]\n",
      " [  1 153  70]\n",
      " [  0 176 156]\n",
      " [  1 171 147]\n",
      " [  1 151  82]\n",
      " [  0 162  58]\n",
      " [  1 190  50]\n",
      " [  1 163 137]\n",
      " [  0 155 144]\n",
      " [  1 184 153]\n",
      " [  1 145  78]\n",
      " [  1 177 117]\n",
      " [  1 178  83]\n",
      " [  0 197 114]\n",
      " [  1 157  74]\n",
      " [  1 159 124]\n",
      " [  0 184 152]\n",
      " [  0 183  50]\n",
      " [  0 166 153]\n",
      " [  1 150  74]\n",
      " [  1 143  88]\n",
      " [  0 146 157]\n",
      " [  0 185 140]\n",
      " [  0 153 133]\n",
      " [  1 147  92]\n",
      " [  0 193  61]\n",
      " [  0 191  68]\n",
      " [  0 189 125]\n",
      " [  0 187 102]\n",
      " [  0 196  50]\n",
      " [  0 181  94]\n",
      " [  0 183  87]\n",
      " [  0 192 108]\n",
      " [  1 159 145]\n",
      " [  1 195  81]\n",
      " [  1 159 120]\n",
      " [  0 198 145]\n",
      " [  0 166 140]\n",
      " [  0 171 152]\n",
      " [  0 180  60]\n",
      " [  0 156 106]\n",
      " [  1 198 109]\n",
      " [  0 169  88]\n",
      " [  0 167  79]\n",
      " [  0 166 144]\n",
      " [  0 140  76]\n",
      " [  0 152 114]\n",
      " [  1 194 106]\n",
      " [  0 183  76]\n",
      " [  0 140 146]\n",
      " [  0 176 121]\n",
      " [  1 178 160]\n",
      " [  0 184 160]\n",
      " [  1 142 131]\n",
      " [  1 142  69]\n",
      " [  1 163 123]\n",
      " [  0 146 110]\n",
      " [  1 143 149]\n",
      " [  1 160 139]\n",
      " [  1 165 143]\n",
      " [  1 187 138]\n",
      " [  1 180  73]\n",
      " [  0 156  89]\n",
      " [  1 182  98]\n",
      " [  1 188 114]\n",
      " [  0 187  92]\n",
      " [  1 197  88]\n",
      " [  1 146 138]\n",
      " [  0 164  71]\n",
      " [  0 192 139]\n",
      " [  1 158  96]\n",
      " [  1 188  81]\n",
      " [  1 178 140]\n",
      " [  1 162 130]\n",
      " [  1 159 109]\n",
      " [  1 154  54]\n",
      " [  0 195  61]\n",
      " [  1 149 100]\n",
      " [  0 186 128]\n",
      " [  1 142  91]\n",
      " [  1 183 105]\n",
      " [  1 174  90]\n",
      " [  0 168 143]\n",
      " [  1 187  96]\n",
      " [  0 155  71]\n",
      " [  1 190 135]\n",
      " [  1 188 100]\n",
      " [  1 144  88]\n",
      " [  0 150  97]\n",
      " [  1 187  80]\n",
      " [  0 172 108]\n",
      " [  0 186 146]\n",
      " [  0 177  81]\n",
      " [  0 180 156]\n",
      " [  0 172 111]\n",
      " [  0 190 156]\n",
      " [  0 177 117]\n",
      " [  0 169 145]\n",
      " [  0 184  86]\n",
      " [  1 148  60]\n",
      " [  0 168  87]\n",
      " [  0 157  60]\n",
      " [  1 188  54]\n",
      " [  1 182  70]\n",
      " [  1 169 136]\n",
      " [  0 145  79]\n",
      " [  1 181 139]\n",
      " [  0 177 101]\n",
      " [  1 159 154]\n",
      " [  1 151  62]\n",
      " [  0 148  54]\n",
      " [  0 167 110]\n",
      " [  0 169 121]\n",
      " [  0 182 126]\n",
      " [  1 179 123]\n",
      " [  0 179 158]\n",
      " [  1 166 107]\n",
      " [  1 198  50]\n",
      " [  1 191  62]\n",
      " [  0 167 135]\n",
      " [  1 151 154]\n",
      " [  1 181 105]\n",
      " [  0 192 101]\n",
      " [  0 189 132]\n",
      " [  0 163 159]\n",
      " [  1 187 136]\n",
      " [  1 180 149]\n",
      " [  0 149  61]\n",
      " [  0 154  92]\n",
      " [  1 183 138]\n",
      " [  0 170 156]\n",
      " [  0 176  54]\n",
      " [  1 178  85]\n",
      " [  0 188 115]\n",
      " [  0 154  96]\n",
      " [  0 166 126]\n",
      " [  0 149 108]\n",
      " [  1 150  95]\n",
      " [  1 140 146]\n",
      " [  1 145  99]\n",
      " [  0 147  94]\n",
      " [  1 183 147]\n",
      " [  1 181 154]\n",
      " [  1 149  61]\n",
      " [  0 172 109]\n",
      " [  0 146 101]\n",
      " [  1 199 156]\n",
      " [  1 166  70]\n",
      " [  0 165  95]\n",
      " [  0 196 159]\n",
      " [  1 174 138]\n",
      " [  1 195  69]\n",
      " [  0 162  68]\n",
      " [  0 171 131]\n",
      " [  0 141 143]\n",
      " [  1 181 111]\n",
      " [  0 195  65]\n",
      " [  1 154 145]\n",
      " [  0 167  85]\n",
      " [  1 184  57]\n",
      " [  1 146  70]\n",
      " [  1 187  62]\n",
      " [  1 174  54]\n",
      " [  0 186 137]\n",
      " [  1 159 104]\n",
      " [  0 156  52]\n",
      " [  1 192 101]\n",
      " [  0 180  59]\n",
      " [  1 171 147]\n",
      " [  1 193  54]\n",
      " [  0 183 150]\n",
      " [  1 182 151]\n",
      " [  0 164 142]\n",
      " [  1 174  90]\n",
      " [  0 199  92]\n",
      " [  0 145 141]\n",
      " [  1 175 135]\n",
      " [  0 186 140]\n",
      " [  1 191  79]\n",
      " [  0 182  84]\n",
      " [  0 175 120]\n",
      " [  1 176  87]\n",
      " [  1 161 115]\n",
      " [  1 169 110]\n",
      " [  0 143 118]\n",
      " [  1 188  57]\n",
      " [  1 165  62]\n",
      " [  0 178  79]\n",
      " [  0 164 126]\n",
      " [  1 181 141]\n",
      " [  0 196 122]\n",
      " [  0 153 146]\n",
      " [  1 160 156]\n",
      " [  0 175  83]\n",
      " [  1 140 143]\n",
      " [  1 198 136]\n",
      " [  0 160 109]\n",
      " [  1 179 110]\n",
      " [  1 184  83]\n",
      " [  1 168  50]\n",
      " [  1 179  93]\n",
      " [  0 184  76]\n",
      " [  1 161 118]\n",
      " [  0 142  86]\n",
      " [  1 174 107]\n",
      " [  1 141  85]\n",
      " [  1 171 141]\n",
      " [  0 184 132]\n",
      " [  1 195 126]\n",
      " [  1 172 105]\n",
      " [  0 150  50]]\n",
      "[4 4 5 5 2 4 4 5 4 5 3 4 2 3 2 5 5 5 5 2 2 4 4 5 2 4 2 4 4 3 1 2 5 5 2 5 4\n",
      " 2 5 5 4 3 5 4 5 5 5 4 5 4 3 2 2 5 1 5 3 5 5 0 4 4 4 5 5 5 3 5 0 4 4 5 5 5\n",
      " 4 4 2 4 5 4 2 5 4 3 2 4 5 2 4 3 1 5 3 4 3 3 4 3 3 4 5 4 5 2 5 5 5 3 2 1 3\n",
      " 5 5 4 2 0 5 5 5 4 4 3 3 4 5 5 0 5 4 5 5 5 5 5 1 2 4 3 0 3 3 3 5 2 5 4 5 5\n",
      " 2 5 3 2 3 5 4 5 3 2 5 4 5 5 5 4 5 5 5 5 5 4 2 4 3 4 3 2 5 3 4 4 2 5 5 5 2\n",
      " 1 5 4 5 3 3 5 3 3 4 3 5 5 2 4 5 3 5 4 5 4 5 3 3 4 2 0 2 5 4 4 4 5 3 2 4 5\n",
      " 4 4 5 4 0 1 5 5 4 3 4 5 4 5 3 4 5 5 1 3 4 5 5 5 5 5 5 5 5 5 3 4 5 4 3 4 5\n",
      " 5 1 3 5 5 4 1 5 4 1 4 1 1 4 5 2 3 1 5 0 5 5 5 3 2 5 5 5 2 3 4 3 5 4 5 1 2\n",
      " 2 5 5 4 5 5 2 5 4 5 4 2 1 3 2 5 5 4 5 5 4 4 4 2]\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, [0,1,2]]\n",
    "y_train = dataset[:train_split, 3]\n",
    "X_test = dataset[train_split:, [0,1,2]]\n",
    "y_test = dataset[train_split:, 3]\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 0.00543878 0.013331 ... -0.022238 -0.00945892 0.0275439]\n",
      " [1 0.00144174 0.0338203 ... 0.023466 0.0234591 0.0364969]\n",
      " [1 0.00459705 -0.0427841 ... -0.00280583 -0.00405907 0.0245653]\n",
      " ...\n",
      " [-1 -0.00806419 -0.0144029 ... -0.0133173 -0.0401071 0.00589705]\n",
      " [-1 -0.0175411 -0.0501311 ... 0.0183344 -0.0016165 -0.0097232]\n",
      " [-1 -0.00874117 0.0151158 ... -0.0409596 -0.0260277 -0.00208328]]\n",
      "torch.Size([800, 1999])\n",
      "torch.Size([800])\n",
      "torch.Size([200, 1999])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized_testing.txt\", sep=' ', header=None, nrows=1000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.01\n",
    "num_iterations = 10000\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradien Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cfbbe",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD Test (This is done by Chris for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c6390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "    \n",
    "#    def step(self):\n",
    " #       for group in self.param_groups:\n",
    "  #          self.update_SGD(self.param_groups, weight_decay = group['weight_decay'], lr = group['lr'])\n",
    "#        \n",
    " #   def update_SGD(param_groups, weight_decay: float, lr: float):\n",
    "  #      for param in group['params']:\n",
    "#            if param.grad is None:\n",
    "#                continue\n",
    " #           grad = param.grad.data\n",
    "  #          param.data.add_(-lr, grad)\n",
    "#            if weight_decay != 0:\n",
    " #               param.data.add_(-lr * weight_decay, param.data)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(3, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)\n",
    "    \n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork()\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientUpdate:\n",
    "    def __init__(self, model, criterion, optimizer, train_data_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data_loader = train_data_loader\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in self.train_data_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self.model.state_dict()\n",
    "\n",
    "def send_client_weights(weights):\n",
    "    print(\"ok\")\n",
    "    \n",
    "def aggregate_weights_on_server(client_weights_list):\n",
    "    print(\"Aggregating client weights on the server...\")\n",
    "    aggregated_weights = {}\n",
    "    # Aggregate the client weights\n",
    "    for key in client_weights_list[0].keys():\n",
    "        aggregated_weights[key] = torch.stack([weights[key] for weights in client_weights_list]).mean(dim=0)\n",
    "    print(\"Client weights aggregated successfully.\")\n",
    "    \n",
    "def federated_learning(model, criterion, optimizer, train_data, num_rounds, batch_size, num_epochs):\n",
    "    num_clients = len(train_data)\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        m = min(num_clients * , 1)\n",
    "        selected_clients = torch.randperm(num_clients)[:m]\n",
    "\n",
    "        for client in selected_clients:\n",
    "            client_data = train_data[client]\n",
    "            client_loader = DataLoader(client_data, batch_size=batch_size)\n",
    "            client_update = ClientUpdate(model, criterion, optimizer, client_loader)\n",
    "            client_weights = client_update.update_weights(num_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights)\n",
    "            \n",
    "            # Collect client weights for aggregation on the server\n",
    "            client_weights_list.append(client_weights)\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = aggregate_weights_on_server(client_weights_list)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        model.load_state_dict(aggregated_weights)\n",
    "\n",
    "    return global_weights\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_rounds = 1\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define your training data for each client\n",
    "#train_data = [\n",
    "#    DataLoader(client1_data, batch_size=32),\n",
    "#    DataLoader(client2_data, batch_size=32),\n",
    "#    # ...\n",
    "#]\n",
    "\n",
    "# Define the training data for each client\n",
    "train_data = []\n",
    "\n",
    "# Client 1 data\n",
    "client1_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,), (0.3081,))\n",
    "])\n",
    "client1_dataset = MNIST(root='./data', train=True, download=True, transform=client1_transform)\n",
    "client1_data = client1_dataset\n",
    "\n",
    "# Client 2 data\n",
    "client2_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,), (0.3081,))\n",
    "])\n",
    "client2_dataset = MNIST(root='./data', train=True, download=True, transform=client2_transform)\n",
    "client2_data = client2_dataset\n",
    "\n",
    "# Add the client data to the train_data list\n",
    "train_data.append(client1_data)\n",
    "train_data.append(client2_data)\n",
    "\n",
    "# Run federated learning\n",
    "global_weights = federated_learning(model, criterion, optimizer, train_data, num_rounds=num_rounds, batch_size=1, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
