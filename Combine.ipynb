{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "#dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, [0,1,2]]\n",
    "#y_train = dataset[:train_split, 3]\n",
    "#X_test = dataset[train_split:, [0,1,2]]\n",
    "#y_test = dataset[train_split:, 3]\n",
    "\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "#prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "#prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "#X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 -0.0108282 -0.0196004 ... -0.0107833 -0.00341473 -0.0131107]\n",
      " [-1 -0.00470349 0.0169135 ... -0.0430199 -0.032248 -0.00424447]\n",
      " [1 0.00250835 0.0168447 ... -0.038255 -0.031005 -0.0168385]\n",
      " ...\n",
      " [-1 0.0103883 -0.0310115 ... 0.00536232 -0.00887889 -0.0570236]\n",
      " [1 0.00903997 0.0155853 ... -0.0331243 -0.0315165 -0.0139837]\n",
      " [-1 -0.0113611 0.0237702 ... -0.0371067 -0.0203854 -0.00976943]]\n",
      "torch.Size([800, 1999])\n",
      "torch.Size([800])\n",
      "torch.Size([200, 1999])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized\", sep=' ', header=None, nrows=1000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dd85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stochastic Gradien Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cfbbe",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD Test (This is done by Chris for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76c6390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=1999, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(-0.1951, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/5000], Loss: 1.06646037\n",
      "tensor(-0.0071, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/5000], Loss: 1.01015317\n",
      "tensor(-0.0336, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/5000], Loss: 1.00486779\n",
      "tensor(-0.0493, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/5000], Loss: 1.00089991\n",
      "tensor(-0.0586, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/5000], Loss: 0.99740100\n",
      "tensor(-0.0643, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/5000], Loss: 0.99407750\n",
      "tensor(-0.0678, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/5000], Loss: 0.99082839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_4576\\304550788.py:18: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  param.data.add_(-lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0699, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/5000], Loss: 0.98761863\n",
      "tensor(-0.0714, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/5000], Loss: 0.98443586\n",
      "tensor(-0.0724, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/5000], Loss: 0.98127580\n",
      "tensor(-0.0731, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/5000], Loss: 0.97813642\n",
      "tensor(-0.0737, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/5000], Loss: 0.97501755\n",
      "tensor(-0.0741, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/5000], Loss: 0.97191846\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/5000], Loss: 0.96883893\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/5000], Loss: 0.96577883\n",
      "tensor(-0.0753, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/5000], Loss: 0.96273780\n",
      "tensor(-0.0756, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/5000], Loss: 0.95971566\n",
      "tensor(-0.0760, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/5000], Loss: 0.95671219\n",
      "tensor(-0.0763, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/5000], Loss: 0.95372725\n",
      "tensor(-0.0767, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/5000], Loss: 0.95076066\n",
      "tensor(-0.0770, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/5000], Loss: 0.94781202\n",
      "tensor(-0.0773, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/5000], Loss: 0.94488144\n",
      "tensor(-0.0776, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/5000], Loss: 0.94196856\n",
      "tensor(-0.0779, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/5000], Loss: 0.93907303\n",
      "tensor(-0.0782, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/5000], Loss: 0.93619502\n",
      "tensor(-0.0786, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/5000], Loss: 0.93333417\n",
      "tensor(-0.0789, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/5000], Loss: 0.93049020\n",
      "tensor(-0.0792, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/5000], Loss: 0.92766309\n",
      "tensor(-0.0795, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/5000], Loss: 0.92485261\n",
      "tensor(-0.0798, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/5000], Loss: 0.92205864\n",
      "tensor(-0.0800, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/5000], Loss: 0.91928083\n",
      "tensor(-0.0803, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/5000], Loss: 0.91651922\n",
      "tensor(-0.0806, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/5000], Loss: 0.91377366\n",
      "tensor(-0.0809, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/5000], Loss: 0.91104376\n",
      "tensor(-0.0812, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/5000], Loss: 0.90832973\n",
      "tensor(-0.0815, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/5000], Loss: 0.90563095\n",
      "tensor(-0.0818, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/5000], Loss: 0.90294749\n",
      "tensor(-0.0820, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/5000], Loss: 0.90027940\n",
      "tensor(-0.0823, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/5000], Loss: 0.89762634\n",
      "tensor(-0.0826, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/5000], Loss: 0.89498800\n",
      "tensor(-0.0828, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/5000], Loss: 0.89236444\n",
      "tensor(-0.0831, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/5000], Loss: 0.88975561\n",
      "tensor(-0.0834, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/5000], Loss: 0.88716108\n",
      "tensor(-0.0836, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/5000], Loss: 0.88458109\n",
      "tensor(-0.0839, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/5000], Loss: 0.88201505\n",
      "tensor(-0.0841, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/5000], Loss: 0.87946326\n",
      "tensor(-0.0844, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/5000], Loss: 0.87692535\n",
      "tensor(-0.0847, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/5000], Loss: 0.87440115\n",
      "tensor(-0.0849, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/5000], Loss: 0.87189072\n",
      "tensor(-0.0851, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/5000], Loss: 0.86939377\n",
      "tensor(-0.0854, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/5000], Loss: 0.86691010\n",
      "tensor(-0.0856, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/5000], Loss: 0.86443990\n",
      "tensor(-0.0859, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/5000], Loss: 0.86198282\n",
      "tensor(-0.0861, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/5000], Loss: 0.85953873\n",
      "tensor(-0.0863, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/5000], Loss: 0.85710770\n",
      "tensor(-0.0866, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/5000], Loss: 0.85468936\n",
      "tensor(-0.0868, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/5000], Loss: 0.85228378\n",
      "tensor(-0.0870, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/5000], Loss: 0.84989077\n",
      "tensor(-0.0873, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/5000], Loss: 0.84751028\n",
      "tensor(-0.0875, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/5000], Loss: 0.84514207\n",
      "tensor(-0.0877, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/5000], Loss: 0.84278607\n",
      "tensor(-0.0879, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/5000], Loss: 0.84044236\n",
      "tensor(-0.0881, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/5000], Loss: 0.83811069\n",
      "tensor(-0.0884, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/5000], Loss: 0.83579087\n",
      "tensor(-0.0886, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/5000], Loss: 0.83348298\n",
      "tensor(-0.0888, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/5000], Loss: 0.83118677\n",
      "tensor(-0.0890, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/5000], Loss: 0.82890213\n",
      "tensor(-0.0892, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/5000], Loss: 0.82662910\n",
      "tensor(-0.0894, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/5000], Loss: 0.82436752\n",
      "tensor(-0.0896, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/5000], Loss: 0.82211733\n",
      "tensor(-0.0898, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/5000], Loss: 0.81987840\n",
      "tensor(-0.0900, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/5000], Loss: 0.81765044\n",
      "tensor(-0.0902, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/5000], Loss: 0.81543374\n",
      "tensor(-0.0904, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/5000], Loss: 0.81322801\n",
      "tensor(-0.0906, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/5000], Loss: 0.81103307\n",
      "tensor(-0.0908, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/5000], Loss: 0.80884898\n",
      "tensor(-0.0910, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/5000], Loss: 0.80667555\n",
      "tensor(-0.0912, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/5000], Loss: 0.80451274\n",
      "tensor(-0.0913, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/5000], Loss: 0.80236048\n",
      "tensor(-0.0915, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/5000], Loss: 0.80021882\n",
      "tensor(-0.0917, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/5000], Loss: 0.79808730\n",
      "tensor(-0.0919, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/5000], Loss: 0.79596621\n",
      "tensor(-0.0921, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/5000], Loss: 0.79385519\n",
      "tensor(-0.0922, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/5000], Loss: 0.79175454\n",
      "tensor(-0.0924, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/5000], Loss: 0.78966367\n",
      "tensor(-0.0926, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/5000], Loss: 0.78758299\n",
      "tensor(-0.0928, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/5000], Loss: 0.78551209\n",
      "tensor(-0.0929, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/5000], Loss: 0.78345102\n",
      "tensor(-0.0931, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/5000], Loss: 0.78139979\n",
      "tensor(-0.0933, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/5000], Loss: 0.77935809\n",
      "tensor(-0.0934, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/5000], Loss: 0.77732599\n",
      "tensor(-0.0936, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/5000], Loss: 0.77530348\n",
      "tensor(-0.0938, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/5000], Loss: 0.77329040\n",
      "tensor(-0.0939, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/5000], Loss: 0.77128673\n",
      "tensor(-0.0941, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/5000], Loss: 0.76929235\n",
      "tensor(-0.0942, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/5000], Loss: 0.76730722\n",
      "tensor(-0.0944, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/5000], Loss: 0.76533121\n",
      "tensor(-0.0945, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/5000], Loss: 0.76336443\n",
      "tensor(-0.0947, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/5000], Loss: 0.76140666\n",
      "tensor(-0.0948, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/5000], Loss: 0.75945771\n",
      "tensor(-0.0950, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/5000], Loss: 0.75751787\n",
      "tensor(-0.0951, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/5000], Loss: 0.75558668\n",
      "tensor(-0.0953, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/5000], Loss: 0.75366449\n",
      "tensor(-0.0954, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/5000], Loss: 0.75175089\n",
      "tensor(-0.0956, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/5000], Loss: 0.74984586\n",
      "tensor(-0.0957, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/5000], Loss: 0.74794966\n",
      "tensor(-0.0958, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/5000], Loss: 0.74606168\n",
      "tensor(-0.0960, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/5000], Loss: 0.74418253\n",
      "tensor(-0.0961, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/5000], Loss: 0.74231154\n",
      "tensor(-0.0963, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/5000], Loss: 0.74044907\n",
      "tensor(-0.0964, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/5000], Loss: 0.73859483\n",
      "tensor(-0.0965, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/5000], Loss: 0.73674881\n",
      "tensor(-0.0967, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/5000], Loss: 0.73491096\n",
      "tensor(-0.0968, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/5000], Loss: 0.73308128\n",
      "tensor(-0.0969, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/5000], Loss: 0.73125958\n",
      "tensor(-0.0970, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/5000], Loss: 0.72944611\n",
      "tensor(-0.0972, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/5000], Loss: 0.72764051\n",
      "tensor(-0.0973, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/5000], Loss: 0.72584283\n",
      "tensor(-0.0974, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/5000], Loss: 0.72405303\n",
      "tensor(-0.0975, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/5000], Loss: 0.72227079\n",
      "tensor(-0.0977, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/5000], Loss: 0.72049665\n",
      "tensor(-0.0978, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/5000], Loss: 0.71873009\n",
      "tensor(-0.0979, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/5000], Loss: 0.71697098\n",
      "tensor(-0.0980, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/5000], Loss: 0.71521974\n",
      "tensor(-0.0981, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/5000], Loss: 0.71347594\n",
      "tensor(-0.0982, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/5000], Loss: 0.71173966\n",
      "tensor(-0.0983, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/5000], Loss: 0.71001089\n",
      "tensor(-0.0985, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/5000], Loss: 0.70828938\n",
      "tensor(-0.0986, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/5000], Loss: 0.70657545\n",
      "tensor(-0.0987, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/5000], Loss: 0.70486856\n",
      "tensor(-0.0988, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/5000], Loss: 0.70316917\n",
      "tensor(-0.0989, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/5000], Loss: 0.70147681\n",
      "tensor(-0.0990, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/5000], Loss: 0.69979179\n",
      "tensor(-0.0991, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/5000], Loss: 0.69811386\n",
      "tensor(-0.0992, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/5000], Loss: 0.69644296\n",
      "tensor(-0.0993, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/5000], Loss: 0.69477904\n",
      "tensor(-0.0994, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/5000], Loss: 0.69312227\n",
      "tensor(-0.0995, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/5000], Loss: 0.69147241\n",
      "tensor(-0.0996, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/5000], Loss: 0.68982935\n",
      "tensor(-0.0997, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/5000], Loss: 0.68819326\n",
      "tensor(-0.0998, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/5000], Loss: 0.68656403\n",
      "tensor(-0.0999, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/5000], Loss: 0.68494135\n",
      "tensor(-0.1000, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/5000], Loss: 0.68332571\n",
      "tensor(-0.1001, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/5000], Loss: 0.68171662\n",
      "tensor(-0.1002, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/5000], Loss: 0.68011421\n",
      "tensor(-0.1002, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/5000], Loss: 0.67851835\n",
      "tensor(-0.1003, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/5000], Loss: 0.67692912\n",
      "tensor(-0.1004, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/5000], Loss: 0.67534643\n",
      "tensor(-0.1005, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/5000], Loss: 0.67377019\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/5000], Loss: 0.67220056\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/5000], Loss: 0.67063719\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/5000], Loss: 0.66908026\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/5000], Loss: 0.66752970\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/5000], Loss: 0.66598541\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/5000], Loss: 0.66444749\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/5000], Loss: 0.66291565\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/5000], Loss: 0.66139007\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/5000], Loss: 0.65987074\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/5000], Loss: 0.65835738\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/5000], Loss: 0.65685022\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/5000], Loss: 0.65534914\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/5000], Loss: 0.65385407\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/5000], Loss: 0.65236491\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/5000], Loss: 0.65088171\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/5000], Loss: 0.64940447\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/5000], Loss: 0.64793307\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/5000], Loss: 0.64646769\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/5000], Loss: 0.64500791\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/5000], Loss: 0.64355397\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/5000], Loss: 0.64210588\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/5000], Loss: 0.64066344\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/5000], Loss: 0.63922673\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/5000], Loss: 0.63779563\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/5000], Loss: 0.63637018\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/5000], Loss: 0.63495034\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/5000], Loss: 0.63353610\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/5000], Loss: 0.63212734\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/5000], Loss: 0.63072413\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/5000], Loss: 0.62932634\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/5000], Loss: 0.62793404\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/5000], Loss: 0.62654710\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/5000], Loss: 0.62516558\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/5000], Loss: 0.62378943\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/5000], Loss: 0.62241858\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/5000], Loss: 0.62105310\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/5000], Loss: 0.61969286\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/5000], Loss: 0.61833781\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/5000], Loss: 0.61698806\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/5000], Loss: 0.61564344\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/5000], Loss: 0.61430401\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/5000], Loss: 0.61296964\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/5000], Loss: 0.61164045\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/5000], Loss: 0.61031634\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/5000], Loss: 0.60899729\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/5000], Loss: 0.60768324\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/5000], Loss: 0.60637414\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/5000], Loss: 0.60507005\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/5000], Loss: 0.60377097\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/5000], Loss: 0.60247678\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/5000], Loss: 0.60118747\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/5000], Loss: 0.59990299\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/5000], Loss: 0.59862351\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/5000], Loss: 0.59734863\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/5000], Loss: 0.59607869\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/5000], Loss: 0.59481359\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/5000], Loss: 0.59355319\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/5000], Loss: 0.59229743\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/5000], Loss: 0.59104651\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/5000], Loss: 0.58980006\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/5000], Loss: 0.58855838\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/5000], Loss: 0.58732146\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/5000], Loss: 0.58608902\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/5000], Loss: 0.58486104\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/5000], Loss: 0.58363783\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/5000], Loss: 0.58241904\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/5000], Loss: 0.58120477\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/5000], Loss: 0.57999504\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/5000], Loss: 0.57878983\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/5000], Loss: 0.57758898\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/5000], Loss: 0.57639265\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/5000], Loss: 0.57520056\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/5000], Loss: 0.57401299\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/5000], Loss: 0.57282972\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/5000], Loss: 0.57165086\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/5000], Loss: 0.57047629\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/5000], Loss: 0.56930602\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/5000], Loss: 0.56814003\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/5000], Loss: 0.56697828\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/5000], Loss: 0.56582075\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/5000], Loss: 0.56466746\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/5000], Loss: 0.56351835\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/5000], Loss: 0.56237340\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/5000], Loss: 0.56123263\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/5000], Loss: 0.56009603\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/5000], Loss: 0.55896354\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/5000], Loss: 0.55783510\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/5000], Loss: 0.55671072\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/5000], Loss: 0.55559045\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/5000], Loss: 0.55447423\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/5000], Loss: 0.55336195\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/5000], Loss: 0.55225372\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/5000], Loss: 0.55114949\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/5000], Loss: 0.55004919\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/5000], Loss: 0.54895288\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/5000], Loss: 0.54786038\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/5000], Loss: 0.54677194\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/5000], Loss: 0.54568732\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/5000], Loss: 0.54460651\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/5000], Loss: 0.54352957\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/5000], Loss: 0.54245657\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/5000], Loss: 0.54138732\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2510/5000], Loss: 0.54032183\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2520/5000], Loss: 0.53926015\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2530/5000], Loss: 0.53820223\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2540/5000], Loss: 0.53714806\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2550/5000], Loss: 0.53609765\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [2560/5000], Loss: 0.53505093\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2570/5000], Loss: 0.53400785\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2580/5000], Loss: 0.53296846\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2590/5000], Loss: 0.53193283\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2600/5000], Loss: 0.53090072\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2610/5000], Loss: 0.52987230\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2620/5000], Loss: 0.52884740\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2630/5000], Loss: 0.52782625\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2640/5000], Loss: 0.52680856\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2650/5000], Loss: 0.52579451\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2660/5000], Loss: 0.52478385\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2670/5000], Loss: 0.52377695\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2680/5000], Loss: 0.52277339\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2690/5000], Loss: 0.52177334\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2700/5000], Loss: 0.52077681\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2710/5000], Loss: 0.51978374\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2720/5000], Loss: 0.51879412\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2730/5000], Loss: 0.51780796\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2740/5000], Loss: 0.51682520\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2750/5000], Loss: 0.51584584\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2760/5000], Loss: 0.51486987\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2770/5000], Loss: 0.51389718\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2780/5000], Loss: 0.51292807\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2790/5000], Loss: 0.51196212\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2800/5000], Loss: 0.51099944\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2810/5000], Loss: 0.51004022\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2820/5000], Loss: 0.50908428\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2830/5000], Loss: 0.50813156\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2840/5000], Loss: 0.50718218\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2850/5000], Loss: 0.50623602\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2860/5000], Loss: 0.50529307\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2870/5000], Loss: 0.50435334\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2880/5000], Loss: 0.50341684\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2890/5000], Loss: 0.50248355\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2900/5000], Loss: 0.50155342\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2910/5000], Loss: 0.50062656\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2920/5000], Loss: 0.49970275\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2930/5000], Loss: 0.49878213\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2940/5000], Loss: 0.49786460\n",
      "tensor(-0.1053, grad_fn=<SelectBackward0>)\n",
      "Epoch [2950/5000], Loss: 0.49695024\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2960/5000], Loss: 0.49603888\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2970/5000], Loss: 0.49513069\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2980/5000], Loss: 0.49422553\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [2990/5000], Loss: 0.49332348\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3000/5000], Loss: 0.49242452\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3010/5000], Loss: 0.49152842\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3020/5000], Loss: 0.49063545\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3030/5000], Loss: 0.48974550\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3040/5000], Loss: 0.48885849\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3050/5000], Loss: 0.48797452\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3060/5000], Loss: 0.48709351\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3070/5000], Loss: 0.48621535\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3080/5000], Loss: 0.48534027\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3090/5000], Loss: 0.48446804\n",
      "tensor(-0.1052, grad_fn=<SelectBackward0>)\n",
      "Epoch [3100/5000], Loss: 0.48359871\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3110/5000], Loss: 0.48273239\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3120/5000], Loss: 0.48186886\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3130/5000], Loss: 0.48100823\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3140/5000], Loss: 0.48015052\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3150/5000], Loss: 0.47929561\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3160/5000], Loss: 0.47844359\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3170/5000], Loss: 0.47759438\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3180/5000], Loss: 0.47674796\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3190/5000], Loss: 0.47590438\n",
      "tensor(-0.1051, grad_fn=<SelectBackward0>)\n",
      "Epoch [3200/5000], Loss: 0.47506359\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3210/5000], Loss: 0.47422549\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3220/5000], Loss: 0.47339028\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3230/5000], Loss: 0.47255778\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3240/5000], Loss: 0.47172809\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3250/5000], Loss: 0.47090110\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3260/5000], Loss: 0.47007692\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3270/5000], Loss: 0.46925527\n",
      "tensor(-0.1050, grad_fn=<SelectBackward0>)\n",
      "Epoch [3280/5000], Loss: 0.46843639\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3290/5000], Loss: 0.46762025\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3300/5000], Loss: 0.46680677\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3310/5000], Loss: 0.46599588\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3320/5000], Loss: 0.46518776\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3330/5000], Loss: 0.46438220\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3340/5000], Loss: 0.46357933\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3350/5000], Loss: 0.46277913\n",
      "tensor(-0.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [3360/5000], Loss: 0.46198142\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3370/5000], Loss: 0.46118632\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3380/5000], Loss: 0.46039382\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3390/5000], Loss: 0.45960400\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3400/5000], Loss: 0.45881662\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3410/5000], Loss: 0.45803192\n",
      "tensor(-0.1048, grad_fn=<SelectBackward0>)\n",
      "Epoch [3420/5000], Loss: 0.45724973\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3430/5000], Loss: 0.45646998\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3440/5000], Loss: 0.45569283\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3450/5000], Loss: 0.45491821\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3460/5000], Loss: 0.45414609\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3470/5000], Loss: 0.45337647\n",
      "tensor(-0.1047, grad_fn=<SelectBackward0>)\n",
      "Epoch [3480/5000], Loss: 0.45260924\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3490/5000], Loss: 0.45184457\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3500/5000], Loss: 0.45108241\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3510/5000], Loss: 0.45032257\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3520/5000], Loss: 0.44956532\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3530/5000], Loss: 0.44881034\n",
      "tensor(-0.1046, grad_fn=<SelectBackward0>)\n",
      "Epoch [3540/5000], Loss: 0.44805795\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3550/5000], Loss: 0.44730785\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3560/5000], Loss: 0.44656026\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3570/5000], Loss: 0.44581497\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3580/5000], Loss: 0.44507208\n",
      "tensor(-0.1045, grad_fn=<SelectBackward0>)\n",
      "Epoch [3590/5000], Loss: 0.44433156\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3600/5000], Loss: 0.44359344\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3610/5000], Loss: 0.44285762\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3620/5000], Loss: 0.44212419\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3630/5000], Loss: 0.44139308\n",
      "tensor(-0.1044, grad_fn=<SelectBackward0>)\n",
      "Epoch [3640/5000], Loss: 0.44066438\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3650/5000], Loss: 0.43993777\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3660/5000], Loss: 0.43921372\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3670/5000], Loss: 0.43849179\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3680/5000], Loss: 0.43777224\n",
      "tensor(-0.1043, grad_fn=<SelectBackward0>)\n",
      "Epoch [3690/5000], Loss: 0.43705499\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3700/5000], Loss: 0.43633991\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3710/5000], Loss: 0.43562713\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3720/5000], Loss: 0.43491662\n",
      "tensor(-0.1042, grad_fn=<SelectBackward0>)\n",
      "Epoch [3730/5000], Loss: 0.43420836\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3740/5000], Loss: 0.43350229\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3750/5000], Loss: 0.43279848\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3760/5000], Loss: 0.43209690\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3770/5000], Loss: 0.43139747\n",
      "tensor(-0.1041, grad_fn=<SelectBackward0>)\n",
      "Epoch [3780/5000], Loss: 0.43070027\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3790/5000], Loss: 0.43000522\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3800/5000], Loss: 0.42931241\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3810/5000], Loss: 0.42862171\n",
      "tensor(-0.1040, grad_fn=<SelectBackward0>)\n",
      "Epoch [3820/5000], Loss: 0.42793319\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3830/5000], Loss: 0.42724687\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3840/5000], Loss: 0.42656258\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3850/5000], Loss: 0.42588055\n",
      "tensor(-0.1039, grad_fn=<SelectBackward0>)\n",
      "Epoch [3860/5000], Loss: 0.42520049\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3870/5000], Loss: 0.42452276\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3880/5000], Loss: 0.42384702\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3890/5000], Loss: 0.42317340\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3900/5000], Loss: 0.42250180\n",
      "tensor(-0.1038, grad_fn=<SelectBackward0>)\n",
      "Epoch [3910/5000], Loss: 0.42183235\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3920/5000], Loss: 0.42116502\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3930/5000], Loss: 0.42049962\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3940/5000], Loss: 0.41983643\n",
      "tensor(-0.1037, grad_fn=<SelectBackward0>)\n",
      "Epoch [3950/5000], Loss: 0.41917518\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3960/5000], Loss: 0.41851604\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3970/5000], Loss: 0.41785896\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3980/5000], Loss: 0.41720381\n",
      "tensor(-0.1036, grad_fn=<SelectBackward0>)\n",
      "Epoch [3990/5000], Loss: 0.41655076\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4000/5000], Loss: 0.41589966\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4010/5000], Loss: 0.41525063\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4020/5000], Loss: 0.41460347\n",
      "tensor(-0.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [4030/5000], Loss: 0.41395843\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4040/5000], Loss: 0.41331527\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4050/5000], Loss: 0.41267413\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4060/5000], Loss: 0.41203499\n",
      "tensor(-0.1034, grad_fn=<SelectBackward0>)\n",
      "Epoch [4070/5000], Loss: 0.41139776\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4080/5000], Loss: 0.41076243\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4090/5000], Loss: 0.41012916\n",
      "tensor(-0.1033, grad_fn=<SelectBackward0>)\n",
      "Epoch [4100/5000], Loss: 0.40949768\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4110/5000], Loss: 0.40886819\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4120/5000], Loss: 0.40824068\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4130/5000], Loss: 0.40761501\n",
      "tensor(-0.1032, grad_fn=<SelectBackward0>)\n",
      "Epoch [4140/5000], Loss: 0.40699124\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4150/5000], Loss: 0.40636939\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4160/5000], Loss: 0.40574935\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4170/5000], Loss: 0.40513125\n",
      "tensor(-0.1031, grad_fn=<SelectBackward0>)\n",
      "Epoch [4180/5000], Loss: 0.40451503\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4190/5000], Loss: 0.40390065\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4200/5000], Loss: 0.40328813\n",
      "tensor(-0.1030, grad_fn=<SelectBackward0>)\n",
      "Epoch [4210/5000], Loss: 0.40267745\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4220/5000], Loss: 0.40206864\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4230/5000], Loss: 0.40146172\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4240/5000], Loss: 0.40085652\n",
      "tensor(-0.1029, grad_fn=<SelectBackward0>)\n",
      "Epoch [4250/5000], Loss: 0.40025318\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4260/5000], Loss: 0.39965162\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4270/5000], Loss: 0.39905190\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4280/5000], Loss: 0.39845398\n",
      "tensor(-0.1028, grad_fn=<SelectBackward0>)\n",
      "Epoch [4290/5000], Loss: 0.39785793\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4300/5000], Loss: 0.39726353\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4310/5000], Loss: 0.39667100\n",
      "tensor(-0.1027, grad_fn=<SelectBackward0>)\n",
      "Epoch [4320/5000], Loss: 0.39608026\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4330/5000], Loss: 0.39549115\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4340/5000], Loss: 0.39490387\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4350/5000], Loss: 0.39431840\n",
      "tensor(-0.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [4360/5000], Loss: 0.39373457\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4370/5000], Loss: 0.39315253\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4380/5000], Loss: 0.39257225\n",
      "tensor(-0.1025, grad_fn=<SelectBackward0>)\n",
      "Epoch [4390/5000], Loss: 0.39199361\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4400/5000], Loss: 0.39141673\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4410/5000], Loss: 0.39084160\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4420/5000], Loss: 0.39026821\n",
      "tensor(-0.1024, grad_fn=<SelectBackward0>)\n",
      "Epoch [4430/5000], Loss: 0.38969639\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4440/5000], Loss: 0.38912627\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4450/5000], Loss: 0.38855794\n",
      "tensor(-0.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [4460/5000], Loss: 0.38799119\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4470/5000], Loss: 0.38742614\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4480/5000], Loss: 0.38686278\n",
      "tensor(-0.1022, grad_fn=<SelectBackward0>)\n",
      "Epoch [4490/5000], Loss: 0.38630107\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4500/5000], Loss: 0.38574100\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4510/5000], Loss: 0.38518262\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4520/5000], Loss: 0.38462579\n",
      "tensor(-0.1021, grad_fn=<SelectBackward0>)\n",
      "Epoch [4530/5000], Loss: 0.38407063\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4540/5000], Loss: 0.38351712\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4550/5000], Loss: 0.38296524\n",
      "tensor(-0.1020, grad_fn=<SelectBackward0>)\n",
      "Epoch [4560/5000], Loss: 0.38241500\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4570/5000], Loss: 0.38186622\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4580/5000], Loss: 0.38131917\n",
      "tensor(-0.1019, grad_fn=<SelectBackward0>)\n",
      "Epoch [4590/5000], Loss: 0.38077369\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4600/5000], Loss: 0.38022977\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4610/5000], Loss: 0.37968743\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4620/5000], Loss: 0.37914675\n",
      "tensor(-0.1018, grad_fn=<SelectBackward0>)\n",
      "Epoch [4630/5000], Loss: 0.37860757\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4640/5000], Loss: 0.37806997\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4650/5000], Loss: 0.37753388\n",
      "tensor(-0.1017, grad_fn=<SelectBackward0>)\n",
      "Epoch [4660/5000], Loss: 0.37699944\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4670/5000], Loss: 0.37646651\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4680/5000], Loss: 0.37593505\n",
      "tensor(-0.1016, grad_fn=<SelectBackward0>)\n",
      "Epoch [4690/5000], Loss: 0.37540522\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4700/5000], Loss: 0.37487683\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4710/5000], Loss: 0.37435010\n",
      "tensor(-0.1015, grad_fn=<SelectBackward0>)\n",
      "Epoch [4720/5000], Loss: 0.37382472\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4730/5000], Loss: 0.37330094\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4740/5000], Loss: 0.37277871\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4750/5000], Loss: 0.37225792\n",
      "tensor(-0.1014, grad_fn=<SelectBackward0>)\n",
      "Epoch [4760/5000], Loss: 0.37173867\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4770/5000], Loss: 0.37122077\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4780/5000], Loss: 0.37070450\n",
      "tensor(-0.1013, grad_fn=<SelectBackward0>)\n",
      "Epoch [4790/5000], Loss: 0.37018967\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4800/5000], Loss: 0.36967629\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4810/5000], Loss: 0.36916438\n",
      "tensor(-0.1012, grad_fn=<SelectBackward0>)\n",
      "Epoch [4820/5000], Loss: 0.36865398\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4830/5000], Loss: 0.36814499\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4840/5000], Loss: 0.36763749\n",
      "tensor(-0.1011, grad_fn=<SelectBackward0>)\n",
      "Epoch [4850/5000], Loss: 0.36713135\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4860/5000], Loss: 0.36662671\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4870/5000], Loss: 0.36612350\n",
      "tensor(-0.1010, grad_fn=<SelectBackward0>)\n",
      "Epoch [4880/5000], Loss: 0.36562172\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4890/5000], Loss: 0.36512133\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4900/5000], Loss: 0.36462238\n",
      "tensor(-0.1009, grad_fn=<SelectBackward0>)\n",
      "Epoch [4910/5000], Loss: 0.36412483\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4920/5000], Loss: 0.36362875\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4930/5000], Loss: 0.36313400\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4940/5000], Loss: 0.36264074\n",
      "tensor(-0.1008, grad_fn=<SelectBackward0>)\n",
      "Epoch [4950/5000], Loss: 0.36214870\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4960/5000], Loss: 0.36165822\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4970/5000], Loss: 0.36116898\n",
      "tensor(-0.1007, grad_fn=<SelectBackward0>)\n",
      "Epoch [4980/5000], Loss: 0.36068124\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [4990/5000], Loss: 0.36019477\n",
      "tensor(-0.1006, grad_fn=<SelectBackward0>)\n",
      "Epoch [5000/5000], Loss: 0.35970971\n",
      "activation_stack.0.weight: tensor([[-0.0149,  0.0335,  0.0247,  ..., -0.0062,  0.0082, -0.0144]])\n",
      "activation_stack.0.bias: tensor([-0.0090])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(1999, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)\n",
    "    \n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork()\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset preprocessing for Federated Learning\n",
    "def split_data_for_client():\n",
    "    dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baec815a",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\School Documents\\CUHK Notes\\FTEC4998 notes\\Testing Enviroment\\Combine.ipynb Cell 17\u001b[0m line \u001b[0;36m<cell line: 51>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m     send_client_weights(global_weights, client_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# Aggregate client weights on the server\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m aggregated_weights \u001b[39m=\u001b[39m aggregate_weights_on_server(global_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39m# Update global weights with aggregated weights\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(aggregated_weights)\n",
      "\u001b[1;32mc:\\Users\\chris\\Desktop\\School Documents\\CUHK Notes\\FTEC4998 notes\\Testing Enviroment\\Combine.ipynb Cell 17\u001b[0m line \u001b[0;36maggregate_weights_on_server\u001b[1;34m(client_weights_list)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m aggregated_weights \u001b[39m=\u001b[39m {}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39m# Aggregate the client weights\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m client_weights_list[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/chris/Desktop/School%20Documents/CUHK%20Notes/FTEC4998%20notes/Testing%20Enviroment/Combine.ipynb#X22sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     aggregated_weights[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([weights[key] \u001b[39mfor\u001b[39;00m weights \u001b[39min\u001b[39;00m client_weights_list])\u001b[39m.\u001b[39mmean(dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientUpdate:\n",
    "    def __init__(self, model, criterion, optimizer, train_data):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data = train_data\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in self.train_data:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self.model.state_dict()\n",
    "\n",
    "def send_client_weights(server, weights):\n",
    "    server.append(weights)\n",
    "    \n",
    "def aggregate_weights_on_server(client_weights_list):\n",
    "    aggregated_weights = {}\n",
    "    # Aggregate the client weights\n",
    "    for key in client_weights_list[0].keys():\n",
    "        aggregated_weights[key] = torch.stack([weights[key] for weights in client_weights_list]).mean(dim=0)\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_epochs = 5000\n",
    "batch_size = 1\n",
    "num_clients = 5\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Add the client data to the train_data list\n",
    "data\n",
    "\n",
    "# Perform training\n",
    "num_clients = len(train_data)\n",
    "global_weights = model.state_dict()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    m = max(num_clients, 1)\n",
    "    selected_clients = torch.randperm(num_clients)[:m]\n",
    "    loss = 0.00\n",
    "\n",
    "    for client in selected_clients:\n",
    "        client_data = train_data[client]\n",
    "        client_update = ClientUpdate(model, criterion, optimizer, client_data[client])\n",
    "        client_weights = client_update.update_weights(num_epochs)\n",
    "\n",
    "        # Send client weights to the server\n",
    "        send_client_weights(global_weights, client_weights)\n",
    "\n",
    "    # Aggregate client weights on the server\n",
    "    aggregated_weights = aggregate_weights_on_server(global_weights)\n",
    "\n",
    "    # Update global weights with aggregated weights\n",
    "    model.load_state_dict(aggregated_weights)\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "\n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
