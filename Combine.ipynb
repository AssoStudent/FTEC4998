{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Optimization for Statistical Learning\" Expirement Notebook\n",
    "## Section 0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Data Importing and Preprocessing\n",
    "In this section, we include packages we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install math\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional\n",
    "from torch.autograd import Variable\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load npy files to restore results in the past**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past_results = np.load('./result.npy', encoding = \"latin1\")\n",
    "#print(past_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2 Global Classes, Functions and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spliting the DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataloader(dataloader, num_piece):\n",
    "    dataset = list(dataloader)\n",
    "    subset_size = len(dataset) // num_piece\n",
    "    remainder = len(dataset) % num_piece\n",
    "    split_subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_piece):\n",
    "        if i < remainder:\n",
    "            end_idx = start_idx + subset_size + 1\n",
    "        else:\n",
    "            end_idx = start_idx + subset_size\n",
    "        subset = dataset[start_idx:end_idx]\n",
    "        split_subsets.append(subset)\n",
    "        start_idx = end_idx\n",
    "    split_dataloaders = []\n",
    "    for subset in split_subsets:\n",
    "        split_dataloader = torch.utils.data.DataLoader(subset, batch_size=dataloader.batch_size, shuffle=True)\n",
    "        split_dataloaders.append(split_dataloader)\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Rate Analaysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, predict):\n",
    "    num_samples = X.shape[0]\n",
    "    error_count = torch.count_nonzero(torch.round(predict) - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Optimizer Class of Vanilia Gradient Descent**\n",
    "\n",
    "The name custom_optimizer_SGD is just for consistency with torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device Class and Federated Learning Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.num_samples = self.X_train.size()[0]\n",
    "        self.num_features = self.X_train.size()[1]\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def update_weights_GDVanilia(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Update weight\n",
    "            outputs = self.model(self.X_train.float())\n",
    "            loss = self.criterion(outputs, self.y_train.float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights_GDStochastic(self, num_epochs, batch_size):\n",
    "        self.model.train()\n",
    "        num_batches = self.num_samples // batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data for each epoch\n",
    "            permutation = torch.randperm(self.num_samples)\n",
    "            X_shuffled = self.X_train[permutation]\n",
    "            y_shuffled = self.y_train[permutation]\n",
    "            for batch in range(num_batches):\n",
    "                # Select the current batch\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                # Update weight\n",
    "                outputs = self.model(X_batch.float())\n",
    "                loss = self.criterion(outputs, y_batch.float())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, input_shape, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_loader, num_epochs, self.optimizer, self.criterion, input_shape, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, dataset_loader, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], dataset_loader)\n",
    "    return client_device\n",
    "    \n",
    "\n",
    "# Define server wise functions\n",
    "def send_client_weights(server_weights, local_weights):\n",
    "    server_weights.append(local_weights)\n",
    "\n",
    "# Total weight processing functions\n",
    "def Federated_Averaging(client_weights_total):\n",
    "    total_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / total_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / total_clients\n",
    "\n",
    "    return aggregate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Linear Training Model\n",
    "In this section, we will focus on the dataset requires no complicated data processing, and mainly linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.0. Data Loading and Preprocessing\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BMI Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BMI Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, [0,1,2]]\n",
    "y_train = dataset[:train_split, 3]\n",
    "X_test = dataset[train_split:, [0,1,2]]\n",
    "y_test = dataset[train_split:, 3]\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"BMI Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Pascal Challenge Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Epsilon Pascal Challenge Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized.txt\", sep=' ', header=None, nrows=20000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(\"X_train_tensor size: \", X_train_tensor.size())\n",
    "print(\"y_train_tensor size: \", y_train_tensor.size())\n",
    "print(\"X_test_tensor size: \", X_test_tensor.size())\n",
    "print(\"y_test_tensor size: \", y_test_tensor.size())\n",
    "\n",
    "# Preset parameters\n",
    "dataset_name = \"Epsilon Datset\"\n",
    "learning_rate_preset = 0.001\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gisette Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gisette Dataset\n",
    "#2 class, 6000 data points, ~5000 features\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"gisette_scale\", sep=' ', header=None)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-2]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "#print(dataset)\n",
    "#print(dataset.shape)\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "denominator = X_train.max(0) - X_train.min(0)\n",
    "X_train_normalized = (X_train - X_train.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "denominator = X_test.max(0) - X_test.min(0)\n",
    "X_test_normalized = (X_test - X_test.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "\n",
    "print(\"X_train_normalized: \", X_train_normalized)\n",
    "print(\"X_test_normalized: \", X_test_normalized)\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"Gisette Datset\"\n",
    "learning_rate_preset = 0.005\n",
    "batch_size_preset = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "### Section 1.1. Vanilia Gradient Descent and Stochastic Gradient Descent\n",
    "\n",
    "Initially test training dataset with custom algorithms and pytorch package. We will verify our result of our algorithms by comparing the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_epochs):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "\n",
    "        # Record the loss\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "    \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_iterations = 100\n",
    "w, b, loss_history, error_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Stochastic Gradient Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "        \n",
    "        # General Output\n",
    "        y_pred = np.dot(X_train_normalized, w) + b\n",
    "        error = y_pred - y_train\n",
    "\n",
    "        # Record the loss\n",
    "        error = y_pred\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "            \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "w, b, loss_history, error_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "**Pytorch Package for Vanilia Gradient Descent**\n",
    "\n",
    "(It is just a Vanilia Gradient Descent... They call it SGD is confusing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch Package for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = torch.matmul(X_batch.float(), w) + b\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # General Output\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2. Linear Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "\n",
    "This neural network is simply a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features=1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "num_class = len(torch.unique(y_train_tensor))\n",
    "print(num_samples)\n",
    "print(num_features)\n",
    "print(num_class)\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "    \n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward propagation to obtain the predicted output\n",
    "        outputs = model(X_batch.float())\n",
    "    \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "    \n",
    "        # Backward propagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # General Output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3. Fedearted Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.1 Fedearted Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with vanilia gradient descent method\n",
    "def FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDVanilia(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(len(value) for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDStochastic(local_update_epochs, int(batch_size / num_clients))\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(len(value) for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.2 Fedearted Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [1]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_VGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_VGD.png')\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_vs_Send_Cost_num_clients_{dataset_name}_VGD_per_clients.png')\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_vs_Send_Cost_num_clients_{dataset_name}_VGD_per_clients.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 100\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [2]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_SGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_SGD.png')\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_vs_Send_Cost_num_clients_{dataset_name}_SGD_per_clients.png')\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_vs_Send_Cost_num_clients_{dataset_name}_SGD_per_clients.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "num_clients_list = [5]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [2]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Non-Linear Training Model\n",
    "Here we focus on dataset requires mainly non-linear training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0 Data Loading and Preprocessing\n",
    "\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"mnist_train.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_train = dataset[:10000, 1:].astype(np.int32)\n",
    "y_train = dataset[:10000, 0].astype(np.int32)\n",
    "\n",
    "# Loading testing data\n",
    "dataset = pd.read_csv(\"mnist_test.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_test = dataset[:1000, 1:].astype(np.int32)\n",
    "y_test = dataset[:1000, 0].astype(np.int32)\n",
    "\n",
    "# Translation of data  \n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "y_train = y_train.astype('float32') \n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "y_test = y_test.astype('float32') \n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor)\n",
    "print(y_train_tensor)\n",
    "print(X_test_tensor)\n",
    "print(y_test_tensor)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"MNIST Dataset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 100\n",
    "CNN_input_shape_preset = (-1, 1, 28, 28)\n",
    "\n",
    "# Number of samples, features, and classes\n",
    "num_samples_preset  = X_train_tensor.size()[0]\n",
    "num_features_preset = X_train_tensor.size()[1]\n",
    "num_classes_preset = len(torch.unique(y_train_tensor))\n",
    "\n",
    "# Translate the tensor to dataset\n",
    "MINST_train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "MINST_test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Translate to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(MINST_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(MINST_test_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train_tensor[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print(y_train_tensor[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10_data(directory):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    \n",
    "    # Load data batches\n",
    "    for i in range(1, 6):\n",
    "        train_batch_file = os.path.join(directory, 'data_batch_{}'.format(i))\n",
    "        train_data_dict = unpickle(train_batch_file)\n",
    "        train_batch_images = train_data_dict[b'data']\n",
    "        train_batch_labels = train_data_dict[b'labels']\n",
    "        \n",
    "        # Reshape the image data\n",
    "        train_batch_images = train_batch_images.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n",
    "        \n",
    "        train_images.append(train_batch_images)\n",
    "        train_labels.extend(train_batch_labels)\n",
    "    \n",
    "    # Load test batch\n",
    "    test_batch_file = os.path.join(directory, 'test_batch')\n",
    "    test_data_dict = unpickle(test_batch_file)\n",
    "    test_batch_images = test_data_dict[b'data']\n",
    "    test_batch_labels = test_data_dict[b'labels']\n",
    "    \n",
    "    # Reshape the test image data\n",
    "    test_batch_images = test_batch_images.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n",
    "\n",
    "    test_images.append(test_batch_images)\n",
    "    test_labels.extend(test_batch_labels)\n",
    "    \n",
    "    train_images = np.concatenate(train_images, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# Specify the path to the 'cifar-10-batches-py' directory\n",
    "data_directory = 'cifar-10-batches-py'\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "train_images, train_labels, test_images, test_labels = load_cifar10_data(data_directory)\n",
    "\n",
    "# Input to X and y\n",
    "X_train = train_images.astype(np.int32)\n",
    "y_train = train_labels.astype(np.int32)\n",
    "X_test = test_images.astype(np.int32)\n",
    "y_test = test_labels.astype(np.int32)\n",
    "\n",
    "# Translation of data\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32') \n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized[:100])\n",
    "y_train_tensor = torch.from_numpy(y_train[:100])\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor)\n",
    "print(y_train_tensor)\n",
    "print(X_test_tensor)\n",
    "print(y_test_tensor)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"CIFAR-10 Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 100\n",
    "CNN_input_shape_preset = (-1, 3, 32, 32)\n",
    "\n",
    "# Number of samples, features, and classes\n",
    "num_samples_preset  = X_train_tensor.size()[0]\n",
    "num_features_preset = X_train_tensor.size()[1]\n",
    "num_classes_preset = len(torch.unique(y_train_tensor))\n",
    "\n",
    "# Translate the tensor to dataset\n",
    "CIFAR_10_train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "CIFAR_10_test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Translate to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(CIFAR_10_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(CIFAR_10_test_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train_tensor[i])\n",
    "    plt.show()\n",
    "    print(y_train_tensor[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Non-Linear Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "class ConvolutionalNeuralNetwork_CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation_stack(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device for CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, optimizer, criterion, train_dataloader, test_dataloader=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_dataloader, num_epochs, self.optimizer, self.criterion, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, train_dataloader_list, test_dataloader_list, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], train_dataloader_list[client], test_dataloader_list[client])\n",
    "    return client_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Training and Testing Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_CNN_model(model, dataset_loader, num_epochs, optimizer, criterion, show_history=True, training=True):\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        single_iteration_count = 0.00\n",
    "        batch_iteration_count = 0.00\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "        for i, (images, labels) in enumerate(dataset_loader):\n",
    "            # Define variables\n",
    "            X_batch = Variable(images.view(CNN_input_shape))\n",
    "            y_batch = Variable(labels)\n",
    "\n",
    "            # Forward propagation to obtain the predicted output\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, y_batch.long())\n",
    "            \n",
    "            # Backward propagation and optimization\n",
    "            if training is True:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record the iteration used\n",
    "            single_iteration_count += 1\n",
    "            batch_iteration_count += len(y_batch)\n",
    "\n",
    "            # Record the loss\n",
    "            loss_count += loss.data.detach().numpy()\n",
    "\n",
    "            # Record the error rate\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            error_count += (predicted != y_batch).float().sum()\n",
    "        \n",
    "        # Summarize the loss rate\n",
    "        loss_rate = loss_count / float(single_iteration_count)\n",
    "        loss_history.append(loss_rate)\n",
    "\n",
    "        # Summarize the error rate\n",
    "        train_error_rate = error_count / float(batch_iteration_count)\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the summarized loss and error every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_rate.item():.8f}, Average Error: {train_error_rate:.16f}')\n",
    "\n",
    "    return loss_history, error_history\n",
    "\n",
    "def fit_CNN_model(model, train_loader, test_loader, num_epochs, optimizer, criterion, show_history=True):\n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "       print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history = iterate_CNN_model(model, train_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history = iterate_CNN_model(model, test_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    # Experiment Result\n",
    "    print(\"!-- CNN Model Result --!\")\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "            \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the CNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 10\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if dataset_name == \"CIFAR-10 Datset\":\n",
    "    model = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "else:\n",
    "    model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "print(model)\n",
    "\n",
    "# Perform training\n",
    "start = timeit.default_timer()\n",
    "fit_CNN_model(model, train_loader, None, num_epochs, optimizer, criterion, show_history=True)\n",
    "stop = timeit.default_timer()\n",
    "train_time = stop - start\n",
    "print(\"Time: \", train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Federated Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def iterate_FedLearn_model(model, dataset_loader, num_epochs, optimizer, criterion, num_clients, local_update_epochs, client_device, iterate_func=iterate_CNN_model, aggregate_func=Federated_Averaging, show_history=True, training = True):\n",
    "    # Initialize cost history for recording\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    time_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Initialize global weights\n",
    "    client_weights = [None] * num_clients\n",
    "    global_weights = model.state_dict()\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    # Iteration\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "\n",
    "            # Local update\n",
    "            client_weights[client], local_loss_history, local_error_history = client_device[client].update_weights(local_update_epochs, iterate_func)\n",
    "            local_loss = sum(local_loss_history) / len(local_loss_history)\n",
    "            local_error = sum(local_error_history) / len(local_error_history)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(value.numel() for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "            # Send client loss and error to the server (we ignore the cost involved here)\n",
    "            loss_count += local_loss\n",
    "            error_count += local_error\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = aggregate_func(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Summarize loss and error\n",
    "        loss_average = loss_count / num_clients\n",
    "        train_error_rate_average = error_count / num_clients\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss_average.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        error_cost_history.append(train_error_rate_average)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Record the time history\n",
    "        current_time = timeit.default_timer()\n",
    "        time_history.append(current_time)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 1 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average.item():.8f}, Average Error: {train_error_rate_average:.16f}, Culminative Send Cost: {send_cost}')\n",
    "    stop_time = timeit.default_timer()\n",
    "    used_time = stop_time - start_time\n",
    "    print(\"Time for all iteration: \", used_time)\n",
    "    return loss_cost_history, error_cost_history, send_cost_history, time_history\n",
    "\n",
    "def fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True):\n",
    "    # Preprocess the client data\n",
    "    #client_dataloader_list = split_dataloader(train_loader, num_clients)\n",
    "\n",
    "    # This is the alternative for preprocessing, will be changed in the future\n",
    "\n",
    "    ### Alternative ###\n",
    "    client_dataloader_list = [] * num_clients\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( num_samples_preset / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        \n",
    "        # Translate the tensor to dataset\n",
    "        client_train_dataset = torch.utils.data.TensorDataset(X_train_client[client], y_train_client[client])\n",
    "\n",
    "        # Translate to DataLoader\n",
    "        client_loader = torch.utils.data.DataLoader(client_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "        client_dataloader_list.append(client_loader)\n",
    "    ### Alternative End ###\n",
    "\n",
    "    # Define the client model, criterion, optimizer, its dataset\n",
    "    client_model_list = [None] * num_clients\n",
    "    client_optimizer_list = [None] * num_clients\n",
    "    client_criterion_list = [None] * num_clients\n",
    "    client_setup_func(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list)\n",
    "    \n",
    "    # Establish client devices\n",
    "    client_device = establish_client_devices(num_clients=num_clients, \n",
    "                                             train_dataloader_list=client_dataloader_list, \n",
    "                                             test_dataloader_list= [None] * num_clients,\n",
    "                                             model_list=client_model_list, \n",
    "                                             optimizer_list=client_optimizer_list,\n",
    "                                             criterion_list=client_criterion_list)\n",
    "\n",
    "    # Perform iteration\n",
    "    train_loss_history = []\n",
    "    train_error_history = []\n",
    "    train_send_cost_history = []\n",
    "    train_time_history = []\n",
    "    test_loss_history = []\n",
    "    test_error_history = []\n",
    "    test_send_cost_history = []\n",
    "    test_time_history = []\n",
    "    \n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "        print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history, train_send_cost_history, train_time_history = iterate_FedLearn_model(model=model, dataset_loader=train_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = True)\n",
    "\n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history, test_send_cost_history, test_time_history = iterate_FedLearn_model(model=model, dataset_loader=test_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = False)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Plot send cost history\n",
    "    if test_loader is not None:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        print(f'Total send cost in testing: {sum(test_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history, label='Training Send Cost')\n",
    "        plt.plot(test_send_cost_history, label='Testing Send Cost')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history)\n",
    "    plt.plot(train_send_cost_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the train time history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_time_history, label='Training Time History')\n",
    "        plt.plot(test_time_history, label='Testing Time History')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_time_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Time used\")\n",
    "    plt.title(\"Iteration Time History\")\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss_history, train_error_history, train_send_cost_history, train_time_history, test_loss_history, test_error_history, test_send_cost_history, test_time_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated Learning Expriment Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_FedLearn_model(global_model_class_setup_func, train_loader, test_loader, num_clients_list, local_update_epochs_list, client_setup_func, iterate_func, aggregate_func, compareClients=False):\n",
    "    # Cost History Total\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "    train_loss_history_total = []\n",
    "    train_error_history_total = []\n",
    "    train_send_cost_history_total = []\n",
    "    train_time_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_error_history_total = []\n",
    "    test_send_cost_history_total = []\n",
    "    test_time_history_total = []\n",
    "    experiment_type = \"test\"\n",
    "\n",
    "    if compareClients is True:\n",
    "        iteration_list = num_clients_list\n",
    "        experiment_type = \"num_clients\"\n",
    "    else:\n",
    "        iteration_list = local_update_epochs_list\n",
    "        experiment_type = \"local_update_epochs\"\n",
    "\n",
    "    for n in range(len(iteration_list)):\n",
    "        if compareClients is True:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            num_clients = num_clients_list[n]\n",
    "            local_update_epochs = local_update_epochs_list[0]\n",
    "        else:\n",
    "            print(f'=== The training for local_update_epochs is {local_update_epochs_list[n]} ===')\n",
    "            num_clients = num_clients_list[0]\n",
    "            local_update_epochs = local_update_epochs_list[n]\n",
    "\n",
    "        # Define global neural network model, loss criterion and optimizer\n",
    "        model, criterion, optimizer = global_model_class_setup_func()\n",
    "        print(model)\n",
    "\n",
    "        # Perform training\n",
    "        train_loss_history, train_error_history, train_send_cost_history, train_time_history, test_loss_history, test_error_history, test_send_cost_history, test_time_history = fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True)\n",
    "\n",
    "        # Record the history of loss, error and send_cost\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        train_send_cost_history_total.append(train_send_cost_history)\n",
    "        train_time_history_total.append(train_time_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "        test_send_cost_history_total.append(test_send_cost_history)\n",
    "        test_time_history_total.append(test_time_history)\n",
    "\n",
    "        # Save the npy result for client (train_loss, train_send_cost, train_time, test_error)\n",
    "        #filename = \"{}_{}_{}_{}epoch_result\".format(dataset_name, experiment_type, iteration_list[n-1], num_epochs)\n",
    "        #with open('filename', 'wb') as f:\n",
    "        #    np.save(f, train_loss_history)\n",
    "        #    np.save(f, train_error_history)\n",
    "        #    np.save(f, train_send_cost_history)\n",
    "        #    np.save(f, train_time_history)\n",
    "        #    np.save(f, test_loss_history)\n",
    "        #    np.save(f, test_error_history)\n",
    "        #    np.save(f, test_send_cost_history)\n",
    "        #    np.save(f, test_time_history)\n",
    "    \n",
    "    # Save the npy result (train_loss_history_total)\n",
    "    total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "    with open(total_filename, 'wb') as f:\n",
    "        np.savez(f, train_loss = train_loss_history_total, train_error = train_error_history_total, train_send_cost = train_send_cost_history_total, train_time = train_time_history_total, test_loss = test_loss_history_total, test_error = test_error_history_total, test_send_cost = test_send_cost_history, test_time = test_time_history_total)\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    # Show Dataset Name\n",
    "    print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "    # Training Result\n",
    "    print(f'!-- Training Result --!')\n",
    "    if compareClients is True:\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_training.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_training.png')\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = train_send_cost_history_total[0]\n",
    "        for sublist in train_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_per_clients_training.png')\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = train_send_cost_history_total[0]\n",
    "        for sublist in train_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_per_clients_training.png')\n",
    "    else:\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_local_update_epochs_training.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_local_update_epochs_training.png')\n",
    "\n",
    "    # Testing Result\n",
    "    print(f'!-- Testing Result --!')\n",
    "    if compareClients is True:\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_testing.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_testing.png')\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = test_send_cost_history_total[0]\n",
    "        for sublist in test_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_per_clients_testing.png')\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = test_send_cost_history_total[0]\n",
    "        for sublist in test_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_per_clients_testing.png')\n",
    "    else:\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_local_update_epochs_testing.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_local_update_epochs_testing.png')\n",
    "\n",
    "    # Compare training and testing result\n",
    "    \n",
    "\n",
    "    # Save the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_global_model_func_CNN():\n",
    "    if dataset_name == \"CIFAR-10 Datset\":\n",
    "        model = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "    else:\n",
    "        model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def simple_client_setup_func_CNN(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list):\n",
    "    for client in range(num_clients):\n",
    "        if dataset_name == \"CIFAR-10 Datset\":\n",
    "            client_model_list[client] = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "        else:\n",
    "            client_model_list[client] = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "        client_criterion_list[client] = nn.CrossEntropyLoss()\n",
    "        client_optimizer_list[client] = torch.optim.SGD(client_model_list[client].parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with CNN model\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "experiment_type = \"num_clients\"\n",
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "\n",
    "# Define the model parameters\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 5\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "num_clients_list = [1,2,5]\n",
    "local_update_epochs_list = [2]\n",
    "\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "\n",
    "# Experiment\n",
    "experiment_FedLearn_model(simple_global_model_func_CNN, train_loader, None, num_clients_list, local_update_epochs_list, simple_client_setup_func_CNN, iterate_CNN_model, Federated_Averaging, compareClients=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with CNN model\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "experiment_type = \"local_update_epochs\"\n",
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "\n",
    "# Define the model parameters\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 5\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "num_clients_list = [2]\n",
    "local_update_epochs_list = [1,2]\n",
    "\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "\n",
    "# Experiment\n",
    "experiment_FedLearn_model(simple_global_model_func_CNN, train_loader, None, num_clients_list, local_update_epochs_list, simple_client_setup_func_CNN, iterate_CNN_model, Federated_Averaging, compareClients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs) #update name of result .npz file\n",
    "result = np.load(total_filename) #load data into 'result'\n",
    "print('filename: ', total_filename)\n",
    "print(result.files) #show attributes inside 'result'\n",
    "\n",
    "data_train_loss = result['train_loss']    #np.load(total_filename['train_loss'])\n",
    "data_train_error = result['train_error']    #np.load(total_filename['train_error'])\n",
    "data_train_send_cost = result['train_send_cost']    #np.load(total_filename['train_send_cost'])\n",
    "data_train_time = result['train_time']    #np.load(total_filename['train_time'])\n",
    "data_test_loss = result['test_loss']    #np.load(total_filename['test_loss'])\n",
    "data_test_error = result['test_error']    #np.load(total_filename['test_error'])\n",
    "data_test_send_cost = result['test_send_cost']    #np.load(total_filename['test_send_cost'])\n",
    "data_test_time = result['test_time']    #np.load(total_filename['test_time'])\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"train loss: \", data_train_loss)\n",
    "print(\"train error: \", data_train_error)\n",
    "print(\"train send cost: \", data_train_send_cost)\n",
    "print(\"train time: \", data_train_time)\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"test loss: \", data_test_loss)\n",
    "print(\"test error: \", data_test_error)\n",
    "print(\"test send cost: \", data_test_send_cost)\n",
    "print(\"test time: \", data_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
