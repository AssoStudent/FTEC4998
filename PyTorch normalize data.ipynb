{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "#Loading training data\n",
    "dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "#Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, [0,1,2]]\n",
    "y_train = dataset[:train_split, 3]\n",
    "X_test = dataset[train_split:, [0,1,2]]\n",
    "y_test = dataset[train_split:, 3]\n",
    "\n",
    "#Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "#normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "#turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9c0f31f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.zeros(3, 1, #tensor size (3,1) for 3 variables and 1 output\n",
    "                                               dtype=torch.float),\n",
    "                                  requires_grad=True) #we can update this value with gradient descent\n",
    "        self.bias = nn.Parameter(torch.zeros(1,\n",
    "                                            dtype=torch.float),\n",
    "                                requires_grad=True)\n",
    "    #define one computation step as forward\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.transpose(self.weights, 0, 1) * x + self.bias #regression formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "3743f7e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.]], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0.], requires_grad=True)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create instance of model\n",
    "model_0 = LinearRegressionModel()\n",
    "\n",
    "#Check nn.Parameters in the nn.Module subclass created\n",
    "list(model_0.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "71c05051",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create loss function (Mean absolute error)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "#Create the optimizer\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), #parameters of target model to optimize\n",
    "                           lr=0.01) #learning rate, how much optimizer should change the parameters each step\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ee736aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | MAE Train Loss: 15.80625 | MAE Test Loss: 12.244346394535613 \n",
      "Epoch: 1 | MAE Train Loss: 12.097977166527667 | MAE Test Loss: 9.519757819975723 \n",
      "Epoch: 2 | MAE Train Loss: 9.406777537929964 | MAE Test Loss: 7.5351409392734325 \n",
      "Epoch: 3 | MAE Train Loss: 7.453224524927168 | MAE Test Loss: 6.088252695956648 \n",
      "Epoch: 4 | MAE Train Loss: 6.034662509015179 | MAE Test Loss: 5.0322527097664445 \n",
      "Epoch: 5 | MAE Train Loss: 5.0041140945370195 | MAE Test Loss: 4.2605070524628905 \n",
      "Epoch: 6 | MAE Train Loss: 4.254983164151073 | MAE Test Loss: 3.695562027486068 \n",
      "Epoch: 7 | MAE Train Loss: 3.7099594883252918 | MAE Test Loss: 3.2811466322701177 \n",
      "Epoch: 8 | MAE Train Loss: 3.312974216251581 | MAE Test Loss: 2.976366017623884 \n",
      "Epoch: 9 | MAE Train Loss: 3.0233621345906996 | MAE Test Loss: 2.7514914684639757 \n",
      "Epoch: 10 | MAE Train Loss: 2.8116312264876924 | MAE Test Loss: 2.5849020034068486 \n",
      "Epoch: 11 | MAE Train Loss: 2.656391563427617 | MAE Test Loss: 2.4608667622588927 \n",
      "Epoch: 12 | MAE Train Loss: 2.542130049293377 | MAE Test Loss: 2.3679348755715517 \n",
      "Epoch: 13 | MAE Train Loss: 2.4575961523002703 | MAE Test Loss: 2.2977650533629017 \n",
      "Epoch: 14 | MAE Train Loss: 2.3946298076285664 | MAE Test Loss: 2.2442774959452136 \n",
      "Epoch: 15 | MAE Train Loss: 2.3473129882391413 | MAE Test Loss: 2.2030370074585512 \n",
      "Epoch: 16 | MAE Train Loss: 2.311353494492593 | MAE Test Loss: 2.1708053803239933 \n",
      "Epoch: 17 | MAE Train Loss: 2.283638501680553 | MAE Test Loss: 2.1452155579649927 \n",
      "Epoch: 18 | MAE Train Loss: 2.261910249975287 | MAE Test Loss: 2.12453534988125 \n",
      "Epoch: 19 | MAE Train Loss: 2.244531311746917 | MAE Test Loss: 2.1074952273483065 \n",
      "Epoch: 20 | MAE Train Loss: 2.230314013388556 | MAE Test Loss: 2.0931633558401597 \n",
      "Epoch: 21 | MAE Train Loss: 2.2183969729033586 | MAE Test Loss: 2.0808545853831535 \n",
      "Epoch: 22 | MAE Train Loss: 2.208155485552093 | MAE Test Loss: 2.070063941288727 \n",
      "Epoch: 23 | MAE Train Loss: 2.1991363618546473 | MAE Test Loss: 2.060418520984638 \n",
      "Epoch: 24 | MAE Train Loss: 2.19101093595203 | MAE Test Loss: 2.051642091345056 \n",
      "Epoch: 25 | MAE Train Loss: 2.183540744420433 | MAE Test Loss: 2.043529650612915 \n",
      "Epoch: 26 | MAE Train Loss: 2.1765529114246456 | MAE Test Loss: 2.035928232261643 \n",
      "Epoch: 27 | MAE Train Loss: 2.169921881289813 | MAE Test Loss: 2.0287235193709585 \n",
      "Epoch: 28 | MAE Train Loss: 2.163556612491497 | MAE Test Loss: 2.0218297508879566 \n",
      "Epoch: 29 | MAE Train Loss: 2.157391084915607 | MAE Test Loss: 2.015182170316969 \n",
      "Epoch: 30 | MAE Train Loss: 2.151377332867764 | MAE Test Loss: 2.008731708732603 \n",
      "Epoch: 31 | MAE Train Loss: 2.1454805246310156 | MAE Test Loss: 2.002440974622836 \n",
      "Epoch: 32 | MAE Train Loss: 2.1396753558391564 | MAE Test Loss: 1.9962814100064463 \n",
      "Epoch: 33 | MAE Train Loss: 2.1339434764926692 | MAE Test Loss: 1.9902310098691445 \n",
      "Epoch: 34 | MAE Train Loss: 2.12827153508325 | MAE Test Loss: 1.9842726492726623 \n",
      "Epoch: 35 | MAE Train Loss: 2.1226497681501075 | MAE Test Loss: 1.9783930460541188 \n",
      "Epoch: 36 | MAE Train Loss: 2.117071095210354 | MAE Test Loss: 1.972581776497402 \n",
      "Epoch: 37 | MAE Train Loss: 2.1115303426610295 | MAE Test Loss: 1.9668305942422222 \n",
      "Epoch: 38 | MAE Train Loss: 2.1060237151105823 | MAE Test Loss: 1.9611328953169902 \n",
      "Epoch: 39 | MAE Train Loss: 2.1005484014882114 | MAE Test Loss: 1.9554834798215706 \n",
      "Epoch: 40 | MAE Train Loss: 2.0951023657855825 | MAE Test Loss: 1.949878123382679 \n",
      "Epoch: 41 | MAE Train Loss: 2.0896840780074064 | MAE Test Loss: 1.9443133818400828 \n",
      "Epoch: 42 | MAE Train Loss: 2.084292390297321 | MAE Test Loss: 1.9387864701129414 \n",
      "Epoch: 43 | MAE Train Loss: 2.078926437806868 | MAE Test Loss: 1.93329508993913 \n",
      "Epoch: 44 | MAE Train Loss: 2.073585557806555 | MAE Test Loss: 1.927837333351349 \n",
      "Epoch: 45 | MAE Train Loss: 2.068269232536434 | MAE Test Loss: 1.9224116475255335 \n",
      "Epoch: 46 | MAE Train Loss: 2.062977064477446 | MAE Test Loss: 1.9170167006812977 \n",
      "Epoch: 47 | MAE Train Loss: 2.0577087088329016 | MAE Test Loss: 1.9116513909540707 \n",
      "Epoch: 48 | MAE Train Loss: 2.052463904372238 | MAE Test Loss: 1.9063147766878477 \n",
      "Epoch: 49 | MAE Train Loss: 2.047242419574098 | MAE Test Loss: 1.9010060691007697 \n",
      "Epoch: 50 | MAE Train Loss: 2.042044051911 | MAE Test Loss: 1.8957245371313995 \n",
      "Epoch: 51 | MAE Train Loss: 2.0368686036445447 | MAE Test Loss: 1.8904696252501352 \n",
      "Epoch: 52 | MAE Train Loss: 2.03171593357487 | MAE Test Loss: 1.8852407778836038 \n",
      "Epoch: 53 | MAE Train Loss: 2.0265858651745963 | MAE Test Loss: 1.8800375893420007 \n",
      "Epoch: 54 | MAE Train Loss: 2.0214782801991724 | MAE Test Loss: 1.874859635755822 \n",
      "Epoch: 55 | MAE Train Loss: 2.0163930325123607 | MAE Test Loss: 1.8697065625610492 \n",
      "Epoch: 56 | MAE Train Loss: 2.011329987488967 | MAE Test Loss: 1.8645780607775104 \n",
      "Epoch: 57 | MAE Train Loss: 2.0062890235877036 | MAE Test Loss: 1.8594738471327332 \n",
      "Epoch: 58 | MAE Train Loss: 2.001270015488646 | MAE Test Loss: 1.8543936487756465 \n",
      "Epoch: 59 | MAE Train Loss: 1.9962728396355829 | MAE Test Loss: 1.8493372522703873 \n",
      "Epoch: 60 | MAE Train Loss: 1.9912973840788495 | MAE Test Loss: 1.8443044075075072 \n",
      "Epoch: 61 | MAE Train Loss: 1.986343516174322 | MAE Test Loss: 1.8392949093515019 \n",
      "Epoch: 62 | MAE Train Loss: 1.9814111209767424 | MAE Test Loss: 1.8343085676236512 \n",
      "Epoch: 63 | MAE Train Loss: 1.9765000769734666 | MAE Test Loss: 1.8293452481054544 \n",
      "Epoch: 64 | MAE Train Loss: 1.9716102917995904 | MAE Test Loss: 1.824404740727824 \n",
      "Epoch: 65 | MAE Train Loss: 1.9667416367265127 | MAE Test Loss: 1.8194868920000324 \n",
      "Epoch: 66 | MAE Train Loss: 1.961893997593587 | MAE Test Loss: 1.814591520680858 \n",
      "Epoch: 67 | MAE Train Loss: 1.957067250032209 | MAE Test Loss: 1.8097185145087238 \n",
      "Epoch: 68 | MAE Train Loss: 1.952261296532813 | MAE Test Loss: 1.804867706665592 \n",
      "Epoch: 69 | MAE Train Loss: 1.9474760219000469 | MAE Test Loss: 1.8000389820995886 \n",
      "Epoch: 70 | MAE Train Loss: 1.9427113237030766 | MAE Test Loss: 1.7952321884589129 \n",
      "Epoch: 71 | MAE Train Loss: 1.9379670823125317 | MAE Test Loss: 1.7904472133485654 \n",
      "Epoch: 72 | MAE Train Loss: 1.9332431972756627 | MAE Test Loss: 1.7856839158140894 \n",
      "Epoch: 73 | MAE Train Loss: 1.9285395565630785 | MAE Test Loss: 1.78094215518935 \n",
      "Epoch: 74 | MAE Train Loss: 1.9238560472364867 | MAE Test Loss: 1.7762218197211883 \n",
      "Epoch: 75 | MAE Train Loss: 1.9191925674760788 | MAE Test Loss: 1.771522795708055 \n",
      "Epoch: 76 | MAE Train Loss: 1.9145490129250688 | MAE Test Loss: 1.7668449741512358 \n",
      "Epoch: 77 | MAE Train Loss: 1.909925284245069 | MAE Test Loss: 1.7621882155188495 \n",
      "Epoch: 78 | MAE Train Loss: 1.905321267894794 | MAE Test Loss: 1.7575524081954703 \n",
      "Epoch: 79 | MAE Train Loss: 1.900736861332592 | MAE Test Loss: 1.7529374418618127 \n",
      "Epoch: 80 | MAE Train Loss: 1.896171962341582 | MAE Test Loss: 1.7483432174114264 \n",
      "Epoch: 81 | MAE Train Loss: 1.891626478517329 | MAE Test Loss: 1.7437695686571892 \n",
      "Epoch: 82 | MAE Train Loss: 1.8871002715634417 | MAE Test Loss: 1.7392164436774038 \n",
      "Epoch: 83 | MAE Train Loss: 1.8825932782987658 | MAE Test Loss: 1.7346836962576835 \n",
      "Epoch: 84 | MAE Train Loss: 1.8781053779941967 | MAE Test Loss: 1.7301712300835024 \n",
      "Epoch: 85 | MAE Train Loss: 1.8736364803553076 | MAE Test Loss: 1.7256789219686433 \n",
      "Epoch: 86 | MAE Train Loss: 1.869186471689617 | MAE Test Loss: 1.72120667662703 \n",
      "Epoch: 87 | MAE Train Loss: 1.8647552627197073 | MAE Test Loss: 1.7167543765261055 \n",
      "Epoch: 88 | MAE Train Loss: 1.8603427558706502 | MAE Test Loss: 1.7123219240862508 \n",
      "Epoch: 89 | MAE Train Loss: 1.8559488477526507 | MAE Test Loss: 1.7079092278669865 \n",
      "Epoch: 90 | MAE Train Loss: 1.8515734534741384 | MAE Test Loss: 1.7035161480466432 \n",
      "Epoch: 91 | MAE Train Loss: 1.8472164563985576 | MAE Test Loss: 1.6991426149924038 \n",
      "Epoch: 92 | MAE Train Loss: 1.8428777782895793 | MAE Test Loss: 1.6947884848333044 \n",
      "Epoch: 93 | MAE Train Loss: 1.8385572978784075 | MAE Test Loss: 1.690453690897814 \n",
      "Epoch: 94 | MAE Train Loss: 1.8342549408107893 | MAE Test Loss: 1.6861381230859156 \n",
      "Epoch: 95 | MAE Train Loss: 1.8299706159902995 | MAE Test Loss: 1.6818416645566447 \n",
      "Epoch: 96 | MAE Train Loss: 1.8257042152247625 | MAE Test Loss: 1.6775642469104841 \n",
      "Epoch: 97 | MAE Train Loss: 1.8214556625729172 | MAE Test Loss: 1.6733057597006227 \n",
      "Epoch: 98 | MAE Train Loss: 1.8172248654980065 | MAE Test Loss: 1.6690660924362342 \n",
      "Epoch: 99 | MAE Train Loss: 1.8130117230968144 | MAE Test Loss: 1.6648451305537446 \n",
      "Epoch: 100 | MAE Train Loss: 1.8088161293388882 | MAE Test Loss: 1.6606427837704643 \n",
      "Epoch: 101 | MAE Train Loss: 1.8046379988408485 | MAE Test Loss: 1.656458972546576 \n",
      "Epoch: 102 | MAE Train Loss: 1.8004772580516324 | MAE Test Loss: 1.6522935801334526 \n",
      "Epoch: 103 | MAE Train Loss: 1.7963338054577576 | MAE Test Loss: 1.6481465215998028 \n",
      "Epoch: 104 | MAE Train Loss: 1.792207553956073 | MAE Test Loss: 1.644017685061011 \n",
      "Epoch: 105 | MAE Train Loss: 1.7880984000981948 | MAE Test Loss: 1.6399070160121347 \n",
      "Epoch: 106 | MAE Train Loss: 1.7840062918850463 | MAE Test Loss: 1.6358143818177813 \n",
      "Epoch: 107 | MAE Train Loss: 1.7799311155539086 | MAE Test Loss: 1.6317396884809028 \n",
      "Epoch: 108 | MAE Train Loss: 1.775872774591194 | MAE Test Loss: 1.6276828605904186 \n",
      "Epoch: 109 | MAE Train Loss: 1.7718311986895032 | MAE Test Loss: 1.6236437897646614 \n",
      "Epoch: 110 | MAE Train Loss: 1.7678062951401692 | MAE Test Loss: 1.6196223885087515 \n",
      "Epoch: 111 | MAE Train Loss: 1.76379797456031 | MAE Test Loss: 1.615618562158057 \n",
      "Epoch: 112 | MAE Train Loss: 1.7598061568339982 | MAE Test Loss: 1.6116322510691634 \n",
      "Epoch: 113 | MAE Train Loss: 1.7558307784100822 | MAE Test Loss: 1.60766332103717 \n",
      "Epoch: 114 | MAE Train Loss: 1.751871723187011 | MAE Test Loss: 1.6037116951965813 \n",
      "Epoch: 115 | MAE Train Loss: 1.7479289179302633 | MAE Test Loss: 1.5997772920364954 \n",
      "Epoch: 116 | MAE Train Loss: 1.7440022810227815 | MAE Test Loss: 1.5958600132997196 \n",
      "Epoch: 117 | MAE Train Loss: 1.7400917285417645 | MAE Test Loss: 1.5919597720645018 \n",
      "Epoch: 118 | MAE Train Loss: 1.736197177117873 | MAE Test Loss: 1.588076473100476 \n",
      "Epoch: 119 | MAE Train Loss: 1.7323185408843393 | MAE Test Loss: 1.584210041652931 \n",
      "Epoch: 120 | MAE Train Loss: 1.7284557487370869 | MAE Test Loss: 1.580360389736257 \n",
      "Epoch: 121 | MAE Train Loss: 1.724608717940347 | MAE Test Loss: 1.576527430246808 \n",
      "Epoch: 122 | MAE Train Loss: 1.720777369894232 | MAE Test Loss: 1.572711071555353 \n",
      "Epoch: 123 | MAE Train Loss: 1.7169616173388973 | MAE Test Loss: 1.5689112230889268 \n",
      "Epoch: 124 | MAE Train Loss: 1.7131613772782022 | MAE Test Loss: 1.5651278097946943 \n",
      "Epoch: 125 | MAE Train Loss: 1.7093765799671938 | MAE Test Loss: 1.5613607524803679 \n",
      "Epoch: 126 | MAE Train Loss: 1.705607150699168 | MAE Test Loss: 1.5576099595148094 \n",
      "Epoch: 127 | MAE Train Loss: 1.701853006967784 | MAE Test Loss: 1.5538753310360156 \n",
      "Epoch: 128 | MAE Train Loss: 1.6981140588741273 | MAE Test Loss: 1.5501568108924413 \n",
      "Epoch: 129 | MAE Train Loss: 1.6943902489023859 | MAE Test Loss: 1.546454295598433 \n",
      "Epoch: 130 | MAE Train Loss: 1.6906814801366417 | MAE Test Loss: 1.5427677158543278 \n",
      "Epoch: 131 | MAE Train Loss: 1.6869876960777184 | MAE Test Loss: 1.5390969747643997 \n",
      "Epoch: 132 | MAE Train Loss: 1.6833088001402694 | MAE Test Loss: 1.535442011499803 \n",
      "Epoch: 133 | MAE Train Loss: 1.6796447368540126 | MAE Test Loss: 1.5318027384418582 \n",
      "Epoch: 134 | MAE Train Loss: 1.6759954291267065 | MAE Test Loss: 1.5281790734793526 \n",
      "Epoch: 135 | MAE Train Loss: 1.6723607995987053 | MAE Test Loss: 1.5245709430619185 \n",
      "Epoch: 136 | MAE Train Loss: 1.6687407758538921 | MAE Test Loss: 1.5209782404248746 \n",
      "Epoch: 137 | MAE Train Loss: 1.66513526379469 | MAE Test Loss: 1.5174009317350106 \n",
      "Epoch: 138 | MAE Train Loss: 1.6615442272130323 | MAE Test Loss: 1.513838911307652 \n",
      "Epoch: 139 | MAE Train Loss: 1.6579675727140788 | MAE Test Loss: 1.5102920920631555 \n",
      "Epoch: 140 | MAE Train Loss: 1.6544052190674912 | MAE Test Loss: 1.5067604233309797 \n",
      "Epoch: 141 | MAE Train Loss: 1.650857119301282 | MAE Test Loss: 1.5032437870638449 \n",
      "Epoch: 142 | MAE Train Loss: 1.6473231637825492 | MAE Test Loss: 1.4997421610035893 \n",
      "Epoch: 143 | MAE Train Loss: 1.6438033289173675 | MAE Test Loss: 1.4962554314450507 \n",
      "Epoch: 144 | MAE Train Loss: 1.640297512232746 | MAE Test Loss: 1.4927835337814628 \n",
      "Epoch: 145 | MAE Train Loss: 1.6368056523849694 | MAE Test Loss: 1.4893264094084273 \n",
      "Epoch: 146 | MAE Train Loss: 1.6333276970840909 | MAE Test Loss: 1.485883938622283 \n",
      "Epoch: 147 | MAE Train Loss: 1.629863535208445 | MAE Test Loss: 1.482456079854176 \n",
      "Epoch: 148 | MAE Train Loss: 1.626413126805908 | MAE Test Loss: 1.4790427470501115 \n",
      "Epoch: 149 | MAE Train Loss: 1.6229763930331687 | MAE Test Loss: 1.4756438745970633 \n",
      "Epoch: 150 | MAE Train Loss: 1.6195532730407436 | MAE Test Loss: 1.4722593971259368 \n",
      "Epoch: 151 | MAE Train Loss: 1.6161437062445572 | MAE Test Loss: 1.4688892412069434 \n",
      "Epoch: 152 | MAE Train Loss: 1.6127476225768795 | MAE Test Loss: 1.465533330597784 \n",
      "Epoch: 153 | MAE Train Loss: 1.609364954392089 | MAE Test Loss: 1.462191572283356 \n",
      "Epoch: 154 | MAE Train Loss: 1.6059956158156605 | MAE Test Loss: 1.458863922968394 \n",
      "Epoch: 155 | MAE Train Loss: 1.6026395641829088 | MAE Test Loss: 1.4555502992622498 \n",
      "Epoch: 156 | MAE Train Loss: 1.5992967231912305 | MAE Test Loss: 1.452250637597879 \n",
      "Epoch: 157 | MAE Train Loss: 1.5959670350340516 | MAE Test Loss: 1.4489648558985186 \n",
      "Epoch: 158 | MAE Train Loss: 1.5926504233300798 | MAE Test Loss: 1.4456928834834246 \n",
      "Epoch: 159 | MAE Train Loss: 1.5893468213913757 | MAE Test Loss: 1.4424346665819123 \n",
      "Epoch: 160 | MAE Train Loss: 1.5860561811036091 | MAE Test Loss: 1.4391901155271585 \n",
      "Epoch: 161 | MAE Train Loss: 1.5827784176037234 | MAE Test Loss: 1.43595917743942 \n",
      "Epoch: 162 | MAE Train Loss: 1.5795134842247336 | MAE Test Loss: 1.4327417931917297 \n",
      "Epoch: 163 | MAE Train Loss: 1.576261324797143 | MAE Test Loss: 1.4295378915435444 \n",
      "Epoch: 164 | MAE Train Loss: 1.5730218726899696 | MAE Test Loss: 1.4263473930435688 \n",
      "Epoch: 165 | MAE Train Loss: 1.5697950548145694 | MAE Test Loss: 1.423170218736208 \n",
      "Epoch: 166 | MAE Train Loss: 1.566580798510905 | MAE Test Loss: 1.4200063369213045 \n",
      "Epoch: 167 | MAE Train Loss: 1.563379075067726 | MAE Test Loss: 1.4168556515530715 \n",
      "Epoch: 168 | MAE Train Loss: 1.560189793038162 | MAE Test Loss: 1.4137181224221782 \n",
      "Epoch: 169 | MAE Train Loss: 1.5570129177180436 | MAE Test Loss: 1.4105936608814749 \n",
      "Epoch: 170 | MAE Train Loss: 1.5538483667757135 | MAE Test Loss: 1.407482200339317 \n",
      "Epoch: 171 | MAE Train Loss: 1.5506960766038052 | MAE Test Loss: 1.4043837013694027 \n",
      "Epoch: 172 | MAE Train Loss: 1.5475560132437207 | MAE Test Loss: 1.4012980684821512 \n",
      "Epoch: 173 | MAE Train Loss: 1.5444280862079056 | MAE Test Loss: 1.3982252732091713 \n",
      "Epoch: 174 | MAE Train Loss: 1.541312269363933 | MAE Test Loss: 1.3951652182698118 \n",
      "Epoch: 175 | MAE Train Loss: 1.5382084747772766 | MAE Test Loss: 1.3921178677658466 \n",
      "Epoch: 176 | MAE Train Loss: 1.5351166676423393 | MAE Test Loss: 1.3890831355088684 \n",
      "Epoch: 177 | MAE Train Loss: 1.5320367680031861 | MAE Test Loss: 1.386060987371814 \n",
      "Epoch: 178 | MAE Train Loss: 1.5289687443786868 | MAE Test Loss: 1.3830513445996753 \n",
      "Epoch: 179 | MAE Train Loss: 1.5259125236313102 | MAE Test Loss: 1.380054133034847 \n",
      "Epoch: 180 | MAE Train Loss: 1.5228680376572432 | MAE Test Loss: 1.37706931873758 \n",
      "Epoch: 181 | MAE Train Loss: 1.5198352565466215 | MAE Test Loss: 1.374096799839374 \n",
      "Epoch: 182 | MAE Train Loss: 1.5168140813889217 | MAE Test Loss: 1.3711365675107756 \n",
      "Epoch: 183 | MAE Train Loss: 1.513804508664291 | MAE Test Loss: 1.3681885208081959 \n",
      "Epoch: 184 | MAE Train Loss: 1.5108064400676158 | MAE Test Loss: 1.3652526156966365 \n",
      "Epoch: 185 | MAE Train Loss: 1.5078198404755543 | MAE Test Loss: 1.3623287875011199 \n",
      "Epoch: 186 | MAE Train Loss: 1.504844644252033 | MAE Test Loss: 1.359416992328044 \n",
      "Epoch: 187 | MAE Train Loss: 1.501880816734094 | MAE Test Loss: 1.3565171533189182 \n",
      "Epoch: 188 | MAE Train Loss: 1.4989282826159465 | MAE Test Loss: 1.353629210304901 \n",
      "Epoch: 189 | MAE Train Loss: 1.4959869876410752 | MAE Test Loss: 1.3507531168532199 \n",
      "Epoch: 190 | MAE Train Loss: 1.4930568874223582 | MAE Test Loss: 1.3478888031924534 \n",
      "Epoch: 191 | MAE Train Loss: 1.4901379211189725 | MAE Test Loss: 1.345036216270242 \n",
      "Epoch: 192 | MAE Train Loss: 1.4872300361820683 | MAE Test Loss: 1.3421953038705925 \n",
      "Epoch: 193 | MAE Train Loss: 1.4843331841874003 | MAE Test Loss: 1.339366013967819 \n",
      "Epoch: 194 | MAE Train Loss: 1.481447316923418 | MAE Test Loss: 1.336548277078343 \n",
      "Epoch: 195 | MAE Train Loss: 1.4785723745392114 | MAE Test Loss: 1.3337420420286854 \n",
      "Epoch: 196 | MAE Train Loss: 1.475708305625178 | MAE Test Loss: 1.330947265050906 \n",
      "Epoch: 197 | MAE Train Loss: 1.4728550712837665 | MAE Test Loss: 1.3281638875297883 \n",
      "Epoch: 198 | MAE Train Loss: 1.4700126156677151 | MAE Test Loss: 1.325391820176497 \n",
      "Epoch: 199 | MAE Train Loss: 1.4671808550466054 | MAE Test Loss: 1.3226310342309127 \n",
      "Epoch: 200 | MAE Train Loss: 1.4643597640977428 | MAE Test Loss: 1.3198814793536149 \n",
      "Epoch: 201 | MAE Train Loss: 1.4615492963230525 | MAE Test Loss: 1.31714310538811 \n",
      "Epoch: 202 | MAE Train Loss: 1.458749405428927 | MAE Test Loss: 1.3144158533613073 \n",
      "Epoch: 203 | MAE Train Loss: 1.4559600385514124 | MAE Test Loss: 1.3116996662301894 \n",
      "Epoch: 204 | MAE Train Loss: 1.453181141470369 | MAE Test Loss: 1.3089944856616678 \n",
      "Epoch: 205 | MAE Train Loss: 1.450412661885426 | MAE Test Loss: 1.30630027140717 \n",
      "Epoch: 206 | MAE Train Loss: 1.4476545612096086 | MAE Test Loss: 1.303616956204546 \n",
      "Epoch: 207 | MAE Train Loss: 1.4449067698843612 | MAE Test Loss: 1.3009444922865783 \n",
      "Epoch: 208 | MAE Train Loss: 1.4421692540835416 | MAE Test Loss: 1.298282825313573 \n",
      "Epoch: 209 | MAE Train Loss: 1.4394419592817174 | MAE Test Loss: 1.295631912890176 \n",
      "Epoch: 210 | MAE Train Loss: 1.4367248509388437 | MAE Test Loss: 1.2929916982966694 \n",
      "Epoch: 211 | MAE Train Loss: 1.4340178672484925 | MAE Test Loss: 1.2903621122380216 \n",
      "Epoch: 212 | MAE Train Loss: 1.4313209505445978 | MAE Test Loss: 1.2877431302846527 \n",
      "Epoch: 213 | MAE Train Loss: 1.4286340801564423 | MAE Test Loss: 1.2851346955773928 \n",
      "Epoch: 214 | MAE Train Loss: 1.425957196611774 | MAE Test Loss: 1.282536739186597 \n",
      "Epoch: 215 | MAE Train Loss: 1.4232902430200416 | MAE Test Loss: 1.2799492221948925 \n",
      "Epoch: 216 | MAE Train Loss: 1.4206331736944642 | MAE Test Loss: 1.2773721046709405 \n",
      "Epoch: 217 | MAE Train Loss: 1.417985964686331 | MAE Test Loss: 1.2748053183518535 \n",
      "Epoch: 218 | MAE Train Loss: 1.415348539854601 | MAE Test Loss: 1.2722488091931612 \n",
      "Epoch: 219 | MAE Train Loss: 1.412720859588075 | MAE Test Loss: 1.2697025385398553 \n",
      "Epoch: 220 | MAE Train Loss: 1.4101028805973008 | MAE Test Loss: 1.2671664678326255 \n",
      "Epoch: 221 | MAE Train Loss: 1.4074945780851356 | MAE Test Loss: 1.2646405447304432 \n",
      "Epoch: 222 | MAE Train Loss: 1.404895893044873 | MAE Test Loss: 1.2621247171079724 \n",
      "Epoch: 223 | MAE Train Loss: 1.4023067759426178 | MAE Test Loss: 1.2596189535444098 \n",
      "Epoch: 224 | MAE Train Loss: 1.3997271999217535 | MAE Test Loss: 1.2571231941160488 \n",
      "Epoch: 225 | MAE Train Loss: 1.3971571199632602 | MAE Test Loss: 1.2546373730152083 \n",
      "Epoch: 226 | MAE Train Loss: 1.3945964638025885 | MAE Test Loss: 1.2521614532574614 \n",
      "Epoch: 227 | MAE Train Loss: 1.3920451986732054 | MAE Test Loss: 1.2496953838993234 \n",
      "Epoch: 228 | MAE Train Loss: 1.3895032762287596 | MAE Test Loss: 1.247239128242646 \n",
      "Epoch: 229 | MAE Train Loss: 1.3869706640167467 | MAE Test Loss: 1.2447926552305093 \n",
      "Epoch: 230 | MAE Train Loss: 1.384447343912321 | MAE Test Loss: 1.242355893829862 \n",
      "Epoch: 231 | MAE Train Loss: 1.381933239826316 | MAE Test Loss: 1.239928807900619 \n",
      "Epoch: 232 | MAE Train Loss: 1.3794283198227257 | MAE Test Loss: 1.2375113328740104 \n",
      "Epoch: 233 | MAE Train Loss: 1.3769325226160707 | MAE Test Loss: 1.2351034689342468 \n",
      "Epoch: 234 | MAE Train Loss: 1.3744458513485487 | MAE Test Loss: 1.2327051332534986 \n",
      "Epoch: 235 | MAE Train Loss: 1.3719682154892625 | MAE Test Loss: 1.2303162881059453 \n",
      "Epoch: 236 | MAE Train Loss: 1.3694995928236877 | MAE Test Loss: 1.2279368699887974 \n",
      "Epoch: 237 | MAE Train Loss: 1.367039923113718 | MAE Test Loss: 1.225566879527961 \n",
      "Epoch: 238 | MAE Train Loss: 1.3645892099935577 | MAE Test Loss: 1.2232062461197564 \n",
      "Epoch: 239 | MAE Train Loss: 1.3621473882088675 | MAE Test Loss: 1.2208549070524497 \n",
      "Epoch: 240 | MAE Train Loss: 1.359714398260514 | MAE Test Loss: 1.2185128420446645 \n",
      "Epoch: 241 | MAE Train Loss: 1.3572902235269875 | MAE Test Loss: 1.216180010042225 \n",
      "Epoch: 242 | MAE Train Loss: 1.3548748255272849 | MAE Test Loss: 1.2138563595496008 \n",
      "Epoch: 243 | MAE Train Loss: 1.3524681447954046 | MAE Test Loss: 1.2115418528933237 \n",
      "Epoch: 244 | MAE Train Loss: 1.3500701602883907 | MAE Test Loss: 1.2092364285727872 \n",
      "Epoch: 245 | MAE Train Loss: 1.3476808136856586 | MAE Test Loss: 1.206940078506959 \n",
      "Epoch: 246 | MAE Train Loss: 1.3453000871369976 | MAE Test Loss: 1.2046527303306578 \n",
      "Epoch: 247 | MAE Train Loss: 1.3429279246493906 | MAE Test Loss: 1.2023743553638773 \n",
      "Epoch: 248 | MAE Train Loss: 1.340564288194217 | MAE Test Loss: 1.200104923353939 \n",
      "Epoch: 249 | MAE Train Loss: 1.3382091644808112 | MAE Test Loss: 1.1978443666409773 \n",
      "Epoch: 250 | MAE Train Loss: 1.3358624893263245 | MAE Test Loss: 1.1955926575250735 \n",
      "Epoch: 251 | MAE Train Loss: 1.3335242252154595 | MAE Test Loss: 1.1933497502778327 \n",
      "Epoch: 252 | MAE Train Loss: 1.3311943288400205 | MAE Test Loss: 1.1911156148662427 \n",
      "Epoch: 253 | MAE Train Loss: 1.3288727876003101 | MAE Test Loss: 1.1888901900909574 \n",
      "Epoch: 254 | MAE Train Loss: 1.3265595295085 | MAE Test Loss: 1.1866734517554607 \n",
      "Epoch: 255 | MAE Train Loss: 1.3242545497335256 | MAE Test Loss: 1.1844653680356543 \n",
      "Epoch: 256 | MAE Train Loss: 1.3219578027581185 | MAE Test Loss: 1.1822658802911157 \n",
      "Epoch: 257 | MAE Train Loss: 1.3196692504832743 | MAE Test Loss: 1.180074949729905 \n",
      "Epoch: 258 | MAE Train Loss: 1.3173888420693607 | MAE Test Loss: 1.1778925398070708 \n",
      "Epoch: 259 | MAE Train Loss: 1.3151165428415865 | MAE Test Loss: 1.1757186117446001 \n",
      "Epoch: 260 | MAE Train Loss: 1.3128523182599534 | MAE Test Loss: 1.1735531279103353 \n",
      "Epoch: 261 | MAE Train Loss: 1.3105961496747904 | MAE Test Loss: 1.1713960378840393 \n",
      "Epoch: 262 | MAE Train Loss: 1.3083479754361371 | MAE Test Loss: 1.1692473449220544 \n",
      "Epoch: 263 | MAE Train Loss: 1.3061078003471498 | MAE Test Loss: 1.1671069585611051 \n",
      "Epoch: 264 | MAE Train Loss: 1.3038755374037203 | MAE Test Loss: 1.164974861758934 \n",
      "Epoch: 265 | MAE Train Loss: 1.3016511742286911 | MAE Test Loss: 1.1628510322471346 \n",
      "Epoch: 266 | MAE Train Loss: 1.2994346902738592 | MAE Test Loss: 1.1607353984637652 \n",
      "Epoch: 267 | MAE Train Loss: 1.2972260334646493 | MAE Test Loss: 1.1586279456628037 \n",
      "Epoch: 268 | MAE Train Loss: 1.295025160520673 | MAE Test Loss: 1.156528603500588 \n",
      "Epoch: 269 | MAE Train Loss: 1.292832037058513 | MAE Test Loss: 1.1544373832710648 \n",
      "Epoch: 270 | MAE Train Loss: 1.290646660051118 | MAE Test Loss: 1.1523542307005168 \n",
      "Epoch: 271 | MAE Train Loss: 1.2884689637888787 | MAE Test Loss: 1.150279107768455 \n",
      "Epoch: 272 | MAE Train Loss: 1.2862989284361188 | MAE Test Loss: 1.1482119812037925 \n",
      "Epoch: 273 | MAE Train Loss: 1.284136523333822 | MAE Test Loss: 1.1461528012612583 \n",
      "Epoch: 274 | MAE Train Loss: 1.2819817031087934 | MAE Test Loss: 1.1441015391234806 \n",
      "Epoch: 275 | MAE Train Loss: 1.2798344424913464 | MAE Test Loss: 1.142058153222874 \n",
      "Epoch: 276 | MAE Train Loss: 1.2776947025507426 | MAE Test Loss: 1.140022611343612 \n",
      "Epoch: 277 | MAE Train Loss: 1.2755624534254326 | MAE Test Loss: 1.1379948759864003 \n",
      "Epoch: 278 | MAE Train Loss: 1.2734376614461698 | MAE Test Loss: 1.1359749167264408 \n",
      "Epoch: 279 | MAE Train Loss: 1.2713202824400163 | MAE Test Loss: 1.1339626950261676 \n",
      "Epoch: 280 | MAE Train Loss: 1.2692102975983126 | MAE Test Loss: 1.1319581921782345 \n",
      "Epoch: 281 | MAE Train Loss: 1.26710769129309 | MAE Test Loss: 1.1299613469000969 \n",
      "Epoch: 282 | MAE Train Loss: 1.2650124057521555 | MAE Test Loss: 1.1279721287801492 \n",
      "Epoch: 283 | MAE Train Loss: 1.2629243949677982 | MAE Test Loss: 1.1259905132255095 \n",
      "Epoch: 284 | MAE Train Loss: 1.2608436567103887 | MAE Test Loss: 1.1240164364838354 \n",
      "Epoch: 285 | MAE Train Loss: 1.2587701292899474 | MAE Test Loss: 1.1220499133536053 \n",
      "Epoch: 286 | MAE Train Loss: 1.256703811262478 | MAE Test Loss: 1.1200908749762248 \n",
      "Epoch: 287 | MAE Train Loss: 1.254644656669099 | MAE Test Loss: 1.118139303206282 \n",
      "Epoch: 288 | MAE Train Loss: 1.2525926310987077 | MAE Test Loss: 1.1161951499464584 \n",
      "Epoch: 289 | MAE Train Loss: 1.250547708662643 | MAE Test Loss: 1.1142583868763924 \n",
      "Epoch: 290 | MAE Train Loss: 1.248509844551235 | MAE Test Loss: 1.112328976685681 \n",
      "Epoch: 291 | MAE Train Loss: 1.246479023870704 | MAE Test Loss: 1.1104069246288684 \n",
      "Epoch: 292 | MAE Train Loss: 1.244455234939443 | MAE Test Loss: 1.1084921601998148 \n",
      "Epoch: 293 | MAE Train Loss: 1.2424384301574798 | MAE Test Loss: 1.1065846458997428 \n",
      "Epoch: 294 | MAE Train Loss: 1.2404285548229388 | MAE Test Loss: 1.1046843663125414 \n",
      "Epoch: 295 | MAE Train Loss: 1.2384255968576388 | MAE Test Loss: 1.1027912614293063 \n",
      "Epoch: 296 | MAE Train Loss: 1.2364295183353262 | MAE Test Loss: 1.100905339760184 \n",
      "Epoch: 297 | MAE Train Loss: 1.2344403121116732 | MAE Test Loss: 1.0990265534326458 \n",
      "Epoch: 298 | MAE Train Loss: 1.232457932331204 | MAE Test Loss: 1.097154852658089 \n",
      "Epoch: 299 | MAE Train Loss: 1.2304823519871135 | MAE Test Loss: 1.095290224263534 \n",
      "Epoch: 300 | MAE Train Loss: 1.2285135408294434 | MAE Test Loss: 1.0934326513742922 \n",
      "Epoch: 301 | MAE Train Loss: 1.226551483485469 | MAE Test Loss: 1.091582066711572 \n",
      "Epoch: 302 | MAE Train Loss: 1.2245961154698377 | MAE Test Loss: 1.089738454703224 \n",
      "Epoch: 303 | MAE Train Loss: 1.2226474462985348 | MAE Test Loss: 1.08790180305093 \n",
      "Epoch: 304 | MAE Train Loss: 1.2207054461015434 | MAE Test Loss: 1.0860720532482493 \n",
      "Epoch: 305 | MAE Train Loss: 1.2187700575819869 | MAE Test Loss: 1.0842491809778927 \n",
      "Epoch: 306 | MAE Train Loss: 1.2168412593988442 | MAE Test Loss: 1.082433149829353 \n",
      "Epoch: 307 | MAE Train Loss: 1.2149190176306903 | MAE Test Loss: 1.0806239569555944 \n",
      "Epoch: 308 | MAE Train Loss: 1.2130033338577964 | MAE Test Loss: 1.0788215356178834 \n",
      "Epoch: 309 | MAE Train Loss: 1.211094141522731 | MAE Test Loss: 1.0770258694588448 \n",
      "Epoch: 310 | MAE Train Loss: 1.2091914503967218 | MAE Test Loss: 1.075236935915299 \n",
      "Epoch: 311 | MAE Train Loss: 1.2072952189724855 | MAE Test Loss: 1.0734547048523988 \n",
      "Epoch: 312 | MAE Train Loss: 1.205405399154594 | MAE Test Loss: 1.071679112134611 \n",
      "Epoch: 313 | MAE Train Loss: 1.2035219706835802 | MAE Test Loss: 1.0699101952627246 \n",
      "Epoch: 314 | MAE Train Loss: 1.2016449319021234 | MAE Test Loss: 1.068147883980993 \n",
      "Epoch: 315 | MAE Train Loss: 1.1997742355170822 | MAE Test Loss: 1.0663921430720076 \n",
      "Epoch: 316 | MAE Train Loss: 1.1979098488798587 | MAE Test Loss: 1.0646429494126035 \n",
      "Epoch: 317 | MAE Train Loss: 1.196051751831813 | MAE Test Loss: 1.0629002799382694 \n",
      "Epoch: 318 | MAE Train Loss: 1.1941999242664356 | MAE Test Loss: 1.0611641116431478 \n",
      "Epoch: 319 | MAE Train Loss: 1.192354346129348 | MAE Test Loss: 1.0594344215800349 \n",
      "Epoch: 320 | MAE Train Loss: 1.1905149974183036 | MAE Test Loss: 1.0577111499435419 \n",
      "Epoch: 321 | MAE Train Loss: 1.1886817970882955 | MAE Test Loss: 1.0559943077863223 \n",
      "Epoch: 322 | MAE Train Loss: 1.1868547813956456 | MAE Test Loss: 1.0542838240694552 \n",
      "Epoch: 323 | MAE Train Loss: 1.1850338827914362 | MAE Test Loss: 1.0525796843984174 \n",
      "Epoch: 324 | MAE Train Loss: 1.1832190914373553 | MAE Test Loss: 1.0508818868600474 \n",
      "Epoch: 325 | MAE Train Loss: 1.1814104042968183 | MAE Test Loss: 1.0491903730340029 \n",
      "Epoch: 326 | MAE Train Loss: 1.1796077440578234 | MAE Test Loss: 1.0475051170376601 \n",
      "Epoch: 327 | MAE Train Loss: 1.1778111114481198 | MAE Test Loss: 1.0458261296143045 \n",
      "Epoch: 328 | MAE Train Loss: 1.1760204935675973 | MAE Test Loss: 1.0441533445105668 \n",
      "Epoch: 329 | MAE Train Loss: 1.1742358531758645 | MAE Test Loss: 1.0424867089095557 \n",
      "Epoch: 330 | MAE Train Loss: 1.172457138056013 | MAE Test Loss: 1.0408262770337842 \n",
      "Epoch: 331 | MAE Train Loss: 1.1706843809486938 | MAE Test Loss: 1.0391719517249762 \n",
      "Epoch: 332 | MAE Train Loss: 1.1689175116833892 | MAE Test Loss: 1.0375237472852414 \n",
      "Epoch: 333 | MAE Train Loss: 1.167156525101756 | MAE Test Loss: 1.0358816351269613 \n",
      "Epoch: 334 | MAE Train Loss: 1.1654014152092593 | MAE Test Loss: 1.0342455709555047 \n",
      "Epoch: 335 | MAE Train Loss: 1.1636521180332495 | MAE Test Loss: 1.0326155052229862 \n",
      "Epoch: 336 | MAE Train Loss: 1.1619086112991757 | MAE Test Loss: 1.0309914740636865 \n",
      "Epoch: 337 | MAE Train Loss: 1.1601709067803396 | MAE Test Loss: 1.029373399443935 \n",
      "Epoch: 338 | MAE Train Loss: 1.1584389563269648 | MAE Test Loss: 1.02776127791803 \n",
      "Epoch: 339 | MAE Train Loss: 1.156712734146056 | MAE Test Loss: 1.0261550597622011 \n",
      "Epoch: 340 | MAE Train Loss: 1.1549922183173311 | MAE Test Loss: 1.0245547635539944 \n",
      "Epoch: 341 | MAE Train Loss: 1.1532774001001436 | MAE Test Loss: 1.022960329518418 \n",
      "Epoch: 342 | MAE Train Loss: 1.1515682525253046 | MAE Test Loss: 1.0213717369369184 \n",
      "Epoch: 343 | MAE Train Loss: 1.149864729544727 | MAE Test Loss: 1.0197889543305367 \n",
      "Epoch: 344 | MAE Train Loss: 1.1481668021150018 | MAE Test Loss: 1.018212001506305 \n",
      "Epoch: 345 | MAE Train Loss: 1.146474490194621 | MAE Test Loss: 1.0166408004224263 \n",
      "Epoch: 346 | MAE Train Loss: 1.1447877465549838 | MAE Test Loss: 1.0150753380051776 \n",
      "Epoch: 347 | MAE Train Loss: 1.1431065348141651 | MAE Test Loss: 1.0135155945408039 \n",
      "Epoch: 348 | MAE Train Loss: 1.141430837757122 | MAE Test Loss: 1.0119615503643153 \n",
      "Epoch: 349 | MAE Train Loss: 1.1397606382123948 | MAE Test Loss: 1.0104131555970883 \n",
      "Epoch: 350 | MAE Train Loss: 1.1380959153370742 | MAE Test Loss: 1.0088704429438398 \n",
      "Epoch: 351 | MAE Train Loss: 1.136436672871461 | MAE Test Loss: 1.0073333422082411 \n",
      "Epoch: 352 | MAE Train Loss: 1.1347828454528774 | MAE Test Loss: 1.0058018400755835 \n",
      "Epoch: 353 | MAE Train Loss: 1.1331344251176052 | MAE Test Loss: 1.0042759114485587 \n",
      "Epoch: 354 | MAE Train Loss: 1.1314913863332834 | MAE Test Loss: 1.0027555259821068 \n",
      "Epoch: 355 | MAE Train Loss: 1.129853701285739 | MAE Test Loss: 1.0012406450732747 \n",
      "Epoch: 356 | MAE Train Loss: 1.1282213609709564 | MAE Test Loss: 0.9997312691914904 \n",
      "Epoch: 357 | MAE Train Loss: 1.126594341356094 | MAE Test Loss: 0.9982273682305788 \n",
      "Epoch: 358 | MAE Train Loss: 1.1249726149216506 | MAE Test Loss: 0.9967289627459326 \n",
      "Epoch: 359 | MAE Train Loss: 1.1233562022404826 | MAE Test Loss: 0.9952359946142891 \n",
      "Epoch: 360 | MAE Train Loss: 1.1217450500684794 | MAE Test Loss: 0.9937484377358838 \n",
      "Epoch: 361 | MAE Train Loss: 1.1201391041625768 | MAE Test Loss: 0.9922662609658947 \n",
      "Epoch: 362 | MAE Train Loss: 1.118538393982521 | MAE Test Loss: 0.9907894588571929 \n",
      "Epoch: 363 | MAE Train Loss: 1.116942858067898 | MAE Test Loss: 0.989317961187243 \n",
      "Epoch: 364 | MAE Train Loss: 1.1153524845664502 | MAE Test Loss: 0.9878518019986939 \n",
      "Epoch: 365 | MAE Train Loss: 1.1137672537110217 | MAE Test Loss: 0.9863909315481744 \n",
      "Epoch: 366 | MAE Train Loss: 1.112187146317296 | MAE Test Loss: 0.98493533176771 \n",
      "Epoch: 367 | MAE Train Loss: 1.1106121467185652 | MAE Test Loss: 0.9834849846335013 \n",
      "Epoch: 368 | MAE Train Loss: 1.1090422392871355 | MAE Test Loss: 0.9820398611074292 \n",
      "Epoch: 369 | MAE Train Loss: 1.1074773976636187 | MAE Test Loss: 0.9805999433000483 \n",
      "Epoch: 370 | MAE Train Loss: 1.105917606379254 | MAE Test Loss: 0.9791651977417117 \n",
      "Epoch: 371 | MAE Train Loss: 1.1043628310180627 | MAE Test Loss: 0.9777356224211765 \n",
      "Epoch: 372 | MAE Train Loss: 1.1028130752831184 | MAE Test Loss: 0.9763111558403275 \n",
      "Epoch: 373 | MAE Train Loss: 1.1012682505258895 | MAE Test Loss: 0.9748918135262981 \n",
      "Epoch: 374 | MAE Train Loss: 1.0997284043407887 | MAE Test Loss: 0.9734775625594864 \n",
      "Epoch: 375 | MAE Train Loss: 1.0981935027615752 | MAE Test Loss: 0.9720684076169167 \n",
      "Epoch: 376 | MAE Train Loss: 1.096663523460448 | MAE Test Loss: 0.97066429868478 \n",
      "Epoch: 377 | MAE Train Loss: 1.0951384482677404 | MAE Test Loss: 0.9692652185542 \n",
      "Epoch: 378 | MAE Train Loss: 1.0936182623382444 | MAE Test Loss: 0.9678711500581336 \n",
      "Epoch: 379 | MAE Train Loss: 1.0921029508633768 | MAE Test Loss: 0.9664820985481922 \n",
      "Epoch: 380 | MAE Train Loss: 1.0905924917792194 | MAE Test Loss: 0.9650979602896769 \n",
      "Epoch: 381 | MAE Train Loss: 1.089086812648585 | MAE Test Loss: 0.9637187947344547 \n",
      "Epoch: 382 | MAE Train Loss: 1.0875859463135034 | MAE Test Loss: 0.9623445624467063 \n",
      "Epoch: 383 | MAE Train Loss: 1.0860898854800438 | MAE Test Loss: 0.9609752319535702 \n",
      "Epoch: 384 | MAE Train Loss: 1.0845985973123062 | MAE Test Loss: 0.9596107988502591 \n",
      "Epoch: 385 | MAE Train Loss: 1.0831120498601692 | MAE Test Loss: 0.9582511851762611 \n",
      "Epoch: 386 | MAE Train Loss: 1.081630200206494 | MAE Test Loss: 0.9568964506540452 \n",
      "Epoch: 387 | MAE Train Loss: 1.0801530810520539 | MAE Test Loss: 0.9555465412875279 \n",
      "Epoch: 388 | MAE Train Loss: 1.0786806672352058 | MAE Test Loss: 0.9542014533840326 \n",
      "Epoch: 389 | MAE Train Loss: 1.0772129271607356 | MAE Test Loss: 0.9528611088387479 \n",
      "Epoch: 390 | MAE Train Loss: 1.0757498182295957 | MAE Test Loss: 0.9515255396086673 \n",
      "Epoch: 391 | MAE Train Loss: 1.0742913372065146 | MAE Test Loss: 0.9501947200698069 \n",
      "Epoch: 392 | MAE Train Loss: 1.072837495361951 | MAE Test Loss: 0.9488686088396128 \n",
      "Epoch: 393 | MAE Train Loss: 1.0713882257581138 | MAE Test Loss: 0.9475472254897769 \n",
      "Epoch: 394 | MAE Train Loss: 1.0699435427270343 | MAE Test Loss: 0.9462305195783418 \n",
      "Epoch: 395 | MAE Train Loss: 1.0685034296473759 | MAE Test Loss: 0.9449184641724105 \n",
      "Epoch: 396 | MAE Train Loss: 1.0670678377781087 | MAE Test Loss: 0.9436110547101995 \n",
      "Epoch: 397 | MAE Train Loss: 1.0656367887357434 | MAE Test Loss: 0.9423082892965992 \n",
      "Epoch: 398 | MAE Train Loss: 1.0642102518394498 | MAE Test Loss: 0.9410101143503624 \n",
      "Epoch: 399 | MAE Train Loss: 1.0627881782105153 | MAE Test Loss: 0.9397165281046872 \n",
      "Epoch: 400 | MAE Train Loss: 1.0613705971443634 | MAE Test Loss: 0.9384274910164498 \n",
      "Epoch: 401 | MAE Train Loss: 1.0599574428190701 | MAE Test Loss: 0.937143036694042 \n",
      "Epoch: 402 | MAE Train Loss: 1.0585487474165904 | MAE Test Loss: 0.9358631142938926 \n",
      "Epoch: 403 | MAE Train Loss: 1.0571444948472468 | MAE Test Loss: 0.9345876824634226 \n",
      "Epoch: 404 | MAE Train Loss: 1.0557446122888665 | MAE Test Loss: 0.9333167668280631 \n",
      "Epoch: 405 | MAE Train Loss: 1.054349129484622 | MAE Test Loss: 0.9320503120489885 \n",
      "Epoch: 406 | MAE Train Loss: 1.0529579908481546 | MAE Test Loss: 0.9307883181418699 \n",
      "Epoch: 407 | MAE Train Loss: 1.0515712331085694 | MAE Test Loss: 0.9295307447922563 \n",
      "Epoch: 408 | MAE Train Loss: 1.0501887840166502 | MAE Test Loss: 0.9282776153455261 \n",
      "Epoch: 409 | MAE Train Loss: 1.0488106660251575 | MAE Test Loss: 0.9270288667451687 \n",
      "Epoch: 410 | MAE Train Loss: 1.0474368215442946 | MAE Test Loss: 0.9257844949650265 \n",
      "Epoch: 411 | MAE Train Loss: 1.0460672477381432 | MAE Test Loss: 0.9245445129123822 \n",
      "Epoch: 412 | MAE Train Loss: 1.0447019573567893 | MAE Test Loss: 0.92330885779296 \n",
      "Epoch: 413 | MAE Train Loss: 1.0433408931461283 | MAE Test Loss: 0.9220775372430898 \n",
      "Epoch: 414 | MAE Train Loss: 1.041984094305505 | MAE Test Loss: 0.9208505146530424 \n",
      "Epoch: 415 | MAE Train Loss: 1.040631496754057 | MAE Test Loss: 0.9196278118388227 \n",
      "Epoch: 416 | MAE Train Loss: 1.0392831159944436 | MAE Test Loss: 0.9184093785111976 \n",
      "Epoch: 417 | MAE Train Loss: 1.0379389118768543 | MAE Test Loss: 0.9171951781589748 \n",
      "Epoch: 418 | MAE Train Loss: 1.0365988460218825 | MAE Test Loss: 0.9159852567291112 \n",
      "Epoch: 419 | MAE Train Loss: 1.035262967227185 | MAE Test Loss: 0.9147795671099995 \n",
      "Epoch: 420 | MAE Train Loss: 1.0339311955246233 | MAE Test Loss: 0.9135780417748732 \n",
      "Epoch: 421 | MAE Train Loss: 1.032603532821947 | MAE Test Loss: 0.9123807312620581 \n",
      "Epoch: 422 | MAE Train Loss: 1.031279994919182 | MAE Test Loss: 0.9111876113323898 \n",
      "Epoch: 423 | MAE Train Loss: 1.0299605282430047 | MAE Test Loss: 0.9099986309204333 \n",
      "Epoch: 424 | MAE Train Loss: 1.0286451184199965 | MAE Test Loss: 0.9088137763292228 \n",
      "Epoch: 425 | MAE Train Loss: 1.0273337536617277 | MAE Test Loss: 0.9076330713699411 \n",
      "Epoch: 426 | MAE Train Loss: 1.026026456753271 | MAE Test Loss: 0.9064564648777964 \n",
      "Epoch: 427 | MAE Train Loss: 1.0247231813477016 | MAE Test Loss: 0.9052839320850584 \n",
      "Epoch: 428 | MAE Train Loss: 1.0234238996647904 | MAE Test Loss: 0.9041154604848961 \n",
      "Epoch: 429 | MAE Train Loss: 1.0221286068945739 | MAE Test Loss: 0.9029510255541073 \n",
      "Epoch: 430 | MAE Train Loss: 1.020837275473377 | MAE Test Loss: 0.9017906249878911 \n",
      "Epoch: 431 | MAE Train Loss: 1.0195499099209644 | MAE Test Loss: 0.9006342717806414 \n",
      "Epoch: 432 | MAE Train Loss: 1.0182665171622998 | MAE Test Loss: 0.8994819436122082 \n",
      "Epoch: 433 | MAE Train Loss: 1.0169870446045282 | MAE Test Loss: 0.8983335890983402 \n",
      "Epoch: 434 | MAE Train Loss: 1.0157114785616936 | MAE Test Loss: 0.8971891951429267 \n",
      "Epoch: 435 | MAE Train Loss: 1.014439807792607 | MAE Test Loss: 0.8960487486801931 \n",
      "Epoch: 436 | MAE Train Loss: 1.013172021082458 | MAE Test Loss: 0.8949122366747009 \n",
      "Epoch: 437 | MAE Train Loss: 1.011908107242813 | MAE Test Loss: 0.8937796647186751 \n",
      "Epoch: 438 | MAE Train Loss: 1.0106480392187573 | MAE Test Loss: 0.8926510014644947 \n",
      "Epoch: 439 | MAE Train Loss: 1.0093918218262417 | MAE Test Loss: 0.8915262135415437 \n",
      "Epoch: 440 | MAE Train Loss: 1.008139419377191 | MAE Test Loss: 0.8904053275579475 \n",
      "Epoch: 441 | MAE Train Loss: 1.006890829696761 | MAE Test Loss: 0.8892883018149256 \n",
      "Epoch: 442 | MAE Train Loss: 1.00564604856695 | MAE Test Loss: 0.8881751033103699 \n",
      "Epoch: 443 | MAE Train Loss: 1.0044050406428087 | MAE Test Loss: 0.8870657488441609 \n",
      "Epoch: 444 | MAE Train Loss: 1.0031677884466326 | MAE Test Loss: 0.8859602167644478 \n",
      "Epoch: 445 | MAE Train Loss: 1.0019343122564985 | MAE Test Loss: 0.8848584744012993 \n",
      "Epoch: 446 | MAE Train Loss: 1.0007045770041139 | MAE Test Loss: 0.8837605288910831 \n",
      "Epoch: 447 | MAE Train Loss: 0.9994785563918261 | MAE Test Loss: 0.8826663484216836 \n",
      "Epoch: 448 | MAE Train Loss: 0.9982562554658017 | MAE Test Loss: 0.881575950432385 \n",
      "Epoch: 449 | MAE Train Loss: 0.9970376569124102 | MAE Test Loss: 0.8804892728666115 \n",
      "Epoch: 450 | MAE Train Loss: 0.9958227391373617 | MAE Test Loss: 0.8794063134447854 \n",
      "Epoch: 451 | MAE Train Loss: 0.9946115005322236 | MAE Test Loss: 0.8783270438068307 \n",
      "Epoch: 452 | MAE Train Loss: 0.9934038753188918 | MAE Test Loss: 0.877251468153764 \n",
      "Epoch: 453 | MAE Train Loss: 0.9921999085088284 | MAE Test Loss: 0.8761796045885614 \n",
      "Epoch: 454 | MAE Train Loss: 0.9909995830113292 | MAE Test Loss: 0.8751113815884647 \n",
      "Epoch: 455 | MAE Train Loss: 0.9898028627488078 | MAE Test Loss: 0.8740468468637805 \n",
      "Epoch: 456 | MAE Train Loss: 0.9886097632055224 | MAE Test Loss: 0.8729858923141386 \n",
      "Epoch: 457 | MAE Train Loss: 0.9874202150168611 | MAE Test Loss: 0.8719285734812013 \n",
      "Epoch: 458 | MAE Train Loss: 0.9862342350297112 | MAE Test Loss: 0.870874898672319 \n",
      "Epoch: 459 | MAE Train Loss: 0.9850518305555319 | MAE Test Loss: 0.8698248144793913 \n",
      "Epoch: 460 | MAE Train Loss: 0.9838729953707205 | MAE Test Loss: 0.8687782945931766 \n",
      "Epoch: 461 | MAE Train Loss: 0.9826976648062065 | MAE Test Loss: 0.8677353330037656 \n",
      "Epoch: 462 | MAE Train Loss: 0.981525868910305 | MAE Test Loss: 0.8666959207279806 \n",
      "Epoch: 463 | MAE Train Loss: 0.9803575594589811 | MAE Test Loss: 0.8656600072769521 \n",
      "Epoch: 464 | MAE Train Loss: 0.9791927316901333 | MAE Test Loss: 0.8646276488570012 \n",
      "Epoch: 465 | MAE Train Loss: 0.9780314024287495 | MAE Test Loss: 0.8635988144071101 \n",
      "Epoch: 466 | MAE Train Loss: 0.9768735447241162 | MAE Test Loss: 0.8625734359772718 \n",
      "Epoch: 467 | MAE Train Loss: 0.9757191249731779 | MAE Test Loss: 0.8615515589589418 \n",
      "Epoch: 468 | MAE Train Loss: 0.9745681573857359 | MAE Test Loss: 0.8605331471907667 \n",
      "Epoch: 469 | MAE Train Loss: 0.9734206019217989 | MAE Test Loss: 0.8595182145660779 \n",
      "Epoch: 470 | MAE Train Loss: 0.9722764793316128 | MAE Test Loss: 0.858506673549339 \n",
      "Epoch: 471 | MAE Train Loss: 0.9711357393425789 | MAE Test Loss: 0.857498581255612 \n",
      "Epoch: 472 | MAE Train Loss: 0.9699983989994294 | MAE Test Loss: 0.8564939347982874 \n",
      "Epoch: 473 | MAE Train Loss: 0.9688644628696206 | MAE Test Loss: 0.8554926467163059 \n",
      "Epoch: 474 | MAE Train Loss: 0.967733880952127 | MAE Test Loss: 0.8544947187492339 \n",
      "Epoch: 475 | MAE Train Loss: 0.9666066206658079 | MAE Test Loss: 0.8535001957896245 \n",
      "Epoch: 476 | MAE Train Loss: 0.9654827223525256 | MAE Test Loss: 0.8525090309487346 \n",
      "Epoch: 477 | MAE Train Loss: 0.964362143801394 | MAE Test Loss: 0.8515211897411777 \n",
      "Epoch: 478 | MAE Train Loss: 0.9632448845548727 | MAE Test Loss: 0.8505366851666059 \n",
      "Epoch: 479 | MAE Train Loss: 0.9621309266280722 | MAE Test Loss: 0.8495555230981131 \n",
      "Epoch: 480 | MAE Train Loss: 0.9610202769715078 | MAE Test Loss: 0.8485776493965483 \n",
      "Epoch: 481 | MAE Train Loss: 0.9599128799971183 | MAE Test Loss: 0.8476030611364049 \n",
      "Epoch: 482 | MAE Train Loss: 0.958808740419766 | MAE Test Loss: 0.8466317762885321 \n",
      "Epoch: 483 | MAE Train Loss: 0.957707868180921 | MAE Test Loss: 0.8456637558909931 \n",
      "Epoch: 484 | MAE Train Loss: 0.9566102352664207 | MAE Test Loss: 0.8446989489274073 \n",
      "Epoch: 485 | MAE Train Loss: 0.9555158255460329 | MAE Test Loss: 0.8437374140143451 \n",
      "Epoch: 486 | MAE Train Loss: 0.9544246562324459 | MAE Test Loss: 0.8427791050231808 \n",
      "Epoch: 487 | MAE Train Loss: 0.9533366858899341 | MAE Test Loss: 0.841823999488408 \n",
      "Epoch: 488 | MAE Train Loss: 0.9522519028588242 | MAE Test Loss: 0.8408720993354715 \n",
      "Epoch: 489 | MAE Train Loss: 0.9511703011501105 | MAE Test Loss: 0.8399234108238083 \n",
      "Epoch: 490 | MAE Train Loss: 0.9500918881888017 | MAE Test Loss: 0.8389779073174113 \n",
      "Epoch: 491 | MAE Train Loss: 0.9490166389968211 | MAE Test Loss: 0.8380355526429233 \n",
      "Epoch: 492 | MAE Train Loss: 0.9479445154709527 | MAE Test Loss: 0.8370963436645763 \n",
      "Epoch: 493 | MAE Train Loss: 0.9468755223214144 | MAE Test Loss: 0.8361602703345064 \n",
      "Epoch: 494 | MAE Train Loss: 0.9458096509443358 | MAE Test Loss: 0.8352273515773547 \n",
      "Epoch: 495 | MAE Train Loss: 0.9447469118492828 | MAE Test Loss: 0.8342975485101188 \n",
      "Epoch: 496 | MAE Train Loss: 0.9436872773919636 | MAE Test Loss: 0.8333708446583963 \n",
      "Epoch: 497 | MAE Train Loss: 0.9426307259774749 | MAE Test Loss: 0.8324472366804493 \n",
      "Epoch: 498 | MAE Train Loss: 0.9415772622077743 | MAE Test Loss: 0.8315267082707262 \n",
      "Epoch: 499 | MAE Train Loss: 0.9405268646066375 | MAE Test Loss: 0.830609236921776 \n",
      "Epoch: 500 | MAE Train Loss: 0.9394795219138325 | MAE Test Loss: 0.8296948256141079 \n",
      "Epoch: 501 | MAE Train Loss: 0.9384352286604656 | MAE Test Loss: 0.8287834646617522 \n",
      "Epoch: 502 | MAE Train Loss: 0.9373939765151157 | MAE Test Loss: 0.8278751602425117 \n",
      "Epoch: 503 | MAE Train Loss: 0.936355735153322 | MAE Test Loss: 0.8269698680616733 \n",
      "Epoch: 504 | MAE Train Loss: 0.9353205026944401 | MAE Test Loss: 0.8260675785458215 \n",
      "Epoch: 505 | MAE Train Loss: 0.9342882709655385 | MAE Test Loss: 0.8251683011272266 \n",
      "Epoch: 506 | MAE Train Loss: 0.9332590474242319 | MAE Test Loss: 0.8242720072568398 \n",
      "Epoch: 507 | MAE Train Loss: 0.9322328082528106 | MAE Test Loss: 0.8233786874241117 \n",
      "Epoch: 508 | MAE Train Loss: 0.9312095453326178 | MAE Test Loss: 0.8224883263876277 \n",
      "Epoch: 509 | MAE Train Loss: 0.9301892379917961 | MAE Test Loss: 0.8216009016319724 \n",
      "Epoch: 510 | MAE Train Loss: 0.9291718753203206 | MAE Test Loss: 0.8207163818995238 \n",
      "Epoch: 511 | MAE Train Loss: 0.9281574206032035 | MAE Test Loss: 0.8198348343590525 \n",
      "Epoch: 512 | MAE Train Loss: 0.9271459038959119 | MAE Test Loss: 0.8189561897343504 \n",
      "Epoch: 513 | MAE Train Loss: 0.9261372956251097 | MAE Test Loss: 0.8180804631720375 \n",
      "Epoch: 514 | MAE Train Loss: 0.9251316157144658 | MAE Test Loss: 0.8172076210758078 \n",
      "Epoch: 515 | MAE Train Loss: 0.9241288285222538 | MAE Test Loss: 0.8163376741592796 \n",
      "Epoch: 516 | MAE Train Loss: 0.9231289038172117 | MAE Test Loss: 0.8154705936843488 \n",
      "Epoch: 517 | MAE Train Loss: 0.9221318563366578 | MAE Test Loss: 0.8146063705708322 \n",
      "Epoch: 518 | MAE Train Loss: 0.9211376783528928 | MAE Test Loss: 0.8137449957581779 \n",
      "Epoch: 519 | MAE Train Loss: 0.9201463621550147 | MAE Test Loss: 0.8128864804250657 \n",
      "Epoch: 520 | MAE Train Loss: 0.9191578776035552 | MAE Test Loss: 0.8120307955233713 \n",
      "Epoch: 521 | MAE Train Loss: 0.9181722394790244 | MAE Test Loss: 0.8111779133178896 \n",
      "Epoch: 522 | MAE Train Loss: 0.9171894250408472 | MAE Test Loss: 0.8103278453801114 \n",
      "Epoch: 523 | MAE Train Loss: 0.9162094043162913 | MAE Test Loss: 0.8094805811782619 \n",
      "Epoch: 524 | MAE Train Loss: 0.9152322072294687 | MAE Test Loss: 0.8086361069736459 \n",
      "Epoch: 525 | MAE Train Loss: 0.9142578142140501 | MAE Test Loss: 0.8077944022041781 \n",
      "Epoch: 526 | MAE Train Loss: 0.9132861773982895 | MAE Test Loss: 0.8069554700872968 \n",
      "Epoch: 527 | MAE Train Loss: 0.9123173297745206 | MAE Test Loss: 0.8061192532175667 \n",
      "Epoch: 528 | MAE Train Loss: 0.9113512296964892 | MAE Test Loss: 0.8052858286605211 \n",
      "Epoch: 529 | MAE Train Loss: 0.9103878978614386 | MAE Test Loss: 0.8044551020841155 \n",
      "Epoch: 530 | MAE Train Loss: 0.9094272988418977 | MAE Test Loss: 0.8036271321903881 \n",
      "Epoch: 531 | MAE Train Loss: 0.9084694385350469 | MAE Test Loss: 0.8028018573125566 \n",
      "Epoch: 532 | MAE Train Loss: 0.9075142995079345 | MAE Test Loss: 0.8019792848766454 \n",
      "Epoch: 533 | MAE Train Loss: 0.9065618907515904 | MAE Test Loss: 0.8011594093288792 \n",
      "Epoch: 534 | MAE Train Loss: 0.9056121679580513 | MAE Test Loss: 0.8003422193460985 \n",
      "Epoch: 535 | MAE Train Loss: 0.9046651610108721 | MAE Test Loss: 0.799527709628619 \n",
      "Epoch: 536 | MAE Train Loss: 0.9037208257097731 | MAE Test Loss: 0.7987158501567944 \n",
      "Epoch: 537 | MAE Train Loss: 0.9027791772869914 | MAE Test Loss: 0.7979066544263371 \n",
      "Epoch: 538 | MAE Train Loss: 0.901840186317014 | MAE Test Loss: 0.7971000763771272 \n",
      "Epoch: 539 | MAE Train Loss: 0.9009038517894415 | MAE Test Loss: 0.7962961799870046 \n",
      "Epoch: 540 | MAE Train Loss: 0.8999701913550673 | MAE Test Loss: 0.7954948623401398 \n",
      "Epoch: 541 | MAE Train Loss: 0.8990391473413055 | MAE Test Loss: 0.7946961757367983 \n",
      "Epoch: 542 | MAE Train Loss: 0.8981107326515059 | MAE Test Loss: 0.7939000859387626 \n",
      "Epoch: 543 | MAE Train Loss: 0.8971849512734609 | MAE Test Loss: 0.79310659256597 \n",
      "Epoch: 544 | MAE Train Loss: 0.8962617708886949 | MAE Test Loss: 0.7923156639764871 \n",
      "Epoch: 545 | MAE Train Loss: 0.8953411937735527 | MAE Test Loss: 0.791527315760576 \n",
      "Epoch: 546 | MAE Train Loss: 0.894423203896116 | MAE Test Loss: 0.7907415320161979 \n",
      "Epoch: 547 | MAE Train Loss: 0.8935078198089268 | MAE Test Loss: 0.7899583091580725 \n",
      "Epoch: 548 | MAE Train Loss: 0.8925949981488521 | MAE Test Loss: 0.7891776007015026 \n",
      "Epoch: 549 | MAE Train Loss: 0.8916847381898065 | MAE Test Loss: 0.7883994408309307 \n",
      "Epoch: 550 | MAE Train Loss: 0.8907770382617395 | MAE Test Loss: 0.7876238183405404 \n",
      "Epoch: 551 | MAE Train Loss: 0.8898718805637376 | MAE Test Loss: 0.7868507205803349 \n",
      "Epoch: 552 | MAE Train Loss: 0.8889692947860126 | MAE Test Loss: 0.7860801230830882 \n",
      "Epoch: 553 | MAE Train Loss: 0.8880692127683385 | MAE Test Loss: 0.785312000683529 \n",
      "Epoch: 554 | MAE Train Loss: 0.8871716590119607 | MAE Test Loss: 0.7845463846788265 \n",
      "Epoch: 555 | MAE Train Loss: 0.8862766209094272 | MAE Test Loss: 0.7837832491629845 \n",
      "Epoch: 556 | MAE Train Loss: 0.8853840778048037 | MAE Test Loss: 0.7830225865429001 \n",
      "Epoch: 557 | MAE Train Loss: 0.8844940231801599 | MAE Test Loss: 0.7822643651164072 \n",
      "Epoch: 558 | MAE Train Loss: 0.883606459689425 | MAE Test Loss: 0.7815086013742908 \n",
      "Epoch: 559 | MAE Train Loss: 0.8827213716836884 | MAE Test Loss: 0.7807552849544697 \n",
      "Epoch: 560 | MAE Train Loss: 0.8818387419979647 | MAE Test Loss: 0.7800043897812301 \n",
      "Epoch: 561 | MAE Train Loss: 0.8809585947381449 | MAE Test Loss: 0.7792559269227621 \n",
      "Epoch: 562 | MAE Train Loss: 0.880080892954933 | MAE Test Loss: 0.7785098916969434 \n",
      "Epoch: 563 | MAE Train Loss: 0.8792056409005168 | MAE Test Loss: 0.777766234123178 \n",
      "Epoch: 564 | MAE Train Loss: 0.8783328275031328 | MAE Test Loss: 0.7770249529627407 \n",
      "Epoch: 565 | MAE Train Loss: 0.8774624105589094 | MAE Test Loss: 0.776286074655129 \n",
      "Epoch: 566 | MAE Train Loss: 0.8765944138234486 | MAE Test Loss: 0.7755495738041347 \n",
      "Epoch: 567 | MAE Train Loss: 0.8757288172944524 | MAE Test Loss: 0.7748154234755805 \n",
      "Epoch: 568 | MAE Train Loss: 0.8748656447742379 | MAE Test Loss: 0.7740836539836293 \n",
      "Epoch: 569 | MAE Train Loss: 0.8740048736403777 | MAE Test Loss: 0.7733542580985423 \n",
      "Epoch: 570 | MAE Train Loss: 0.8731464976531994 | MAE Test Loss: 0.7726271747894511 \n",
      "Epoch: 571 | MAE Train Loss: 0.8722904568635637 | MAE Test Loss: 0.7719024508555347 \n",
      "Epoch: 572 | MAE Train Loss: 0.8714367989655576 | MAE Test Loss: 0.7711800204778382 \n",
      "Epoch: 573 | MAE Train Loss: 0.8705855100484376 | MAE Test Loss: 0.7704599169550381 \n",
      "Epoch: 574 | MAE Train Loss: 0.8697365781486583 | MAE Test Loss: 0.7697421157662235 \n",
      "Epoch: 575 | MAE Train Loss: 0.8688899708227048 | MAE Test Loss: 0.7690266273417515 \n",
      "Epoch: 576 | MAE Train Loss: 0.8680457084157794 | MAE Test Loss: 0.7683134445908554 \n",
      "Epoch: 577 | MAE Train Loss: 0.8672037848606682 | MAE Test Loss: 0.767602525247265 \n",
      "Epoch: 578 | MAE Train Loss: 0.8663641544424646 | MAE Test Loss: 0.7668938565611605 \n",
      "Epoch: 579 | MAE Train Loss: 0.8655268567186762 | MAE Test Loss: 0.766187470617987 \n",
      "Epoch: 580 | MAE Train Loss: 0.8646918698271542 | MAE Test Loss: 0.765483327288105 \n",
      "Epoch: 581 | MAE Train Loss: 0.8638591583580555 | MAE Test Loss: 0.7647814546649159 \n",
      "Epoch: 582 | MAE Train Loss: 0.8630287559029842 | MAE Test Loss: 0.7640818109459637 \n",
      "Epoch: 583 | MAE Train Loss: 0.8622006171281391 | MAE Test Loss: 0.7633844225501044 \n",
      "Epoch: 584 | MAE Train Loss: 0.8613747656446723 | MAE Test Loss: 0.7626892315977198 \n",
      "Epoch: 585 | MAE Train Loss: 0.8605511529808998 | MAE Test Loss: 0.7619962824164137 \n",
      "Epoch: 586 | MAE Train Loss: 0.8597298159968961 | MAE Test Loss: 0.7613055351181562 \n",
      "Epoch: 587 | MAE Train Loss: 0.8589107195390687 | MAE Test Loss: 0.7606169981938594 \n",
      "Epoch: 588 | MAE Train Loss: 0.8580938739904653 | MAE Test Loss: 0.7599306497359224 \n",
      "Epoch: 589 | MAE Train Loss: 0.8572792574890741 | MAE Test Loss: 0.7592464563847097 \n",
      "Epoch: 590 | MAE Train Loss: 0.8564668861805733 | MAE Test Loss: 0.7585644201468119 \n",
      "Epoch: 591 | MAE Train Loss: 0.8556567194004806 | MAE Test Loss: 0.7578845930995082 \n",
      "Epoch: 592 | MAE Train Loss: 0.8548487492290404 | MAE Test Loss: 0.7572069010523232 \n",
      "Epoch: 593 | MAE Train Loss: 0.8540430072469369 | MAE Test Loss: 0.7565313463499248 \n",
      "Epoch: 594 | MAE Train Loss: 0.8532394529326354 | MAE Test Loss: 0.755857963643671 \n",
      "Epoch: 595 | MAE Train Loss: 0.8524380654642607 | MAE Test Loss: 0.7551866959746312 \n",
      "Epoch: 596 | MAE Train Loss: 0.8516388893373119 | MAE Test Loss: 0.7545175874912159 \n",
      "Epoch: 597 | MAE Train Loss: 0.8508418690069304 | MAE Test Loss: 0.7538505904384032 \n",
      "Epoch: 598 | MAE Train Loss: 0.8500470141669627 | MAE Test Loss: 0.7531857302920764 \n",
      "Epoch: 599 | MAE Train Loss: 0.8492543388327413 | MAE Test Loss: 0.7525229678629174 \n",
      "Epoch: 600 | MAE Train Loss: 0.8484638085699592 | MAE Test Loss: 0.7518622974956206 \n",
      "Epoch: 601 | MAE Train Loss: 0.8476754272917834 | MAE Test Loss: 0.7512036943434139 \n",
      "Epoch: 602 | MAE Train Loss: 0.8468891674018959 | MAE Test Loss: 0.7505471698399685 \n",
      "Epoch: 603 | MAE Train Loss: 0.8461050363502993 | MAE Test Loss: 0.7498927176811722 \n",
      "Epoch: 604 | MAE Train Loss: 0.8453230287523132 | MAE Test Loss: 0.7492403465014458 \n",
      "Epoch: 605 | MAE Train Loss: 0.8445431552782428 | MAE Test Loss: 0.748590035064864 \n",
      "Epoch: 606 | MAE Train Loss: 0.8437653945112074 | MAE Test Loss: 0.7479417594260926 \n",
      "Epoch: 607 | MAE Train Loss: 0.8429897284283328 | MAE Test Loss: 0.7472955133621693 \n",
      "Epoch: 608 | MAE Train Loss: 0.8422161517629224 | MAE Test Loss: 0.746651308322525 \n",
      "Epoch: 609 | MAE Train Loss: 0.8414446718809199 | MAE Test Loss: 0.7460091204298036 \n",
      "Epoch: 610 | MAE Train Loss: 0.840675270865858 | MAE Test Loss: 0.7453689866480377 \n",
      "Epoch: 611 | MAE Train Loss: 0.83990792883362 | MAE Test Loss: 0.7447308578917525 \n",
      "Epoch: 612 | MAE Train Loss: 0.8391426552819301 | MAE Test Loss: 0.7440947456208478 \n",
      "Epoch: 613 | MAE Train Loss: 0.8383794575216174 | MAE Test Loss: 0.7434606259948991 \n",
      "Epoch: 614 | MAE Train Loss: 0.8376183088491004 | MAE Test Loss: 0.7428284930266553 \n",
      "Epoch: 615 | MAE Train Loss: 0.8368592130605718 | MAE Test Loss: 0.7421983406368173 \n",
      "Epoch: 616 | MAE Train Loss: 0.8361021561075901 | MAE Test Loss: 0.7415701889465286 \n",
      "Epoch: 617 | MAE Train Loss: 0.8353471237203969 | MAE Test Loss: 0.7409440059734291 \n",
      "Epoch: 618 | MAE Train Loss: 0.8345941200163717 | MAE Test Loss: 0.7403197535105726 \n",
      "Epoch: 619 | MAE Train Loss: 0.8338431115882156 | MAE Test Loss: 0.7396974577061298 \n",
      "Epoch: 620 | MAE Train Loss: 0.8330941305380627 | MAE Test Loss: 0.7390770952354994 \n",
      "Epoch: 621 | MAE Train Loss: 0.8323471506872583 | MAE Test Loss: 0.7384587218602071 \n",
      "Epoch: 622 | MAE Train Loss: 0.8316021738166472 | MAE Test Loss: 0.7378422877152284 \n",
      "Epoch: 623 | MAE Train Loss: 0.8308592004879269 | MAE Test Loss: 0.7372277693989043 \n",
      "Epoch: 624 | MAE Train Loss: 0.830118213428843 | MAE Test Loss: 0.7366151464651405 \n",
      "Epoch: 625 | MAE Train Loss: 0.8293791917798142 | MAE Test Loss: 0.736004472092162 \n",
      "Epoch: 626 | MAE Train Loss: 0.8286421410153354 | MAE Test Loss: 0.735395696216002 \n",
      "Epoch: 627 | MAE Train Loss: 0.8279070617855119 | MAE Test Loss: 0.7347887984748378 \n",
      "Epoch: 628 | MAE Train Loss: 0.8271739332786557 | MAE Test Loss: 0.734183787654397 \n",
      "Epoch: 629 | MAE Train Loss: 0.8264427665291654 | MAE Test Loss: 0.7335807026145537 \n",
      "Epoch: 630 | MAE Train Loss: 0.8257135511289505 | MAE Test Loss: 0.732979461111024 \n",
      "Epoch: 631 | MAE Train Loss: 0.8249862597519337 | MAE Test Loss: 0.7323801076300992 \n",
      "Epoch: 632 | MAE Train Loss: 0.8242609192793372 | MAE Test Loss: 0.7317826630979519 \n",
      "Epoch: 633 | MAE Train Loss: 0.8235375156713085 | MAE Test Loss: 0.7311870451794926 \n",
      "Epoch: 634 | MAE Train Loss: 0.8228160217027544 | MAE Test Loss: 0.7305932810113323 \n",
      "Epoch: 635 | MAE Train Loss: 0.8220964522053158 | MAE Test Loss: 0.7300013322019322 \n",
      "Epoch: 636 | MAE Train Loss: 0.821378782868296 | MAE Test Loss: 0.7294112536626269 \n",
      "Epoch: 637 | MAE Train Loss: 0.8206630110856207 | MAE Test Loss: 0.7288229804956419 \n",
      "Epoch: 638 | MAE Train Loss: 0.8199491218631423 | MAE Test Loss: 0.7282365831069919 \n",
      "Epoch: 639 | MAE Train Loss: 0.8192371410458547 | MAE Test Loss: 0.7276519801477329 \n",
      "Epoch: 640 | MAE Train Loss: 0.8185270334527429 | MAE Test Loss: 0.7270691805005762 \n",
      "Epoch: 641 | MAE Train Loss: 0.817818810308841 | MAE Test Loss: 0.7264882085460764 \n",
      "Epoch: 642 | MAE Train Loss: 0.8171124538155187 | MAE Test Loss: 0.7259090463604057 \n",
      "Epoch: 643 | MAE Train Loss: 0.8164079843533782 | MAE Test Loss: 0.7253316394811131 \n",
      "Epoch: 644 | MAE Train Loss: 0.8157053578363541 | MAE Test Loss: 0.7247560454999566 \n",
      "Epoch: 645 | MAE Train Loss: 0.8150045761034193 | MAE Test Loss: 0.7241822277603917 \n",
      "Epoch: 646 | MAE Train Loss: 0.8143056558507465 | MAE Test Loss: 0.723610195044342 \n",
      "Epoch: 647 | MAE Train Loss: 0.8136085595418153 | MAE Test Loss: 0.7230399136738661 \n",
      "Epoch: 648 | MAE Train Loss: 0.8129132998767197 | MAE Test Loss: 0.7224713924809544 \n",
      "Epoch: 649 | MAE Train Loss: 0.8122198880970715 | MAE Test Loss: 0.7219046405488295 \n",
      "Epoch: 650 | MAE Train Loss: 0.8115282867946954 | MAE Test Loss: 0.7213396083852712 \n",
      "Epoch: 651 | MAE Train Loss: 0.8108384891140415 | MAE Test Loss: 0.7207763524804836 \n",
      "Epoch: 652 | MAE Train Loss: 0.8101505047398401 | MAE Test Loss: 0.7202148356778391 \n",
      "Epoch: 653 | MAE Train Loss: 0.8094643503451332 | MAE Test Loss: 0.7196550692518223 \n",
      "Epoch: 654 | MAE Train Loss: 0.8087799809314514 | MAE Test Loss: 0.7190970001305085 \n",
      "Epoch: 655 | MAE Train Loss: 0.8080974053183956 | MAE Test Loss: 0.7185406230469364 \n",
      "Epoch: 656 | MAE Train Loss: 0.8074166191034472 | MAE Test Loss: 0.717985981078346 \n",
      "Epoch: 657 | MAE Train Loss: 0.8067376048259538 | MAE Test Loss: 0.717433050100351 \n",
      "Epoch: 658 | MAE Train Loss: 0.8060603543393811 | MAE Test Loss: 0.7168817977900473 \n",
      "Epoch: 659 | MAE Train Loss: 0.8053848725903212 | MAE Test Loss: 0.7163322968745499 \n",
      "Epoch: 660 | MAE Train Loss: 0.8047111770746694 | MAE Test Loss: 0.7157844624242643 \n",
      "Epoch: 661 | MAE Train Loss: 0.8040392492538816 | MAE Test Loss: 0.7152382741739396 \n",
      "Epoch: 662 | MAE Train Loss: 0.8033690658115162 | MAE Test Loss: 0.7146937910635804 \n",
      "Epoch: 663 | MAE Train Loss: 0.8027006285944177 | MAE Test Loss: 0.7141510082261424 \n",
      "Epoch: 664 | MAE Train Loss: 0.8020339333651219 | MAE Test Loss: 0.7136098567111608 \n",
      "Epoch: 665 | MAE Train Loss: 0.8013689698808342 | MAE Test Loss: 0.7130703933382798 \n",
      "Epoch: 666 | MAE Train Loss: 0.8007057473969466 | MAE Test Loss: 0.7125325512945836 \n",
      "Epoch: 667 | MAE Train Loss: 0.8000442482327736 | MAE Test Loss: 0.7119963588204479 \n",
      "Epoch: 668 | MAE Train Loss: 0.7993844472685511 | MAE Test Loss: 0.7114617778566977 \n",
      "Epoch: 669 | MAE Train Loss: 0.7987263613421094 | MAE Test Loss: 0.7109288506525596 \n",
      "Epoch: 670 | MAE Train Loss: 0.7980699809864751 | MAE Test Loss: 0.7103975448069827 \n",
      "Epoch: 671 | MAE Train Loss: 0.7974153113470585 | MAE Test Loss: 0.7098678831182185 \n",
      "Epoch: 672 | MAE Train Loss: 0.7967623390513083 | MAE Test Loss: 0.7093398608250018 \n",
      "Epoch: 673 | MAE Train Loss: 0.7961110599958887 | MAE Test Loss: 0.7088134257050758 \n",
      "Epoch: 674 | MAE Train Loss: 0.7954614753912204 | MAE Test Loss: 0.7082885922104317 \n",
      "Epoch: 675 | MAE Train Loss: 0.7948135419119693 | MAE Test Loss: 0.7077653363008591 \n",
      "Epoch: 676 | MAE Train Loss: 0.7941672948060552 | MAE Test Loss: 0.7072437007216983 \n",
      "Epoch: 677 | MAE Train Loss: 0.7935227246956622 | MAE Test Loss: 0.7067236835805193 \n",
      "Epoch: 678 | MAE Train Loss: 0.7928798204053941 | MAE Test Loss: 0.7062051989052534 \n",
      "Epoch: 679 | MAE Train Loss: 0.7922385637051714 | MAE Test Loss: 0.7056883233586797 \n",
      "Epoch: 680 | MAE Train Loss: 0.7915989648718667 | MAE Test Loss: 0.7051730015328126 \n",
      "Epoch: 681 | MAE Train Loss: 0.7909610322483762 | MAE Test Loss: 0.7046592488360738 \n",
      "Epoch: 682 | MAE Train Loss: 0.7903247229774806 | MAE Test Loss: 0.7041470914485964 \n",
      "Epoch: 683 | MAE Train Loss: 0.7896900597038907 | MAE Test Loss: 0.7036364566375923 \n",
      "Epoch: 684 | MAE Train Loss: 0.7890570397173048 | MAE Test Loss: 0.70312737727682 \n",
      "Epoch: 685 | MAE Train Loss: 0.7884256313819197 | MAE Test Loss: 0.702619879498544 \n",
      "Epoch: 686 | MAE Train Loss: 0.7877958572812789 | MAE Test Loss: 0.7021138629545888 \n",
      "Epoch: 687 | MAE Train Loss: 0.7871676814200332 | MAE Test Loss: 0.7016094156843685 \n",
      "Epoch: 688 | MAE Train Loss: 0.7865411389032678 | MAE Test Loss: 0.7011064888304851 \n",
      "Epoch: 689 | MAE Train Loss: 0.7859161816975272 | MAE Test Loss: 0.7006051254874154 \n",
      "Epoch: 690 | MAE Train Loss: 0.7852928432306728 | MAE Test Loss: 0.7001052252273867 \n",
      "Epoch: 691 | MAE Train Loss: 0.7846710877056424 | MAE Test Loss: 0.6996068794755145 \n",
      "Epoch: 692 | MAE Train Loss: 0.7840509432367939 | MAE Test Loss: 0.6991100667845286 \n",
      "Epoch: 693 | MAE Train Loss: 0.7834323951405758 | MAE Test Loss: 0.6986147036674764 \n",
      "Epoch: 694 | MAE Train Loss: 0.7828154186168147 | MAE Test Loss: 0.6981208682068982 \n",
      "Epoch: 695 | MAE Train Loss: 0.7822000242292744 | MAE Test Loss: 0.6976285088460206 \n",
      "Epoch: 696 | MAE Train Loss: 0.7815861733666392 | MAE Test Loss: 0.6971376161152079 \n",
      "Epoch: 697 | MAE Train Loss: 0.7809739090530133 | MAE Test Loss: 0.6966482343456467 \n",
      "Epoch: 698 | MAE Train Loss: 0.7803632223023789 | MAE Test Loss: 0.6961603023054016 \n",
      "Epoch: 699 | MAE Train Loss: 0.7797540506384631 | MAE Test Loss: 0.6956738726302688 \n",
      "Epoch: 700 | MAE Train Loss: 0.7791464491595489 | MAE Test Loss: 0.6951888918655387 \n",
      "Epoch: 701 | MAE Train Loss: 0.77854041928609 | MAE Test Loss: 0.6947053480331435 \n",
      "Epoch: 702 | MAE Train Loss: 0.7779358936142187 | MAE Test Loss: 0.6942232935280177 \n",
      "Epoch: 703 | MAE Train Loss: 0.7773329270235936 | MAE Test Loss: 0.6937426676002734 \n",
      "Epoch: 704 | MAE Train Loss: 0.7767314574712387 | MAE Test Loss: 0.6932635264043068 \n",
      "Epoch: 705 | MAE Train Loss: 0.7761315332681115 | MAE Test Loss: 0.6927858123969941 \n",
      "Epoch: 706 | MAE Train Loss: 0.775533162319913 | MAE Test Loss: 0.6923095143698742 \n",
      "Epoch: 707 | MAE Train Loss: 0.7749362776770304 | MAE Test Loss: 0.6918346674230278 \n",
      "Epoch: 708 | MAE Train Loss: 0.7743409233224066 | MAE Test Loss: 0.6913612281923243 \n",
      "Epoch: 709 | MAE Train Loss: 0.7737470482137777 | MAE Test Loss: 0.6908892526298638 \n",
      "Epoch: 710 | MAE Train Loss: 0.7731547004233935 | MAE Test Loss: 0.6904186700273269 \n",
      "Epoch: 711 | MAE Train Loss: 0.7725638709890041 | MAE Test Loss: 0.6899494827523489 \n",
      "Epoch: 712 | MAE Train Loss: 0.7719745102534326 | MAE Test Loss: 0.689481746635901 \n",
      "Epoch: 713 | MAE Train Loss: 0.771386666146091 | MAE Test Loss: 0.689015376595165 \n",
      "Epoch: 714 | MAE Train Loss: 0.7708002795340922 | MAE Test Loss: 0.688550449446246 \n",
      "Epoch: 715 | MAE Train Loss: 0.7702154024625272 | MAE Test Loss: 0.6880868845000736 \n",
      "Epoch: 716 | MAE Train Loss: 0.7696319697516207 | MAE Test Loss: 0.6876246918360479 \n",
      "Epoch: 717 | MAE Train Loss: 0.7690500281689371 | MAE Test Loss: 0.6871638701523064 \n",
      "Epoch: 718 | MAE Train Loss: 0.768469534520883 | MAE Test Loss: 0.6867044582054426 \n",
      "Epoch: 719 | MAE Train Loss: 0.7678905261132578 | MAE Test Loss: 0.6862463923944258 \n",
      "Epoch: 720 | MAE Train Loss: 0.7673129484521256 | MAE Test Loss: 0.685789745066361 \n",
      "Epoch: 721 | MAE Train Loss: 0.7667368594445152 | MAE Test Loss: 0.6853344358981016 \n",
      "Epoch: 722 | MAE Train Loss: 0.7661621943776888 | MAE Test Loss: 0.6848804655813974 \n",
      "Epoch: 723 | MAE Train Loss: 0.765588943940845 | MAE Test Loss: 0.6844278849813852 \n",
      "Epoch: 724 | MAE Train Loss: 0.7650171616306565 | MAE Test Loss: 0.6839765803330244 \n",
      "Epoch: 725 | MAE Train Loss: 0.7644467983239095 | MAE Test Loss: 0.6835266788132944 \n",
      "Epoch: 726 | MAE Train Loss: 0.7638779006078537 | MAE Test Loss: 0.6830780957484046 \n",
      "Epoch: 727 | MAE Train Loss: 0.7633104101204234 | MAE Test Loss: 0.6826308320555023 \n",
      "Epoch: 728 | MAE Train Loss: 0.7627443176995297 | MAE Test Loss: 0.6821849382099878 \n",
      "Epoch: 729 | MAE Train Loss: 0.7621796764879978 | MAE Test Loss: 0.6817403560399106 \n",
      "Epoch: 730 | MAE Train Loss: 0.7616164267659955 | MAE Test Loss: 0.6812971190969961 \n",
      "Epoch: 731 | MAE Train Loss: 0.7610546114021794 | MAE Test Loss: 0.6808551861719938 \n",
      "Epoch: 732 | MAE Train Loss: 0.7604941810090556 | MAE Test Loss: 0.680414553526037 \n",
      "Epoch: 733 | MAE Train Loss: 0.7599351323956222 | MAE Test Loss: 0.6799752712471522 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 734 | MAE Train Loss: 0.7593775183531679 | MAE Test Loss: 0.6795372816219828 \n",
      "Epoch: 735 | MAE Train Loss: 0.7588212795741079 | MAE Test Loss: 0.6791005641896687 \n",
      "Epoch: 736 | MAE Train Loss: 0.7582664028355509 | MAE Test Loss: 0.6786651319921908 \n",
      "Epoch: 737 | MAE Train Loss: 0.7577128950531813 | MAE Test Loss: 0.6782309888351286 \n",
      "Epoch: 738 | MAE Train Loss: 0.757160808113205 | MAE Test Loss: 0.6777981065817589 \n",
      "Epoch: 739 | MAE Train Loss: 0.7566100736980484 | MAE Test Loss: 0.6773665495361925 \n",
      "Epoch: 740 | MAE Train Loss: 0.7560606936853306 | MAE Test Loss: 0.6769362700738979 \n",
      "Epoch: 741 | MAE Train Loss: 0.7555127248224774 | MAE Test Loss: 0.6765072403962951 \n",
      "Epoch: 742 | MAE Train Loss: 0.7549660990297195 | MAE Test Loss: 0.6760795249736211 \n",
      "Epoch: 743 | MAE Train Loss: 0.7544208181656245 | MAE Test Loss: 0.6756530006595062 \n",
      "Epoch: 744 | MAE Train Loss: 0.7538768791755542 | MAE Test Loss: 0.6752278250296966 \n",
      "Epoch: 745 | MAE Train Loss: 0.7533343186715478 | MAE Test Loss: 0.6748038497262192 \n",
      "Epoch: 746 | MAE Train Loss: 0.752793103676345 | MAE Test Loss: 0.6743811575169689 \n",
      "Epoch: 747 | MAE Train Loss: 0.7522532112462038 | MAE Test Loss: 0.6739597154791293 \n",
      "Epoch: 748 | MAE Train Loss: 0.7517146477507342 | MAE Test Loss: 0.6735394461038243 \n",
      "Epoch: 749 | MAE Train Loss: 0.7511774106805186 | MAE Test Loss: 0.6731204491999352 \n",
      "Epoch: 750 | MAE Train Loss: 0.7506414870526272 | MAE Test Loss: 0.6727027275348301 \n",
      "Epoch: 751 | MAE Train Loss: 0.7501069281530851 | MAE Test Loss: 0.6722862250500891 \n",
      "Epoch: 752 | MAE Train Loss: 0.7495736761674017 | MAE Test Loss: 0.6718709492101367 \n",
      "Epoch: 753 | MAE Train Loss: 0.7490417432126686 | MAE Test Loss: 0.6714568855141744 \n",
      "Epoch: 754 | MAE Train Loss: 0.7485111111800526 | MAE Test Loss: 0.671044024707299 \n",
      "Epoch: 755 | MAE Train Loss: 0.7479817823788004 | MAE Test Loss: 0.6706323690430466 \n",
      "Epoch: 756 | MAE Train Loss: 0.7474537485612621 | MAE Test Loss: 0.670221909208738 \n",
      "Epoch: 757 | MAE Train Loss: 0.7469270120035799 | MAE Test Loss: 0.6698127052677088 \n",
      "Epoch: 758 | MAE Train Loss: 0.7464016135025254 | MAE Test Loss: 0.6694046959581804 \n",
      "Epoch: 759 | MAE Train Loss: 0.7458775010365469 | MAE Test Loss: 0.6689978718813794 \n",
      "Epoch: 760 | MAE Train Loss: 0.7453546768325843 | MAE Test Loss: 0.6685922354990663 \n",
      "Epoch: 761 | MAE Train Loss: 0.744833132779365 | MAE Test Loss: 0.6681877833872276 \n",
      "Epoch: 762 | MAE Train Loss: 0.7443128659644317 | MAE Test Loss: 0.66778449551133 \n",
      "Epoch: 763 | MAE Train Loss: 0.7437938638614906 | MAE Test Loss: 0.6673823789841231 \n",
      "Epoch: 764 | MAE Train Loss: 0.743276138282124 | MAE Test Loss: 0.6669814889963843 \n",
      "Epoch: 765 | MAE Train Loss: 0.7427596763218561 | MAE Test Loss: 0.6665817696869205 \n",
      "Epoch: 766 | MAE Train Loss: 0.7422444799984216 | MAE Test Loss: 0.6661832010631181 \n",
      "Epoch: 767 | MAE Train Loss: 0.7417305368675149 | MAE Test Loss: 0.6657857963463152 \n",
      "Epoch: 768 | MAE Train Loss: 0.7412178536366156 | MAE Test Loss: 0.6653895521559514 \n",
      "Epoch: 769 | MAE Train Loss: 0.7407064274299492 | MAE Test Loss: 0.6649944485306757 \n",
      "Epoch: 770 | MAE Train Loss: 0.7401962458641436 | MAE Test Loss: 0.6646004986977593 \n",
      "Epoch: 771 | MAE Train Loss: 0.7396873156168237 | MAE Test Loss: 0.6642077356352912 \n",
      "Epoch: 772 | MAE Train Loss: 0.7391796194724567 | MAE Test Loss: 0.6638161197562913 \n",
      "Epoch: 773 | MAE Train Loss: 0.7386731689694791 | MAE Test Loss: 0.6634256375840792 \n",
      "Epoch: 774 | MAE Train Loss: 0.7381679469535644 | MAE Test Loss: 0.6630362959777235 \n",
      "Epoch: 775 | MAE Train Loss: 0.737663964956318 | MAE Test Loss: 0.6626480750370786 \n",
      "Epoch: 776 | MAE Train Loss: 0.7371612107146781 | MAE Test Loss: 0.6622609880012695 \n",
      "Epoch: 777 | MAE Train Loss: 0.7366596908473847 | MAE Test Loss: 0.661875021569245 \n",
      "Epoch: 778 | MAE Train Loss: 0.7361593883633494 | MAE Test Loss: 0.6614901789770344 \n",
      "Epoch: 779 | MAE Train Loss: 0.7356602619103801 | MAE Test Loss: 0.6611064440506376 \n",
      "Epoch: 780 | MAE Train Loss: 0.7351623521748387 | MAE Test Loss: 0.6607238201736467 \n",
      "Epoch: 781 | MAE Train Loss: 0.7346656516808523 | MAE Test Loss: 0.660342297444221 \n",
      "Epoch: 782 | MAE Train Loss: 0.7341701624102596 | MAE Test Loss: 0.6599618958594472 \n",
      "Epoch: 783 | MAE Train Loss: 0.7336758862594762 | MAE Test Loss: 0.6595826424352623 \n",
      "Epoch: 784 | MAE Train Loss: 0.733182811004294 | MAE Test Loss: 0.6592044872950181 \n",
      "Epoch: 785 | MAE Train Loss: 0.7326909341032091 | MAE Test Loss: 0.6588274272721156 \n",
      "Epoch: 786 | MAE Train Loss: 0.7322002528721455 | MAE Test Loss: 0.6584513959249858 \n",
      "Epoch: 787 | MAE Train Loss: 0.7317107217001824 | MAE Test Loss: 0.6580764534033079 \n",
      "Epoch: 788 | MAE Train Loss: 0.7312223809037836 | MAE Test Loss: 0.6577026503173166 \n",
      "Epoch: 789 | MAE Train Loss: 0.7307352230084095 | MAE Test Loss: 0.6573299228840811 \n",
      "Epoch: 790 | MAE Train Loss: 0.7302492546831714 | MAE Test Loss: 0.6569582748762361 \n",
      "Epoch: 791 | MAE Train Loss: 0.7297644687052612 | MAE Test Loss: 0.6565876469833457 \n",
      "Epoch: 792 | MAE Train Loss: 0.7292808151545879 | MAE Test Loss: 0.6562181462995221 \n",
      "Epoch: 793 | MAE Train Loss: 0.7287983339553528 | MAE Test Loss: 0.6558497087393796 \n",
      "Epoch: 794 | MAE Train Loss: 0.7283170316992382 | MAE Test Loss: 0.6554823382288635 \n",
      "Epoch: 795 | MAE Train Loss: 0.7278369012630885 | MAE Test Loss: 0.6551159756590297 \n",
      "Epoch: 796 | MAE Train Loss: 0.727357892944531 | MAE Test Loss: 0.654750728159346 \n",
      "Epoch: 797 | MAE Train Loss: 0.7268800465028786 | MAE Test Loss: 0.6543865221011117 \n",
      "Epoch: 798 | MAE Train Loss: 0.7264033549826224 | MAE Test Loss: 0.6540233708772214 \n",
      "Epoch: 799 | MAE Train Loss: 0.7259278248973737 | MAE Test Loss: 0.6536612155793 \n",
      "Epoch: 800 | MAE Train Loss: 0.7254534067613805 | MAE Test Loss: 0.6533001633710099 \n",
      "Epoch: 801 | MAE Train Loss: 0.7249801401695279 | MAE Test Loss: 0.6529401478322846 \n",
      "Epoch: 802 | MAE Train Loss: 0.7245080139416522 | MAE Test Loss: 0.652581119376676 \n",
      "Epoch: 803 | MAE Train Loss: 0.7240369921570291 | MAE Test Loss: 0.6522231307138507 \n",
      "Epoch: 804 | MAE Train Loss: 0.7235671190336308 | MAE Test Loss: 0.6518662168539567 \n",
      "Epoch: 805 | MAE Train Loss: 0.7230983782209179 | MAE Test Loss: 0.651510281218349 \n",
      "Epoch: 806 | MAE Train Loss: 0.7226307343462682 | MAE Test Loss: 0.6511553674739476 \n",
      "Epoch: 807 | MAE Train Loss: 0.7221642183227764 | MAE Test Loss: 0.6508015362391989 \n",
      "Epoch: 808 | MAE Train Loss: 0.721698836074668 | MAE Test Loss: 0.6504486580390019 \n",
      "Epoch: 809 | MAE Train Loss: 0.7212345343796007 | MAE Test Loss: 0.6500968094148268 \n",
      "Epoch: 810 | MAE Train Loss: 0.7207713620464753 | MAE Test Loss: 0.6497459628233643 \n",
      "Epoch: 811 | MAE Train Loss: 0.7203092606943609 | MAE Test Loss: 0.64939614007043 \n",
      "Epoch: 812 | MAE Train Loss: 0.7198482837816648 | MAE Test Loss: 0.6490473140930232 \n",
      "Epoch: 813 | MAE Train Loss: 0.7193884239707427 | MAE Test Loss: 0.648699489376018 \n",
      "Epoch: 814 | MAE Train Loss: 0.7189296238080555 | MAE Test Loss: 0.64835267210055 \n",
      "Epoch: 815 | MAE Train Loss: 0.7184719447442931 | MAE Test Loss: 0.6480067955365172 \n",
      "Epoch: 816 | MAE Train Loss: 0.7180153252723928 | MAE Test Loss: 0.6476619592420767 \n",
      "Epoch: 817 | MAE Train Loss: 0.717559808425153 | MAE Test Loss: 0.6473180580828741 \n",
      "Epoch: 818 | MAE Train Loss: 0.7171053464486401 | MAE Test Loss: 0.6469751606983241 \n",
      "Epoch: 819 | MAE Train Loss: 0.7166519918166501 | MAE Test Loss: 0.6466332401521419 \n",
      "Epoch: 820 | MAE Train Loss: 0.7161996866060938 | MAE Test Loss: 0.6462923013306489 \n",
      "Epoch: 821 | MAE Train Loss: 0.7157484750942332 | MAE Test Loss: 0.6459523413655424 \n",
      "Epoch: 822 | MAE Train Loss: 0.7152983548574918 | MAE Test Loss: 0.6456133579461466 \n",
      "Epoch: 823 | MAE Train Loss: 0.7148492730577702 | MAE Test Loss: 0.6452752850276176 \n",
      "Epoch: 824 | MAE Train Loss: 0.7144012359672288 | MAE Test Loss: 0.6449381989865854 \n",
      "Epoch: 825 | MAE Train Loss: 0.7139542918178358 | MAE Test Loss: 0.6446020813967055 \n",
      "Epoch: 826 | MAE Train Loss: 0.7135083791649484 | MAE Test Loss: 0.6442669287731577 \n",
      "Epoch: 827 | MAE Train Loss: 0.713063545972634 | MAE Test Loss: 0.6439327392739214 \n",
      "Epoch: 828 | MAE Train Loss: 0.7126197396825167 | MAE Test Loss: 0.6435995092463799 \n",
      "Epoch: 829 | MAE Train Loss: 0.7121770081632386 | MAE Test Loss: 0.6432671649825789 \n",
      "Epoch: 830 | MAE Train Loss: 0.711735294930698 | MAE Test Loss: 0.6429358303519106 \n",
      "Epoch: 831 | MAE Train Loss: 0.7112946471691214 | MAE Test Loss: 0.6426053924882169 \n",
      "Epoch: 832 | MAE Train Loss: 0.7108550218349456 | MAE Test Loss: 0.6422759045177163 \n",
      "Epoch: 833 | MAE Train Loss: 0.7104164120563742 | MAE Test Loss: 0.6419473623705961 \n",
      "Epoch: 834 | MAE Train Loss: 0.7099788654643362 | MAE Test Loss: 0.6416197089560024 \n",
      "Epoch: 835 | MAE Train Loss: 0.7095423345189026 | MAE Test Loss: 0.6412930602005147 \n",
      "Epoch: 836 | MAE Train Loss: 0.7091068538528817 | MAE Test Loss: 0.6409672785487917 \n",
      "Epoch: 837 | MAE Train Loss: 0.7086723757424321 | MAE Test Loss: 0.6406424337348615 \n",
      "Epoch: 838 | MAE Train Loss: 0.7082389019758233 | MAE Test Loss: 0.6403185212747552 \n",
      "Epoch: 839 | MAE Train Loss: 0.7078064799476023 | MAE Test Loss: 0.6399955489635747 \n",
      "Epoch: 840 | MAE Train Loss: 0.7073750541956721 | MAE Test Loss: 0.6396734333102162 \n",
      "Epoch: 841 | MAE Train Loss: 0.7069446221913613 | MAE Test Loss: 0.6393522420107557 \n",
      "Epoch: 842 | MAE Train Loss: 0.706515235145601 | MAE Test Loss: 0.6390319832226988 \n",
      "Epoch: 843 | MAE Train Loss: 0.7060868378147089 | MAE Test Loss: 0.6387125732570553 \n",
      "Epoch: 844 | MAE Train Loss: 0.7056594276534055 | MAE Test Loss: 0.6383941446990478 \n",
      "Epoch: 845 | MAE Train Loss: 0.7052330476335495 | MAE Test Loss: 0.6380765597846219 \n",
      "Epoch: 846 | MAE Train Loss: 0.7048076504078662 | MAE Test Loss: 0.6377598973896184 \n",
      "Epoch: 847 | MAE Train Loss: 0.7043832342774361 | MAE Test Loss: 0.637444073562637 \n",
      "Epoch: 848 | MAE Train Loss: 0.7039597966747702 | MAE Test Loss: 0.6371292210155646 \n",
      "Epoch: 849 | MAE Train Loss: 0.7035373804355263 | MAE Test Loss: 0.6368152018936695 \n",
      "Epoch: 850 | MAE Train Loss: 0.7031159383785699 | MAE Test Loss: 0.6365020387696115 \n",
      "Epoch: 851 | MAE Train Loss: 0.7026954734281798 | MAE Test Loss: 0.6361897694608009 \n",
      "Epoch: 852 | MAE Train Loss: 0.7022759704922341 | MAE Test Loss: 0.6358784362154493 \n",
      "Epoch: 853 | MAE Train Loss: 0.7018574751718392 | MAE Test Loss: 0.6355679514957032 \n",
      "Epoch: 854 | MAE Train Loss: 0.701439950575417 | MAE Test Loss: 0.6352582964974981 \n",
      "Epoch: 855 | MAE Train Loss: 0.7010233862254641 | MAE Test Loss: 0.6349495418570846 \n",
      "Epoch: 856 | MAE Train Loss: 0.7006077838767069 | MAE Test Loss: 0.6346416689121328 \n",
      "Epoch: 857 | MAE Train Loss: 0.7001931330930334 | MAE Test Loss: 0.6343346626795243 \n",
      "Epoch: 858 | MAE Train Loss: 0.6997794837206127 | MAE Test Loss: 0.6340284762852055 \n",
      "Epoch: 859 | MAE Train Loss: 0.6993667862427767 | MAE Test Loss: 0.6337231805429233 \n",
      "Epoch: 860 | MAE Train Loss: 0.6989550423956842 | MAE Test Loss: 0.6334187568166798 \n",
      "Epoch: 861 | MAE Train Loss: 0.6985442418043782 | MAE Test Loss: 0.6331151457076964 \n",
      "Epoch: 862 | MAE Train Loss: 0.6981343869741006 | MAE Test Loss: 0.6328124018726765 \n",
      "Epoch: 863 | MAE Train Loss: 0.6977254713412119 | MAE Test Loss: 0.6325104658366563 \n",
      "Epoch: 864 | MAE Train Loss: 0.6973174974107248 | MAE Test Loss: 0.6322094453616327 \n",
      "Epoch: 865 | MAE Train Loss: 0.6969105023984152 | MAE Test Loss: 0.6319092278003657 \n",
      "Epoch: 866 | MAE Train Loss: 0.6965044449515736 | MAE Test Loss: 0.6316098842741534 \n",
      "Epoch: 867 | MAE Train Loss: 0.6960993267745778 | MAE Test Loss: 0.6313113388724216 \n",
      "Epoch: 868 | MAE Train Loss: 0.6956951421093571 | MAE Test Loss: 0.631013646528658 \n",
      "Epoch: 869 | MAE Train Loss: 0.6952918844342266 | MAE Test Loss: 0.6307167569196339 \n",
      "Epoch: 870 | MAE Train Loss: 0.6948895532059332 | MAE Test Loss: 0.6304207157471465 \n",
      "Epoch: 871 | MAE Train Loss: 0.6944881449980747 | MAE Test Loss: 0.6301254632348371 \n",
      "Epoch: 872 | MAE Train Loss: 0.6940876623151775 | MAE Test Loss: 0.6298310544987376 \n",
      "Epoch: 873 | MAE Train Loss: 0.6936880986630793 | MAE Test Loss: 0.6295374229117368 \n",
      "Epoch: 874 | MAE Train Loss: 0.6932894454041296 | MAE Test Loss: 0.6292446305158088 \n",
      "Epoch: 875 | MAE Train Loss: 0.6928917072555597 | MAE Test Loss: 0.6289526750359153 \n",
      "Epoch: 876 | MAE Train Loss: 0.6924948822377257 | MAE Test Loss: 0.6286615060787959 \n",
      "Epoch: 877 | MAE Train Loss: 0.6920989699231737 | MAE Test Loss: 0.6283711694716428 \n",
      "Epoch: 878 | MAE Train Loss: 0.6917039668164341 | MAE Test Loss: 0.628081614801318 \n",
      "Epoch: 879 | MAE Train Loss: 0.6913098725227887 | MAE Test Loss: 0.6277928879306476 \n",
      "Epoch: 880 | MAE Train Loss: 0.6909166835273383 | MAE Test Loss: 0.6275049703556802 \n",
      "Epoch: 881 | MAE Train Loss: 0.6905243897843641 | MAE Test Loss: 0.6272178278863398 \n",
      "Epoch: 882 | MAE Train Loss: 0.6901329990529941 | MAE Test Loss: 0.6269315064387323 \n",
      "Epoch: 883 | MAE Train Loss: 0.6897425077894507 | MAE Test Loss: 0.6266459555502509 \n",
      "Epoch: 884 | MAE Train Loss: 0.689352915680167 | MAE Test Loss: 0.6263612147158037 \n",
      "Epoch: 885 | MAE Train Loss: 0.6889642083217431 | MAE Test Loss: 0.6260772882134302 \n",
      "Epoch: 886 | MAE Train Loss: 0.6885763946628083 | MAE Test Loss: 0.6257941255508437 \n",
      "Epoch: 887 | MAE Train Loss: 0.6881894744379158 | MAE Test Loss: 0.6255117663614668 \n",
      "Epoch: 888 | MAE Train Loss: 0.687803433296047 | MAE Test Loss: 0.6252301566718795 \n",
      "Epoch: 889 | MAE Train Loss: 0.6874182845540853 | MAE Test Loss: 0.6249493460440791 \n",
      "Epoch: 890 | MAE Train Loss: 0.6870340111266922 | MAE Test Loss: 0.6246692966362793 \n",
      "Epoch: 891 | MAE Train Loss: 0.6866505765538322 | MAE Test Loss: 0.6243899838112478 \n",
      "Epoch: 892 | MAE Train Loss: 0.6862680180972672 | MAE Test Loss: 0.6241114698907938 \n",
      "Epoch: 893 | MAE Train Loss: 0.6858863401037626 | MAE Test Loss: 0.6238337626899841 \n",
      "Epoch: 894 | MAE Train Loss: 0.6855055379960371 | MAE Test Loss: 0.6235567854497643 \n",
      "Epoch: 895 | MAE Train Loss: 0.685125606392558 | MAE Test Loss: 0.6232805943926957 \n",
      "Epoch: 896 | MAE Train Loss: 0.6847465390332108 | MAE Test Loss: 0.6230051451300778 \n",
      "Epoch: 897 | MAE Train Loss: 0.6843683464101782 | MAE Test Loss: 0.6227304260975477 \n",
      "Epoch: 898 | MAE Train Loss: 0.6839909720713024 | MAE Test Loss: 0.6224565031387032 \n",
      "Epoch: 899 | MAE Train Loss: 0.6836144644430817 | MAE Test Loss: 0.6221832993251898 \n",
      "Epoch: 900 | MAE Train Loss: 0.6832388181954394 | MAE Test Loss: 0.6219108873035128 \n",
      "Epoch: 901 | MAE Train Loss: 0.6828640349688201 | MAE Test Loss: 0.6216392487687871 \n",
      "Epoch: 902 | MAE Train Loss: 0.6824901050577225 | MAE Test Loss: 0.6213682714510674 \n",
      "Epoch: 903 | MAE Train Loss: 0.6821169889901284 | MAE Test Loss: 0.621098079651513 \n",
      "Epoch: 904 | MAE Train Loss: 0.6817447305344688 | MAE Test Loss: 0.6208286550813918 \n",
      "Epoch: 905 | MAE Train Loss: 0.6813733200255285 | MAE Test Loss: 0.6205599368160303 \n",
      "Epoch: 906 | MAE Train Loss: 0.6810027600420394 | MAE Test Loss: 0.6202919302589545 \n",
      "Epoch: 907 | MAE Train Loss: 0.6806330025183986 | MAE Test Loss: 0.6200247112851067 \n",
      "Epoch: 908 | MAE Train Loss: 0.6802640929767716 | MAE Test Loss: 0.6197581923357343 \n",
      "Epoch: 909 | MAE Train Loss: 0.6798960286483303 | MAE Test Loss: 0.6194924302312821 \n",
      "Epoch: 910 | MAE Train Loss: 0.6795288033617187 | MAE Test Loss: 0.6192273717808746 \n",
      "Epoch: 911 | MAE Train Loss: 0.6791623736279011 | MAE Test Loss: 0.6189630175403918 \n",
      "Epoch: 912 | MAE Train Loss: 0.6787967814325753 | MAE Test Loss: 0.6186994140258866 \n",
      "Epoch: 913 | MAE Train Loss: 0.6784320230321867 | MAE Test Loss: 0.6184365243619652 \n",
      "Epoch: 914 | MAE Train Loss: 0.6780680628129403 | MAE Test Loss: 0.6181743919582057 \n",
      "Epoch: 915 | MAE Train Loss: 0.6777049305648387 | MAE Test Loss: 0.6179129450353738 \n",
      "Epoch: 916 | MAE Train Loss: 0.6773426312269848 | MAE Test Loss: 0.6176521898162161 \n",
      "Epoch: 917 | MAE Train Loss: 0.6769811172333322 | MAE Test Loss: 0.6173921858642105 \n",
      "Epoch: 918 | MAE Train Loss: 0.6766204260523521 | MAE Test Loss: 0.6171328612219988 \n",
      "Epoch: 919 | MAE Train Loss: 0.6762605625769236 | MAE Test Loss: 0.6168742330352801 \n",
      "Epoch: 920 | MAE Train Loss: 0.6759014771311523 | MAE Test Loss: 0.6166163394983724 \n",
      "Epoch: 921 | MAE Train Loss: 0.6755432116386462 | MAE Test Loss: 0.6163591298602575 \n",
      "Epoch: 922 | MAE Train Loss: 0.6751857664439612 | MAE Test Loss: 0.6161026109032322 \n",
      "Epoch: 923 | MAE Train Loss: 0.674829094322469 | MAE Test Loss: 0.615846820638865 \n",
      "Epoch: 924 | MAE Train Loss: 0.6744732370317482 | MAE Test Loss: 0.6155917172821503 \n",
      "Epoch: 925 | MAE Train Loss: 0.674118149546799 | MAE Test Loss: 0.6153372791171499 \n",
      "Epoch: 926 | MAE Train Loss: 0.6737638778323679 | MAE Test Loss: 0.61508355835565 \n",
      "Epoch: 927 | MAE Train Loss: 0.6734104060602686 | MAE Test Loss: 0.6148305187790319 \n",
      "Epoch: 928 | MAE Train Loss: 0.6730576991978816 | MAE Test Loss: 0.6145781980960423 \n",
      "Epoch: 929 | MAE Train Loss: 0.6727057987276959 | MAE Test Loss: 0.6143264951017711 \n",
      "Epoch: 930 | MAE Train Loss: 0.6723546642314996 | MAE Test Loss: 0.6140755180120765 \n",
      "Epoch: 931 | MAE Train Loss: 0.6720043306740761 | MAE Test Loss: 0.6138252146109112 \n",
      "Epoch: 932 | MAE Train Loss: 0.6716547555790804 | MAE Test Loss: 0.6135756172066941 \n",
      "Epoch: 933 | MAE Train Loss: 0.6713059705934932 | MAE Test Loss: 0.6133266189046382 \n",
      "Epoch: 934 | MAE Train Loss: 0.670957947245281 | MAE Test Loss: 0.6130783388954203 \n",
      "Epoch: 935 | MAE Train Loss: 0.6706107182688725 | MAE Test Loss: 0.6128307251319998 \n",
      "Epoch: 936 | MAE Train Loss: 0.6702642413694312 | MAE Test Loss: 0.6125838097848486 \n",
      "Epoch: 937 | MAE Train Loss: 0.6699185480720837 | MAE Test Loss: 0.6123374969945916 \n",
      "Epoch: 938 | MAE Train Loss: 0.6695736079809975 | MAE Test Loss: 0.6120918949838439 \n",
      "Epoch: 939 | MAE Train Loss: 0.6692294557662706 | MAE Test Loss: 0.6118469357294839 \n",
      "Epoch: 940 | MAE Train Loss: 0.6688860418318031 | MAE Test Loss: 0.611602683554465 \n",
      "Epoch: 941 | MAE Train Loss: 0.6685434125544312 | MAE Test Loss: 0.6113590866638241 \n",
      "Epoch: 942 | MAE Train Loss: 0.6682015259100602 | MAE Test Loss: 0.6111161167922591 \n",
      "Epoch: 943 | MAE Train Loss: 0.6678604175049669 | MAE Test Loss: 0.6108737985285683 \n",
      "Epoch: 944 | MAE Train Loss: 0.6675200486076353 | MAE Test Loss: 0.6106321139752525 \n",
      "Epoch: 945 | MAE Train Loss: 0.6671804102601588 | MAE Test Loss: 0.6103911272696056 \n",
      "Epoch: 946 | MAE Train Loss: 0.6668415485940848 | MAE Test Loss: 0.6101507377774019 \n",
      "Epoch: 947 | MAE Train Loss: 0.6665034241904593 | MAE Test Loss: 0.6099110263145088 \n",
      "Epoch: 948 | MAE Train Loss: 0.6661660658813826 | MAE Test Loss: 0.6096719575138695 \n",
      "Epoch: 949 | MAE Train Loss: 0.6658294393818867 | MAE Test Loss: 0.6094335134996803 \n",
      "Epoch: 950 | MAE Train Loss: 0.6654935357929147 | MAE Test Loss: 0.609195758224613 \n",
      "Epoch: 951 | MAE Train Loss: 0.6651584010023621 | MAE Test Loss: 0.608958635623055 \n",
      "Epoch: 952 | MAE Train Loss: 0.6648239842619804 | MAE Test Loss: 0.6087221214663061 \n",
      "Epoch: 953 | MAE Train Loss: 0.6644903300615466 | MAE Test Loss: 0.6084862411036935 \n",
      "Epoch: 954 | MAE Train Loss: 0.6641574000539141 | MAE Test Loss: 0.6082509881899736 \n",
      "Epoch: 955 | MAE Train Loss: 0.6638251836387984 | MAE Test Loss: 0.6080164150308578 \n",
      "Epoch: 956 | MAE Train Loss: 0.6634937282489043 | MAE Test Loss: 0.6077824543332178 \n",
      "Epoch: 957 | MAE Train Loss: 0.6631629852033625 | MAE Test Loss: 0.6075490551932822 \n",
      "Epoch: 958 | MAE Train Loss: 0.6628329555371721 | MAE Test Loss: 0.6073162811754109 \n",
      "Epoch: 959 | MAE Train Loss: 0.6625036426196068 | MAE Test Loss: 0.6070841752868096 \n",
      "Epoch: 960 | MAE Train Loss: 0.6621750755894473 | MAE Test Loss: 0.6068526749463425 \n",
      "Epoch: 961 | MAE Train Loss: 0.661847215012511 | MAE Test Loss: 0.6066218062299905 \n",
      "Epoch: 962 | MAE Train Loss: 0.6615200650510864 | MAE Test Loss: 0.6063915887980873 \n",
      "Epoch: 963 | MAE Train Loss: 0.6611936581368649 | MAE Test Loss: 0.6061619225098783 \n",
      "Epoch: 964 | MAE Train Loss: 0.6608679558093125 | MAE Test Loss: 0.6059328549025624 \n",
      "Epoch: 965 | MAE Train Loss: 0.660542954082053 | MAE Test Loss: 0.6057044121481777 \n",
      "Epoch: 966 | MAE Train Loss: 0.6602186571449268 | MAE Test Loss: 0.6054766254615718 \n",
      "Epoch: 967 | MAE Train Loss: 0.6598950956586559 | MAE Test Loss: 0.6052494323900268 \n",
      "Epoch: 968 | MAE Train Loss: 0.6595722304008099 | MAE Test Loss: 0.6050228430706037 \n",
      "Epoch: 969 | MAE Train Loss: 0.6592500583633653 | MAE Test Loss: 0.6047968558858281 \n",
      "Epoch: 970 | MAE Train Loss: 0.6589285781513473 | MAE Test Loss: 0.6045714610457529 \n",
      "Epoch: 971 | MAE Train Loss: 0.6586078404555398 | MAE Test Loss: 0.6043466649231897 \n",
      "Epoch: 972 | MAE Train Loss: 0.6582877916701954 | MAE Test Loss: 0.6041224659088373 \n",
      "Epoch: 973 | MAE Train Loss: 0.6579684304071639 | MAE Test Loss: 0.603898850462245 \n",
      "Epoch: 974 | MAE Train Loss: 0.657649756785758 | MAE Test Loss: 0.6036758288692747 \n",
      "Epoch: 975 | MAE Train Loss: 0.6573317678908035 | MAE Test Loss: 0.6034533995284945 \n",
      "Epoch: 976 | MAE Train Loss: 0.6570144623407883 | MAE Test Loss: 0.6032316094181713 \n",
      "Epoch: 977 | MAE Train Loss: 0.6566978776729598 | MAE Test Loss: 0.6030103468389572 \n",
      "Epoch: 978 | MAE Train Loss: 0.6563819776681488 | MAE Test Loss: 0.6027896715810599 \n",
      "Epoch: 979 | MAE Train Loss: 0.6560667567897973 | MAE Test Loss: 0.6025695820536312 \n",
      "Epoch: 980 | MAE Train Loss: 0.655752213665243 | MAE Test Loss: 0.602350064582698 \n",
      "Epoch: 981 | MAE Train Loss: 0.6554383483385811 | MAE Test Loss: 0.6021311296252334 \n",
      "Epoch: 982 | MAE Train Loss: 0.6551251580012785 | MAE Test Loss: 0.6019127755982586 \n",
      "Epoch: 983 | MAE Train Loss: 0.6548126412873106 | MAE Test Loss: 0.6016950009213745 \n",
      "Epoch: 984 | MAE Train Loss: 0.6545007968328372 | MAE Test Loss: 0.601477852296046 \n",
      "Epoch: 985 | MAE Train Loss: 0.6541896618692261 | MAE Test Loss: 0.601261279793681 \n",
      "Epoch: 986 | MAE Train Loss: 0.6538791963635548 | MAE Test Loss: 0.6010452818417445 \n",
      "Epoch: 987 | MAE Train Loss: 0.6535693989586862 | MAE Test Loss: 0.6008298691056908 \n",
      "Epoch: 988 | MAE Train Loss: 0.653260266974331 | MAE Test Loss: 0.6006149660063522 \n",
      "Epoch: 989 | MAE Train Loss: 0.6529518045312206 | MAE Test Loss: 0.6004006327314071 \n",
      "Epoch: 990 | MAE Train Loss: 0.6526440061497489 | MAE Test Loss: 0.6001868677186424 \n",
      "Epoch: 991 | MAE Train Loss: 0.652336870481393 | MAE Test Loss: 0.599973653327022 \n",
      "Epoch: 992 | MAE Train Loss: 0.6520303891572011 | MAE Test Loss: 0.5997610040828879 \n",
      "Epoch: 993 | MAE Train Loss: 0.6517245678721627 | MAE Test Loss: 0.5995489184317295 \n",
      "Epoch: 994 | MAE Train Loss: 0.6514194052842333 | MAE Test Loss: 0.599337407204404 \n",
      "Epoch: 995 | MAE Train Loss: 0.6511148988159227 | MAE Test Loss: 0.599126456510523 \n",
      "Epoch: 996 | MAE Train Loss: 0.6508110483920169 | MAE Test Loss: 0.5989160487266789 \n",
      "Epoch: 997 | MAE Train Loss: 0.6505078456954392 | MAE Test Loss: 0.5987061983869398 \n",
      "Epoch: 998 | MAE Train Loss: 0.6502052963905983 | MAE Test Loss: 0.5984969164155555 \n",
      "Epoch: 999 | MAE Train Loss: 0.6499033979580771 | MAE Test Loss: 0.5982881266698794 \n"
     ]
    }
   ],
   "source": [
    "#ACTUAL TRAINING OF THE MODEL\n",
    "\n",
    "torch.manual_seed(42)\n",
    "#no. of times the model will pass over the training data\n",
    "epochs = 1000\n",
    "\n",
    "#Create empty loss list to track loss values\n",
    "train_loss_values = []\n",
    "test_loss_values = []\n",
    "epoch_count = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    ###TRAINING!!!\n",
    "    \n",
    "    #Put model in training mode\n",
    "    model_0.train()\n",
    "    \n",
    "    # 1. Forward pass on train data\n",
    "    y_pred = model_0(X_train_tensor) #weight prediction\n",
    "    #calculated index prediction\n",
    "    index_pred = torch.sum(torch.mul(X_train_tensor, y_pred), dim=1) + model_0.bias #sum product of weight and data\n",
    "        \n",
    "    # 2. Calculate loss from prediction\n",
    "    loss = loss_fn(index_pred.double(), y_train_tensor.double())\n",
    "    \n",
    "    # 3. Zero grad of optimizer\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 4. Loss backwards\n",
    "    loss.backward()\n",
    "    \n",
    "    # 5. Progress the optimizer\n",
    "    optimizer.step()\n",
    "    \n",
    "    ###TESTING ACCURACY!!!\n",
    "    \n",
    "    #Put model in evaluation mode\n",
    "    model_0.eval()\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        # 1. Forward pass on test data\n",
    "        test_pred = model_0(X_test_tensor)\n",
    "        \n",
    "        # 2. Calculate loss on test data\n",
    "        test_sum_prod = torch.sum(torch.mul(X_test_tensor, test_pred), dim=1) + model_0.bias\n",
    "        test_loss = loss_fn(test_sum_prod, y_test_tensor)\n",
    "        \n",
    "        #Print out what is happening (each 10 steps)\n",
    "        if epoch % 1 == 0:\n",
    "            epoch_count.append(epoch)\n",
    "            train_loss_values.append(loss.detach().numpy())\n",
    "            test_loss_values.append(test_loss.detach().numpy())\n",
    "            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c17bfd4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjMAAAHFCAYAAAAHcXhbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABXVUlEQVR4nO3deXwTZf4H8M/kTq/0gLYUCpSz3DfIoYAoUA5BcHWR2xVBLvEERDnFiifrssLiuqA/5dAVWFRu5VJAEAQREEShFKGWo/eR5nh+fySZNrSFkqSdlH7er9e8kjwzmXwzYe1nn+eZGUkIIUBERERUSamULoCIiIjIGwwzREREVKkxzBAREVGlxjBDRERElRrDDBEREVVqDDNERERUqTHMEBERUaXGMENERESVGsMMERERVWoMM1QlSJJUpmXXrl1efc7cuXMhSZJH7921a5dPavB3Y8aMQd26dW+53XvvvYeVK1eWay2XLl3C3LlzcfTo0TJtX1V+I6LKRqN0AUQVYf/+/W6vFyxYgJ07d+Kbb75xa2/atKlXn/P444+jb9++Hr23bdu22L9/v9c13Cnee+89VKtWDWPGjCm3z7h06RLmzZuHunXronXr1uX2OURUvhhmqEq466673F5Xr14dKpWqWPuNcnNzERAQUObPqVWrFmrVquVRjSEhIbesh0gJNpsNVqsVer1e6VKISsRhJiKnHj16oHnz5tizZw+6dOmCgIAAPPbYYwCAtWvXonfv3qhRowaMRiOaNGmCGTNmICcnx20fJQ0z1a1bFwMGDMCWLVvQtm1bGI1GxMfH4z//+Y/bdiUNYYwZMwZBQUE4e/Ys+vXrh6CgIMTGxuLZZ5+F2Wx2e//Fixfx0EMPITg4GKGhoRg+fDgOHToESZJuOVxz5coVTJw4EU2bNkVQUBAiIyNx7733Yu/evW7bnT9/HpIk4c0338Tbb7+NuLg4BAUFoXPnzjhw4ECx/a5cuRKNGzeGXq9HkyZN8NFHH920jqLH7MSJE9i9e7c8BFh0aCozMxPPPfcc4uLioNPpULNmTUybNq3Y7/HZZ5+hU6dOMJlMCAgIQL169eTfdNeuXejQoQMAYOzYsfLnzJ07t0w1FrVx40Z07twZAQEBCA4Oxv3331+sN/DKlSt44oknEBsbC71ej+rVq6Nr167YsWOHvM2PP/6IAQMGIDIyEnq9HjExMejfvz8uXrx4yxq2bNmCXr16yd+1SZMmSExMlNf36NEDPXr0KPa+G4f9XL/x66+/jldeeQVxcXHQ6/X49NNPodPp8PLLLxfbxy+//AJJkvDuu+/KbSkpKRg/fjxq1aoFnU6HuLg4zJs3D1ar1e29S5cuRatWrRAUFITg4GDEx8fjxRdfvOX3JSqKPTNERVy+fBkjRozACy+8gFdffRUqlSPv//rrr+jXrx+mTZuGwMBA/PLLL1i0aBEOHjxYbKiqJMeOHcOzzz6LGTNmICoqCv/+97/xt7/9DQ0aNMA999xz0/daLBY88MAD+Nvf/oZnn30We/bswYIFC2AymTB79mwAQE5ODnr27Inr169j0aJFaNCgAbZs2YJHHnmkTN/7+vXrAIA5c+YgOjoa2dnZWL9+PXr06IGvv/662B/Bf/7zn4iPj8fixYsBAC+//DL69euHc+fOwWQyAXAEmbFjx2LQoEF46623kJGRgblz58JsNsvHtTTr16/HQw89BJPJhPfeew8A5F6B3NxcdO/eHRcvXsSLL76Ili1b4sSJE5g9ezaOHz+OHTt2QJIk7N+/H4888ggeeeQRzJ07FwaDAUlJSfLv1bZtW6xYsQJjx47FSy+9hP79+wPAbfesrVq1CsOHD0fv3r2xevVqmM1mvP766/Kx69atGwBg5MiROHLkCBYuXIhGjRohPT0dR44cwbVr1wA4fsP7778fcXFx+Oc//4moqCikpKRg586dyMrKumkNH3zwAcaNG4fu3btj2bJliIyMxJkzZ/Dzzz/f1ncp6t1330WjRo3w5ptvIiQkBA0bNsSAAQPw4YcfYt68eW6/4YoVK6DT6TB8+HAAjiDTsWNHqFQqzJ49G/Xr18f+/fvxyiuv4Pz581ixYgUAYM2aNZg4cSKmTJmCN998EyqVCmfPnsXJkyc9rpuqKEFUBY0ePVoEBga6tXXv3l0AEF9//fVN32u324XFYhG7d+8WAMSxY8fkdXPmzBE3/s+qTp06wmAwiKSkJLktLy9PhIeHi/Hjx8ttO3fuFADEzp073eoEID799FO3ffbr1080btxYfv3Pf/5TABCbN2922278+PECgFixYsVNv9ONrFarsFgsolevXuLBBx+U28+dOycAiBYtWgir1Sq3Hzx4UAAQq1evFkIIYbPZRExMjGjbtq2w2+3ydufPnxdarVbUqVPnljU0a9ZMdO/evVh7YmKiUKlU4tChQ27t//3vfwUAsWnTJiGEEG+++aYAINLT00v9jEOHDt3W8bnxN3J9zxYtWgibzSZvl5WVJSIjI0WXLl3ktqCgIDFt2rRS9/3DDz8IAGLDhg1lqqXoZ4WEhIhu3bq5Hesbde/evcTjOXr0aLffw/Ub169fXxQUFLhtu3HjRgFAbNu2TW6zWq0iJiZGDB06VG4bP368CAoKcvs3L0Thb3LixAkhhBCTJ08WoaGht/N1iUrEYSaiIsLCwnDvvfcWa//999/x6KOPIjo6Gmq1GlqtFt27dwcAnDp16pb7bd26NWrXri2/NhgMaNSoEZKSkm75XkmSMHDgQLe2li1bur139+7dCA4OLjb5eNiwYbfcv8uyZcvQtm1bGAwGaDQaaLVafP311yV+v/79+0OtVrvVA0Cu6fTp07h06RIeffRRt2G3OnXqoEuXLmWuqSRffvklmjdvjtatW8NqtcpLnz593IbpXENIDz/8MD799FP88ccfXn1uSVzfc+TIkW49FUFBQRg6dCgOHDiA3NxcAEDHjh2xcuVKvPLKKzhw4AAsFovbvho0aICwsDBMnz4dy5YtK3PvxL59+5CZmYmJEyd6fCZdSR544AFotVq3toSEBERHR8s9KwCwdetWXLp0SR6+Axy/Uc+ePRETE+P2GyUkJABw/HsFHMckPT0dw4YNw//+9z9cvXrVZ/VT1cIwQ1REjRo1irVlZ2fj7rvvxvfff49XXnkFu3btwqFDh7Bu3ToAQF5e3i33GxERUaxNr9eX6b0BAQEwGAzF3pufny+/vnbtGqKiooq9t6S2krz99tt48skn0alTJ3z++ec4cOAADh06hL59+5ZY443fxzUE5NrWNXQSHR1d7L0ltd2OP//8Ez/99BO0Wq3bEhwcDCGE/AfxnnvuwYYNG2C1WjFq1CjUqlULzZs3x+rVq736/KJc37OkfzcxMTGw2+1IS0sD4Jh3NXr0aPz73/9G586dER4ejlGjRiElJQUAYDKZsHv3brRu3RovvvgimjVrhpiYGMyZM6dY8CnqypUrAG5/eOxWSvpOGo0GI0eOxPr165Geng7AMZxYo0YN9OnTR97uzz//xBdffFHsN2rWrBkAyL/RyJEj8Z///AdJSUkYOnQoIiMj0alTJ2zfvt2n34XufJwzQ1RESf/P9ptvvsGlS5ewa9cuuTcGgPwfc38QERGBgwcPFmt3/aG8lY8//hg9evTA0qVL3dpvNVfjZvWU9vllrak01apVg9FoLDaBuuh6l0GDBmHQoEEwm804cOAAEhMT8eijj6Ju3bro3LmzV3UAhd/z8uXLxdZdunQJKpUKYWFhcl2LFy/G4sWLceHCBWzcuBEzZsxAamoqtmzZAgBo0aIF1qxZAyEEfvrpJ6xcuRLz58+H0WjEjBkzSqyhevXqAHDLScIGgwEZGRnF2kvrDSmtl2fs2LF44403sGbNGjzyyCPYuHEjpk2b5tZTV61aNbRs2RILFy4scR8xMTFu+xs7dixycnKwZ88ezJkzBwMGDMCZM2dQp06dm34nIhf2zBDdgus/6jeelvqvf/1LiXJK1L17d2RlZWHz5s1u7WvWrCnT+yVJKvb9fvrpp2Jn5JRV48aNUaNGDaxevRpCCLk9KSkJ+/btK9M+Suu5GjBgAH777TdERESgffv2xZaSLsin1+vRvXt3LFq0CIDjrCFXO1C23rWSNG7cGDVr1sSqVavcvmdOTg4+//xz+QynG9WuXRuTJ0/G/fffjyNHjhRbL0kSWrVqhXfeeQehoaElbuPSpUsXmEwmLFu2zK2GG9WtWxdnzpxxOwvu2rVrZf49XJo0aYJOnTphxYoVWLVqFcxmM8aOHeu2zYABA/Dzzz+jfv36Jf5GRcOMS2BgIBISEjBr1iwUFBTgxIkTt1UXVW3smSG6hS5duiAsLAwTJkzAnDlzoNVq8cknn+DYsWNKlyYbPXo03nnnHYwYMQKvvPIKGjRogM2bN2Pr1q0AcMuzhwYMGIAFCxZgzpw56N69O06fPo358+cjLi6u2Km0ZaFSqbBgwQI8/vjjePDBBzFu3Dikp6dj7ty5ZR5mcvVSrF27FvXq1YPBYECLFi0wbdo0fP7557jnnnvw9NNPo2XLlrDb7bhw4QK2bduGZ599Fp06dcLs2bNx8eJF9OrVC7Vq1UJ6ejr+/ve/u813ql+/PoxGIz755BM0adIEQUFBiImJKfGPbWnf8/XXX8fw4cMxYMAAjB8/HmazGW+88QbS09Px2muvAQAyMjLQs2dPPProo4iPj0dwcDAOHTqELVu2YMiQIQAc80zee+89DB48GPXq1YMQAuvWrUN6ejruv//+UmsICgrCW2+9hccffxz33Xcfxo0bh6ioKJw9exbHjh3DkiVLADiGdP71r39hxIgRGDduHK5du4bXX38dISEhZfquRT322GMYP348Ll26hC5duqBx48Zu6+fPn4/t27ejS5cumDp1Kho3boz8/HycP38emzZtwrJly1CrVi2MGzcORqMRXbt2RY0aNZCSkoLExESYTCZ5zhNRmSg5+5hIKaWdzdSsWbMSt9+3b5/o3LmzCAgIENWrVxePP/64OHLkSLEzYUo7m6l///7F9nnj2SWlnc10Y52lfc6FCxfEkCFDRFBQkAgODhZDhw4VmzZtEgDE//73v9IOhRBCCLPZLJ577jlRs2ZNYTAYRNu2bcWGDRtKPdPljTfeKLYPAGLOnDlubf/+979Fw4YNhU6nE40aNRL/+c9/iu2zNOfPnxe9e/cWwcHBAoDbe7Kzs8VLL70kGjduLHQ6nTCZTKJFixbi6aefFikpKUIIIb788kuRkJAgatasKXQ6nYiMjBT9+vUTe/fudfuc1atXi/j4eKHVakv8DkWV9BsJIcSGDRtEp06dhMFgEIGBgaJXr17iu+++k9fn5+eLCRMmiJYtW4qQkBBhNBpF48aNxZw5c0ROTo4QQohffvlFDBs2TNSvX18YjUZhMplEx44dxcqVK295rIQQYtOmTaJ79+4iMDBQBAQEiKZNm4pFixa5bfPhhx+KJk2aCIPBIJo2bSrWrl17W7+xS0ZGhjAajQKAeP/990vc5sqVK2Lq1KkiLi5OaLVaER4eLtq1aydmzZolsrOz5Xp69uwpoqKihE6nEzExMeLhhx8WP/30U5m+M5GLJMRN+iWJqFJ79dVX8dJLL+HChQs+nyBKROQvOMxEdIdwDSfEx8fDYrHgm2++wbvvvosRI0YwyBDRHY1hhugOERAQgHfeeQfnz5+H2WxG7dq1MX36dLz00ktKl0ZEVK44zERERESVGk/NJiIiokqNYYaIiIgqNYYZIiIiqtTu+AnAdrsdly5dQnBwsE9vwkZERETlRwiBrKwsxMTE3PLCn3d8mLl06RJiY2OVLoOIiIg8kJycfMvLS9zxYSY4OBiA42B4ctluIiIiqniZmZmIjY2V/47fzB0fZlxDSyEhIQwzRERElUxZpohwAjARERFVagwzREREVKkxzBAREVGldsfPmSEiojuXzWaDxWJRugzygFarhVqt9sm+GGaIiKjSEUIgJSUF6enpSpdCXggNDUV0dLTX14FTNMzs2bMHb7zxBg4fPozLly9j/fr1GDx4sNs2p06dwvTp07F7927Y7XY0a9YMn376KWrXrq1M0UREpDhXkImMjERAQAAvilrJCCGQm5uL1NRUAECNGjW82p+iYSYnJwetWrXC2LFjMXTo0GLrf/vtN3Tr1g1/+9vfMG/ePJhMJpw6dQoGg0GBaomIyB/YbDY5yERERChdDnnIaDQCAFJTUxEZGenVkJOiYSYhIQEJCQmlrp81axb69euH119/XW6rV69eRZRGRER+yjVHJiAgQOFKyFuu39BisXgVZvz2bCa73Y6vvvoKjRo1Qp8+fRAZGYlOnTphw4YNSpdGRER+gENLlZ+vfkO/DTOpqanIzs7Ga6+9hr59+2Lbtm148MEHMWTIEOzevbvU95nNZmRmZrotREREdOfy2zBjt9sBAIMGDcLTTz+N1q1bY8aMGRgwYACWLVtW6vsSExNhMpnkhTeZJCKiO1mPHj0wbdo0xfehJL8NM9WqVYNGo0HTpk3d2ps0aYILFy6U+r6ZM2ciIyNDXpKTk8u7VCIioluSJOmmy5gxYzza77p167BgwQLfFlvJ+O11ZnQ6HTp06IDTp0+7tZ85cwZ16tQp9X16vR56vb68y0NWvgUZeRYE6DQID9SV++cREVHldvnyZfn52rVrMXv2bLe/ca6ze1wsFgu0Wu0t9xseHu67IispRXtmsrOzcfToURw9ehQAcO7cORw9elTueXn++eexdu1avP/++zh79iyWLFmCL774AhMnTlSwaoeP9ieh26KdeG3zKaVLISKiSiA6OlpeTCYTJEmSX+fn5yM0NBSffvopevToAYPBgI8//hjXrl3DsGHDUKtWLQQEBKBFixZYvXq1235vHCKqW7cuXn31VTz22GMIDg5G7dq1sXz58tuqNS0tDaNGjUJYWBgCAgKQkJCAX3/9VV6flJSEgQMHIiwsDIGBgWjWrBk2bdokv3f48OGoXr06jEYjGjZsiBUrVnh+4MpA0Z6ZH374AT179pRfP/PMMwCA0aNHY+XKlXjwwQexbNkyJCYmYurUqWjcuDE+//xzdOvWTamSZa4J2EIoWwcRETkuwpZnsSny2Uat2mdn5UyfPh1vvfUWVqxYAb1ej/z8fLRr1w7Tp09HSEgIvvrqK4wcORL16tVDp06dSt3PW2+9hQULFuDFF1/Ef//7Xzz55JO45557EB8fX6Y6xowZg19//RUbN25ESEgIpk+fjn79+uHkyZPQarWYNGkSCgoKsGfPHgQGBuLkyZMICgoCALz88ss4efIkNm/ejGrVquHs2bPIy8vzyfEpjaJhpkePHhC3SAOPPfYYHnvssQqqqOxUzn+4zDJERMrLs9jQdPZWRT775Pw+CND55s/ptGnTMGTIELe25557Tn4+ZcoUbNmyBZ999tlNw0y/fv3kUYzp06fjnXfewa5du8oUZlwh5rvvvkOXLl0AAJ988gliY2OxYcMG/OUvf8GFCxcwdOhQtGjRAoD7NeAuXLiANm3aoH379gAcPUXlzW8nAPs7Vwa3s2uGiIh8xBUAXGw2GxYuXIiWLVsiIiICQUFB2LZt201PhAGAli1bys9dw1muWwfcyqlTp6DRaNzCUkREBBo3boxTpxxTK6ZOnYpXXnkFXbt2xZw5c/DTTz/J2z755JNYs2YNWrdujRdeeAH79u0r0+d6w28nAPs7lTzOpGwdRETkGOo5Ob+PYp/tK4GBgW6v33rrLbzzzjtYvHgxWrRogcDAQEybNg0FBQU33c+NE4clSZIveXIrpY2YCCHk4bTHH38cffr0wVdffYVt27YhMTERb731FqZMmYKEhAQkJSXhq6++wo4dO9CrVy9MmjQJb775Zpk+3xPsmfGQK8uwZ4aISHmSJCFAp1FkKc8rEe/duxeDBg3CiBEj0KpVK9SrV89tIm55aNq0KaxWK77//nu57dq1azhz5gyaNGkit8XGxmLChAlYt24dnn32Wbz//vvyuurVq2PMmDH4+OOPsXjx4tuegHy72DPjIYlzZoiIqJw1aNAAn3/+Ofbt24ewsDC8/fbbSElJcQsVvtawYUMMGjQI48aNw7/+9S8EBwdjxowZqFmzJgYNGgTAMbcnISEBjRo1QlpaGr755hu5ptmzZ6Ndu3Zo1qwZzGYzvvzyy3KtF2DPjMcK58woWgYREd3BXn75ZbRt2xZ9+vRBjx49EB0djcGDB5f7565YsQLt2rXDgAED0LlzZwghsGnTJnn4ymazYdKkSWjSpAn69u2Lxo0b47333gPguE7czJkz0bJlS9xzzz1Qq9VYs2ZNudYriVudTlTJZWZmwmQyISMjAyEhIT7b74rvzmHeFycxoGUNLHm0rc/2S0REN5efn49z584hLi4OBoNB6XLICzf7LW/n7zd7ZjzEU7OJiIj8A8OMhwovmsc4Q0REpCSGGQ/JE4CZZYiIiBTFMOMhXjSPiIjIPzDMeIj3ZiIiIvIPDDMe4gRgIiIi/8Aw4yHXMBMnABMRESmLYcZDKk4AJiIi8gsMM57ivZmIiIj8AsOMh+RhJkWrICIiKjtJkrBhwwaly/A5hhkPcZiJiIhuhyRJN13GjBnj8b7r1q2LxYsX+6zWyoZ3zfaQxGEmIiK6DZcvX5afr127FrNnz8bp06flNqPRqERZdwT2zHjI1TNDRERUFtHR0fJiMpkgSZJb2549e9CuXTsYDAbUq1cP8+bNg9Vqld8/d+5c1K5dG3q9HjExMZg6dSoAoEePHkhKSsLTTz8t9/KU1fHjx3HvvffCaDQiIiICTzzxBLKzs+X1u3btQseOHREYGIjQ0FB07doVSUlJAIBjx46hZ8+eCA4ORkhICNq1a4cffvjBR0fr9rBnxkPsmSEi8iNCAJZcZT5bG1D4R8FDW7duxYgRI/Duu+/i7rvvxm+//YYnnngCADBnzhz897//xTvvvIM1a9agWbNmSElJwbFjxwAA69atQ6tWrfDEE09g3LhxZf7M3Nxc9O3bF3fddRcOHTqE1NRUPP7445g8eTJWrlwJq9WKwYMHY9y4cVi9ejUKCgpw8OBBOSwNHz4cbdq0wdKlS6FWq3H06FFotVqvjoOnGGY8xHszERH5EUsu8GqMMp/94iVAF+jVLhYuXIgZM2Zg9OjRAIB69ephwYIFeOGFFzBnzhxcuHAB0dHRuO+++6DValG7dm107NgRABAeHg61Wo3g4GBER0eX+TM/+eQT5OXl4aOPPkJgoKP+JUuWYODAgVi0aBG0Wi0yMjIwYMAA1K9fHwDQpEkT+f0XLlzA888/j/j4eABAw4YNvToG3uAwk4d4byYiIvKVw4cPY/78+QgKCpKXcePG4fLly8jNzcVf/vIX5OXloV69ehg3bhzWr1/vNgTliVOnTqFVq1ZykAGArl27wm634/Tp0wgPD8eYMWPQp08fDBw4EH//+9/d5v0888wzePzxx3Hffffhtddew2+//eZVPd5gz4yHeG8mIiI/og1w9JAo9dlestvtmDdvHoYMGVJsncFgQGxsLE6fPo3t27djx44dmDhxIt544w3s3r3b46EdIUSp82tc7StWrMDUqVOxZcsWrF27Fi+99BK2b9+Ou+66C3PnzsWjjz6Kr776Cps3b8acOXOwZs0aPPjggx7V4w2GGQ/x3kxERH5Ekrwe6lFS27Ztcfr0aTRo0KDUbYxGIx544AE88MADmDRpEuLj43H8+HG0bdsWOp0ONpvttj6zadOm+PDDD5GTkyP3znz33XdQqVRo1KiRvF2bNm3Qpk0bzJw5E507d8aqVatw1113AQAaNWqERo0a4emnn8awYcOwYsUKRcIMh5k8xHszERGRr8yePRsfffQR5s6dixMnTuDUqVNyTwgArFy5Eh988AF+/vln/P777/i///s/GI1G1KlTB4DjOjN79uzBH3/8gatXr5bpM4cPHw6DwYDRo0fj559/xs6dOzFlyhSMHDkSUVFROHfuHGbOnIn9+/cjKSkJ27Ztw5kzZ9CkSRPk5eVh8uTJ2LVrF5KSkvDdd9/h0KFDbnNqKhLDjIc4AZiIiHylT58++PLLL7F9+3Z06NABd911F95++205rISGhuL9999H165d0bJlS3z99df44osvEBERAQCYP38+zp8/j/r166N69epl+syAgABs3boV169fR4cOHfDQQw+hV69eWLJkibz+l19+wdChQ9GoUSM88cQTmDx5MsaPHw+1Wo1r165h1KhRaNSoER5++GEkJCRg3rx55XOAbkESd3jXQmZmJkwmEzIyMhASEuKz/W49kYLx/3cYbWuHYt3Erj7bLxER3Vx+fj7OnTuHuLg4GAwGpcshL9zst7ydv9/smfEQ781ERETkHxhmPMR7MxEREfkHhhkPFZ6azTRDRESkJIYZD/HUbCIiIv/AMOMp3puJiEhR7Bmv/Hz1GzLMeKjwOjOKlkFEVOW4rnibm6vQjSXJZ1y/obc3qOQVgD3ECcBERMpQq9UIDQ1FamoqAMf1UEq7LD/5JyEEcnNzkZqaitDQUKjVaq/2p2iY2bNnD9544w0cPnwYly9fxvr16zF48OAStx0/fjyWL1+Od955B9OmTavQOksicZiJiEgxrrtDuwINVU6hoaG3dafv0igaZnJyctCqVSuMHTsWQ4cOLXW7DRs24Pvvv0dMjEK3dy+Biv8vgIhIMZIkoUaNGoiMjITFYlG6HPKAVqv1ukfGRdEwk5CQgISEhJtu88cff2Dy5MnYunUr+vfvX0GV3ZoryrBnhohIOWq12md/EKny8us5M3a7HSNHjsTzzz+PZs2alek9ZrMZZrNZfp2ZmVk+xcnXmSmf3RMREVHZ+PXZTIsWLYJGo8HUqVPL/J7ExESYTCZ5iY2NLZfaeJ0ZIiIi/+C3Yebw4cP4+9//jpUrV97WLPWZM2ciIyNDXpKTk8ulPg4zERER+Qe/DTN79+5FamoqateuDY1GA41Gg6SkJDz77LOoW7duqe/T6/UICQlxW8qDSuUaZyqX3RMREVEZ+e2cmZEjR+K+++5za+vTpw9GjhyJsWPHKlRVIfbMEBER+QdFw0x2djbOnj0rvz537hyOHj2K8PBw1K5dGxEREW7ba7VaREdHo3HjxhVdajES58wQERH5BUXDzA8//ICePXvKr5955hkAwOjRo7Fy5UqFqiobXjSPiIjIPygaZnr06HFbN5k6f/58+RVzm3hvJiIiIv/gtxOA/R3vzUREROQfGGY8JMkXzWOaISIiUhLDjId40TwiIiL/wDDjJU4AJiIiUhbDjIck3puJiIjILzDMeIjDTERERP6BYcZDnABMRETkHxhmPMRTs4mIiPwDw4yHeG8mIiIi/8Aw4yGJN80mIiLyCwwzHpI4zEREROQXGGY8xGEmIiIi/8Aw4yEVx5mIiIj8AsOMh1xZhj0zREREymKY8ZAEXjSPiIjIHzDMeIg9M0RERP6BYcZDvDcTERGRf2CY8RDvzUREROQfGGY8xHszERER+QeGGQ/x3kxERET+gWHGQ7xoHhERkX9gmPEUr5lHRETkFxhmPMRhJiIiIv/AMOMhqchzTgImIiJSDsOMh+R7M4G9M0REREpimPFQkSzDScBEREQKYpjxkFRkoIlRhoiISDkMMx6Sihw5dswQEREph2HGQ0UnAHOYiYiISDkMMx4qOgGYiIiIlMMw4yFVylGMUm9FF9XP7JkhIiJSEMOMhzTndmG+9kM8oNrHOTNEREQKYpjxlHMGsAqCZzMREREpSNEws2fPHgwcOBAxMTGQJAkbNmyQ11ksFkyfPh0tWrRAYGAgYmJiMGrUKFy6dEm5gouQVI45MypJcJiJiIhIQYqGmZycHLRq1QpLliwpti43NxdHjhzByy+/jCNHjmDdunU4c+YMHnjgAQUqLU6S1I5H2CHsChdDRERUhWmU/PCEhAQkJCSUuM5kMmH79u1ubf/4xz/QsWNHXLhwAbVr166IEkslqYoOM7FnhoiISCmKhpnblZGRAUmSEBoaWuo2ZrMZZrNZfp2ZmVkutUhF5szYmWWIiIgUU2kmAOfn52PGjBl49NFHERISUup2iYmJMJlM8hIbG1su9UgqxzCTCnbOmSEiIlJQpQgzFosFf/3rX2G32/Hee+/ddNuZM2ciIyNDXpKTk8unKGfPjAROACYiIlKS3w8zWSwWPPzwwzh37hy++eabm/bKAIBer4dery//wpxXAFZB8DozRERECvLrMOMKMr/++it27tyJiIgIpUsq5DZnhmmGiIhIKYqGmezsbJw9e1Z+fe7cORw9ehTh4eGIiYnBQw89hCNHjuDLL7+EzWZDSkoKACA8PBw6nU6psp1cPTN2TgAmIiJSkKJh5ocffkDPnj3l18888wwAYPTo0Zg7dy42btwIAGjdurXb+3bu3IkePXpUVJklKzJnRrBnhoiISDGKhpkePXrcNAj4dUhwCzMK10JERFSFVYqzmfwS58wQERH5BYYZT/GieURERH6BYcZTcpjhRfOIiIiUxDDjKU4AJiIi8gsMM54qctE8DjMREREph2HGU5wATERE5BcYZjzlCjOSHXa7wrUQERFVYQwznuKNJomIiPwCw4ynigwzMcsQEREph2HGUzw1m4iIyC8wzHiKE4CJiIj8AsOMp5ynZks8NZuIiEhRDDOeKjLMxIvmERERKYdhxlNyzwzYM0NERKQghhlPcc4MERGRX2CY8ZR8nRmezURERKQkhhlP8TozREREfoFhxlMcZiIiIvILDDOecjubSeFaiIiIqjCGGU/x3kxERER+gWHGU5wzQ0RE5BcYZjzFezMRERH5BYYZTzkvmqeSeDsDIiIiJTHMeIpzZoiIiPwCw4yn3ObMMMwQEREphWHGY85hJtg5zERERKQghhlPycNM4DATERGRghhmPOV2byaFayEiIqrCGGY8xTkzREREfoFhxlO8NxMREZFfYJjxVNGL5tkVroWIiKgKY5jxlPOiebzODBERkbIYZjzFezMRERH5BUXDzJ49ezBw4EDExMRAkiRs2LDBbb0QAnPnzkVMTAyMRiN69OiBEydOKFPsjThnhoiIyC8oGmZycnLQqlUrLFmypMT1r7/+Ot5++20sWbIEhw4dQnR0NO6//35kZWVVcKUlcLvRpMK1EBERVWEaJT88ISEBCQkJJa4TQmDx4sWYNWsWhgwZAgD48MMPERUVhVWrVmH8+PEVWWpxvDcTERGRX/DbOTPnzp1DSkoKevfuLbfp9Xp0794d+/btK/V9ZrMZmZmZbku54HVmiIiI/ILfhpmUlBQAQFRUlFt7VFSUvK4kiYmJMJlM8hIbG1s+BXKYiYiIyC/4bZhxkZynQLsIIYq1FTVz5kxkZGTIS3JycnkVBgBQSxxmIiIiUpKic2ZuJjo6GoCjh6ZGjRpye2pqarHemqL0ej30en251+fqmQEAwa4ZIiIixfhtz0xcXByio6Oxfft2ua2goAC7d+9Gly5dFKzMqUiYsQteApiIiEgpivbMZGdn4+zZs/Lrc+fO4ejRowgPD0ft2rUxbdo0vPrqq2jYsCEaNmyIV199FQEBAXj00UcVrNqp6FAX72dARESkGEXDzA8//ICePXvKr5955hkAwOjRo7Fy5Uq88MILyMvLw8SJE5GWloZOnTph27ZtCA4OVqrkQkWHmdgzQ0REpBhJ3OHnFWdmZsJkMiEjIwMhISG+27E5C0isBQBYfvd+PNGrqe/2TUREVMXdzt9vv50z4/fcemZsChZCRERUtTHMeKpImAGHmYiIiBTDMOMpt1OzGWaIiIiUwjDjKQ4zERER+QWGGU+xZ4aIiMgvMMx4inNmiIiI/ALDjKckCQKOC+cJG4eZiIiIlMIw4wW78/BxzgwREZFyGGa8IJxDTZwzQ0REpByGGS+4wgzs7JkhIiJSCsOMF+xQOx45zERERKQYhhlvuO6czQnAREREimGY8YKQ1M4nnDNDRESkFIYZL/BsJiIiIuUxzHhDPpuJYYaIiEgpDDNeKDybicNMRERESmGY8YJgzwwREZHiGGa8IFyHj3NmiIiIFMMw4wWezURERKQ8hhlvcJiJiIhIcQwzXpAnAHOYiYiISDEehZnk5GRcvHhRfn3w4EFMmzYNy5cv91lhlYE8zGQXyhZCRERUhXkUZh599FHs3LkTAJCSkoL7778fBw8exIsvvoj58+f7tEC/xp4ZIiIixXkUZn7++Wd07NgRAPDpp5+iefPm2LdvH1atWoWVK1f6sj6/xlOziYiIlOdRmLFYLNDr9QCAHTt24IEHHgAAxMfH4/Lly76rzt85w4zEnhkiIiLFeBRmmjVrhmXLlmHv3r3Yvn07+vbtCwC4dOkSIiIifFqgX3POmRE8NZuIiEgxHoWZRYsW4V//+hd69OiBYcOGoVWrVgCAjRs3ysNPVQFvZ0BERKQ8jSdv6tGjB65evYrMzEyEhYXJ7U888QQCAgJ8Vpzf4zATERGR4jzqmcnLy4PZbJaDTFJSEhYvXozTp08jMjLSpwX6NZXr1GyGGSIiIqV4FGYGDRqEjz76CACQnp6OTp064a233sLgwYOxdOlSnxboz+RhJnCYiYiISCkehZkjR47g7rvvBgD897//RVRUFJKSkvDRRx/h3Xff9WmBfk2+zgzDDBERkVI8CjO5ubkIDg4GAGzbtg1DhgyBSqXCXXfdhaSkJJ8W6NfkKwAzzBARESnFozDToEEDbNiwAcnJydi6dSt69+4NAEhNTUVISIhPC/RnEicAExERKc6jMDN79mw899xzqFu3Ljp27IjOnTsDcPTStGnTxmfFWa1WvPTSS4iLi4PRaES9evUwf/582P2kJ0S4JgBzmImIiEgxHp2a/dBDD6Fbt264fPmyfI0ZAOjVqxcefPBBnxW3aNEiLFu2DB9++CGaNWuGH374AWPHjoXJZMJTTz3ls8/xmMQwQ0REpDSPwgwAREdHIzo6GhcvXoQkSahZs6bPL5i3f/9+DBo0CP379wcA1K1bF6tXr8YPP/zg08/xFIeZiIiIlOfRMJPdbsf8+fNhMplQp04d1K5dG6GhoViwYIFPh4C6deuGr7/+GmfOnAEAHDt2DN9++y369etX6nvMZjMyMzPdlvIiVK4ww54ZIiIipXjUMzNr1ix88MEHeO2119C1a1cIIfDdd99h7ty5yM/Px8KFC31S3PTp05GRkYH4+Hio1WrYbDYsXLgQw4YNK/U9iYmJmDdvnk8+/1YkDjMREREpzqMw8+GHH+Lf//63fLdsAGjVqhVq1qyJiRMn+izMrF27Fh9//DFWrVqFZs2a4ejRo5g2bRpiYmIwevToEt8zc+ZMPPPMM/LrzMxMxMbG+qSeYlQcZiIiIlKaR2Hm+vXriI+PL9YeHx+P69eve12Uy/PPP48ZM2bgr3/9KwCgRYsWSEpKQmJiYqlhRq/XQ6/X+6yGm5J7ZkTFfB4REREV49GcmVatWmHJkiXF2pcsWYKWLVt6XZRLbm4uVCr3EtVqtd+cms2eGSIiIuV51DPz+uuvo3///tixYwc6d+4MSZKwb98+JCcnY9OmTT4rbuDAgVi4cCFq166NZs2a4ccff8Tbb7+Nxx57zGef4Q3XnBmGGSIiIuV41DPTvXt3nDlzBg8++CDS09Nx/fp1DBkyBCdOnMCKFSt8Vtw//vEPPPTQQ5g4cSKaNGmC5557DuPHj8eCBQt89hlecV00DxxmIiIiUookhO8mfBw7dgxt27aFzeY/PRWZmZkwmUzIyMjw+a0W0lc/gdDTa7FENRyTZ7/n030TERFVZbfz99ujnhlyUnGYiYiISGkMM16QrwAMP5mQTEREVAUxzHhD7plhmCEiIlLKbZ3NNGTIkJuuT09P96aWSkdyhhkVh5mIiIgUc1thxmQy3XL9qFGjvCqoMpHYM0NERKS42wozvjzt+o6gdhw+hhkiIiLlcM6MF1QqR5hRgcNMRERESmGY8YLk7JlRCRt8eLkeIiIiug0MM16QnD0zathhZ5YhIiJSBMOMFyS1YwKwBjZY/eXml0RERFUMw4wXVOoiPTPMMkRERIpgmPGCPMwk2dkzQ0REpBCGGS8U9szYYOOkGSIiIkUwzHih6DATwwwREZEyGGa8UHg2E3tmiIiIlMIw4w2V62wmO6wMM0RERIpgmPGGfAVgDjMREREphWHGG1LhdWYYZoiIiJTBMOONIlcA5jATERGRMhhmvOGcM8OzmYiIiJTDMOMN1wRgicNMRERESmGY8QYnABMRESmOYcYbzjDjODWbtzMgIiJSAsOMNyTXnBkb7II9M0REREpgmPFGkQnAVhvDDBERkRIYZryh4r2ZiIiIlMYw4w1V4UXzeJ0ZIiIiZTDMeKPo2UycM0NERKQIhhlvFL2dAefMEBERKYJhxhuuCcASb2dARESkFIYZbxSZAMxTs4mIiJTBMOMNVeF1ZtgzQ0REpAyGGW+4nZrNKwATEREpwe/DzB9//IERI0YgIiICAQEBaN26NQ4fPqx0WQ5FbmdgY5YhIiJShEbpAm4mLS0NXbt2Rc+ePbF582ZERkbit99+Q2hoqNKlOUiOLKiGjT0zRERECvHrMLNo0SLExsZixYoVclvdunWVK+hGbjea5JwZIiIiJfj1MNPGjRvRvn17/OUvf0FkZCTatGmD999/X+myCslzZmy8nQEREZFC/DrM/P7771i6dCkaNmyIrVu3YsKECZg6dSo++uijUt9jNpuRmZnptpQbtRYAoJHssFg5zERERKQEvx5mstvtaN++PV599VUAQJs2bXDixAksXboUo0aNKvE9iYmJmDdvXsUUqCo8fHZrQcV8JhEREbnx656ZGjVqoGnTpm5tTZo0wYULF0p9z8yZM5GRkSEvycnJ5Vegs2cGAOw2S/l9DhEREZXKr3tmunbtitOnT7u1nTlzBnXq1Cn1PXq9Hnq9vrxLc1AVCTPsmSEiIlKEX/fMPP300zhw4ABeffVVnD17FqtWrcLy5csxadIkpUtzKNozY2XPDBERkRL8Osx06NAB69evx+rVq9G8eXMsWLAAixcvxvDhw5UuzUGSYIfjlgZ2G3tmiIiIlODXw0wAMGDAAAwYMEDpMkplkzRQCRsEe2aIiIgU4dc9M5WB3XlGk+AEYCIiIkUwzHjJLjnCDM9mIiIiUgbDjJeExJ4ZIiIiJTHMeMmuckwABufMEBERKYJhxktyz4ydYYaIiEgJDDNeEq5bGnCYiYiISBEMM14SzqsAC5tV4UqIiIiqJoYZL8k9M3ZeNI+IiEgJDDNecvXMgD0zREREimCY8VJhzwznzBARESmBYcZbzp4Zyc6eGSIiIiUwzHhL7eiZkXg2ExERkSIYZrzlOpuJPTNERESKYJjxljzMxJ4ZIiIiJTDMeEvtCDMqwZ4ZIiIiJTDMeElSs2eGiIhISQwz3tLoAAAqTgAmIiJSBMOMl1QaveNR8ArARERESmCY8ZKkdYQZNW9nQEREpAiGGS+p5DDDYSYiIiIlMMx4SaUxAAA0wgK7XShcDRERUdXDMOMltdYxAVgHKwpsdoWrISIiqnoYZryk1jp6ZnSwwGxlmCEiIqpoDDNeUjvnzOgkKwoYZoiIiCocw4yXJOep2VoOMxERESmCYcZbzjCjg4U9M0RERApgmPGWusgEYIYZIiKiCscw4y1nmNFL7JkhIiJSAsOMt9zmzNgULoaIiKjqYZjxlto1Z8bKU7OJiIgUwDDjLY1rzgyHmYiIiJTAMOMtTgAmIiJSFMOMt5xzZvQSrwBMRESkhEoVZhITEyFJEqZNm6Z0KYW0AQAAAwqQb+EEYCIioopWacLMoUOHsHz5crRs2VLpUtw575rNMENERKSMShFmsrOzMXz4cLz//vsICwtTuhx3WiMAwCgVIL+AYYaIiKiiVYowM2nSJPTv3x/33Xef0qUU5wwzAFBgzlWwECIioqpJo3QBt7JmzRocOXIEhw4dKtP2ZrMZZrNZfp2ZmVlepTloCsOMrYBhhoiIqKL5dc9McnIynnrqKXz88ccwGAxlek9iYiJMJpO8xMbGlm+Rag1skhoAYDXnle9nERERUTF+HWYOHz6M1NRUtGvXDhqNBhqNBrt378a7774LjUYDWwm3D5g5cyYyMjLkJTk5udzrtKkcQctewDBDRERU0fx6mKlXr144fvy4W9vYsWMRHx+P6dOnQ61WF3uPXq+HXq+vqBIBAFa1ATpbDuwcZiIiIqpwfh1mgoOD0bx5c7e2wMBAREREFGtXkt15fyb2zBAREVU8vx5mqizsaud8HivDDBERUUXz656ZkuzatUvpEoqxO89oEuyZISIiqnDsmfEFDXtmiIiIlMIw4wu6IACAZMlRuBAiIqKqh2HGByS9I8yoLdkKV0JERFT1MMz4gMrgCDNaK0/NJiIiqmgMMz6gNoQAALS2XAghFK6GiIioamGY8QGtMRgAEIB85PLO2URERBWKYcYHNM4wE4Q8ZJutCldDRERUtTDM+ICkd/bMSPkMM0RERBWMYcYXdIEAgCDkI4dhhoiIqEIxzPiC8zozQVIeMvMYZoiIiCoSw4wvGEwAgBDkIj2vQOFiiIiIqhaGGV8whgIATFIO0nMtytZCRERUxTDM+IIhFABgQg4y8hhmiIiIKhLDjC84e2b0kgXZ2ZnK1kJERFTFMMz4gi4YduehLMhKU7gYIiKiqoVhxhdUKli0jlsaFGRfV7gYIiKiqoVhxkes+lDHY841ZQshIiKqYhhmfCWwGgBAlXNF4UKIiIiqFoYZH1EFRwEAdPlXYbfzztlEREQVhWHGR3SmaABAGNKRlssL5xEREVUUhhkfUTt7ZqojA5fS8xWuhoiIqOpgmPGVoEgAQJSUhuS0XIWLISIiqjoYZnwlNBYAECNdw4XrDDNEREQVhWHGV0y1AQC1pCs4fyVb4WKIiIiqDoYZX3H2zARJ+bh4+bLCxRAREVUdDDO+ojXCGug4o8maegY2np5NRERUIRhmfEgdGQ8AiLUn43RKlsLVEBERVQ0MMz4kOcNMvJSMfb9dVbgaIiKiqoFhxpdqtgMAtFedxsZjlxQuhoiIqGpgmPGlOl0BAM2lczh38TKHmoiIiCoAw4wvmWoC4fWglgTaq05j4aZTEIITgYmIiMoTw4yvxd0DAHhA8z32nLmC+V+ehMVmV7goIiKiOxfDjK+1GQUAeECzH9WQgRXfnUf/d/fisx+SkWO2KlwcERHRnUcSd/g4SGZmJkwmEzIyMhASElIxH/rv+4CLh3Cx1gAMuDQa6bkWAIBeo8I9jaqjT7No3NOwGiJDDBVTDxERUSVzO3+//bpnJjExER06dEBwcDAiIyMxePBgnD59Wumybq3Pq4CkRq2LX+JAh28xvXd91IkIgNlqx/aTf+K5z46h46tfo887e/DKlyex+8wV5BXYlK6aiIioUvLrnpm+ffvir3/9Kzp06ACr1YpZs2bh+PHjOHnyJAIDA8u0D0V6ZgBg/z+BrS86nke3hOg+HaeCu2LLyVTsOnMFx//IQNEjr1Or0L5uGLrUj8Bd9SLQslYodBq/zppERETl5nb+fvt1mLnRlStXEBkZid27d+Oee+4p03sUCzMAcGwNsPkFID/D8TqkJtB0MNBkIK6Ht8a+c2nYe+Yq9v56BZcy8t3eatCq0K5OGO6Ki8Bd9SPQspYJeo26YusnIiJSyB0bZs6ePYuGDRvi+PHjaN68eYnbmM1mmM1m+XVmZiZiY2OVCTMAkH0F2P8P4Mj/AXnXC9sDI4EGvYD690LU64Hf8wLw3dmr+P736zjw+zVcyylw241e4ww39SLQMS4crWqFwqhjuCEiojvTHRlmhBAYNGgQ0tLSsHfv3lK3mzt3LubNm1esXbEw42LJB87uAE59AZzZXNhb4xLdEqh/L9CgF0Stjjh73YIDv1/Dgd+v4/tz13A12z3caFQSmsaEoG3tMLStE4Z2dcIQYzJAkqQK/FJERETl444MM5MmTcJXX32Fb7/9FrVq1Sp1O7/rmSmJzQIk7QN++wb47Wsg5bj7eo0BqNUBqHs3ULcbRM12+C3Ngv3OXpsfzl/Hn5nmYruNDjGgXZ3CcNO0Rgjn3RARUaV0x4WZKVOmYMOGDdizZw/i4uJu672Kzpkpq+xU4PddwNmvHQEnJ9V9vRxuusnh5lIOcDgpDUeS0nDkQhpOXMqEze7+U+o0KjStEYJWtUxoUSsUrWqZUK96ENQq9t4QEZF/u2PCjBACU6ZMwfr167Fr1y40bNjwtvdRKcJMUUIAV38Fzu8Fzn/rWG4MN2q946aWsR2B2E5AbEfkakPx08UMOeAcvpAmX9+mqECdGs1qmtCypgktYx0Bp3Z4AIeniIjIr9wxYWbixIlYtWoV/ve//6Fx48Zyu8lkgtFoLNM+Kl2YuZEQwLWz7uEm+8/i24XXl4MNYjtBVG+Mc9fycPyPDPx0MQPHL2bg50sZyC3hejYmoxYtaprQpEYwmtQIQdOYENSvHgStmkNURESkjDsmzJTWW7BixQqMGTOmTPuo9GHmRq5wk/y9czkIXPml+HZ6E1CrvaMHp2ZbIKYNbIFR+O1KNo4lp+P4Hxk4djEDpy5loqCEe0fp1Co0iAxC05gQR8BxLqYAbQV8SSIiqurumDDjC3dcmClJ7nXgj8OFAefiYcCSU3y74Bggpg1Qsw0Q4wg4BbpQnPkzCycuZeDU5SycvJSJU5czkVXKfaRiTAY0qRGC+BrBaBgZjIZRQahfPQgGLU8TJyIi32GYKaJKhJkb2axA6glHr80fR4BLPzp7b0r4qcPqOgJOjdZAdAsgugVEYHVcTMvDycuZcrg5lZKJ5Ot5JX6cSgJqhwegYVQwGkYGoVFUMBpEBqFBJEMOERF5hmGmiCoZZkpizgZSfnIEG1fAuf5bydsGRgLRzR3hJqqF43lEQ2RaBH65nIVTlzNx+s8snP0zG2dSs0qcaAwAkivkRDrCTb3qgahXLRBx1QIRHqjjpGMiIioVw0wRDDM3kZcGXD7mCDcpPwEpPzvm45TUg6PWA5HxhQEnMh6oHg8RGIkrOQWOYPNnFn5Nzcavtwg5ABBi0CCuehDqO8NNXHXnY7VABOg05fediYioUmCYKYJh5jYV5ACppxwX8vvzZ+fjCaAgu+TtDaFA9XigemMgsonjsXo8RFA0ruZY8OufWTjzZxZ+v5qDc1dz8PuVHFzKyMPN/tVFhxhQr3og6lYLRO3wAHmJDQ+AycgJyEREVQHDTBEMMz5gtwPp5x09N3/+7Hi88guQdg4Qxc+EAuA4m6p6YzncIKIBEFEfCK2DfKHG+Ws5OHclRw45ruX6DfekupHJqHULN7XDA1AnwvFYw2SAhqeTExHdERhmimCYKUeWfMew1JVfCpfUX4DrvwOi+PVsAACSGgir4ww3DYDweoXPQ2oiPd+K3509OBeu5eDC9Vznkoer2cVv4VCUWiWhZqgRseFGxJiMiAk1omaYETVDHc9rmAyckExEVEkwzBTBMKMAqxm49htw5ZQj3Fz71RF6rv0GWHJLf5/G4Lj4X0Q9R8gJreM42yqsLmCKRa5dheTreXLASZaDjmMpsJbSS1REtSA9aoYaEOMMODXdHg2cmExE5CcYZopgmPEjQgBZlwuDTdHHtHOAveRr2wAAJBUQUtMRbOSQUxh27MZquJJTgKRrubiYlotL6Xn4Iz0ff6TnOZ6n5SHPUkpvUREGrQpRIQZEhRgQHWJAVIje8dxU2BYZoodewx4eIqLyxDBTBMNMJWGzAulJjiGqq78Caecdr9POA2lJgLXka9zItAFAaG3AVMuxhNQqfG6qCREcgwyLChfTHOHGEXbycKlI4EnNuvkwVlHhgTpEBusRbXIFHMdjtEmP6kEGVA/WIyJIx1tCEBF5iGGmCIaZO4AQjjuLy+HmvCPguAJPxkWUeDr5jQIj5XADU6wz9Lie14RZH46ULAv+zDQjJTMff2bk48/MfMfzzHy5vSzDWS5hAVpUC9KjerAe1YL0RZ7r5LbIYD3CA3WcvExEVATDTBEMM1WAtQDISHYGmz8c4SbzouMx46Kj7VY9O4BjKCsoCgiOBoJrOJ/XKHwdHA0RHI10hODP7AKkuMJOhhl/ZjnCT0pmPq5mm3E1uwA2e9n/pyVJQHiAzi3shAXqEBFY5DFAh4ggx2NogA5qFef2ENGdi2GmCIYZghCO+1e5BRznkukMP1mXSz/N/EYqDRAU7Qw5RYJPUHUgsDoQGAl7QDWkq0JxxazB1WwzrmSZ5ccrzrDjaruWbcZt5B4AjvATatQiPFBXbAkLKLktQKfm5GYiqjQYZopgmKEysduAnCuOUJOV4nz884bXKY5tyjKk5aINcAScoEhn0Kle5HU1IDAStoDqSFeZkGox4mqOxdGzk1WAazkFSMtxPuYW4HqOY8nIK/3KyjejUUkIDdDCZNQiNEDneDRqYXK1udpveB1i0HAIjIgq3O38/eZ144kAQKUu7Gm5GZvFMX9HDjjOJTvVEXRyrgDZV4CcVMCa7zgVPT3JsZRCDSACQISkBgLCAWO44zEgAjCGATERRV6Hw2oIR6YUgmsiGFctBlzPs+F6bgGuZztCjysEXS+yFNjssNoFrmYX4Gp2AYAS7qp+E8F6DUwB2sIwZNQhxKhBiEGLYIMGwc7Hkl4HGTQcEiOicsUwQ3Q71FrnBOKaN99OCMctILJTgZyrjnBTNOjc+Dw/w3GhQVcgugkNgHDn0hCSI/AUCTsIiADCQwFjKGAIhTCYYNaGIBuByBCBSLMH4JotAOkFQHquBel5FmTkWZCRa0F6nqPnJz3X8TrL7DhdPstsRZbZiotpZZh7VIIgvcYZcooHnmCDFiFG56NBI7cF6jQI0msQqFcjUK+BXqPiMBkRlYjDTET+wGoGcq855vbkXgPynI+5aTe8vu58fh0wZ3r3mRqD495axlDAYHI8N5jcXtv0JuSqgpAlBSJDGJFuMyDNasBViw7pBSpk5lmQlW9Fltnx6HqdmW9FVr4F5ts48+uW5aokBOrdA06QXoNAncb53NHmag/QqZ3batze52rjafNE/o3DTESVjUYPhMQ4lrKyFjjufF406MjB57qjtyc/A8hPB/LSC5/nZwIQjmGw7BTHUgo1gGDnUqwytQ7QBwP6kMLH8GDnc8di1QYhXxWIPFUAcmBEljAiUxiQYTMizabHdasB1wtUyDLbkJXvCkKOxxyzFdlmK/ItjkBktQtHD5KHc4ZupNOoEKhTI0CngVGnRoBODaPW8ejWplMjQKspfO5cDFrHdm7tWsf7dBoGJaKKxDBDVFlpdEBwlGO5HXa7o1cnP90RcPLSb3heQgDKSwfMWY7F4pxvYytwhqhrpZcIIMi5VC9tI0kN6IIAXaBzCQCCCl/bNQGwaAJgURlhVhmRBwPyJANyYUCO0CPbrkOmzYBMuw7pVh3SrVpct2qRWSAhx1wYinILbMg2W+XrBBVY7Siw2pGW65tw5Pa9VVKR4KORQ5LRGYIMWjUMGpXzuaqwzfVao75hnQr6G9qMzu05H4mIYYao6lGpHENJxlDP3m+zOuYDucKNvGSW8PrGthsWCMdcIXOGYympXAB65xJ0O3WqdY5ApA0EAgOBUCOgDYBdY4RVrYdVpYdFZUCBpIdZ0qNA0iNf6JAPPXKFFrlCh1yhQ7ZNi2y7Flk2LbKsGmTatMiwapBu0SDDokaOxY7cAhvyCmywOs+xt9qFY/gt3wqg7FeW9oRWLcGgUUN/Q/gpGoj0WjUMGjWMOkco0mlU0GtUzsfC1/pS2kt+rYJOzXlM5B8YZojo9qg13oUhF7vd0ctjzgIKch0BqSDHsVhyCp+7LdmOM8Ruts5W4Ni/rQDIcw7FFaECoHMuPqExAAYDEBwAoTXCrjbApjHAptLBKulhkXSwSFpYJB0KoEUBtDBDBzO0MAst8qFBvl2LPKFBrl2LXLsGuXYNcmxq5NjUyLZpkGXVIMuqQrZVjUyrGllWNczQwQ4VLDYBi80qT9auaDqNCnq1CnqtI9zotWrno8r9sZRwpNOooFNL0KpVjuXG12oVdBoJOrUaWrXkXO9aJznX3/BarYKKPVZVCsMMESlDpSqcX+NL1gJnGMotDDkFOYAlz3ElaEueI/RYnKfOW/IKT6O35BVZcp3tJWxvK9LbYs13LPnpkOCYZ1SutyHVQP4vt1BpINR6CLUOdrUedpUeNpXOGaS0zkUDKzSwQIMCaGFxPRdqFAgNzNA4Hu1qmKFGvk2NfKFBnl2NfLsaeTYV8uxq5NrUyLWrkWtVyfsqgAYWmxoWqwYFZg1yoEW6c/8WqAEoFyjUKkkON3qNqkg4Kj0AucKUVi0VCUyO1xq1BI3K9VwFjUpyLK71KpW8jUYtFbY5t9GoJWiLrFM712mLrFOrJbmNw4e3h2GGiO4sGp1jMYaV32fYbaUHHVdgsuY7zlKz5jsClttr56PNXLyt1G2d7UXuLi/ZrZDsVsCSU74BCoAnSc0uaWFXa2GTNLA7F5ukgQ0a2CQVrNDABjVskiP8WKGBVahghRoWqGERjkerUKNAqJwBzPHcYlfBLNQw21UosKthFo79WaFyhjc1rDbnUqCW9+nYRu38HA3y4VjnWK+BHSpYhQo2qGCDGlaoHG1QO2qFCnZIKO+gJklwBByV5AxA7uFHrSoMRxq1ClqVs02tcgte6iLhSOPcxvFYuJ9i7a7X6lLa3dY72muFGVEnIrBcj8nNMMwQEd0ulbpwwnJFs1mLhKAbg5DZsc6SD9gtzgBU4Hi0FTgu+mgzF3le0voC5zaWwjbrDe+xlfAeqxk3Xh1bJSxQWS3l+4dG5VwqmA0q2CVHwLEXee4KQY4ApJLbXAHJKhwBySZUsDjbHcGqaGgqDFM2oYLVqoLNqi4xYBWGL+fnONfZnfsofJRQINzXudpdz0XRdqGCDVKJ+3K9r2j7Q12bYdqADhX/QzgxzBARVSZqjWNRIkjdjBCOHqtiYccZmOxWZ8ByPtqtjiAkP7rWuba7YZui77VZHJ8lP79xvzfso9hnlLSdc52wFe77JtSwQy3s0MK53e1esU264bGS++nqYwAYZoiIqDKTpMKghQClq/ENu90ZbqzOgON8LEubsBcGKa/biu7/Fm12ZyATduc29iLbFn0UJbTZCr+z2/vsJWxrd9umZZ1SL75QIRhmiIiISqJyjmGptUpXQrfAy1QSERFRpcYwQ0RERJUawwwRERFVagwzREREVKkxzBAREVGlxjBDRERElRrDDBEREVVqlSLMvPfee4iLi4PBYEC7du2wd+9epUsiIiIiP+H3YWbt2rWYNm0aZs2ahR9//BF33303EhIScOHCBaVLIyIiIj8gCSFu944SFapTp05o27Ytli5dKrc1adIEgwcPRmJi4i3fn5mZCZPJhIyMDISEhJRnqUREROQjt/P32697ZgoKCnD48GH07t3brb13797Yt29fie8xm83IzMx0W4iIiOjO5ddh5urVq7DZbIiKinJrj4qKQkpKSonvSUxMhMlkkpfY2NiKKJWIiIgU4tdhxkWS3O+RLoQo1uYyc+ZMZGRkyEtycnJFlEhEREQK8eu7ZlerVg1qtbpYL0xqamqx3hoXvV4PvV5fEeURERGRH/DrMKPT6dCuXTts374dDz74oNy+fft2DBo0qEz7cM1v5twZIiKiysP1d7ss5yn5dZgBgGeeeQYjR45E+/bt0blzZyxfvhwXLlzAhAkTyvT+rKwsAODcGSIiokooKysLJpPpptv4fZh55JFHcO3aNcyfPx+XL19G8+bNsWnTJtSpU6dM74+JiUFycjKCg4NLnWfjqczMTMTGxiI5OZmnfZcjHueKweNcMXicKw6PdcUor+MshEBWVhZiYmJuua3fX2fGn/EaNhWDx7li8DhXDB7nisNjXTH84ThXirOZiIiIiErDMENERESVGsOMF/R6PebMmcNTwcsZj3PF4HGuGDzOFYfHumL4w3HmnBkiIiKq1NgzQ0RERJUawwwRERFVagwzREREVKkxzBAREVGlxjDjoffeew9xcXEwGAxo164d9u7dq3RJlUpiYiI6dOiA4OBgREZGYvDgwTh9+rTbNkIIzJ07FzExMTAajejRowdOnDjhto3ZbMaUKVNQrVo1BAYG4oEHHsDFixcr8qtUGomJiZAkCdOmTZPbeIx9548//sCIESMQERGBgIAAtG7dGocPH5bX81h7z2q14qWXXkJcXByMRiPq1auH+fPnw263y9vwOHtmz549GDhwIGJiYiBJEjZs2OC23lfHNS0tDSNHjoTJZILJZMLIkSORnp7u/RcQdNvWrFkjtFqteP/998XJkyfFU089JQIDA0VSUpLSpVUaffr0EStWrBA///yzOHr0qOjfv7+oXbu2yM7Olrd57bXXRHBwsPj888/F8ePHxSOPPCJq1KghMjMz5W0mTJggatasKbZv3y6OHDkievbsKVq1aiWsVqsSX8tvHTx4UNStW1e0bNlSPPXUU3I7j7FvXL9+XdSpU0eMGTNGfP/99+LcuXNix44d4uzZs/I2PNbee+WVV0RERIT48ssvxblz58Rnn30mgoKCxOLFi+VteJw9s2nTJjFr1izx+eefCwBi/fr1but9dVz79u0rmjdvLvbt2yf27dsnmjdvLgYMGOB1/QwzHujYsaOYMGGCW1t8fLyYMWOGQhVVfqmpqQKA2L17txBCCLvdLqKjo8Vrr70mb5Ofny9MJpNYtmyZEEKI9PR0odVqxZo1a+Rt/vjjD6FSqcSWLVsq9gv4saysLNGwYUOxfft20b17dznM8Bj7zvTp00W3bt1KXc9j7Rv9+/cXjz32mFvbkCFDxIgRI4QQPM6+cmOY8dVxPXnypAAgDhw4IG+zf/9+AUD88ssvXtXMYabbVFBQgMOHD6N3795u7b1798a+ffsUqqryy8jIAACEh4cDAM6dO4eUlBS346zX69G9e3f5OB8+fBgWi8Vtm5iYGDRv3py/RRGTJk1C//79cd9997m18xj7zsaNG9G+fXv85S9/QWRkJNq0aYP3339fXs9j7RvdunXD119/jTNnzgAAjh07hm+//Rb9+vUDwONcXnx1XPfv3w+TyYROnTrJ29x1110wmUxeH3u/v2u2v7l69SpsNhuioqLc2qOiopCSkqJQVZWbEALPPPMMunXrhubNmwOAfCxLOs5JSUnyNjqdDmFhYcW24W/hsGbNGhw5cgSHDh0qto7H2Hd+//13LF26FM888wxefPFFHDx4EFOnToVer8eoUaN4rH1k+vTpyMjIQHx8PNRqNWw2GxYuXIhhw4YB4L/p8uKr45qSkoLIyMhi+4+MjPT62DPMeEiSJLfXQohibVQ2kydPxk8//YRvv/222DpPjjN/C4fk5GQ89dRT2LZtGwwGQ6nb8Rh7z263o3379nj11VcBAG3atMGJEyewdOlSjBo1St6Ox9o7a9euxccff4xVq1ahWbNmOHr0KKZNm4aYmBiMHj1a3o7HuXz44riWtL0vjj2HmW5TtWrVoFari6XI1NTUYqmVbm3KlCnYuHEjdu7ciVq1asnt0dHRAHDT4xwdHY2CggKkpaWVuk1VdvjwYaSmpqJdu3bQaDTQaDTYvXs33n33XWg0GvkY8Rh7r0aNGmjatKlbW5MmTXDhwgUA/PfsK88//zxmzJiBv/71r2jRogVGjhyJp59+GomJiQB4nMuLr45rdHQ0/vzzz2L7v3LlitfHnmHmNul0OrRr1w7bt293a9++fTu6dOmiUFWVjxACkydPxrp16/DNN98gLi7ObX1cXByio6PdjnNBQQF2794tH+d27dpBq9W6bXP58mX8/PPP/C0A9OrVC8ePH8fRo0flpX379hg+fDiOHj2KevXq8Rj7SNeuXYtdWuDMmTOoU6cOAP579pXc3FyoVO5/ttRqtXxqNo9z+fDVce3cuTMyMjJw8OBBeZvvv/8eGRkZ3h97r6YPV1GuU7M/+OADcfLkSTFt2jQRGBgozp8/r3RplcaTTz4pTCaT2LVrl7h8+bK85Obmytu89tprwmQyiXXr1onjx4+LYcOGlXgqYK1atcSOHTvEkSNHxL333lvlT7G8maJnMwnBY+wrBw8eFBqNRixcuFD8+uuv4pNPPhEBAQHi448/lrfhsfbe6NGjRc2aNeVTs9etWyeqVasmXnjhBXkbHmfPZGVliR9//FH8+OOPAoB4++23xY8//ihfcsRXx7Vv376iZcuWYv/+/WL//v2iRYsWPDVbSf/85z9FnTp1hE6nE23btpVPKaayAVDismLFCnkbu90u5syZI6Kjo4Verxf33HOPOH78uNt+8vLyxOTJk0V4eLgwGo1iwIAB4sKFCxX8bSqPG8MMj7HvfPHFF6J58+ZCr9eL+Ph4sXz5crf1PNbey8zMFE899ZSoXbu2MBgMol69emLWrFnCbDbL2/A4e2bnzp0l/jd59OjRQgjfHddr166J4cOHi+DgYBEcHCyGDx8u0tLSvK5fEkII7/p2iIiIiJTDOTNERERUqTHMEBERUaXGMENERESVGsMMERERVWoMM0RERFSpMcwQERFRpcYwQ0RERJUawwwRVQmSJGHDhg1Kl0FE5YBhhojK3ZgxYyBJUrGlb9++SpdGRHcAjdIFEFHV0LdvX6xYscKtTa/XK1QNEd1J2DNDRBVCr9cjOjrabQkLCwPgGAJaunQpEhISYDQaERcXh88++8zt/cePH8e9994Lo9GIiIgIPPHEE8jOznbb5j//+Q+aNWsGvV6PGjVqYPLkyW7rr169igcffBABAQFo2LAhNm7cKK9LS0vD8OHDUb16dRiNRjRs2LBY+CIi/8QwQ0R+4eWXX8bQoUNx7NgxjBgxAsOGDcOpU6cAALm5uejbty/CwsJw6NAhfPbZZ9ixY4dbWFm6dCkmTZqEJ554AsePH8fGjRvRoEEDt8+YN28eHn74Yfz000/o168fhg8fjuvXr8uff/LkSWzevBmnTp3C0qVLUa1atYo7AETkOa9vVUlEdAujR48WarVaBAYGui3z588XQjjuoj5hwgS393Tq1Ek8+eSTQgghli9fLsLCwkR2dra8/quvvhIqlUqkpKQIIYSIiYkRs2bNKrUGAOKll16SX2dnZwtJksTmzZuFEEIMHDhQjB071jdfmIgqFOfMEFGF6NmzJ5YuXerWFh4eLj/v3Lmz27rOnTvj6NGjAIBTp06hVatWCAwMlNd37doVdrsdp0+fhiRJuHTpEnr16nXTGlq2bCk/DwwMRHBwMFJTUwEATz75JIYOHYojR46gd+/eGDx4MLp06eLRdyWiisUwQ0QVIjAwsNiwz61IkgQAEELIz0vaxmg0lml/Wq222HvtdjsAICEhAUlJSfjqq6+wY8cO9OrVC5MmTcKbb755WzUTUcXjnBki8gsHDhwo9jo+Ph4A0LRpUxw9ehQ5OTny+u+++w4qlQqNGjVCcHAw6tati6+//tqrGqpXr44xY8bg448/xuLFi7F8+XKv9kdEFYM9M0RUIcxmM1JSUtzaNBqNPMn2s88+Q/v27dGtWzd88sknOHjwID744AMAwPDhwzFnzhyMHj0ac+fOxZUrVzBlyhSMHDkSUVFRAIC5c+diwoQJiIyMREJCArKysvDdd99hypQpZapv9uzZaNeuHZo1awaz2Ywvv/wSTZo08eERIKLywjBDRBViy5YtqFGjhltb48aN8csvvwBwnGm0Zs0aTJw4EdHR0fjkk0/QtGlTAEBAQAC2bt2Kp556Ch06dEBAQACGDh2Kt99+W97X6NGjkZ+fj3feeQfPPfccqlWrhoceeqjM9el0OsycORPnz5+H0WjE3XffjTVr1vjgmxNReZOEEELpIoioapMkCevXr8fgwYOVLoWIKiHOmSEiIqJKjWGGiIiIKjXOmSEixXG0m4i8wZ4ZIiIiqtQYZoiIiKhSY5ghIiKiSo1hhoiIiCo1hhkiIiKq1BhmiIiIqFJjmCEiIqJKjWGGiIiIKjWGGSIiIqrU/h9mKdhZQZof3QAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the loss curves\n",
    "# Plot the loss curves\n",
    "plt.plot(epoch_count, train_loss_values, label=\"Train loss\")\n",
    "plt.plot(epoch_count, test_loss_values, label=\"Test loss\")\n",
    "plt.title(\"Training and test loss curves\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "e2ece1c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model learned the following values for weights and bias:\n",
      "OrderedDict([('weights', tensor([[-1.5060],\n",
      "        [-2.5791],\n",
      "        [ 1.6318]])), ('bias', tensor([1.8871]))])\n"
     ]
    }
   ],
   "source": [
    "#Find model's learnt parameters\n",
    "# Find our model's learned parameters\n",
    "print(\"The model learned the following values for weights and bias:\")\n",
    "print(model_0.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a4eb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
