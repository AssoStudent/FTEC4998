{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Optimization for Statistical Learning\" Expirement Notebook\n",
    "## Section 0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Data Importing and Preprocessing\n",
    "In this section, we include packages we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install math\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2 Global Classes, Functions and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spliting the DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataloader(dataloader, num_piece):\n",
    "    dataset = list(dataloader)\n",
    "    subset_size = len(dataset) // num_piece\n",
    "    remainder = len(dataset) % num_piece\n",
    "    split_subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_piece):\n",
    "        if i < remainder:\n",
    "            end_idx = start_idx + subset_size + 1\n",
    "        else:\n",
    "            end_idx = start_idx + subset_size\n",
    "        subset = dataset[start_idx:end_idx]\n",
    "        split_subsets.append(subset)\n",
    "        start_idx = end_idx\n",
    "    split_dataloaders = []\n",
    "    for subset in split_subsets:\n",
    "        split_dataloader = torch.utils.data.DataLoader(subset, batch_size=dataloader.batch_size, shuffle=True)\n",
    "        split_dataloaders.append(split_dataloader)\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Rate Analaysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, predict):\n",
    "    num_samples = X.shape[0]\n",
    "    error_count = torch.count_nonzero(torch.round(predict) - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Optimizer Class of Vanilia Gradient Descent**\n",
    "\n",
    "The name custom_optimizer_SGD is just for consistency with torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device Class and Federated Learning Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.num_samples = self.X_train.size()[0]\n",
    "        self.num_features = self.X_train.size()[1]\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def update_weights_GDVanilia(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Update weight\n",
    "            outputs = self.model(self.X_train.float())\n",
    "            loss = self.criterion(outputs, self.y_train.float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights_GDStochastic(self, num_epochs, batch_size):\n",
    "        self.model.train()\n",
    "        num_batches = self.num_samples // batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data for each epoch\n",
    "            permutation = torch.randperm(self.num_samples)\n",
    "            X_shuffled = self.X_train[permutation]\n",
    "            y_shuffled = self.y_train[permutation]\n",
    "            for batch in range(num_batches):\n",
    "                # Select the current batch\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                # Update weight\n",
    "                outputs = self.model(X_batch.float())\n",
    "                loss = self.criterion(outputs, y_batch.float())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, input_shape, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_loader, num_epochs, self.optimizer, self.criterion, input_shape, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, dataset_loader, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], dataset_loader)\n",
    "    return client_device\n",
    "    \n",
    "\n",
    "# Define server wise functions\n",
    "def send_client_weights(server_weights, local_weights):\n",
    "    server_weights.append(local_weights)\n",
    "\n",
    "# Total weight processing functions\n",
    "def Federated_Averaging(client_weights_total):\n",
    "    aggregate_weights = {}\n",
    "    num_clients = len(client_weights_total)\n",
    "    \n",
    "    # Iterate over the parameters of the model\n",
    "    for param_name in client_weights_total[0].keys():\n",
    "        # Initialize the aggregated parameter tensor\n",
    "        aggregated_param = client_weights_total[0][param_name].clone()\n",
    "\n",
    "        # Sum the parameter tensors from all clients\n",
    "        for client_weights in client_weights_total[1:]:\n",
    "            aggregated_param += client_weights[param_name]\n",
    "\n",
    "        # Calculate the average parameter value\n",
    "        aggregated_param /= num_clients\n",
    "\n",
    "        # Assign the averaged parameter to the aggregate_weights dictionary\n",
    "        aggregate_weights[param_name] = aggregated_param\n",
    "\n",
    "    return aggregate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Linear Training Model\n",
    "In this section, we will focus on the dataset requires no complicated data processing, and mainly linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.0. Data Loading and Preprocessing\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BMI Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BMI Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, [0,1,2]]\n",
    "y_train = dataset[:train_split, 3]\n",
    "X_test = dataset[train_split:, [0,1,2]]\n",
    "y_test = dataset[train_split:, 3]\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"BMI Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Pascal Challenge Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Epsilon Pascal Challenge Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized\", sep=' ', header=None, nrows=20000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Preset parameters\n",
    "dataset_name = \"Epsilon Datset\"\n",
    "learning_rate_preset = 0.001\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gisette Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gisette Dataset\n",
    "#2 class, 6000 data points, ~5000 features\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"gisette_scale\", sep=' ', header=None)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-2]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "#print(dataset)\n",
    "#print(dataset.shape)\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "denominator = X_train.max(0) - X_train.min(0)\n",
    "X_train_normalized = (X_train - X_train.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "denominator = X_test.max(0) - X_test.min(0)\n",
    "X_test_normalized = (X_test - X_test.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "\n",
    "print(\"X_train_normalized: \", X_train_normalized)\n",
    "print(\"X_test_normalized: \", X_test_normalized)\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"Gisette Datset\"\n",
    "learning_rate_preset = 0.005\n",
    "batch_size_preset = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "### Section 1.1. Vanilia Gradient Descent and Stochastic Gradient Descent\n",
    "\n",
    "Initially test training dataset with custom algorithms and pytorch package. We will verify our result of our algorithms by comparing the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_epochs):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "\n",
    "        # Record the loss\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "    \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_iterations = 100\n",
    "w, b, loss_history, error_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Stochastic Gradient Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "        \n",
    "        # General Output\n",
    "        y_pred = np.dot(X_train_normalized, w) + b\n",
    "        error = y_pred - y_train\n",
    "\n",
    "        # Record the loss\n",
    "        error = y_pred\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "            \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "w, b, loss_history, error_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "**Pytorch Package for Vanilia Gradient Descent**\n",
    "\n",
    "(It is just a Vanilia Gradient Descent... They call it SGD is confusing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch Package for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = torch.matmul(X_batch.float(), w) + b\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # General Output\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2. Linear Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "\n",
    "This neural network is simply a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features=1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "num_class = len(torch.unique(y_train_tensor))\n",
    "print(num_samples)\n",
    "print(num_features)\n",
    "print(num_class)\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "    \n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward propagation to obtain the predicted output\n",
    "        outputs = model(X_batch.float())\n",
    "    \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "    \n",
    "        # Backward propagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # General Output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3. Fedearted Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.1 Fedearted Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with vanilia gradient descent method\n",
    "def FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDVanilia(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(len(value) for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDStochastic(local_update_epochs, int(batch_size / num_clients))\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(len(value) for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.2 Fedearted Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [1]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_VGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_VGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 100\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [2]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_SGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_SGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "num_clients_list = [5]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [2]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Non-Linear Training Model\n",
    "Here we focus on dataset requires mainly non-linear training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0 Data Loading and Preprocessing\n",
    "\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]\n",
      "\n",
      "\n",
      " [[[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]\n",
      "\n",
      "  [[0.]\n",
      "   [0.]\n",
      "   [0.]\n",
      "   ...\n",
      "   [0.]\n",
      "   [0.]\n",
      "   [0.]]]]\n",
      "[0. 4. 1. 9. 2. 1. 3. 1. 4. 3. 5. 3. 6. 1. 7. 2. 8. 6. 9. 4. 0. 9. 1. 1.\n",
      " 2. 4. 3. 2. 7. 3. 8. 6. 9. 0. 5. 6. 0. 7. 6. 1. 8. 7. 9. 3. 9. 8. 5. 9.\n",
      " 3. 3. 0. 7. 4. 9. 8. 0. 9. 4. 1. 4. 4. 6. 0. 4. 5. 6. 1. 0. 0. 1. 7. 1.\n",
      " 6. 3. 0. 2. 1. 1. 7. 9. 0. 2. 6. 7. 8. 3. 9. 0. 4. 6. 7. 4. 6. 8. 0. 7.\n",
      " 8. 3. 1. 5. 7. 1. 7. 1. 1. 6. 3. 0. 2. 9. 3. 1. 1. 0. 4. 9. 2. 0. 0. 2.\n",
      " 0. 2. 7. 1. 8. 6. 4. 1. 6. 3. 4. 5. 9. 1. 3. 3. 8. 5. 4. 7. 7. 4. 2. 8.\n",
      " 5. 8. 6. 7. 3. 4. 6. 1. 9. 9. 6. 0. 3. 7. 2. 8. 2. 9. 4. 4. 6. 4. 9. 7.\n",
      " 0. 9. 2. 9. 5. 1. 5. 9. 1. 2. 3. 2. 3. 5. 9. 1. 7. 6. 2. 8. 2. 2. 5. 0.\n",
      " 7. 4. 9. 7. 8. 3. 2. 1. 1. 8. 3. 6. 1. 0. 3. 1. 0. 0. 1. 7. 2. 7. 3. 0.\n",
      " 4. 6. 5. 2. 6. 4. 7. 1. 8. 9. 9. 3. 0. 7. 1. 0. 2. 0. 3. 5. 4. 6. 5. 8.\n",
      " 6. 3. 7. 5. 8. 0. 9. 1. 0. 3. 1. 2. 2. 3. 3. 6. 4. 7. 5. 0. 6. 2. 7. 9.\n",
      " 8. 5. 9. 2. 1. 1. 4. 4. 5. 6. 4. 1. 2. 5. 3. 9. 3. 9. 0. 5. 9. 6. 5. 7.\n",
      " 4. 1. 3. 4. 0. 4. 8. 0. 4. 3. 6. 8. 7. 6. 0. 9. 7. 5. 7. 2. 1. 1. 6. 8.\n",
      " 9. 4. 1. 5. 2. 2. 9. 0. 3. 9. 6. 7. 2. 0. 3. 5. 4. 3. 6. 5. 8. 9. 5. 4.\n",
      " 7. 4. 2. 7. 3. 4. 8. 9. 1. 9. 2. 8. 7. 9. 1. 8. 7. 4. 1. 3. 1. 1. 0. 2.\n",
      " 3. 9. 4. 9. 2. 1. 6. 8. 4. 7. 7. 4. 4. 9. 2. 5. 7. 2. 4. 4. 2. 1. 9. 7.\n",
      " 2. 8. 7. 6. 9. 2. 2. 3. 8. 1. 6. 5. 1. 1. 0. 2. 6. 4. 5. 8. 3. 1. 5. 1.\n",
      " 9. 2. 7. 4. 4. 4. 8. 1. 5. 8. 9. 5. 6. 7. 9. 9. 3. 7. 0. 9. 0. 6. 6. 2.\n",
      " 3. 9. 0. 7. 5. 4. 8. 0. 9. 4. 1. 2. 8. 7. 1. 2. 6. 1. 0. 3. 0. 1. 1. 8.\n",
      " 2. 0. 3. 9. 4. 0. 5. 0. 6. 1. 7. 7. 8. 1. 9. 2. 0. 5. 1. 2. 2. 7. 3. 5.\n",
      " 4. 9. 7. 1. 8. 3. 9. 6. 0. 3. 1. 1. 2. 6. 3. 5. 7. 6. 8. 3. 9. 5. 8. 5.\n",
      " 7. 6. 1. 1. 3. 1. 7. 5. 5. 5. 2. 5. 8. 7. 0. 9. 7. 7. 5. 0. 9. 0. 0. 8.\n",
      " 9. 2. 4. 8. 1. 6. 1. 6. 5. 1. 8. 3. 4. 0. 5. 5. 8. 3. 6. 2. 3. 9. 2. 1.\n",
      " 1. 5. 2. 1. 3. 2. 8. 7. 3. 7. 2. 4. 6. 9. 7. 2. 4. 2. 8. 1. 1. 3. 8. 4.\n",
      " 0. 6. 5. 9. 3. 0. 9. 2. 4. 7. 1. 2. 9. 4. 2. 6. 1. 8. 9. 0. 6. 6. 7. 9.\n",
      " 9. 8. 0. 1. 4. 4. 6. 7. 1. 5. 7. 0. 3. 5. 8. 4. 7. 1. 2. 5. 9. 5. 6. 7.\n",
      " 5. 9. 8. 8. 3. 6. 9. 7. 0. 7. 5. 7. 1. 1. 0. 7. 9. 2. 3. 7. 3. 2. 4. 1.\n",
      " 6. 2. 7. 5. 5. 7. 4. 0. 2. 6. 3. 6. 4. 0. 4. 2. 6. 0. 0. 0. 0. 3. 1. 6.\n",
      " 2. 2. 3. 1. 4. 1. 5. 4. 6. 4. 7. 2. 8. 7. 9. 2. 0. 5. 1. 4. 2. 8. 3. 2.\n",
      " 4. 1. 5. 4. 6. 0. 7. 9. 8. 4. 9. 8. 0. 1. 1. 0. 2. 2. 3. 2. 4. 4. 5. 8.\n",
      " 6. 5. 7. 7. 8. 8. 9. 7. 4. 7. 3. 2. 0. 8. 6. 8. 6. 1. 6. 8. 9. 4. 0. 9.\n",
      " 0. 4. 1. 5. 4. 7. 5. 3. 7. 4. 9. 8. 5. 8. 6. 3. 8. 6. 9. 9. 1. 8. 3. 5.\n",
      " 8. 6. 5. 9. 7. 2. 5. 0. 8. 5. 1. 1. 0. 9. 1. 8. 6. 7. 0. 9. 3. 0. 8. 8.\n",
      " 9. 6. 7. 8. 4. 7. 5. 9. 2. 6. 7. 4. 5. 9. 2. 3. 1. 6. 3. 9. 2. 2. 5. 6.\n",
      " 8. 0. 7. 7. 1. 9. 8. 7. 0. 9. 9. 4. 6. 2. 8. 5. 1. 4. 1. 5. 5. 1. 7. 3.\n",
      " 6. 4. 3. 2. 5. 6. 4. 4. 0. 4. 4. 6. 7. 2. 4. 3. 3. 8. 0. 0. 3. 2. 2. 9.\n",
      " 8. 2. 3. 7. 0. 1. 1. 0. 2. 3. 3. 8. 4. 3. 5. 7. 6. 4. 7. 7. 8. 5. 9. 7.\n",
      " 0. 3. 1. 6. 2. 4. 3. 4. 4. 7. 5. 9. 6. 9. 0. 7. 1. 4. 2. 7. 3. 6. 7. 5.\n",
      " 8. 4. 5. 5. 2. 7. 1. 1. 5. 6. 8. 5. 8. 4. 0. 7. 9. 9. 2. 9. 7. 7. 8. 7.\n",
      " 4. 2. 6. 9. 1. 7. 0. 6. 4. 2. 5. 7. 0. 7. 1. 0. 3. 7. 6. 5. 0. 6. 1. 5.\n",
      " 1. 7. 8. 5. 0. 3. 4. 7. 7. 5. 7. 8. 6. 9. 3. 8. 6. 1. 0. 9. 7. 1. 3. 0.\n",
      " 5. 6. 4. 4. 2. 4. 4. 3. 1. 7. 7. 6. 0. 3. 6. 0.]\n",
      "torch.Size([1000, 28, 28, 1])\n",
      "torch.Size([1000])\n",
      "torch.Size([1000, 28, 28, 1])\n",
      "torch.Size([1000])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n"
     ]
    }
   ],
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"mnist_train.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_train = dataset[:1000, 1:].astype(np.int32)\n",
    "y_train = dataset[:1000, 0].astype(np.int32)\n",
    "\n",
    "# Loading testing data\n",
    "dataset = pd.read_csv(\"mnist_test.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_test = dataset[:1000, 1:].astype(np.int32)\n",
    "y_test = dataset[:1000, 0].astype(np.int32)\n",
    "\n",
    "# Translation of data  \n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32') \n",
    "y_train = y_train.astype('float32') \n",
    "y_train = y_train.astype('float32') \n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train / 255\n",
    "X_test_normalized = X_test / 255\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"MNIST Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 100\n",
    "\n",
    "# Number of samples, features, and classes\n",
    "num_samples_preset  = X_train_tensor.size()[0]\n",
    "num_features_preset = X_train_tensor.size()[1]\n",
    "num_classes_preset = len(torch.unique(y_train_tensor))\n",
    "\n",
    "# Translate the tensor to dataset\n",
    "MINST_train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "MINST_test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Translate to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(MINST_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(MINST_test_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Non-Linear Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device for CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, train_dataloader, test_dataloader=None):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_dataloader, num_epochs, self.optimizer, self.criterion, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, train_dataloader_list, test_dataloader_list, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], train_dataloader_list[client], test_dataloader_list[client])\n",
    "    return client_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Training and Testing Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_CNN_model(model, dataset_loader, num_epochs, optimizer, criterion, show_history=True, training=True):\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        single_iteration_count = 0.00\n",
    "        batch_iteration_count = 0.00\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "        for i, (images, labels) in enumerate(dataset_loader):\n",
    "            # Define variables\n",
    "            X_batch = Variable(images.view(CNN_input_shape))\n",
    "            y_batch = Variable(labels)\n",
    "\n",
    "            # Forward propagation to obtain the predicted output\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, y_batch.long())\n",
    "            \n",
    "            # Backward propagation and optimization\n",
    "            if training is True:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record the iteration used\n",
    "            single_iteration_count += 1\n",
    "            batch_iteration_count += len(y_batch)\n",
    "\n",
    "            # Record the loss\n",
    "            loss_count += loss.data.detach().numpy()\n",
    "\n",
    "            # Record the error rate\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            error_count += (predicted != y_batch).float().sum()\n",
    "        \n",
    "        # Summarize the loss rate\n",
    "        loss_rate = loss_count / float(single_iteration_count)\n",
    "        loss_history.append(loss_rate)\n",
    "\n",
    "        # Summarize the error rate\n",
    "        train_error_rate = error_count / float(batch_iteration_count)\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the summarized loss and error every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_rate.item():.8f}, Average Error: {train_error_rate:.16f}')\n",
    "\n",
    "    return loss_history, error_history\n",
    "\n",
    "def fit_CNN_model(model, train_loader, test_loader, num_epochs, optimizer, criterion, show_history=True):\n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "       print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history = iterate_CNN_model(model, train_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history = iterate_CNN_model(model, test_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    # Experiment Result\n",
    "    print(\"!-- CNN Model Result --!\")\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "            \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the CNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset: MNIST Datset\n",
      "ConvolutionalNeuralNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "Epoch [1/10], Average Loss: 2.30307088, Average Error: 0.9430000185966492\n",
      "Epoch [10/10], Average Loss: 2.19398568, Average Error: 0.5500000119209290\n",
      "[2.303070878982544, 2.294379782676697, 2.2858983755111693, 2.276969885826111, 2.267872858047485, 2.257234001159668, 2.24512197971344, 2.2309871912002563, 2.2137262344360353, 2.193985676765442]\n",
      "[tensor(0.9430), tensor(0.8460), tensor(0.7520), tensor(0.7890), tensor(0.7270), tensor(0.6850), tensor(0.6190), tensor(0.5620), tensor(0.5690), tensor(0.5500)]\n",
      "!-- CNN Model Result --!\n",
      "conv1.weight: tensor([[[[ 0.0644,  0.1978, -0.0827, -0.1208,  0.0049],\n",
      "          [-0.1832, -0.0873, -0.0678,  0.1847, -0.0521],\n",
      "          [ 0.1757,  0.0159, -0.1590,  0.0775, -0.1402],\n",
      "          [-0.0992,  0.1645, -0.0370,  0.0266,  0.1432],\n",
      "          [ 0.0684,  0.0388,  0.1614,  0.0159, -0.1303]]],\n",
      "\n",
      "\n",
      "        [[[-0.1249,  0.1966, -0.1592, -0.0331,  0.1047],\n",
      "          [ 0.0021, -0.1632,  0.0826,  0.1475,  0.0934],\n",
      "          [ 0.1740, -0.1345, -0.0578,  0.1949,  0.1944],\n",
      "          [-0.0721,  0.1997, -0.1609, -0.0355,  0.1779],\n",
      "          [ 0.1726,  0.0634,  0.1108, -0.1050,  0.0845]]],\n",
      "\n",
      "\n",
      "        [[[-0.0752,  0.0687, -0.0932,  0.1271,  0.1252],\n",
      "          [-0.1722, -0.1067,  0.1262,  0.1832,  0.0816],\n",
      "          [ 0.1207,  0.1752, -0.1879, -0.1608, -0.1267],\n",
      "          [ 0.1122, -0.0928, -0.1362,  0.0946, -0.0591],\n",
      "          [-0.0002,  0.1297, -0.1323,  0.1642,  0.1402]]],\n",
      "\n",
      "\n",
      "        [[[-0.0389,  0.0994,  0.0600,  0.0477,  0.1277],\n",
      "          [ 0.1776, -0.1231, -0.0963,  0.1703, -0.1641],\n",
      "          [-0.1302,  0.0014, -0.1832,  0.1031,  0.1024],\n",
      "          [ 0.1590,  0.0570, -0.0355, -0.1084, -0.1188],\n",
      "          [ 0.1479, -0.0625,  0.0837,  0.0123, -0.0293]]],\n",
      "\n",
      "\n",
      "        [[[-0.1198,  0.0660,  0.0609, -0.1756,  0.1190],\n",
      "          [-0.0754,  0.1123,  0.1231,  0.1335,  0.2031],\n",
      "          [-0.1197,  0.0757,  0.0130, -0.0701, -0.1757],\n",
      "          [-0.0599, -0.0487,  0.0145,  0.0742, -0.1507],\n",
      "          [ 0.0436, -0.0340, -0.1136,  0.1581, -0.0768]]],\n",
      "\n",
      "\n",
      "        [[[-0.0049,  0.1522, -0.0776, -0.0032,  0.1907],\n",
      "          [-0.0147, -0.0425,  0.1136,  0.0655, -0.1847],\n",
      "          [ 0.0822,  0.0974, -0.1728, -0.0209, -0.0185],\n",
      "          [ 0.0378,  0.1311, -0.0759, -0.0449,  0.1721],\n",
      "          [ 0.1809, -0.1108, -0.0314, -0.1696,  0.0132]]],\n",
      "\n",
      "\n",
      "        [[[-0.1319, -0.1967,  0.0533, -0.1948,  0.1076],\n",
      "          [-0.1493, -0.0933, -0.1724, -0.1019,  0.0559],\n",
      "          [-0.0502, -0.1199,  0.1204, -0.1718, -0.0992],\n",
      "          [ 0.1569, -0.0830, -0.1096, -0.0363,  0.1425],\n",
      "          [ 0.0733,  0.1991, -0.1292,  0.0111,  0.2012]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1619, -0.1867,  0.0486, -0.1382,  0.1159],\n",
      "          [ 0.1380, -0.1140, -0.1082,  0.1872,  0.0736],\n",
      "          [-0.0397, -0.1906,  0.1678,  0.1061,  0.0174],\n",
      "          [ 0.1394,  0.1940, -0.1733,  0.0566, -0.1144],\n",
      "          [-0.1908,  0.0965,  0.1783,  0.0540,  0.1503]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1609,  0.1779,  0.0276, -0.1640,  0.1817],\n",
      "          [ 0.0456,  0.1242, -0.1801, -0.1701,  0.1277],\n",
      "          [-0.0722,  0.0839, -0.0658, -0.1328,  0.0584],\n",
      "          [-0.1420,  0.1164,  0.1135, -0.0730, -0.1092],\n",
      "          [-0.1983, -0.1976,  0.0631,  0.1327,  0.1857]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0841, -0.0389, -0.1586, -0.0860, -0.1918],\n",
      "          [-0.1119, -0.0426, -0.0816,  0.0387,  0.1174],\n",
      "          [-0.0663, -0.0450,  0.1104, -0.1096,  0.0726],\n",
      "          [-0.1962,  0.0684, -0.1823, -0.0272, -0.0515],\n",
      "          [ 0.1541, -0.0613,  0.1326, -0.1673,  0.1871]]],\n",
      "\n",
      "\n",
      "        [[[-0.1409, -0.0882,  0.0686, -0.0635,  0.0569],\n",
      "          [-0.0541, -0.1127,  0.1477,  0.1290, -0.1834],\n",
      "          [ 0.0718, -0.1218,  0.0243,  0.1433,  0.0688],\n",
      "          [ 0.1455,  0.0908, -0.1215, -0.1743, -0.0100],\n",
      "          [ 0.0956, -0.0445,  0.1036, -0.1626, -0.1539]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0802, -0.1069,  0.0599,  0.0058,  0.1564],\n",
      "          [ 0.0130, -0.0604,  0.1460,  0.0564, -0.0642],\n",
      "          [ 0.1741,  0.1296, -0.0592, -0.0354, -0.0818],\n",
      "          [ 0.0576, -0.1580, -0.1938,  0.1016,  0.1447],\n",
      "          [-0.1165,  0.0948,  0.1254, -0.1032,  0.0783]]],\n",
      "\n",
      "\n",
      "        [[[-0.0592, -0.0049, -0.1989, -0.0541,  0.1848],\n",
      "          [-0.0815, -0.0715, -0.0091, -0.1555,  0.1394],\n",
      "          [ 0.0114, -0.1050,  0.1525, -0.0729, -0.0235],\n",
      "          [ 0.0415,  0.0156, -0.0427,  0.0791,  0.1889],\n",
      "          [ 0.1645, -0.1483,  0.0951, -0.0971,  0.1088]]],\n",
      "\n",
      "\n",
      "        [[[-0.0705, -0.1329,  0.1346,  0.1068, -0.0291],\n",
      "          [-0.1295, -0.1192,  0.1263,  0.0613,  0.1167],\n",
      "          [ 0.1350, -0.0284,  0.1869, -0.0967, -0.1006],\n",
      "          [ 0.1781,  0.0806, -0.0406,  0.0542, -0.0664],\n",
      "          [ 0.1022, -0.1298,  0.1183, -0.0348,  0.1217]]],\n",
      "\n",
      "\n",
      "        [[[ 0.1965, -0.1404,  0.0567,  0.0278, -0.1068],\n",
      "          [ 0.0946, -0.1365, -0.1282,  0.1638, -0.0773],\n",
      "          [ 0.0442, -0.0383,  0.0094, -0.0549,  0.0246],\n",
      "          [ 0.0518, -0.1599, -0.0993, -0.0867, -0.0523],\n",
      "          [-0.0510, -0.0388, -0.1000,  0.1152,  0.0491]]],\n",
      "\n",
      "\n",
      "        [[[-0.0442, -0.0611, -0.0453, -0.1786,  0.1106],\n",
      "          [-0.1203,  0.1896, -0.1259, -0.0723,  0.1413],\n",
      "          [-0.0357,  0.0591,  0.1877,  0.1535,  0.0645],\n",
      "          [ 0.1459, -0.1029, -0.0420, -0.0775,  0.1173],\n",
      "          [-0.0306,  0.0926,  0.0731, -0.1124, -0.1787]]]])\n",
      "conv1.bias: tensor([-0.0502,  0.0650,  0.0823,  0.2000,  0.1105,  0.0842,  0.0061,  0.0847,\n",
      "         0.0329,  0.1842,  0.1043, -0.1046,  0.1244,  0.1744,  0.1535,  0.1755])\n",
      "conv2.weight: tensor([[[[-3.7839e-02,  4.6571e-02,  2.6271e-02, -4.1046e-04,  1.3922e-02],\n",
      "          [-2.3259e-02, -2.5895e-02,  6.0965e-04,  1.6010e-02, -2.2198e-02],\n",
      "          [-4.6052e-02,  1.2285e-02,  3.1658e-02,  4.1482e-02,  3.6329e-02],\n",
      "          [-8.1938e-03,  2.5733e-02,  3.2268e-02,  4.1189e-02,  1.5258e-02],\n",
      "          [ 2.8619e-02,  2.4908e-02, -5.8173e-03, -7.7989e-03, -4.8774e-02]],\n",
      "\n",
      "         [[ 3.9060e-02,  4.5996e-02,  2.4163e-02, -1.9999e-02, -3.4010e-03],\n",
      "          [-7.0798e-03,  2.0598e-02,  4.6624e-02,  1.2138e-02,  7.0338e-03],\n",
      "          [ 4.0361e-02, -3.2063e-02,  4.0524e-02,  4.0883e-02,  2.9058e-02],\n",
      "          [-3.6857e-02,  5.5269e-03, -4.8928e-02, -3.2045e-02, -3.3024e-02],\n",
      "          [ 4.4679e-02, -5.1006e-02, -2.3581e-02, -4.5452e-02, -3.9031e-02]],\n",
      "\n",
      "         [[ 2.0475e-02, -5.7608e-04, -6.6982e-03,  1.9320e-02, -3.7602e-03],\n",
      "          [-4.0975e-02,  8.6224e-03,  2.6538e-02, -2.3104e-02, -4.0265e-02],\n",
      "          [-1.8939e-02,  5.2140e-02,  5.5462e-03,  4.1408e-02,  3.2440e-02],\n",
      "          [-9.1393e-03,  3.2655e-02,  4.4304e-03,  5.5799e-03, -3.4102e-02],\n",
      "          [ 3.7018e-02,  3.0999e-02,  4.5680e-02,  1.2962e-02,  2.1983e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 5.4552e-03,  2.3540e-03,  1.2959e-02,  4.4303e-02, -5.4804e-03],\n",
      "          [-7.3630e-03,  4.7014e-03,  3.7358e-02,  1.2606e-02,  2.8042e-02],\n",
      "          [ 3.5371e-02, -3.9920e-02,  4.1229e-02,  4.2375e-02, -2.8543e-02],\n",
      "          [-4.6303e-02, -2.1366e-02,  3.9730e-02,  6.2753e-03,  2.5742e-02],\n",
      "          [-3.3831e-02, -4.7246e-02,  3.1906e-02, -2.9345e-02,  3.6714e-02]],\n",
      "\n",
      "         [[-3.2263e-02, -6.0890e-03, -3.6738e-02, -4.6619e-02,  4.8364e-02],\n",
      "          [ 2.0462e-02, -1.5003e-02, -3.5823e-02,  4.3643e-02,  4.7832e-02],\n",
      "          [ 3.8213e-02,  1.9983e-02,  3.0129e-02, -3.7356e-02, -1.2107e-02],\n",
      "          [-4.8518e-02,  3.5195e-02, -2.4464e-03, -1.6005e-02,  3.7193e-02],\n",
      "          [ 4.5059e-02,  1.5221e-02,  4.9247e-02, -1.4931e-02, -2.4896e-03]],\n",
      "\n",
      "         [[-3.0888e-02,  1.8342e-02,  9.2291e-03, -3.6317e-02,  4.4954e-02],\n",
      "          [-1.5537e-02, -4.0250e-02,  1.0921e-02,  2.3167e-02,  4.3758e-02],\n",
      "          [ 1.6080e-02,  2.9790e-02, -4.4772e-03,  2.0832e-02, -2.6279e-02],\n",
      "          [ 7.5957e-03,  1.2970e-02, -2.1866e-02, -5.1072e-02, -4.3307e-02],\n",
      "          [ 7.1548e-03,  4.4737e-02,  9.8045e-03, -1.1644e-02, -2.1879e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 4.8427e-02, -2.5046e-02,  2.0939e-02,  1.6655e-02,  1.4250e-03],\n",
      "          [-2.7425e-02, -1.9165e-02, -2.8957e-02, -6.4537e-03, -1.4447e-02],\n",
      "          [ 4.9297e-02, -4.2870e-02,  1.6320e-02,  4.3587e-02,  8.0485e-03],\n",
      "          [ 2.6754e-02,  4.7402e-02,  3.2132e-03, -2.9559e-02,  2.4974e-02],\n",
      "          [-3.8865e-03, -2.8123e-02,  2.5723e-02, -4.7627e-02, -3.0854e-02]],\n",
      "\n",
      "         [[ 2.3170e-02, -3.9411e-02,  8.7973e-03,  4.8934e-02,  3.3567e-02],\n",
      "          [-1.8153e-02, -4.0574e-02, -9.9470e-03,  4.3987e-02,  3.6197e-02],\n",
      "          [-4.0044e-02, -9.6769e-03, -1.0887e-02, -4.6527e-02, -3.0737e-03],\n",
      "          [-4.7558e-02,  2.2486e-02,  3.1006e-02,  4.2059e-02,  2.2410e-02],\n",
      "          [-5.2697e-03,  2.6928e-02,  9.6923e-03,  4.0430e-02, -5.4291e-03]],\n",
      "\n",
      "         [[ 2.8223e-02,  4.9932e-04,  4.9137e-03, -8.8372e-03, -4.0954e-02],\n",
      "          [-1.2017e-02,  4.5006e-02, -4.3662e-02, -3.2921e-03, -8.2704e-03],\n",
      "          [-2.2241e-02,  2.8991e-02, -4.6065e-02,  2.0643e-02, -4.7477e-02],\n",
      "          [ 1.2340e-03,  1.0285e-02, -1.7847e-02, -4.0607e-02, -1.0811e-03],\n",
      "          [ 3.4366e-02,  2.6097e-02,  1.0406e-02, -2.3616e-02,  3.5625e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.0506e-02, -2.7015e-02, -3.0593e-03,  4.8438e-02, -2.2806e-02],\n",
      "          [-2.0990e-02, -3.0457e-02,  2.0833e-02, -2.3527e-02,  3.8748e-02],\n",
      "          [ 4.3830e-02,  1.6678e-02,  1.5400e-02, -4.1874e-02,  2.5464e-02],\n",
      "          [ 2.2306e-02, -1.8490e-03,  3.0253e-02,  4.2559e-02,  6.4952e-03],\n",
      "          [ 2.7780e-02,  1.6814e-02, -4.9825e-02,  6.0482e-03, -3.9899e-02]],\n",
      "\n",
      "         [[-2.4444e-02, -3.4175e-02,  6.0576e-03,  4.8371e-02, -1.2028e-02],\n",
      "          [ 3.5701e-02, -6.2239e-03,  7.4316e-03,  4.5282e-02,  2.4347e-03],\n",
      "          [ 3.5602e-02,  8.8894e-03,  1.8715e-02, -1.7437e-02, -3.3191e-02],\n",
      "          [-2.5473e-02,  2.8348e-02,  6.4781e-03, -2.2968e-02,  5.8108e-03],\n",
      "          [ 3.9816e-02,  4.0604e-02,  9.9786e-03,  2.0547e-02,  3.4530e-02]],\n",
      "\n",
      "         [[-1.4544e-03, -4.7860e-02, -2.7418e-02, -1.4093e-02, -3.8954e-02],\n",
      "          [-4.1115e-02, -2.8458e-02,  4.0610e-02,  2.2691e-02, -4.5601e-02],\n",
      "          [-4.0579e-02,  4.0770e-02,  2.6612e-02, -4.6175e-02,  4.5481e-02],\n",
      "          [-1.1788e-02, -2.5677e-02, -2.0980e-02,  3.1304e-02, -4.4169e-02],\n",
      "          [-3.2716e-02,  2.2470e-02, -1.1499e-02,  4.0450e-02, -1.5106e-02]]],\n",
      "\n",
      "\n",
      "        [[[-2.9197e-02, -4.2131e-02,  4.5193e-02, -3.6112e-02, -2.9714e-02],\n",
      "          [ 1.6409e-02, -6.8179e-03,  4.6163e-02,  1.0808e-02,  3.2982e-03],\n",
      "          [ 1.3911e-02, -4.9966e-02, -4.2566e-02, -4.3899e-02,  3.1423e-02],\n",
      "          [-1.6693e-02,  9.1088e-03,  6.7345e-03, -1.1716e-03, -3.3755e-02],\n",
      "          [ 4.0744e-02, -3.1000e-02,  9.2480e-03, -4.0972e-02, -1.3810e-02]],\n",
      "\n",
      "         [[ 3.2563e-02, -1.4154e-02, -2.1171e-02, -3.9193e-02,  1.8165e-02],\n",
      "          [-3.2104e-02,  3.1785e-02,  2.8513e-02, -4.5997e-02,  9.9157e-03],\n",
      "          [-1.5230e-02, -4.7432e-02, -1.8319e-02, -4.2077e-02,  2.0790e-02],\n",
      "          [ 2.9915e-02, -1.4531e-02,  1.4379e-02, -4.6923e-02, -4.5446e-02],\n",
      "          [-4.0793e-02,  3.1359e-02, -1.4693e-02,  4.0672e-03, -1.5254e-02]],\n",
      "\n",
      "         [[-1.1612e-02,  4.1955e-02,  4.6912e-02,  2.0630e-02,  3.8107e-02],\n",
      "          [-9.2674e-05, -4.2751e-02,  4.0662e-02, -1.3721e-02, -3.8273e-02],\n",
      "          [-2.8241e-03,  1.5936e-02,  2.9758e-02,  4.3804e-02,  3.2293e-02],\n",
      "          [-2.8189e-02, -1.8615e-02,  6.8135e-03,  3.9727e-03, -6.6176e-03],\n",
      "          [ 9.5406e-04, -3.1509e-02,  3.6886e-02, -2.3965e-02,  4.1343e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.1615e-02, -2.8854e-02,  4.0468e-02, -4.0071e-02,  4.7110e-02],\n",
      "          [-4.4065e-02,  2.5550e-02, -2.4193e-02, -1.9455e-02,  7.5642e-03],\n",
      "          [-1.4344e-02,  2.5988e-02,  4.5194e-02, -1.8832e-02, -5.0028e-02],\n",
      "          [-2.6472e-02, -1.7935e-02, -4.1343e-02,  3.7107e-03, -4.5801e-02],\n",
      "          [-4.0514e-02, -3.8357e-02,  3.5042e-03, -2.1933e-03,  4.2951e-02]],\n",
      "\n",
      "         [[-4.5389e-02, -5.6366e-03, -4.2537e-02,  1.1967e-02,  3.7647e-03],\n",
      "          [ 2.3307e-02,  4.0255e-02,  3.2693e-02, -1.8784e-02, -4.9236e-02],\n",
      "          [-3.4643e-02,  1.9411e-02,  3.2192e-02,  2.3543e-02,  1.7623e-02],\n",
      "          [ 2.1295e-02, -2.8876e-02, -1.5750e-02, -3.0380e-02,  4.3210e-02],\n",
      "          [ 3.1511e-02, -6.2512e-03,  4.3731e-02,  6.3776e-03,  1.7101e-02]],\n",
      "\n",
      "         [[-2.2251e-02,  3.6588e-02,  2.2653e-02,  3.7101e-02,  1.3313e-02],\n",
      "          [ 4.5684e-02, -2.5432e-02,  2.5946e-02, -1.9282e-03,  4.3654e-02],\n",
      "          [ 4.1285e-02,  5.2098e-03,  2.3110e-03,  1.6797e-02,  4.4588e-02],\n",
      "          [-3.7941e-02, -3.4365e-02,  1.6731e-02,  2.5641e-02, -3.6388e-02],\n",
      "          [-6.4525e-03,  4.7199e-02,  3.2022e-02, -3.3977e-02, -1.0855e-02]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.8264e-02,  5.5963e-04,  2.5864e-02, -1.1133e-03, -2.7146e-02],\n",
      "          [ 3.0177e-02,  4.7772e-03, -8.6845e-03,  2.3413e-02, -2.9334e-03],\n",
      "          [ 3.1637e-02, -2.2665e-03, -3.5620e-02, -1.6036e-02, -3.6166e-03],\n",
      "          [ 6.9816e-03, -2.6582e-02, -2.1670e-02, -1.2515e-02, -1.2479e-02],\n",
      "          [-1.8743e-02,  1.8883e-02,  2.0185e-02,  5.0152e-02, -6.7070e-03]],\n",
      "\n",
      "         [[-5.2055e-02,  3.2017e-02,  1.1940e-02,  1.3996e-02,  3.3723e-02],\n",
      "          [ 2.4291e-02,  2.1067e-02,  1.0191e-02,  4.2860e-02, -4.6046e-02],\n",
      "          [-3.9458e-02,  4.1180e-04,  2.4846e-02,  4.8967e-02,  8.9122e-03],\n",
      "          [ 4.9420e-02,  4.6698e-03,  7.7682e-03,  4.8436e-02, -4.5205e-02],\n",
      "          [-3.5003e-02,  1.8785e-03, -4.6340e-02, -3.3297e-02, -1.0464e-02]],\n",
      "\n",
      "         [[ 2.5681e-02, -6.7826e-03, -4.4138e-04, -2.2637e-02,  2.9172e-02],\n",
      "          [ 3.4098e-02, -3.6132e-02,  1.9285e-02,  5.8026e-03, -3.1395e-02],\n",
      "          [ 4.7171e-02,  1.7207e-02,  3.6458e-02,  2.5381e-02,  3.1692e-02],\n",
      "          [ 2.0549e-02, -2.8213e-02,  3.8102e-02,  2.4357e-02,  2.7445e-02],\n",
      "          [ 6.2454e-03, -3.2832e-02, -3.9819e-02, -2.4325e-02,  6.1751e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3175e-02, -1.1329e-02, -3.6488e-02,  2.9299e-02,  2.4376e-02],\n",
      "          [-2.9491e-03, -1.7644e-02, -2.3790e-02,  2.9785e-02,  3.4157e-02],\n",
      "          [ 2.5173e-03,  4.3227e-02,  5.7047e-02,  3.2414e-02, -3.3027e-02],\n",
      "          [-1.4035e-02,  2.8532e-02,  3.6475e-02, -4.5794e-02, -1.9505e-02],\n",
      "          [ 4.9580e-02, -4.6590e-02,  8.7467e-03, -1.6582e-03,  4.1400e-02]],\n",
      "\n",
      "         [[-1.6139e-03,  5.2468e-03,  1.2620e-02, -2.8065e-02, -1.2341e-02],\n",
      "          [ 1.6713e-02,  9.8760e-03, -4.4862e-02,  4.0121e-02, -4.1099e-02],\n",
      "          [ 1.4957e-02, -2.7586e-02, -1.2861e-02,  4.7659e-02, -1.7492e-02],\n",
      "          [ 1.5784e-02,  2.1304e-02, -3.4517e-02,  3.1846e-02,  4.6747e-02],\n",
      "          [-1.9754e-02,  4.5257e-02, -3.1272e-02,  7.4361e-03, -3.3691e-02]],\n",
      "\n",
      "         [[ 6.6240e-03,  3.7192e-02,  3.8838e-02, -2.0595e-02, -2.9437e-02],\n",
      "          [-4.0229e-02,  1.5403e-02, -2.0061e-02,  4.8011e-02, -1.5569e-02],\n",
      "          [ 3.0832e-02,  3.1596e-02,  4.5579e-02, -2.2288e-02, -3.6042e-02],\n",
      "          [ 5.2997e-04,  4.0008e-02,  2.5141e-03, -2.9079e-02, -4.7670e-02],\n",
      "          [ 5.0790e-02, -2.1046e-02,  5.5282e-03,  1.4087e-02, -4.3747e-02]]],\n",
      "\n",
      "\n",
      "        [[[ 1.1311e-02,  4.2143e-02, -1.3128e-03, -2.3847e-02, -1.5696e-02],\n",
      "          [ 2.7987e-02,  3.5940e-02,  1.0639e-02, -1.5282e-03,  1.1232e-02],\n",
      "          [-3.9966e-02,  4.4905e-02, -1.9260e-03,  1.7717e-02, -5.2620e-03],\n",
      "          [ 3.4155e-02, -3.3472e-02, -1.7803e-02,  2.8073e-02,  2.4569e-02],\n",
      "          [ 3.1466e-02, -3.8804e-02, -4.5053e-02,  4.0748e-02, -3.1006e-02]],\n",
      "\n",
      "         [[ 1.0079e-03,  3.8839e-02, -3.6652e-02, -3.5738e-02,  3.0690e-02],\n",
      "          [-8.0051e-03,  6.1673e-03, -4.5449e-02,  1.6799e-02,  4.4786e-02],\n",
      "          [-1.7861e-02,  6.7915e-03,  3.5243e-02, -2.5379e-02, -1.9526e-02],\n",
      "          [-1.7612e-02, -2.4857e-02,  4.7502e-02, -1.5061e-02, -4.1331e-02],\n",
      "          [-2.9244e-02, -2.1416e-02,  2.6489e-02,  4.0825e-02,  3.6400e-02]],\n",
      "\n",
      "         [[ 2.0734e-02, -4.0528e-02,  4.1874e-02,  1.0325e-02, -4.1681e-02],\n",
      "          [-1.7736e-02, -2.3585e-02, -1.5616e-02,  2.2667e-02, -1.4075e-03],\n",
      "          [-4.0799e-03,  1.0421e-02,  4.2444e-02, -4.8223e-02, -1.0233e-02],\n",
      "          [-2.4938e-02,  8.5813e-04,  8.5259e-03,  1.1342e-02,  3.8841e-02],\n",
      "          [-3.4526e-02, -3.9556e-02, -3.4707e-02,  2.7489e-02,  3.1446e-03]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.0090e-02,  5.3528e-03, -3.3701e-03, -4.7553e-03,  9.1273e-03],\n",
      "          [ 6.6203e-03,  1.4083e-02,  3.7079e-02, -6.5315e-03,  1.0231e-02],\n",
      "          [ 3.2787e-02,  4.5484e-02,  2.0583e-02, -4.4859e-02,  2.9981e-02],\n",
      "          [ 3.8048e-02, -1.9834e-02, -3.3666e-03, -6.8836e-03,  6.1187e-03],\n",
      "          [-1.1912e-02,  2.1262e-02,  4.1972e-02,  4.4062e-02,  4.1184e-02]],\n",
      "\n",
      "         [[ 4.4095e-02,  4.9711e-02, -2.5144e-02, -1.1775e-02,  3.7201e-02],\n",
      "          [-4.5933e-02,  2.4501e-02, -7.3551e-03,  2.1739e-02,  1.1587e-02],\n",
      "          [ 3.4706e-02, -4.0558e-02,  4.5751e-02,  4.8995e-02, -4.2367e-02],\n",
      "          [ 3.7761e-02,  3.4236e-02,  1.1869e-02, -1.5179e-02, -4.1255e-02],\n",
      "          [-3.4195e-02,  2.8713e-03,  4.1005e-02, -2.6493e-02,  5.2598e-03]],\n",
      "\n",
      "         [[ 3.6509e-02, -4.4609e-02, -1.8312e-02, -4.0693e-02, -4.5595e-02],\n",
      "          [ 1.9154e-02, -4.4257e-02,  7.7035e-03, -4.0646e-02,  4.1945e-02],\n",
      "          [ 2.5994e-02,  1.1712e-02, -1.8415e-02, -3.4443e-02, -1.0844e-02],\n",
      "          [-4.3743e-02,  3.8138e-02, -1.7301e-02,  2.2682e-02, -4.4187e-03],\n",
      "          [ 3.9142e-02, -3.6639e-02,  1.2871e-02,  8.1144e-03, -4.9346e-02]]],\n",
      "\n",
      "\n",
      "        [[[-1.5683e-03,  3.4360e-02,  1.0799e-02,  1.9335e-02,  3.9859e-02],\n",
      "          [-2.8998e-02,  1.2878e-02,  2.0133e-02,  2.1875e-02, -4.7693e-02],\n",
      "          [-3.6130e-02,  4.5585e-02, -4.7042e-02,  2.7583e-02,  4.6912e-02],\n",
      "          [ 4.6396e-02,  2.8560e-02,  3.9870e-02,  1.7433e-02, -4.5014e-02],\n",
      "          [-1.9068e-02, -2.9662e-02,  8.7803e-03,  2.1385e-02, -4.1525e-02]],\n",
      "\n",
      "         [[-3.7867e-02, -1.7137e-02,  6.2019e-03,  4.2192e-03, -5.4121e-03],\n",
      "          [ 4.5047e-02,  2.6601e-02, -1.4299e-02,  4.3960e-02, -2.2390e-02],\n",
      "          [-1.7742e-02, -1.0720e-02, -2.1544e-02,  2.9951e-02,  4.7597e-02],\n",
      "          [-4.5745e-02, -1.4785e-02, -2.4274e-02, -4.5535e-02,  3.8066e-02],\n",
      "          [-3.2244e-02,  4.3837e-02,  2.7196e-03,  4.8461e-02,  2.2205e-03]],\n",
      "\n",
      "         [[-2.9290e-02, -4.2510e-03,  3.3829e-02, -3.5129e-02, -2.4186e-02],\n",
      "          [ 1.6299e-02,  3.7061e-04,  4.8985e-02,  4.4713e-02,  1.2739e-02],\n",
      "          [ 1.6123e-02,  1.1085e-03,  3.8055e-02, -1.7346e-02,  4.5058e-02],\n",
      "          [ 4.8179e-02,  2.1847e-02,  3.3589e-02, -2.5205e-02, -3.7609e-02],\n",
      "          [ 1.8802e-02, -3.1249e-02,  1.2361e-03, -2.4178e-02,  4.7971e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.9416e-03, -3.2923e-03, -4.1272e-02, -2.8167e-02, -3.0210e-02],\n",
      "          [-2.9349e-02, -2.4094e-02, -2.4881e-03, -3.8310e-02,  3.7643e-02],\n",
      "          [ 2.9425e-02, -2.5655e-02, -1.6986e-02, -4.8288e-02, -4.7839e-02],\n",
      "          [-3.4389e-03, -4.5043e-02,  1.8046e-02, -3.7557e-02,  5.1508e-02],\n",
      "          [-1.8497e-02, -2.1331e-02,  3.2610e-02,  4.8383e-02,  2.5586e-02]],\n",
      "\n",
      "         [[ 2.6590e-02, -2.0994e-02, -3.2226e-02, -4.9896e-02,  1.0637e-02],\n",
      "          [-2.9445e-02,  3.8091e-02,  4.5937e-02, -2.0452e-02,  2.3332e-02],\n",
      "          [ 3.4821e-02, -7.1748e-03, -3.6875e-02,  1.8497e-02,  4.3658e-02],\n",
      "          [ 5.8466e-03,  4.9258e-02, -4.6978e-02, -5.1792e-03,  1.7908e-02],\n",
      "          [ 2.5357e-02, -4.4844e-02,  6.4969e-03,  1.0390e-02,  2.6762e-02]],\n",
      "\n",
      "         [[-1.2464e-02, -3.6343e-02, -3.2631e-02,  1.7851e-02, -3.3347e-02],\n",
      "          [ 1.7102e-02,  4.5419e-02,  1.1795e-02, -3.1051e-02, -3.5805e-02],\n",
      "          [-1.6385e-02,  1.6925e-02, -3.9009e-02, -4.3872e-02,  3.9011e-02],\n",
      "          [ 1.6722e-02,  1.3412e-02,  3.4876e-02,  2.1293e-02, -8.5295e-03],\n",
      "          [ 2.2183e-02,  1.9432e-02,  3.5191e-02, -7.6294e-03, -2.2984e-02]]]])\n",
      "conv2.bias: tensor([-0.0514, -0.0439,  0.0088,  0.0373, -0.0264,  0.0129,  0.0397,  0.0139,\n",
      "        -0.0495,  0.0029, -0.0115, -0.0501,  0.0375, -0.0263,  0.0527,  0.0062,\n",
      "        -0.0326,  0.0036, -0.0487,  0.0302,  0.0398, -0.0036, -0.0076,  0.0275,\n",
      "         0.0319,  0.0116,  0.0293, -0.0129, -0.0060,  0.0030,  0.0121, -0.0255])\n",
      "fc1.weight: tensor([[-0.0042, -0.0060,  0.0012,  ...,  0.0010,  0.0247, -0.0020],\n",
      "        [ 0.0111, -0.0191,  0.0074,  ..., -0.0012, -0.0027, -0.0243],\n",
      "        [ 0.0103, -0.0155, -0.0012,  ..., -0.0094,  0.0054,  0.0079],\n",
      "        ...,\n",
      "        [-0.0038, -0.0025,  0.0205,  ..., -0.0056, -0.0186,  0.0222],\n",
      "        [-0.0177, -0.0213, -0.0245,  ..., -0.0145, -0.0165,  0.0123],\n",
      "        [-0.0077, -0.0083,  0.0164,  ..., -0.0078, -0.0188,  0.0182]])\n",
      "fc1.bias: tensor([ 0.0087,  0.0281,  0.0075,  0.0032,  0.0226,  0.0175, -0.0296, -0.0075,\n",
      "        -0.0296, -0.0201])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 10\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "CNN_input_shape = (-1, 1, 28, 28)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "print(model)\n",
    "\n",
    "# Perform training\n",
    "fit_CNN_model(model, train_loader, None, num_epochs, optimizer, criterion, show_history=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Federated Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def iterate_FedLearn_model(model, dataset_loader, num_epochs, optimizer, criterion, num_clients, local_update_epochs, client_device, iterate_func=iterate_CNN_model, aggregate_func=Federated_Averaging, show_history=True, training = True):\n",
    "    # Initialize cost history for recording\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Initialize global weights\n",
    "    client_weights = [None] * num_clients\n",
    "    global_weights = model.state_dict()\n",
    "    \n",
    "    # Iteration\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "\n",
    "            # Local update\n",
    "            client_weights[client], local_loss_history, local_error_history = client_device[client].update_weights(local_update_epochs, iterate_func)\n",
    "            local_loss = sum(local_loss_history) / len(local_loss_history)\n",
    "            local_error = sum(local_error_history) / len(local_error_history)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(len(value) for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "            # Send client loss and error to the server (we ignore the cost involved here)\n",
    "            loss_count += local_loss\n",
    "            error_count += local_error\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = aggregate_func(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Summarize loss and error\n",
    "        loss_average = loss_count / num_clients\n",
    "        train_error_rate_average = error_count / num_clients\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss_average.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        error_cost_history.append(train_error_rate_average)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 1 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average.item():.8f}, Average Error: {train_error_rate_average:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    return loss_cost_history, error_cost_history, send_cost_history\n",
    "\n",
    "def fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True):\n",
    "    # Preprocess the client data\n",
    "    #client_dataloader_list = split_dataloader(train_loader, num_clients)\n",
    "\n",
    "    # This is the alternative for preprocessing, will be changed in the future\n",
    "\n",
    "    ### Alternative ###\n",
    "    client_dataloader_list = [] * num_clients\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( num_samples_preset / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        \n",
    "        # Translate the tensor to dataset\n",
    "        client_train_dataset = torch.utils.data.TensorDataset(X_train_client[client], y_train_client[client])\n",
    "\n",
    "        # Translate to DataLoader\n",
    "        client_loader = torch.utils.data.DataLoader(client_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "        client_dataloader_list.append(client_loader)\n",
    "    ### Alternative End ###\n",
    "\n",
    "    # Define the client model, criterion, optimizer, its dataset\n",
    "    client_model_list = [None] * num_clients\n",
    "    client_optimizer_list = [None] * num_clients\n",
    "    client_criterion_list = [None] * num_clients\n",
    "    client_setup_func(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list)\n",
    "    \n",
    "    # Establish client devices\n",
    "    client_device = establish_client_devices(num_clients=num_clients, \n",
    "                                             train_dataloader_list=client_dataloader_list, \n",
    "                                             test_dataloader_list= [None] * num_clients,\n",
    "                                             model_list=client_model_list, \n",
    "                                             optimizer_list=client_optimizer_list,\n",
    "                                             criterion_list=client_criterion_list)\n",
    "\n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "        print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history, train_send_cost_history = iterate_FedLearn_model(model=model, dataset_loader=train_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = True)\n",
    "\n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history, test_send_cost_history = iterate_FedLearn_model(model=model, dataset_loader=test_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = False)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Plot send cost history\n",
    "    if test_loader is not None:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        print(f'Total send cost in testing: {sum(test_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history, label='Training Send Cost')\n",
    "        plt.plot(test_send_cost_history, label='Testing Send Cost')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history)\n",
    "    plt.plot(train_send_cost_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss_history, train_error_history, train_send_cost_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated Learning Expriment Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_FedLearn_model(model, train_loader, test_loader, criterion, optimizer, num_clients_list, local_update_epochs_list, client_setup_func, iterate_func, aggregate_func, compareClients=False):\n",
    "    # Cost History Total\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "    loss_cost_history_total = []\n",
    "    error_cost_history_total = []\n",
    "    send_cost_history_total = []\n",
    "\n",
    "    if compareClients is True:\n",
    "        iteration_list = num_clients_list\n",
    "    else:\n",
    "        iteration_list = local_update_epochs_list\n",
    "\n",
    "    for n in range(len(iteration_list)):\n",
    "        if compareClients is True:\n",
    "            print(f'=== The training for num_clients is {n+1} ===')\n",
    "            num_clients = num_clients_list[n]\n",
    "            local_update_epochs = local_update_epochs_list[0]\n",
    "        else:\n",
    "            print(f'=== The training for local_update_epochs is {n+1} ===')\n",
    "            num_clients = num_clients_list[0]\n",
    "            local_update_epochs = local_update_epochs_list[n]\n",
    "        # Perform training\n",
    "        loss_cost_history, error_cost_history, send_cost_history = fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True)\n",
    "\n",
    "        # Record the history of loss, error and send_cost\n",
    "        loss_cost_history_total.append(loss_cost_history)\n",
    "        error_cost_history_total.append(error_cost_history)\n",
    "        send_cost_history_total.append(send_cost_history)\n",
    "\n",
    "    print(f'=== The Experiment Result ===')\n",
    "    # Show Dataset Name\n",
    "    print(f'Current Dataset: {dataset_name}')\n",
    "    if compareClients is True:\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients.png')\n",
    "    else:\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_local_update_epochs.png')\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.show()\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_local_update_epochs.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_client_setup_func_CNN(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list):\n",
    "    for client in range(num_clients):\n",
    "        client_model_list[client] = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "        client_optimizer_list[client] = nn.CrossEntropyLoss()\n",
    "        client_criterion_list[client] = torch.optim.SGD(client_model_list[client].parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset: MNIST Datset\n",
      "ConvolutionalNeuralNetwork(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu1): ReLU()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (relu2): ReLU()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n",
      "=== The training for local_update_epochs is 1 ===\n",
      "Epoch [1/50], Average Loss: 2.12152768, Average Error: 0.5584000349044800, Culminative Send Cost: 116\n",
      "Epoch [2/50], Average Loss: 0.73666229, Average Error: 0.1812500059604645, Culminative Send Cost: 232\n",
      "Epoch [3/50], Average Loss: 0.35544257, Average Error: 0.1053000092506409, Culminative Send Cost: 348\n",
      "Epoch [4/50], Average Loss: 0.25542109, Average Error: 0.0709000006318092, Culminative Send Cost: 464\n",
      "Epoch [5/50], Average Loss: 0.19691329, Average Error: 0.0523499958217144, Culminative Send Cost: 580\n",
      "Epoch [6/50], Average Loss: 0.15497022, Average Error: 0.0414999984204769, Culminative Send Cost: 696\n",
      "Epoch [7/50], Average Loss: 0.12541337, Average Error: 0.0336499996483326, Culminative Send Cost: 812\n",
      "Epoch [8/50], Average Loss: 0.10171885, Average Error: 0.0276500023901463, Culminative Send Cost: 928\n",
      "Epoch [9/50], Average Loss: 0.08191101, Average Error: 0.0204499978572130, Culminative Send Cost: 1044\n",
      "Epoch [10/50], Average Loss: 0.06589034, Average Error: 0.0143499998375773, Culminative Send Cost: 1160\n",
      "Epoch [11/50], Average Loss: 0.05472385, Average Error: 0.0099499998614192, Culminative Send Cost: 1276\n",
      "Epoch [12/50], Average Loss: 0.04395555, Average Error: 0.0070499991998076, Culminative Send Cost: 1392\n",
      "Epoch [13/50], Average Loss: 0.03645773, Average Error: 0.0048500001430511, Culminative Send Cost: 1508\n"
     ]
    }
   ],
   "source": [
    "### Experiment for local_update_epochs with CNN model\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the model parameters\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 50\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "num_clients_list = [1]\n",
    "local_update_epochs_list = [20]\n",
    "\n",
    "# Define global neural network model, loss criterion and optimizer\n",
    "model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "CNN_input_shape = (-1, 1, 28, 28)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "print(model)\n",
    "\n",
    "# Experiment\n",
    "experiment_FedLearn_model(model, train_loader, None, criterion, optimizer, num_clients_list, local_update_epochs_list, simple_client_setup_func_CNN, iterate_CNN_model, Federated_Averaging, compareClients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
