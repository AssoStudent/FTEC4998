{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install math\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BMI Dataset\n",
    "\n",
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "#dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, [0,1,2]]\n",
    "#y_train = dataset[:train_split, 3]\n",
    "#X_test = dataset[train_split:, [0,1,2]]\n",
    "#y_test = dataset[train_split:, 3]\n",
    "\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "#prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "#prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "#X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Epsilon Pascal Challenge Dataset\n",
    "\n",
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"epsilon_normalized\", sep=' ', header=None, nrows=20000)\n",
    "#dataset = dataset.to_numpy()\n",
    "#for i in range(1, dataset.shape[1]-1):\n",
    "#    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "#dataset = dataset[:, :-1]\n",
    "#np.random.shuffle(dataset)\n",
    "\n",
    "#for i in range(1, dataset.shape[0]):\n",
    "#    if dataset[i - 1, 0] == -1:\n",
    "#        dataset[i - 1, 0] = 0\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "#y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "#X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "#y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = X_train\n",
    "#X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#print(X_train_tensor.size())\n",
    "#print(y_train_tensor.size())\n",
    "#print(X_test_tensor.size())\n",
    "#print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[0 4 1 ... 0 1 2]\n"
     ]
    }
   ],
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"mnist_train.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.int32)\n",
    "y_train = dataset[:train_split, 0].astype(np.int32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.int32)\n",
    "y_test = dataset[train_split:, 0].astype(np.int32)\n",
    "\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"mnist_test.csv\")\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "denominator = X_train.max(0) - X_train.min(0)\n",
    "X_train_normalized = (X_train - X_train.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "denominator = X_test.max(0) - X_test.min(0)\n",
    "X_test_normalized = (X_test - X_test.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "denominator = X_prediction.max(0) - X_prediction.min(0)\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom Vanilia Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "Epoch [1/100], Loss: 28.17117023\n",
      "0.6965471143408579\n",
      "Epoch [10/100], Loss: 18.24058317\n",
      "1.2312888300286804\n",
      "Epoch [20/100], Loss: 12.91475868\n",
      "1.5945998179825525\n",
      "Epoch [30/100], Loss: 10.38817490\n",
      "1.8428506458704415\n",
      "Epoch [40/100], Loss: 9.12953929\n",
      "2.01384109929673\n",
      "Epoch [50/100], Loss: 8.44964850\n",
      "2.1329186190900233\n",
      "Epoch [60/100], Loss: 8.03797406\n",
      "2.217080938894724\n",
      "Epoch [70/100], Loss: 7.75429734\n",
      "2.2777270528724984\n",
      "Epoch [80/100], Loss: 7.53502030\n",
      "2.3225018377792757\n",
      "Epoch [90/100], Loss: 7.35103733\n",
      "2.356532936248861\n",
      "Epoch [100/100], Loss: 7.18879028\n",
      "Learned parameters:\n",
      "w0 = 0.0\n",
      "w1 = 0.0\n",
      "w2 = 0.0\n",
      "w3 = 0.0\n",
      "w4 = 0.0\n",
      "w5 = 0.0\n",
      "w6 = 0.0\n",
      "w7 = 0.0\n",
      "w8 = 0.0\n",
      "w9 = 0.0\n",
      "w10 = 0.0\n",
      "w11 = 0.0\n",
      "w12 = 7.373905711133619e-06\n",
      "w13 = 6.76765704266697e-06\n",
      "w14 = -7.93325225071123e-07\n",
      "w15 = -7.93325225071123e-07\n",
      "w16 = 0.0\n",
      "w17 = 0.0\n",
      "w18 = 0.0\n",
      "w19 = 0.0\n",
      "w20 = 0.0\n",
      "w21 = 0.0\n",
      "w22 = 0.0\n",
      "w23 = 0.0\n",
      "w24 = 0.0\n",
      "w25 = 0.0\n",
      "w26 = 0.0\n",
      "w27 = 0.0\n",
      "w28 = 0.0\n",
      "w29 = 0.0\n",
      "w30 = 0.0\n",
      "w31 = 0.0\n",
      "w32 = 5.8518308579610834e-06\n",
      "w33 = 6.961509064569088e-06\n",
      "w34 = 2.3907531762131517e-05\n",
      "w35 = 3.521008246667493e-05\n",
      "w36 = 4.658394678839096e-05\n",
      "w37 = 6.193044090222266e-05\n",
      "w38 = 9.628255785651139e-05\n",
      "w39 = 0.00013524788023795727\n",
      "w40 = 0.00016410616336938791\n",
      "w41 = 0.00018787785292742604\n",
      "w42 = 0.00019357486440811276\n",
      "w43 = 0.00016119655659981637\n",
      "w44 = 0.00014601651866195272\n",
      "w45 = 0.00013629583547914117\n",
      "w46 = 8.673141304442236e-05\n",
      "w47 = 7.57031961015649e-05\n",
      "w48 = 6.496173272193261e-05\n",
      "w49 = 2.084712722113425e-05\n",
      "w50 = 1.4130124921368004e-05\n",
      "w51 = 6.374903292047739e-06\n",
      "w52 = 0.0\n",
      "w53 = 0.0\n",
      "w54 = 0.0\n",
      "w55 = 0.0\n",
      "w56 = 0.0\n",
      "w57 = 0.0\n",
      "w58 = 4.2494037335111024e-06\n",
      "w59 = -6.88886597894158e-06\n",
      "w60 = 1.5835408464970164e-05\n",
      "w61 = 1.848936990512601e-05\n",
      "w62 = 4.567801274915555e-05\n",
      "w63 = 0.00012709683612965975\n",
      "w64 = 0.00022782654705317046\n",
      "w65 = 0.0003896181743440637\n",
      "w66 = 0.000687530146341734\n",
      "w67 = 0.0010523102741530766\n",
      "w68 = 0.0016437896946147098\n",
      "w69 = 0.0022066488166437035\n",
      "w70 = 0.002782105402473064\n",
      "w71 = 0.003159517571876108\n",
      "w72 = 0.0033689381303177376\n",
      "w73 = 0.0032431981390613315\n",
      "w74 = 0.0028216276874048853\n",
      "w75 = 0.0020986900655987346\n",
      "w76 = 0.0012812585172034437\n",
      "w77 = 0.0006492590699699466\n",
      "w78 = 0.0003006785448271831\n",
      "w79 = 0.00010821004744418264\n",
      "w80 = 3.431756601230169e-05\n",
      "w81 = 5.94772259197995e-06\n",
      "w82 = 0.0\n",
      "w83 = 0.0\n",
      "w84 = 0.0\n",
      "w85 = 0.0\n",
      "w86 = 4.049486281178141e-06\n",
      "w87 = -2.1442943041046425e-06\n",
      "w88 = 7.774417387069438e-06\n",
      "w89 = 4.115016873414979e-06\n",
      "w90 = 6.214178510589081e-05\n",
      "w91 = 0.00016842890094953317\n",
      "w92 = 0.0002737431235267055\n",
      "w93 = 0.0005190413292504439\n",
      "w94 = 0.000976568879518399\n",
      "w95 = 0.0015421655996408538\n",
      "w96 = 0.0022572190579832484\n",
      "w97 = 0.0031977043447005567\n",
      "w98 = 0.004681318127859444\n",
      "w99 = 0.005855766631187814\n",
      "w100 = 0.006520781628296135\n",
      "w101 = 0.0068703348685884985\n",
      "w102 = 0.006342760455276462\n",
      "w103 = 0.004819949811316936\n",
      "w104 = 0.002998298352910865\n",
      "w105 = 0.0017589792499099043\n",
      "w106 = 0.0009042262097772846\n",
      "w107 = 0.00039368250920984125\n",
      "w108 = 0.00015949417912685667\n",
      "w109 = 2.9900039745169715e-05\n",
      "w110 = 8.348150842926002e-06\n",
      "w111 = 0.0\n",
      "w112 = 0.0\n",
      "w113 = -5.099482536803252e-07\n",
      "w114 = 5.393969668606825e-06\n",
      "w115 = -2.0974596368327155e-06\n",
      "w116 = 8.291524893547909e-08\n",
      "w117 = -2.2941218263450065e-05\n",
      "w118 = -5.222314729624032e-07\n",
      "w119 = 7.476571137446296e-06\n",
      "w120 = 2.6636977787775776e-05\n",
      "w121 = 0.0002178306146374806\n",
      "w122 = 0.0005418760658734517\n",
      "w123 = 0.0006831946458878486\n",
      "w124 = 0.0007794052327211096\n",
      "w125 = 0.0011270023666809044\n",
      "w126 = 0.0018106829860486326\n",
      "w127 = 0.002254047674545502\n",
      "w128 = 0.0029729576885400184\n",
      "w129 = 0.003154091630553143\n",
      "w130 = 0.002917994200592929\n",
      "w131 = 0.0022300166693047553\n",
      "w132 = 0.0019582356170653125\n",
      "w133 = 0.001950170562238272\n",
      "w134 = 0.001598712498018974\n",
      "w135 = 0.001159546213724002\n",
      "w136 = 0.0006030526413819085\n",
      "w137 = 0.00018124510190685556\n",
      "w138 = 4.603649899100699e-05\n",
      "w139 = 1.9656340737633854e-05\n",
      "w140 = 0.0\n",
      "w141 = 0.0\n",
      "w142 = 1.0258369907430022e-05\n",
      "w143 = -8.375824552296023e-06\n",
      "w144 = -1.1140064079361305e-05\n",
      "w145 = -7.492760742258117e-05\n",
      "w146 = -9.16534203682976e-05\n",
      "w147 = -5.5872144881479206e-05\n",
      "w148 = 0.0003075768697906906\n",
      "w149 = 0.0010631980159991321\n",
      "w150 = 0.001974408277076679\n",
      "w151 = 0.0035311926247839063\n",
      "w152 = 0.0056678656696044615\n",
      "w153 = 0.006996247101447832\n",
      "w154 = 0.007799794092349125\n",
      "w155 = 0.008163541952203795\n",
      "w156 = 0.00787403810853611\n",
      "w157 = 0.006818612186332052\n",
      "w158 = 0.005208241922232966\n",
      "w159 = 0.0039556854867826485\n",
      "w160 = 0.003923116845971073\n",
      "w161 = 0.004264765245596394\n",
      "w162 = 0.004117100898526469\n",
      "w163 = 0.00327843978599769\n",
      "w164 = 0.0019427835817708676\n",
      "w165 = 0.0007065409348873709\n",
      "w166 = 0.00012878190460246407\n",
      "w167 = 2.0225754789583974e-05\n",
      "w168 = 0.0\n",
      "w169 = 1.4256907393686406e-07\n",
      "w170 = 3.9437395902763706e-05\n",
      "w171 = 4.1148422099042716e-05\n",
      "w172 = 0.00011954269989511654\n",
      "w173 = 0.00031273374338780145\n",
      "w174 = 0.0007840758237759553\n",
      "w175 = 0.0017709825870758807\n",
      "w176 = 0.003800276170527637\n",
      "w177 = 0.00709904186936873\n",
      "w178 = 0.011294225270608487\n",
      "w179 = 0.017072922023960303\n",
      "w180 = 0.023744515916175896\n",
      "w181 = 0.02976592035807614\n",
      "w182 = 0.03354169665778433\n",
      "w183 = 0.03408073136585063\n",
      "w184 = 0.0306851785145898\n",
      "w185 = 0.02460217801647903\n",
      "w186 = 0.017692293686020155\n",
      "w187 = 0.01189988255836117\n",
      "w188 = 0.008549841793214273\n",
      "w189 = 0.0076287252446641625\n",
      "w190 = 0.007003646241807827\n",
      "w191 = 0.005831739146061582\n",
      "w192 = 0.003719609047838395\n",
      "w193 = 0.0015767054551747116\n",
      "w194 = 0.0003346268827094773\n",
      "w195 = 3.68193281713514e-05\n",
      "w196 = 7.485301759736532e-06\n",
      "w197 = 2.3951090105862362e-05\n",
      "w198 = 0.00014469456300927666\n",
      "w199 = 0.0004165531912235655\n",
      "w200 = 0.001060507373725376\n",
      "w201 = 0.0021970802334270677\n",
      "w202 = 0.004407110439738463\n",
      "w203 = 0.008247450824787414\n",
      "w204 = 0.014127356283411757\n",
      "w205 = 0.021627367345241174\n",
      "w206 = 0.03143989164494077\n",
      "w207 = 0.04296022150557642\n",
      "w208 = 0.053492130188147455\n",
      "w209 = 0.06054835373105431\n",
      "w210 = 0.06206272836831124\n",
      "w211 = 0.05990486435210803\n",
      "w212 = 0.05394797599149879\n",
      "w213 = 0.04504136446291251\n",
      "w214 = 0.035098643285266996\n",
      "w215 = 0.02516344206988224\n",
      "w216 = 0.01738935023803221\n",
      "w217 = 0.012235325413345278\n",
      "w218 = 0.00914595100322537\n",
      "w219 = 0.007878175889795305\n",
      "w220 = 0.005295495934240847\n",
      "w221 = 0.0023109805744109735\n",
      "w222 = 0.0004910095100277343\n",
      "w223 = 4.490287363649144e-05\n",
      "w224 = 7.485301759736532e-06\n",
      "w225 = 8.357655690692256e-05\n",
      "w226 = 0.0004887536487441354\n",
      "w227 = 0.0013729009924955498\n",
      "w228 = 0.002922096556833909\n",
      "w229 = 0.005495845268764706\n",
      "w230 = 0.009913993081183367\n",
      "w231 = 0.017378257015277235\n",
      "w232 = 0.027394392956043546\n",
      "w233 = 0.03993931423947495\n",
      "w234 = 0.054971517182739564\n",
      "w235 = 0.06807862556688797\n",
      "w236 = 0.07521877424629642\n",
      "w237 = 0.07357776106300032\n",
      "w238 = 0.06603357896672976\n",
      "w239 = 0.05881310457404922\n",
      "w240 = 0.05526323527670241\n",
      "w241 = 0.05220140642809621\n",
      "w242 = 0.046198531573641795\n",
      "w243 = 0.03551313372512767\n",
      "w244 = 0.023613771682979054\n",
      "w245 = 0.015024464524137748\n",
      "w246 = 0.009799915965500406\n",
      "w247 = 0.007670179007873317\n",
      "w248 = 0.0056389639318579375\n",
      "w249 = 0.002603207751386093\n",
      "w250 = 0.0004903590959633372\n",
      "w251 = 5.018904607564512e-05\n",
      "w252 = 7.485301759736532e-06\n",
      "w253 = 0.00013498067745007546\n",
      "w254 = 0.0007711765432117091\n",
      "w255 = 0.0020740133611362576\n",
      "w256 = 0.004334503521847024\n",
      "w257 = 0.008268842966240338\n",
      "w258 = 0.014633391611969577\n",
      "w259 = 0.02448339250998646\n",
      "w260 = 0.038101756091031114\n",
      "w261 = 0.053880671993583665\n",
      "w262 = 0.06890688246112117\n",
      "w263 = 0.07689090851439066\n",
      "w264 = 0.0735378732401106\n",
      "w265 = 0.060224278356505616\n",
      "w266 = 0.0461146225774933\n",
      "w267 = 0.03911579450982044\n",
      "w268 = 0.043801322641418214\n",
      "w269 = 0.052147921197215535\n",
      "w270 = 0.053043140521343096\n",
      "w271 = 0.043091762613189775\n",
      "w272 = 0.02734213352899099\n",
      "w273 = 0.014422691717296453\n",
      "w274 = 0.007148217720346936\n",
      "w275 = 0.004908189687725441\n",
      "w276 = 0.0041212519060171045\n",
      "w277 = 0.0020858224861027147\n",
      "w278 = 0.0004227716681046686\n",
      "w279 = 5.4373392837988304e-05\n",
      "w280 = 1.0862680116504888e-05\n",
      "w281 = 0.00015273643425962576\n",
      "w282 = 0.0007356669694105371\n",
      "w283 = 0.0020138060636568666\n",
      "w284 = 0.004709646899655242\n",
      "w285 = 0.00911420606683497\n",
      "w286 = 0.01634138545527651\n",
      "w287 = 0.027145668288129032\n",
      "w288 = 0.04217511697232344\n",
      "w289 = 0.0580519546830975\n",
      "w290 = 0.069517638426624\n",
      "w291 = 0.0697137356326827\n",
      "w292 = 0.05742725438617683\n",
      "w293 = 0.03907222908229449\n",
      "w294 = 0.02459014994979716\n",
      "w295 = 0.023888150826678417\n",
      "w296 = 0.03849994209973106\n",
      "w297 = 0.05646962315920187\n",
      "w298 = 0.06188134046064082\n",
      "w299 = 0.04990596858106235\n",
      "w300 = 0.028322796174763173\n",
      "w301 = 0.010885062952205997\n",
      "w302 = 0.002035477672797948\n",
      "w303 = 0.0002709102980474194\n",
      "w304 = 0.001692129405613224\n",
      "w305 = 0.0013757297522204526\n",
      "w306 = 0.0002967629491879215\n",
      "w307 = 3.724826298431021e-05\n",
      "w308 = 1.80036309111787e-05\n",
      "w309 = 0.00010079418842251168\n",
      "w310 = 0.0005696166539344962\n",
      "w311 = 0.001630933858878912\n",
      "w312 = 0.00419178287349146\n",
      "w313 = 0.008449526925570576\n",
      "w314 = 0.01531375402182434\n",
      "w315 = 0.026467374739785693\n",
      "w316 = 0.04173788856448054\n",
      "w317 = 0.056616114163099564\n",
      "w318 = 0.06338744074489579\n",
      "w319 = 0.05831292965809452\n",
      "w320 = 0.04453027381823147\n",
      "w321 = 0.027859064129719252\n",
      "w322 = 0.01755740521149045\n",
      "w323 = 0.023897378021054936\n",
      "w324 = 0.0469523745574875\n",
      "w325 = 0.06985370764650499\n",
      "w326 = 0.07326791487505992\n",
      "w327 = 0.05391675586376242\n",
      "w328 = 0.026416681540079347\n",
      "w329 = 0.005562820095471656\n",
      "w330 = -0.0030647311652068857\n",
      "w331 = -0.0035484295673714732\n",
      "w332 = -0.00033063837691594443\n",
      "w333 = 0.0006552217477743278\n",
      "w334 = 0.00015280677495235678\n",
      "w335 = 2.0253737777364088e-05\n",
      "w336 = 1.2198407528509246e-05\n",
      "w337 = 7.56327341064376e-05\n",
      "w338 = 0.0003597413143872867\n",
      "w339 = 0.001213746865797768\n",
      "w340 = 0.0034815473754130186\n",
      "w341 = 0.006817146680417616\n",
      "w342 = 0.012588980383478013\n",
      "w343 = 0.02354282123397789\n",
      "w344 = 0.0385458418509605\n",
      "w345 = 0.05205335249028588\n",
      "w346 = 0.0567645804006629\n",
      "w347 = 0.05231541981702497\n",
      "w348 = 0.04247613748995188\n",
      "w349 = 0.03107748706855863\n",
      "w350 = 0.02760120264709142\n",
      "w351 = 0.04161968497210835\n",
      "w352 = 0.06892731640967835\n",
      "w353 = 0.08825828749311315\n",
      "w354 = 0.08096085045848603\n",
      "w355 = 0.05280415103687253\n",
      "w356 = 0.021371911680208345\n",
      "w357 = 0.0009892632303024533\n",
      "w358 = -0.006253131468764507\n",
      "w359 = -0.0054393194877856875\n",
      "w360 = -0.0014515395446697626\n",
      "w361 = 0.0002682839844030119\n",
      "w362 = 7.12983753500969e-05\n",
      "w363 = 1.6774232656924822e-05\n",
      "w364 = -1.2952774324029825e-06\n",
      "w365 = 3.0282277890507046e-05\n",
      "w366 = 0.00019246154416908897\n",
      "w367 = 0.000877228329036905\n",
      "w368 = 0.0027608620705292805\n",
      "w369 = 0.0045408428901400595\n",
      "w370 = 0.008925972522705843\n",
      "w371 = 0.018845898517559318\n",
      "w372 = 0.03374086058241594\n",
      "w373 = 0.04704261084948363\n",
      "w374 = 0.0531293660597337\n",
      "w375 = 0.05332079852721253\n",
      "w376 = 0.050527423685122703\n",
      "w377 = 0.04549966755788024\n",
      "w378 = 0.04922463590473157\n",
      "w379 = 0.06760722013947845\n",
      "w380 = 0.09215676597412921\n",
      "w381 = 0.10032813119331092\n",
      "w382 = 0.08049445322872129\n",
      "w383 = 0.04625068033677186\n",
      "w384 = 0.015234978561657203\n",
      "w385 = -0.001849493795543835\n",
      "w386 = -0.007040024875412638\n",
      "w387 = -0.005626281221899065\n",
      "w388 = -0.001569357736018318\n",
      "w389 = 0.0002313817986198915\n",
      "w390 = 3.8649147074712606e-05\n",
      "w391 = 3.4880431488884017e-06\n",
      "w392 = 5.150199533030058e-06\n",
      "w393 = 1.3927587968435216e-05\n",
      "w394 = 8.676114140714239e-05\n",
      "w395 = 0.0006166478753113112\n",
      "w396 = 0.0016822465692894412\n",
      "w397 = 0.0016332089077250092\n",
      "w398 = 0.004320669940454496\n",
      "w399 = 0.012678129904463407\n",
      "w400 = 0.0272770621627305\n",
      "w401 = 0.04212610605908235\n",
      "w402 = 0.05223157691991138\n",
      "w403 = 0.05846795023392295\n",
      "w404 = 0.060753613701267875\n",
      "w405 = 0.06093851138442785\n",
      "w406 = 0.06899424913898292\n",
      "w407 = 0.08649357821519771\n",
      "w408 = 0.1045625563175683\n",
      "w409 = 0.10069242789918609\n",
      "w410 = 0.07169408685882081\n",
      "w411 = 0.03645165619340558\n",
      "w412 = 0.010448177058225263\n",
      "w413 = -0.0026459043201439354\n",
      "w414 = -0.005998738001919299\n",
      "w415 = -0.004613510293885142\n",
      "w416 = -0.001406453963168968\n",
      "w417 = 0.0002636194031061695\n",
      "w418 = -2.499080748756539e-06\n",
      "w419 = -2.0377237349284577e-06\n",
      "w420 = 6.9045422254618675e-06\n",
      "w421 = 1.2316306174894552e-06\n",
      "w422 = 4.386672544531421e-05\n",
      "w423 = 0.00047127459586032483\n",
      "w424 = 0.00034258289804499695\n",
      "w425 = -0.001905068507457514\n",
      "w426 = -0.0010427528315182192\n",
      "w427 = 0.005468768292779887\n",
      "w428 = 0.01972851277665775\n",
      "w429 = 0.037238587243094846\n",
      "w430 = 0.051714648386376204\n",
      "w431 = 0.06117020179065629\n",
      "w432 = 0.06503654889875515\n",
      "w433 = 0.06684180160392651\n",
      "w434 = 0.07522068161954293\n",
      "w435 = 0.09167393445812785\n",
      "w436 = 0.1032485007359883\n",
      "w437 = 0.09072554131768297\n",
      "w438 = 0.05804258163013218\n",
      "w439 = 0.02673389937171713\n",
      "w440 = 0.007105900607143761\n",
      "w441 = -0.002096465191803833\n",
      "w442 = -0.004521209555520812\n",
      "w443 = -0.003428381387381113\n",
      "w444 = -0.0009929941966215013\n",
      "w445 = 0.0002971036087667163\n",
      "w446 = -2.3182158584208315e-05\n",
      "w447 = -2.9197651696853434e-06\n",
      "w448 = 9.272954231425271e-06\n",
      "w449 = -5.790965395447985e-07\n",
      "w450 = 2.2965431247191074e-06\n",
      "w451 = 0.00024329921433512302\n",
      "w452 = -0.001142073248018156\n",
      "w453 = -0.005449306456021224\n",
      "w454 = -0.006549891626026423\n",
      "w455 = -0.001713959145837522\n",
      "w456 = 0.01194882308007248\n",
      "w457 = 0.030962538753815372\n",
      "w458 = 0.0478947893938338\n",
      "w459 = 0.05740381846417882\n",
      "w460 = 0.059543887849442555\n",
      "w461 = 0.06009245175832294\n",
      "w462 = 0.07006105705842992\n",
      "w463 = 0.08718004433734344\n",
      "w464 = 0.09344054667795439\n",
      "w465 = 0.07522252188659942\n",
      "w466 = 0.044311099482586984\n",
      "w467 = 0.019200809570150242\n",
      "w468 = 0.004737869743317433\n",
      "w469 = -0.0020299048991042116\n",
      "w470 = -0.003371410098523183\n",
      "w471 = -0.0024316633167862406\n",
      "w472 = -0.0006499794532189841\n",
      "w473 = 9.852111897196219e-05\n",
      "w474 = -2.695539233227058e-05\n",
      "w475 = -2.559687181137971e-06\n",
      "w476 = 0.0\n",
      "w477 = 1.1114395915959e-07\n",
      "w478 = 1.4875214132005888e-06\n",
      "w479 = -5.3413663308442784e-05\n",
      "w480 = -0.002670509301180901\n",
      "w481 = -0.008395439011872393\n",
      "w482 = -0.010847370294553437\n",
      "w483 = -0.0073563386455255\n",
      "w484 = 0.005344172003078431\n",
      "w485 = 0.024278827616824202\n",
      "w486 = 0.04052480580730015\n",
      "w487 = 0.04763745364195604\n",
      "w488 = 0.0466126467485391\n",
      "w489 = 0.04840667284609251\n",
      "w490 = 0.06250051979298796\n",
      "w491 = 0.07874817356176027\n",
      "w492 = 0.07961633139478008\n",
      "w493 = 0.059990480843566174\n",
      "w494 = 0.034324440381082545\n",
      "w495 = 0.014860941456974057\n",
      "w496 = 0.0034696925388693354\n",
      "w497 = -0.0017585465092751632\n",
      "w498 = -0.002799311310433048\n",
      "w499 = -0.0017597773148339305\n",
      "w500 = -0.0006394311970878161\n",
      "w501 = -0.00018548466546313973\n",
      "w502 = -4.873489135110103e-05\n",
      "w503 = -6.472813613308015e-06\n",
      "w504 = 1.127725400124781e-05\n",
      "w505 = 1.425272686525769e-06\n",
      "w506 = 3.89386104618908e-06\n",
      "w507 = -0.00030035319547696985\n",
      "w508 = -0.003589700960526068\n",
      "w509 = -0.009689963219202362\n",
      "w510 = -0.012543142629688251\n",
      "w511 = -0.010054562201727483\n",
      "w512 = 0.0009065509945253437\n",
      "w513 = 0.017892634135426334\n",
      "w514 = 0.032027775966056146\n",
      "w515 = 0.03688149544796423\n",
      "w516 = 0.03586484699573912\n",
      "w517 = 0.04162766607819845\n",
      "w518 = 0.05719084259799593\n",
      "w519 = 0.0694909214763332\n",
      "w520 = 0.06616682335511678\n",
      "w521 = 0.04884914347311873\n",
      "w522 = 0.028589148076768222\n",
      "w523 = 0.012107413990103868\n",
      "w524 = 0.002838457344102485\n",
      "w525 = -0.0016174333227628095\n",
      "w526 = -0.0023218085713825154\n",
      "w527 = -0.0015793053683151605\n",
      "w528 = -0.000990392080131041\n",
      "w529 = -0.00035716007984919906\n",
      "w530 = -3.9385305478314136e-05\n",
      "w531 = -4.6772390468668775e-06\n",
      "w532 = 0.0\n",
      "w533 = 4.336607607628769e-06\n",
      "w534 = -1.6572997107190607e-05\n",
      "w535 = -0.0004947659231903334\n",
      "w536 = -0.003844527817914903\n",
      "w537 = -0.009338573299854956\n",
      "w538 = -0.012311555979913737\n",
      "w539 = -0.010486371861792864\n",
      "w540 = -0.0018228633838023678\n",
      "w541 = 0.011514208239577937\n",
      "w542 = 0.023709536879497575\n",
      "w543 = 0.028931206390445555\n",
      "w544 = 0.03256646509596713\n",
      "w545 = 0.04154056626538399\n",
      "w546 = 0.055023307043438205\n",
      "w547 = 0.061716176488019706\n",
      "w548 = 0.05585916705084102\n",
      "w549 = 0.04108272826866784\n",
      "w550 = 0.02398506514388449\n",
      "w551 = 0.010453921618667164\n",
      "w552 = 0.002532149767658953\n",
      "w553 = -0.0013621577662642017\n",
      "w554 = -0.0018897108166602344\n",
      "w555 = -0.0015201225110168994\n",
      "w556 = -0.0010012077801032219\n",
      "w557 = -0.00035222964762498637\n",
      "w558 = -3.9910247899099485e-05\n",
      "w559 = -3.462000681673026e-06\n",
      "w560 = 0.0\n",
      "w561 = -1.122552143575365e-06\n",
      "w562 = -5.2957350515744306e-05\n",
      "w563 = -0.0005940816093918166\n",
      "w564 = -0.003471884090554091\n",
      "w565 = -0.007745834630488778\n",
      "w566 = -0.01052506526063331\n",
      "w567 = -0.009843130027346951\n",
      "w568 = -0.004647072037251645\n",
      "w569 = 0.004335281496772887\n",
      "w570 = 0.015198291665996812\n",
      "w571 = 0.02412106712122525\n",
      "w572 = 0.033900406130997325\n",
      "w573 = 0.04529288693560973\n",
      "w574 = 0.054223963692717064\n",
      "w575 = 0.05538186817278398\n",
      "w576 = 0.04783798459395115\n",
      "w577 = 0.035208681695638615\n",
      "w578 = 0.02120014176024681\n",
      "w579 = 0.009873025274242293\n",
      "w580 = 0.002778409454326748\n",
      "w581 = -0.0004981244667165339\n",
      "w582 = -0.0011794519606569895\n",
      "w583 = -0.001128781295925348\n",
      "w584 = -0.0007934352273909191\n",
      "w585 = -0.00022951519125519125\n",
      "w586 = -3.845920849470725e-05\n",
      "w587 = 4.558215051813742e-06\n",
      "w588 = -3.0204194961798415e-06\n",
      "w589 = -1.4062447756599074e-06\n",
      "w590 = -6.56274233953418e-05\n",
      "w591 = -0.0005496762874370347\n",
      "w592 = -0.0024866546535671397\n",
      "w593 = -0.005350515222257296\n",
      "w594 = -0.007346475625046187\n",
      "w595 = -0.007845660146260813\n",
      "w596 = -0.005626729656284282\n",
      "w597 = 0.0004214341333482236\n",
      "w598 = 0.010617423313546814\n",
      "w599 = 0.023261037697321508\n",
      "w600 = 0.03656152604694065\n",
      "w601 = 0.047307382037118\n",
      "w602 = 0.051548579098248726\n",
      "w603 = 0.048963291046211915\n",
      "w604 = 0.041661421325270606\n",
      "w605 = 0.030880546415702374\n",
      "w606 = 0.019235234345009206\n",
      "w607 = 0.009911011017575869\n",
      "w608 = 0.004169273950240758\n",
      "w609 = 0.0010981577455585632\n",
      "w610 = -0.00010023965835700911\n",
      "w611 = -0.0005938050935042822\n",
      "w612 = -0.0005010384622473092\n",
      "w613 = -0.00014328688305400875\n",
      "w614 = -2.8850187091335598e-05\n",
      "w615 = -8.121796250286633e-06\n",
      "w616 = -3.0204194961798415e-06\n",
      "w617 = 6.428460117536739e-07\n",
      "w618 = -4.2388587322850335e-05\n",
      "w619 = -0.0003252027674174686\n",
      "w620 = -0.001236699468486316\n",
      "w621 = -0.00263500050971541\n",
      "w622 = -0.0035571799135636914\n",
      "w623 = -0.003642234086438457\n",
      "w624 = -0.0016375271853029992\n",
      "w625 = 0.0036494178199804857\n",
      "w626 = 0.014009790015271074\n",
      "w627 = 0.027054408426540378\n",
      "w628 = 0.03903627411186522\n",
      "w629 = 0.045671305684579284\n",
      "w630 = 0.046704909723004485\n",
      "w631 = 0.0431351462677548\n",
      "w632 = 0.036712596584654485\n",
      "w633 = 0.028232737363633555\n",
      "w634 = 0.018746486841420246\n",
      "w635 = 0.010798938633325258\n",
      "w636 = 0.005616923613857379\n",
      "w637 = 0.0023892009720929614\n",
      "w638 = 0.0006580767216301078\n",
      "w639 = -4.343866049765738e-05\n",
      "w640 = -0.0001661042455842631\n",
      "w641 = -4.655317131070733e-05\n",
      "w642 = -1.4201589920959294e-05\n",
      "w643 = -8.121796250286633e-06\n",
      "w644 = 0.0\n",
      "w645 = 0.0\n",
      "w646 = -1.20893339560339e-05\n",
      "w647 = -7.569586122713538e-05\n",
      "w648 = -0.00025858670728564243\n",
      "w649 = -0.00033192800738738167\n",
      "w650 = 0.0004732574775931643\n",
      "w651 = 0.002464836597694849\n",
      "w652 = 0.006291016553134285\n",
      "w653 = 0.01280833062884986\n",
      "w654 = 0.023006935627683626\n",
      "w655 = 0.03474091453650923\n",
      "w656 = 0.04359255406487466\n",
      "w657 = 0.0471952997060245\n",
      "w658 = 0.0453423175283073\n",
      "w659 = 0.04067204866657917\n",
      "w660 = 0.03429862177502064\n",
      "w661 = 0.02660359705027239\n",
      "w662 = 0.01804039398053129\n",
      "w663 = 0.011021245037794886\n",
      "w664 = 0.006118908699601564\n",
      "w665 = 0.002860995181408174\n",
      "w666 = 0.001179076651498254\n",
      "w667 = 0.0004096251746984112\n",
      "w668 = 7.600305145970241e-05\n",
      "w669 = -1.4913723993489007e-06\n",
      "w670 = -1.1157876336504152e-05\n",
      "w671 = 0.0\n",
      "w672 = 0.0\n",
      "w673 = 0.0\n",
      "w674 = 2.3745863902770874e-06\n",
      "w675 = 8.099429708445616e-06\n",
      "w676 = 0.00012794697258336512\n",
      "w677 = 0.0005809428745549699\n",
      "w678 = 0.002176423045508543\n",
      "w679 = 0.00576699252173529\n",
      "w680 = 0.011475755615229111\n",
      "w681 = 0.019478336117682935\n",
      "w682 = 0.02901242861421699\n",
      "w683 = 0.03706243425445149\n",
      "w684 = 0.041734437476972065\n",
      "w685 = 0.04200011634118278\n",
      "w686 = 0.03862005598444971\n",
      "w687 = 0.03342438219570489\n",
      "w688 = 0.027578857591506224\n",
      "w689 = 0.02094621703531836\n",
      "w690 = 0.014246796173028053\n",
      "w691 = 0.008966356798903632\n",
      "w692 = 0.005225451598839388\n",
      "w693 = 0.002677899636314418\n",
      "w694 = 0.0012033268630641295\n",
      "w695 = 0.0005280172028391087\n",
      "w696 = 9.181335341553204e-05\n",
      "w697 = -7.28327869584259e-07\n",
      "w698 = -2.98468313477058e-06\n",
      "w699 = 0.0\n",
      "w700 = 0.0\n",
      "w701 = 0.0\n",
      "w702 = 1.051828477442723e-05\n",
      "w703 = 2.572887398644448e-05\n",
      "w704 = 0.00017538416617982695\n",
      "w705 = 0.0006388181202609459\n",
      "w706 = 0.0018915364001075392\n",
      "w707 = 0.00436795816702622\n",
      "w708 = 0.00844567521114861\n",
      "w709 = 0.013612761630387682\n",
      "w710 = 0.01914436681858975\n",
      "w711 = 0.022843609338660562\n",
      "w712 = 0.02467062323604876\n",
      "w713 = 0.02436359238166035\n",
      "w714 = 0.022325112490356167\n",
      "w715 = 0.019585764238239982\n",
      "w716 = 0.01655494479404477\n",
      "w717 = 0.012801720070779387\n",
      "w718 = 0.00890290603761671\n",
      "w719 = 0.005697291192876512\n",
      "w720 = 0.0032384400434998114\n",
      "w721 = 0.0016939870289717893\n",
      "w722 = 0.0007471109544135433\n",
      "w723 = 0.00028048485510380217\n",
      "w724 = 5.0058719587422534e-05\n",
      "w725 = 8.380493106625662e-06\n",
      "w726 = -2.4849500262343195e-06\n",
      "w727 = 0.0\n",
      "w728 = 0.0\n",
      "w729 = 0.0\n",
      "w730 = 0.0\n",
      "w731 = 1.3510303166956137e-05\n",
      "w732 = 5.7189501371570004e-05\n",
      "w733 = 0.00024854670492013636\n",
      "w734 = 0.0008129830565820684\n",
      "w735 = 0.0017999893406311642\n",
      "w736 = 0.0035696010097834648\n",
      "w737 = 0.005406525475830994\n",
      "w738 = 0.0072679813408445615\n",
      "w739 = 0.008820175658766473\n",
      "w740 = 0.009581651563923434\n",
      "w741 = 0.009566155102438925\n",
      "w742 = 0.008897001717056402\n",
      "w743 = 0.007467540016209934\n",
      "w744 = 0.006266253765762321\n",
      "w745 = 0.004714925742096438\n",
      "w746 = 0.003215951842858261\n",
      "w747 = 0.001990171639161363\n",
      "w748 = 0.001135451063481368\n",
      "w749 = 0.0005633496091657458\n",
      "w750 = 0.00018124728054872715\n",
      "w751 = 6.710613990650622e-05\n",
      "w752 = 2.3366461667196288e-06\n",
      "w753 = 8.297944710595277e-06\n",
      "w754 = 0.0\n",
      "w755 = 0.0\n",
      "w756 = 0.0\n",
      "w757 = 0.0\n",
      "w758 = 0.0\n",
      "w759 = 0.0\n",
      "w760 = 8.448303155945396e-06\n",
      "w761 = 2.1116504023621273e-05\n",
      "w762 = 6.293896454263733e-05\n",
      "w763 = 0.00013812944597643955\n",
      "w764 = 0.00017335368893882396\n",
      "w765 = 0.00026056607984811775\n",
      "w766 = 0.0004562660898683092\n",
      "w767 = 0.0005774661999686002\n",
      "w768 = 0.0007892047628653778\n",
      "w769 = 0.0008366029958855019\n",
      "w770 = 0.001059129172997557\n",
      "w771 = 0.0009573599102742247\n",
      "w772 = 0.0007911236827992229\n",
      "w773 = 0.0005738384711303368\n",
      "w774 = 0.00036048367712495597\n",
      "w775 = 0.0001740981089479602\n",
      "w776 = 0.00011298838663031808\n",
      "w777 = 5.4430083330774625e-05\n",
      "w778 = 4.426279586771734e-05\n",
      "w779 = 2.3596307629512754e-05\n",
      "w780 = 0.0\n",
      "w781 = 0.0\n",
      "w782 = 0.0\n",
      "w783 = 0.0\n",
      "b = 0.1305484932998259\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.8990)\n",
      "Test error rate: tensor(0.8978)\n"
     ]
    }
   ],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(y_pred[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_iterations}], Loss: {cost.item():.8f}')\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.001\n",
    "num_iterations = 100\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2361821287482853\n",
      "Epoch [1/10], Loss: 4.26444719\n",
      "3.681250269966303\n",
      "Epoch [10/10], Loss: 4.84625479\n",
      "Learned parameters:\n",
      "w0 = 0.0\n",
      "w1 = 0.0\n",
      "w2 = 0.0\n",
      "w3 = 0.0\n",
      "w4 = 0.0\n",
      "w5 = 0.0\n",
      "w6 = 0.0\n",
      "w7 = 0.0\n",
      "w8 = 0.0\n",
      "w9 = 0.0\n",
      "w10 = 0.0\n",
      "w11 = 0.0\n",
      "w12 = 0.000990900281483295\n",
      "w13 = 0.0001353646228264518\n",
      "w14 = -0.001119537335524304\n",
      "w15 = -0.001119537335524304\n",
      "w16 = 0.0\n",
      "w17 = 0.0\n",
      "w18 = 0.0\n",
      "w19 = 0.0\n",
      "w20 = 0.0\n",
      "w21 = 0.0\n",
      "w22 = 0.0\n",
      "w23 = 0.0\n",
      "w24 = 0.0\n",
      "w25 = 0.0\n",
      "w26 = 0.0\n",
      "w27 = 0.0\n",
      "w28 = 0.0\n",
      "w29 = 0.0\n",
      "w30 = 0.0\n",
      "w31 = 0.0\n",
      "w32 = 0.0014986690051980761\n",
      "w33 = 0.0017449061370374548\n",
      "w34 = 0.006020259084254055\n",
      "w35 = 0.008281523323500446\n",
      "w36 = 0.01275452997244562\n",
      "w37 = 0.015629007778843147\n",
      "w38 = 0.021883517314918857\n",
      "w39 = 0.023859609365041657\n",
      "w40 = 0.018437796199158616\n",
      "w41 = 0.022331188072382345\n",
      "w42 = 0.02885674184248017\n",
      "w43 = 0.021360485726556355\n",
      "w44 = 0.011484330665532835\n",
      "w45 = 0.015060412011213406\n",
      "w46 = 0.010782419243972911\n",
      "w47 = 0.012905258098371274\n",
      "w48 = 0.013977819597020371\n",
      "w49 = 0.005490503977846101\n",
      "w50 = 0.0017306289542489035\n",
      "w51 = 0.0006495796323275722\n",
      "w52 = 0.0\n",
      "w53 = 0.0\n",
      "w54 = 0.0\n",
      "w55 = 0.0\n",
      "w56 = 0.0\n",
      "w57 = 0.0\n",
      "w58 = -0.00021632825568205534\n",
      "w59 = -0.0024710147071278247\n",
      "w60 = 0.003631915681875321\n",
      "w61 = 0.0013527227872118929\n",
      "w62 = 0.012273891575526977\n",
      "w63 = 0.035276639172405505\n",
      "w64 = 0.06677266598262646\n",
      "w65 = 0.11261102331368235\n",
      "w66 = 0.15518353903284401\n",
      "w67 = 0.15542107386848128\n",
      "w68 = 0.18421086408998824\n",
      "w69 = 0.2166971257767704\n",
      "w70 = 0.2572282904044538\n",
      "w71 = 0.26589420133648617\n",
      "w72 = 0.28378185616723384\n",
      "w73 = 0.2598488549759282\n",
      "w74 = 0.25550538031829256\n",
      "w75 = 0.24128458075985953\n",
      "w76 = 0.20088352922302802\n",
      "w77 = 0.13048512699824874\n",
      "w78 = 0.07222779826095549\n",
      "w79 = 0.028986695463578575\n",
      "w80 = 0.009888626220535131\n",
      "w81 = -0.0011120206462939364\n",
      "w82 = 0.0\n",
      "w83 = 0.0\n",
      "w84 = 0.0\n",
      "w85 = 0.0\n",
      "w86 = -0.00032728723363412214\n",
      "w87 = -0.001852439949233864\n",
      "w88 = 0.0033887990837740824\n",
      "w89 = 0.005611929565756513\n",
      "w90 = 0.03450128731270653\n",
      "w91 = 0.06463045268646975\n",
      "w92 = 0.10126134668428534\n",
      "w93 = 0.15485205122513862\n",
      "w94 = 0.17426832850630877\n",
      "w95 = 0.11789056937459101\n",
      "w96 = 0.08338699361898586\n",
      "w97 = 0.08354289605711743\n",
      "w98 = 0.15442315801999337\n",
      "w99 = 0.18651635101745434\n",
      "w100 = 0.21009133711872777\n",
      "w101 = 0.28054969369524113\n",
      "w102 = 0.4025061967227181\n",
      "w103 = 0.4517055438442491\n",
      "w104 = 0.428202722503639\n",
      "w105 = 0.3391321707791265\n",
      "w106 = 0.22293000014037387\n",
      "w107 = 0.11983923289305781\n",
      "w108 = 0.059916890795014464\n",
      "w109 = 0.008988407782209974\n",
      "w110 = 0.0038102917292890067\n",
      "w111 = 0.0\n",
      "w112 = 0.0\n",
      "w113 = 0.0010266032017500098\n",
      "w114 = 0.0018165569461244917\n",
      "w115 = 0.0021896063461684\n",
      "w116 = 0.0063212513662176326\n",
      "w117 = 0.013431652310341113\n",
      "w118 = 0.04136315202245225\n",
      "w119 = 0.07741171052627316\n",
      "w120 = 0.10099520071659622\n",
      "w121 = 0.14710241779888542\n",
      "w122 = 0.14274756653081824\n",
      "w123 = 0.07179302430810305\n",
      "w124 = -0.04936629456654419\n",
      "w125 = -0.05317239863238771\n",
      "w126 = 0.03434085462416292\n",
      "w127 = -0.025615746871679314\n",
      "w128 = -0.027803632960757742\n",
      "w129 = -0.02841106889404538\n",
      "w130 = 0.010898299151589769\n",
      "w131 = 0.030514244296548153\n",
      "w132 = 0.09875739979086097\n",
      "w133 = 0.16946864086768973\n",
      "w134 = 0.18988759265304783\n",
      "w135 = 0.1886394618055325\n",
      "w136 = 0.12821982464204268\n",
      "w137 = 0.042075645446070224\n",
      "w138 = 0.011504843470843235\n",
      "w139 = 0.007542766693783671\n",
      "w140 = 0.0\n",
      "w141 = 0.0\n",
      "w142 = 0.0015630418429411243\n",
      "w143 = -0.003392207305211672\n",
      "w144 = -0.004839323308448571\n",
      "w145 = -0.004623672889969416\n",
      "w146 = 0.002329265989906301\n",
      "w147 = 0.003686072435358151\n",
      "w148 = 0.033946261188084664\n",
      "w149 = 0.06125282442141578\n",
      "w150 = 0.03542156629546198\n",
      "w151 = 0.011786498713506947\n",
      "w152 = 0.05014581583792983\n",
      "w153 = -0.06836091939640371\n",
      "w154 = -0.07853165928336962\n",
      "w155 = -0.14301053209339648\n",
      "w156 = -0.09658493422904471\n",
      "w157 = -0.02454395374394824\n",
      "w158 = 0.019858569527553972\n",
      "w159 = -0.05096088405488201\n",
      "w160 = -0.03809145440978925\n",
      "w161 = -0.0011993876117504043\n",
      "w162 = 0.058478494395019155\n",
      "w163 = 0.09157097363698467\n",
      "w164 = 0.14980569063505897\n",
      "w165 = 0.10018802572655824\n",
      "w166 = 0.022920825340164413\n",
      "w167 = 0.007148256530762794\n",
      "w168 = 0.0\n",
      "w169 = -0.0007983635824099323\n",
      "w170 = 0.00433118230921139\n",
      "w171 = -0.007429188500287828\n",
      "w172 = -0.01706885456076923\n",
      "w173 = 0.01084478172899705\n",
      "w174 = -0.012159742302687597\n",
      "w175 = -0.03827246487217726\n",
      "w176 = -0.01114305477177411\n",
      "w177 = 0.017531084666652427\n",
      "w178 = -0.017324933313300175\n",
      "w179 = 0.02525915603492633\n",
      "w180 = -0.018213078240981083\n",
      "w181 = 0.0035112481649735203\n",
      "w182 = 0.02228280914571718\n",
      "w183 = 0.08903864402372466\n",
      "w184 = 0.06539074165773799\n",
      "w185 = 0.12083888235598109\n",
      "w186 = 0.04547557074185387\n",
      "w187 = -0.013552279227488534\n",
      "w188 = -0.08430147862961045\n",
      "w189 = -0.07125853970924385\n",
      "w190 = -0.06375101677359035\n",
      "w191 = 0.04702454982803915\n",
      "w192 = 0.14918293006414304\n",
      "w193 = 0.1378319160851525\n",
      "w194 = 0.028472175871145304\n",
      "w195 = 0.006048243641283804\n",
      "w196 = -0.0005813986866422978\n",
      "w197 = -0.0004871888205968648\n",
      "w198 = 0.017561920809187316\n",
      "w199 = 0.008334947782095776\n",
      "w200 = -0.01371092457161816\n",
      "w201 = -0.02921418485395417\n",
      "w202 = -0.06155451480963849\n",
      "w203 = 0.009380037004838067\n",
      "w204 = 0.07102519131809186\n",
      "w205 = 0.018299340597697722\n",
      "w206 = 0.08768718606721516\n",
      "w207 = 0.05360155047057882\n",
      "w208 = -0.01772543292543946\n",
      "w209 = 0.008170965907417392\n",
      "w210 = 0.05520578618525404\n",
      "w211 = 0.1978296612452035\n",
      "w212 = 0.1307087133485961\n",
      "w213 = -0.0618656226939585\n",
      "w214 = -0.07291651062397726\n",
      "w215 = -0.09737246278007565\n",
      "w216 = 0.02360887746658878\n",
      "w217 = -0.018504827211662873\n",
      "w218 = -0.11588766982643423\n",
      "w219 = 0.08895998949788184\n",
      "w220 = 0.2593553008464233\n",
      "w221 = 0.17082037093524544\n",
      "w222 = 0.04129481978108221\n",
      "w223 = 0.00651506156857249\n",
      "w224 = -0.0005813986866422978\n",
      "w225 = 5.524182642681855e-05\n",
      "w226 = 0.03382890262198455\n",
      "w227 = 0.03979620818546637\n",
      "w228 = 0.005137343502931183\n",
      "w229 = -0.04719645424546827\n",
      "w230 = -0.032067219148511406\n",
      "w231 = 0.06862178474317567\n",
      "w232 = 0.09859913896214938\n",
      "w233 = 0.01635648374058999\n",
      "w234 = 0.0580560504361507\n",
      "w235 = 0.07685911708705002\n",
      "w236 = 0.07984273399912281\n",
      "w237 = 0.029505507767051187\n",
      "w238 = -0.006025715445218709\n",
      "w239 = 0.025836466070964858\n",
      "w240 = 0.0032268030777259805\n",
      "w241 = -0.14716812665432277\n",
      "w242 = -0.10047246640552134\n",
      "w243 = -0.0838685746962036\n",
      "w244 = -0.03819890074885396\n",
      "w245 = 0.008067329674900486\n",
      "w246 = -0.0229595998708229\n",
      "w247 = 0.1587391073066076\n",
      "w248 = 0.34892916039007577\n",
      "w249 = 0.2494241519337069\n",
      "w250 = 0.048376516305652144\n",
      "w251 = 0.006424080608329086\n",
      "w252 = -0.0005813986866422978\n",
      "w253 = 0.006643606418627041\n",
      "w254 = 0.039613843925481326\n",
      "w255 = 0.05547831640185125\n",
      "w256 = -0.002514942024295841\n",
      "w257 = -0.008335099343121604\n",
      "w258 = 0.0035846726136569093\n",
      "w259 = 0.028534483697684507\n",
      "w260 = 0.12751225672103694\n",
      "w261 = 0.09589605617915932\n",
      "w262 = 0.14924839770532544\n",
      "w263 = 0.23573055091398618\n",
      "w264 = 0.15704584305655078\n",
      "w265 = -0.049964150795131106\n",
      "w266 = -0.1103374387247866\n",
      "w267 = -0.14313372545982264\n",
      "w268 = -0.06086082641635032\n",
      "w269 = -0.06400386590186334\n",
      "w270 = -0.08307722258681777\n",
      "w271 = -0.042371565171683064\n",
      "w272 = -0.02366433272386065\n",
      "w273 = 0.015723820607816137\n",
      "w274 = 0.00023230869521410446\n",
      "w275 = 0.12877921371308\n",
      "w276 = 0.2869130149216639\n",
      "w277 = 0.21176341647109093\n",
      "w278 = 0.04443714171582576\n",
      "w279 = 0.007229431117921013\n",
      "w280 = 0.0019772437692024553\n",
      "w281 = 0.012198543873274745\n",
      "w282 = 0.025723632591997593\n",
      "w283 = 0.015669721471797144\n",
      "w284 = 0.013492613691264066\n",
      "w285 = 0.0364790143895641\n",
      "w286 = 0.09949985064369865\n",
      "w287 = 0.07468838676379959\n",
      "w288 = 0.10574561989931447\n",
      "w289 = 0.0929676286375349\n",
      "w290 = 0.1710737663540837\n",
      "w291 = 0.23254511284883073\n",
      "w292 = 0.1454219641421279\n",
      "w293 = -0.014331943410478646\n",
      "w294 = -0.26954153086336563\n",
      "w295 = -0.2588652275078532\n",
      "w296 = -0.043161580706505505\n",
      "w297 = -0.07668083470462247\n",
      "w298 = -0.03692574301763853\n",
      "w299 = 0.019514875065973096\n",
      "w300 = -0.08902263509331894\n",
      "w301 = -0.06107956440031259\n",
      "w302 = -0.017691242538442634\n",
      "w303 = 0.030424303271831012\n",
      "w304 = 0.14752579048800438\n",
      "w305 = 0.15322842815006693\n",
      "w306 = 0.04262557962922182\n",
      "w307 = 0.0037205155810690798\n",
      "w308 = 0.0019223437814792626\n",
      "w309 = 0.006475847714998225\n",
      "w310 = 0.013659443787096597\n",
      "w311 = 0.02627892859132183\n",
      "w312 = 0.06937010941100862\n",
      "w313 = 0.1639632894189238\n",
      "w314 = 0.16960275388422727\n",
      "w315 = 0.16586770584112315\n",
      "w316 = 0.14027791225005432\n",
      "w317 = 0.16931233161458653\n",
      "w318 = 0.12729172403824038\n",
      "w319 = 0.16444679107952992\n",
      "w320 = 0.3233477696195135\n",
      "w321 = 0.28131934755274984\n",
      "w322 = -0.14950869156297952\n",
      "w323 = -0.18612721099890522\n",
      "w324 = 0.04398604564641361\n",
      "w325 = 0.059983376887813925\n",
      "w326 = 0.0406321327063056\n",
      "w327 = -0.02162240148447367\n",
      "w328 = 0.024275127971201967\n",
      "w329 = -0.028481268933068054\n",
      "w330 = -0.009357049259188725\n",
      "w331 = -0.04966248308217703\n",
      "w332 = 0.050760952393156256\n",
      "w333 = 0.09216925939407214\n",
      "w334 = 0.020612680168175374\n",
      "w335 = 0.002596648902569856\n",
      "w336 = -0.00020005007904833207\n",
      "w337 = 0.006630904598225769\n",
      "w338 = 0.017971678744913764\n",
      "w339 = 0.06058565109706639\n",
      "w340 = 0.1797218336606712\n",
      "w341 = 0.2519030632538056\n",
      "w342 = 0.18867124112567968\n",
      "w343 = 0.14945696581716694\n",
      "w344 = 0.05028806601996311\n",
      "w345 = 0.13106981578325277\n",
      "w346 = 0.019887638316286246\n",
      "w347 = 0.13535909542865485\n",
      "w348 = 0.36344104720493187\n",
      "w349 = 0.21653929272177314\n",
      "w350 = -0.19228073200129123\n",
      "w351 = 0.0677488347364673\n",
      "w352 = 0.24143288059851745\n",
      "w353 = 0.2255702282482998\n",
      "w354 = 0.12017859557022151\n",
      "w355 = 0.1135041592893593\n",
      "w356 = 0.0415400869610653\n",
      "w357 = 0.05422569255979983\n",
      "w358 = 0.010009656654982744\n",
      "w359 = -0.08132693725865695\n",
      "w360 = -0.04494047634718973\n",
      "w361 = 0.018077831563905186\n",
      "w362 = 0.004347509257913979\n",
      "w363 = -0.0001087932970455757\n",
      "w364 = -0.0006909367956140908\n",
      "w365 = 0.004474550609432767\n",
      "w366 = 0.015283307412074116\n",
      "w367 = 0.08011014773660512\n",
      "w368 = 0.22854999917564342\n",
      "w369 = 0.1763939723777311\n",
      "w370 = 0.06946423620759436\n",
      "w371 = 0.0472978638909824\n",
      "w372 = 0.08544529454230751\n",
      "w373 = 0.038836517126565995\n",
      "w374 = -0.07351763895152849\n",
      "w375 = 0.07305517105275143\n",
      "w376 = 0.4147192767827455\n",
      "w377 = 0.009686706723314054\n",
      "w378 = -0.16409842638270974\n",
      "w379 = 0.2423887258638653\n",
      "w380 = 0.213852442476928\n",
      "w381 = 0.2022513961276355\n",
      "w382 = 0.1440013176762711\n",
      "w383 = 0.1722512651212118\n",
      "w384 = 0.05353400408012169\n",
      "w385 = 0.019712455138840342\n",
      "w386 = -0.05697839857600439\n",
      "w387 = -0.1389865334865599\n",
      "w388 = -0.02917342209375959\n",
      "w389 = 0.013319916249783043\n",
      "w390 = -0.010800379953913211\n",
      "w391 = -0.003630529076994459\n",
      "w392 = 0.00016335053113068486\n",
      "w393 = 0.002501438890214816\n",
      "w394 = 0.010580849788008202\n",
      "w395 = 0.0765587022294696\n",
      "w396 = 0.14455455556738658\n",
      "w397 = -0.025328843345261913\n",
      "w398 = -0.13819980833607898\n",
      "w399 = -0.08857735659919742\n",
      "w400 = -0.04521627141322624\n",
      "w401 = -0.09323156363395906\n",
      "w402 = -0.20096221769652636\n",
      "w403 = 0.14419926033825667\n",
      "w404 = 0.34304241892406295\n",
      "w405 = -0.1266492820488557\n",
      "w406 = 0.04476657908209238\n",
      "w407 = 0.03735681709167675\n",
      "w408 = 0.2737503615214139\n",
      "w409 = 0.19578882069910183\n",
      "w410 = -0.01736905849382265\n",
      "w411 = 0.03889048230505321\n",
      "w412 = -0.06775058113423235\n",
      "w413 = -0.10575759540615934\n",
      "w414 = -0.06767562653845714\n",
      "w415 = -0.10003583179132065\n",
      "w416 = -0.009389580426311907\n",
      "w417 = 0.028877198495035186\n",
      "w418 = -0.019871306390939467\n",
      "w419 = -0.004041678125399134\n",
      "w420 = 0.0005895375538002541\n",
      "w421 = -0.00010383199374777422\n",
      "w422 = 0.007213456510045388\n",
      "w423 = 0.07097622714483416\n",
      "w424 = 0.0406781407250265\n",
      "w425 = -0.18845971914148313\n",
      "w426 = -0.1480129596544034\n",
      "w427 = -0.13537907442308075\n",
      "w428 = -0.1250868682001497\n",
      "w429 = -0.08616114856919695\n",
      "w430 = -0.08414248388637519\n",
      "w431 = 0.14700234404125537\n",
      "w432 = 0.17630469015857764\n",
      "w433 = -0.09514229985851869\n",
      "w434 = 0.007538518140843081\n",
      "w435 = -0.006135638807512215\n",
      "w436 = 0.3290427054505303\n",
      "w437 = 0.138524686223964\n",
      "w438 = -0.06005754729865033\n",
      "w439 = -0.0734961132654343\n",
      "w440 = -0.1108801552653174\n",
      "w441 = -0.046241356832813114\n",
      "w442 = 0.00041081778528054795\n",
      "w443 = -0.05422365891518648\n",
      "w444 = 0.009588654208138581\n",
      "w445 = 0.018362844725826534\n",
      "w446 = -0.03233996514073546\n",
      "w447 = -0.003852378182651743\n",
      "w448 = 0.00225270283411058\n",
      "w449 = -0.0017331499474082376\n",
      "w450 = 0.004188813789132028\n",
      "w451 = 0.07355740454448274\n",
      "w452 = 0.005245584849171219\n",
      "w453 = -0.16188392305995425\n",
      "w454 = -0.06685575164633444\n",
      "w455 = -0.05601571624569555\n",
      "w456 = -0.07992192519861065\n",
      "w457 = -0.08036072891302115\n",
      "w458 = -0.008180519560834548\n",
      "w459 = 0.19858855567145187\n",
      "w460 = 0.1900016709668288\n",
      "w461 = -0.10946826765826995\n",
      "w462 = -0.09972142651783797\n",
      "w463 = 0.10604196805354213\n",
      "w464 = 0.2765042627218508\n",
      "w465 = -0.024065762869188236\n",
      "w466 = -0.17091636788114342\n",
      "w467 = -0.15245206872554437\n",
      "w468 = -0.10068323279419923\n",
      "w469 = -0.11377719643732884\n",
      "w470 = -0.028140461845776617\n",
      "w471 = -0.04100780698661096\n",
      "w472 = -0.02914846646621961\n",
      "w473 = -0.05661417503978366\n",
      "w474 = -0.045632501512840436\n",
      "w475 = -0.002923350505031219\n",
      "w476 = 0.0\n",
      "w477 = -0.0019216552737385685\n",
      "w478 = 0.006428010619154839\n",
      "w479 = 0.06512509189578425\n",
      "w480 = -0.007109044520781755\n",
      "w481 = -0.11725952787534108\n",
      "w482 = -0.09893870060237919\n",
      "w483 = -0.1262515948062838\n",
      "w484 = -0.01929325028245383\n",
      "w485 = 0.07154581884795652\n",
      "w486 = 0.07819850110630687\n",
      "w487 = 0.25612813854246075\n",
      "w488 = 0.10744935095977853\n",
      "w489 = -0.07447010916190636\n",
      "w490 = -0.03492969185078203\n",
      "w491 = 0.12281133482170643\n",
      "w492 = 0.06318557835464474\n",
      "w493 = -0.1056567519596838\n",
      "w494 = -0.05418013330075942\n",
      "w495 = -0.0946244084148988\n",
      "w496 = -0.020281125777033452\n",
      "w497 = -0.10332335518780414\n",
      "w498 = -0.0982812084825795\n",
      "w499 = -0.028134544749232628\n",
      "w500 = -0.07477390437301681\n",
      "w501 = -0.11166452045346979\n",
      "w502 = -0.04724315847493078\n",
      "w503 = -0.008608843716758062\n",
      "w504 = 0.0016827388527501875\n",
      "w505 = 0.00039731500006190396\n",
      "w506 = 0.006989938498227022\n",
      "w507 = 0.05065908887761699\n",
      "w508 = -0.010380716495685057\n",
      "w509 = -0.10993487455988088\n",
      "w510 = -0.05613090093647461\n",
      "w511 = -0.13322278239774424\n",
      "w512 = -0.08735269037818519\n",
      "w513 = 0.05031029869173638\n",
      "w514 = 0.15225028427830467\n",
      "w515 = 0.19516317438447792\n",
      "w516 = -0.01816272275712138\n",
      "w517 = -0.15668916661394575\n",
      "w518 = -0.044767857910020706\n",
      "w519 = -0.008083217213026878\n",
      "w520 = -0.013006037052395648\n",
      "w521 = -0.004918306586858751\n",
      "w522 = 0.01568645687616371\n",
      "w523 = -0.039143650747325724\n",
      "w524 = -0.003969154310303202\n",
      "w525 = -0.08882070512626773\n",
      "w526 = -0.05624145283181815\n",
      "w527 = -0.07323915448334582\n",
      "w528 = -0.1774235838812639\n",
      "w529 = -0.10200382699638481\n",
      "w530 = -0.015430699699713082\n",
      "w531 = -0.003897358635283592\n",
      "w532 = 0.0\n",
      "w533 = 0.001151868143902936\n",
      "w534 = 0.0070582407364833365\n",
      "w535 = 0.009683711603387244\n",
      "w536 = 0.005063160347395276\n",
      "w537 = -0.05179029645624994\n",
      "w538 = -0.025820074219690885\n",
      "w539 = -0.05675848438812486\n",
      "w540 = -0.046252824673606474\n",
      "w541 = -0.054615820592452194\n",
      "w542 = 0.038945195840804685\n",
      "w543 = 0.02829435113368608\n",
      "w544 = -0.060013172844572193\n",
      "w545 = -0.23176788287255182\n",
      "w546 = -0.09422316775426375\n",
      "w547 = -0.07386374619438751\n",
      "w548 = -0.002285014313703004\n",
      "w549 = -0.031205946992350585\n",
      "w550 = -0.11875509313419591\n",
      "w551 = -0.061790121208342756\n",
      "w552 = -0.06385979895287179\n",
      "w553 = -0.10600973554711543\n",
      "w554 = -0.04069232375158269\n",
      "w555 = -0.11824456521239657\n",
      "w556 = -0.1700636510942125\n",
      "w557 = -0.09658227030419753\n",
      "w558 = -0.02305826878078901\n",
      "w559 = -0.0018580133057974366\n",
      "w560 = 0.0\n",
      "w561 = -0.0008740724184346316\n",
      "w562 = 0.0004017849867829704\n",
      "w563 = -0.034838857242815206\n",
      "w564 = -0.0697549560823499\n",
      "w565 = -0.003843885248661712\n",
      "w566 = -0.014070211359439666\n",
      "w567 = -0.007054511404513991\n",
      "w568 = -0.04558112573979194\n",
      "w569 = -0.13246119639473994\n",
      "w570 = -0.09489462523821647\n",
      "w571 = -0.09480129612912926\n",
      "w572 = -0.022808568969561332\n",
      "w573 = 0.000968139924200912\n",
      "w574 = -0.06517947496117009\n",
      "w575 = -0.03413573227493716\n",
      "w576 = -0.07602215961237933\n",
      "w577 = -0.1133934829753095\n",
      "w578 = -0.0816757498870057\n",
      "w579 = -0.05057072125960459\n",
      "w580 = -0.06774636955531486\n",
      "w581 = -0.08633432173611437\n",
      "w582 = -0.06472952860800868\n",
      "w583 = -0.12875727447077542\n",
      "w584 = -0.14215230976815466\n",
      "w585 = -0.06900240877684574\n",
      "w586 = -0.01791984582910993\n",
      "w587 = -0.00012405929905528714\n",
      "w588 = -0.0009309191658200007\n",
      "w589 = 0.0006234406192443933\n",
      "w590 = -0.0064440855061559076\n",
      "w591 = -0.06757672193754824\n",
      "w592 = -0.09318591790689945\n",
      "w593 = -0.04784767093294029\n",
      "w594 = 0.025098207776016377\n",
      "w595 = 0.018886726051749486\n",
      "w596 = -0.07638847008082544\n",
      "w597 = -0.11133940310614047\n",
      "w598 = -0.09714986501373343\n",
      "w599 = -0.06148075893229404\n",
      "w600 = 0.044346628447791095\n",
      "w601 = 0.03450214502053067\n",
      "w602 = -0.03522814273064557\n",
      "w603 = -0.11554710055854792\n",
      "w604 = 0.0023751215472450804\n",
      "w605 = -0.02192164191711774\n",
      "w606 = 0.01063672964643808\n",
      "w607 = -0.03168159162414933\n",
      "w608 = -0.0466911295360736\n",
      "w609 = -0.10721838629353461\n",
      "w610 = -0.1021877780669758\n",
      "w611 = -0.12171271377993037\n",
      "w612 = -0.10099194342606245\n",
      "w613 = -0.059558144647146156\n",
      "w614 = -0.013430227700939986\n",
      "w615 = -0.002391702494725085\n",
      "w616 = -0.0009309191658200007\n",
      "w617 = 0.0013876208848594197\n",
      "w618 = -0.00837076268092975\n",
      "w619 = -0.04572416194022375\n",
      "w620 = -0.07484997632958341\n",
      "w621 = -0.10010472564935417\n",
      "w622 = -0.09006816345214426\n",
      "w623 = -0.10583635429583649\n",
      "w624 = -0.09934580466500284\n",
      "w625 = -0.14283101716626273\n",
      "w626 = -0.05443245530832614\n",
      "w627 = 0.0015280067430644864\n",
      "w628 = 0.01597845870815806\n",
      "w629 = -0.060177128659253266\n",
      "w630 = -0.06816499926488864\n",
      "w631 = -0.13872793079150234\n",
      "w632 = -0.06603245985504654\n",
      "w633 = -0.05879275124424743\n",
      "w634 = -0.05028226605306046\n",
      "w635 = -0.10726293371253273\n",
      "w636 = -0.08591774996293554\n",
      "w637 = -0.07676993486176067\n",
      "w638 = -0.07739043405399756\n",
      "w639 = -0.06193297048725438\n",
      "w640 = -0.054798729256520275\n",
      "w641 = -0.03348466890386316\n",
      "w642 = -0.007396196015158139\n",
      "w643 = -0.002391702494725085\n",
      "w644 = 0.0\n",
      "w645 = 0.0\n",
      "w646 = -0.0027783377485257944\n",
      "w647 = -0.011038823599724093\n",
      "w648 = -0.040748922373751426\n",
      "w649 = -0.07621036649888831\n",
      "w650 = -0.10091461790381859\n",
      "w651 = -0.09534798377302302\n",
      "w652 = -0.02579681100529205\n",
      "w653 = -0.11526438185749491\n",
      "w654 = -0.07384926058462503\n",
      "w655 = -0.01095365187361981\n",
      "w656 = 0.0019813231341583525\n",
      "w657 = 0.07847610347740372\n",
      "w658 = 0.0361612293117613\n",
      "w659 = -0.11813404376883863\n",
      "w660 = -0.08528664921529998\n",
      "w661 = -0.05561297967898043\n",
      "w662 = -0.04828749364088264\n",
      "w663 = -0.00437254485684084\n",
      "w664 = 0.08848488030567142\n",
      "w665 = 0.08238353315330345\n",
      "w666 = 0.059208255787754\n",
      "w667 = 0.04020313505301796\n",
      "w668 = -0.0036095610981399795\n",
      "w669 = -0.011780413799603666\n",
      "w670 = -0.006902815096253219\n",
      "w671 = 0.0\n",
      "w672 = 0.0\n",
      "w673 = 0.0\n",
      "w674 = 0.0010634098329465086\n",
      "w675 = -0.0100555760219911\n",
      "w676 = 0.002336539212462632\n",
      "w677 = -0.006507213485399955\n",
      "w678 = -0.00866356640076636\n",
      "w679 = 0.13394474084124744\n",
      "w680 = 0.14969081756879152\n",
      "w681 = 0.10154443980484174\n",
      "w682 = 0.09426585485368362\n",
      "w683 = 0.024961286937366076\n",
      "w684 = 0.025921730541012895\n",
      "w685 = 0.04834393080434278\n",
      "w686 = 0.024446047885953767\n",
      "w687 = 0.025306462231010525\n",
      "w688 = 0.0875330978860755\n",
      "w689 = 0.11599893210335555\n",
      "w690 = 0.18124645123669947\n",
      "w691 = 0.2855012910223311\n",
      "w692 = 0.36627088193791424\n",
      "w693 = 0.26096230773439016\n",
      "w694 = 0.15532286806442758\n",
      "w695 = 0.10020788461627513\n",
      "w696 = 0.01027019027665186\n",
      "w697 = -0.009219873000059829\n",
      "w698 = -0.004620396871219901\n",
      "w699 = 0.0\n",
      "w700 = 0.0\n",
      "w701 = 0.0\n",
      "w702 = 0.0017613899359561396\n",
      "w703 = 0.002456634016042846\n",
      "w704 = 0.030782338436920256\n",
      "w705 = 0.10316821044366065\n",
      "w706 = 0.20234560463630868\n",
      "w707 = 0.3567871533009015\n",
      "w708 = 0.4093359150069499\n",
      "w709 = 0.4170241407000241\n",
      "w710 = 0.3883486507882763\n",
      "w711 = 0.3532043271367992\n",
      "w712 = 0.36947226454912113\n",
      "w713 = 0.3610350788705921\n",
      "w714 = 0.4059125831909614\n",
      "w715 = 0.5079186013496251\n",
      "w716 = 0.5794354362025985\n",
      "w717 = 0.5849676192411997\n",
      "w718 = 0.5178562130902654\n",
      "w719 = 0.4524483729937353\n",
      "w720 = 0.3840301866847617\n",
      "w721 = 0.26950767941968884\n",
      "w722 = 0.13325240428406837\n",
      "w723 = 0.06147646097642432\n",
      "w724 = 0.011539172054113696\n",
      "w725 = -0.0007741490625334687\n",
      "w726 = -0.004153799230385649\n",
      "w727 = 0.0\n",
      "w728 = 0.0\n",
      "w729 = 0.0\n",
      "w730 = 0.0\n",
      "w731 = 0.0037790932267070154\n",
      "w732 = 0.014184668630612308\n",
      "w733 = 0.05336067622373052\n",
      "w734 = 0.11575157725074586\n",
      "w735 = 0.18774735135500553\n",
      "w736 = 0.26261915157312005\n",
      "w737 = 0.28775741725954485\n",
      "w738 = 0.27644442230414606\n",
      "w739 = 0.2920985120521227\n",
      "w740 = 0.3162189677760798\n",
      "w741 = 0.36900465742285166\n",
      "w742 = 0.38884040976176654\n",
      "w743 = 0.3444995497634833\n",
      "w744 = 0.3207941286456629\n",
      "w745 = 0.26513430762370493\n",
      "w746 = 0.23478469893598317\n",
      "w747 = 0.16790465048237105\n",
      "w748 = 0.13882816681014531\n",
      "w749 = 0.09186333821830957\n",
      "w750 = 0.0356000656159223\n",
      "w751 = 0.014151211839897827\n",
      "w752 = -0.00191215955495488\n",
      "w753 = 0.0018923857848574897\n",
      "w754 = 0.0\n",
      "w755 = 0.0\n",
      "w756 = 0.0\n",
      "w757 = 0.0\n",
      "w758 = 0.0\n",
      "w759 = 0.0\n",
      "w760 = 0.0013907128321996626\n",
      "w761 = 0.0034989170931566172\n",
      "w762 = 0.010775419267440746\n",
      "w763 = 0.01576822639386029\n",
      "w764 = 0.012182830561364258\n",
      "w765 = 0.019029597136593895\n",
      "w766 = 0.03546508695776502\n",
      "w767 = 0.03718497768776495\n",
      "w768 = 0.05320600584275695\n",
      "w769 = 0.05327034176082498\n",
      "w770 = 0.08323293152500849\n",
      "w771 = 0.06617489880514676\n",
      "w772 = 0.04776217022062682\n",
      "w773 = 0.033691171980004486\n",
      "w774 = 0.021406640343527628\n",
      "w775 = 0.007828749895152802\n",
      "w776 = 0.014483186883929314\n",
      "w777 = 0.010754357194513585\n",
      "w778 = 0.013086055623331578\n",
      "w779 = 0.008067736614453432\n",
      "w780 = 0.0\n",
      "w781 = 0.0\n",
      "w782 = 0.0\n",
      "w783 = 0.0\n",
      "b = 2.9423954632557083\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.7859)\n",
      "Test error rate: tensor(0.7796)\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(y_pred[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 28.17116928, Error: 0.9014770984649658\n",
      "Epoch [10/1000], Loss: 25.62903404, Error: 0.9065605401992798\n",
      "Epoch [20/1000], Loss: 23.18855286, Error: 0.9622700214385986\n",
      "Epoch [30/1000], Loss: 21.09553909, Error: 0.918519139289856\n",
      "Epoch [40/1000], Loss: 19.29952049, Error: 0.8817266821861267\n",
      "Epoch [50/1000], Loss: 17.75737762, Error: 0.851996898651123\n",
      "Epoch [60/1000], Loss: 16.43225861, Error: 0.8339132070541382\n",
      "Epoch [70/1000], Loss: 15.29268169, Error: 0.8269755840301514\n",
      "Epoch [80/1000], Loss: 14.31173992, Error: 0.8287255764007568\n",
      "Epoch [90/1000], Loss: 13.46644688, Error: 0.8376632928848267\n",
      "Epoch [100/1000], Loss: 12.73715878, Error: 0.8488093614578247\n",
      "Epoch [110/1000], Loss: 12.10709095, Error: 0.8583720326423645\n",
      "Epoch [120/1000], Loss: 11.56189728, Error: 0.8679139018058777\n",
      "Epoch [130/1000], Loss: 11.08932781, Error: 0.8746224045753479\n",
      "Epoch [140/1000], Loss: 10.67889977, Error: 0.8797683119773865\n",
      "Epoch [150/1000], Loss: 10.32166958, Error: 0.8829559087753296\n",
      "Epoch [160/1000], Loss: 10.00998688, Error: 0.8859976530075073\n",
      "Epoch [170/1000], Loss: 9.73731136, Error: 0.8881434798240662\n",
      "Epoch [180/1000], Loss: 9.49806118, Error: 0.8903310298919678\n",
      "Epoch [190/1000], Loss: 9.28745747, Error: 0.8912477493286133\n",
      "Epoch [200/1000], Loss: 9.10142040, Error: 0.8927894234657288\n",
      "Epoch [210/1000], Loss: 8.93646336, Error: 0.894872784614563\n",
      "Epoch [220/1000], Loss: 8.78960228, Error: 0.8949561715126038\n",
      "Epoch [230/1000], Loss: 8.65828800, Error: 0.8959770202636719\n",
      "Epoch [240/1000], Loss: 8.54034328, Error: 0.8968311548233032\n",
      "Epoch [250/1000], Loss: 8.43390274, Error: 0.8976228833198547\n",
      "Epoch [260/1000], Loss: 8.33737469, Error: 0.897372841835022\n",
      "Epoch [270/1000], Loss: 8.24939442, Error: 0.8978520631790161\n",
      "Epoch [280/1000], Loss: 8.16879749, Error: 0.8983728885650635\n",
      "Epoch [290/1000], Loss: 8.09458637, Error: 0.8986645340919495\n",
      "Epoch [300/1000], Loss: 8.02591038, Error: 0.8988937139511108\n",
      "Epoch [310/1000], Loss: 7.96203709, Error: 0.8992895483970642\n",
      "Epoch [320/1000], Loss: 7.90234184, Error: 0.8991228938102722\n",
      "Epoch [330/1000], Loss: 7.84628916, Error: 0.899414598941803\n",
      "Epoch [340/1000], Loss: 7.79342222, Error: 0.8994354009628296\n",
      "Epoch [350/1000], Loss: 7.74334621, Error: 0.8997271060943604\n",
      "Epoch [360/1000], Loss: 7.69572687, Error: 0.899497926235199\n",
      "Epoch [370/1000], Loss: 7.65027332, Error: 0.899497926235199\n",
      "Epoch [380/1000], Loss: 7.60673904, Error: 0.8992270827293396\n",
      "Epoch [390/1000], Loss: 7.56491232, Error: 0.8991020917892456\n",
      "Epoch [400/1000], Loss: 7.52460814, Error: 0.8988520503044128\n",
      "Epoch [410/1000], Loss: 7.48567009, Error: 0.8988937139511108\n",
      "Epoch [420/1000], Loss: 7.44796371, Error: 0.8993104100227356\n",
      "Epoch [430/1000], Loss: 7.41137266, Error: 0.899456262588501\n",
      "Epoch [440/1000], Loss: 7.37579441, Error: 0.8992687463760376\n",
      "Epoch [450/1000], Loss: 7.34114313, Error: 0.899414598941803\n",
      "Epoch [460/1000], Loss: 7.30734491, Error: 0.8993520736694336\n",
      "Epoch [470/1000], Loss: 7.27433252, Error: 0.8991228938102722\n",
      "Epoch [480/1000], Loss: 7.24205256, Error: 0.8990812301635742\n",
      "Epoch [490/1000], Loss: 7.21045208, Error: 0.8991020917892456\n",
      "Epoch [500/1000], Loss: 7.17949057, Error: 0.8990395665168762\n",
      "Epoch [510/1000], Loss: 7.14913082, Error: 0.8987895846366882\n",
      "Epoch [520/1000], Loss: 7.11933708, Error: 0.8986645340919495\n",
      "Epoch [530/1000], Loss: 7.09008455, Error: 0.8983103632926941\n",
      "Epoch [540/1000], Loss: 7.06134319, Error: 0.8981020450592041\n",
      "Epoch [550/1000], Loss: 7.03309441, Error: 0.8977478742599487\n",
      "Epoch [560/1000], Loss: 7.00531578, Error: 0.8977686762809753\n",
      "Epoch [570/1000], Loss: 6.97798967, Error: 0.8976436853408813\n",
      "Epoch [580/1000], Loss: 6.95110035, Error: 0.8977478742599487\n",
      "Epoch [590/1000], Loss: 6.92463255, Error: 0.8975186944007874\n",
      "Epoch [600/1000], Loss: 6.89857388, Error: 0.8973937034606934\n",
      "Epoch [610/1000], Loss: 6.87291193, Error: 0.897081196308136\n",
      "Epoch [620/1000], Loss: 6.84763622, Error: 0.896914541721344\n",
      "Epoch [630/1000], Loss: 6.82273674, Error: 0.8969770073890686\n",
      "Epoch [640/1000], Loss: 6.79820299, Error: 0.8968311548233032\n",
      "Epoch [650/1000], Loss: 6.77402782, Error: 0.8968103528022766\n",
      "Epoch [660/1000], Loss: 6.75020123, Error: 0.8963936567306519\n",
      "Epoch [670/1000], Loss: 6.72671747, Error: 0.8963103294372559\n",
      "Epoch [680/1000], Loss: 6.70356941, Error: 0.8960811495780945\n",
      "Epoch [690/1000], Loss: 6.68074846, Error: 0.8959561586380005\n",
      "Epoch [700/1000], Loss: 6.65825081, Error: 0.8958519697189331\n",
      "Epoch [710/1000], Loss: 6.63606787, Error: 0.8959561586380005\n",
      "Epoch [720/1000], Loss: 6.61419630, Error: 0.8958311676979065\n",
      "Epoch [730/1000], Loss: 6.59262896, Error: 0.8957061767578125\n",
      "Epoch [740/1000], Loss: 6.57136106, Error: 0.8957686424255371\n",
      "Epoch [750/1000], Loss: 6.55038643, Error: 0.8956645131111145\n",
      "Epoch [760/1000], Loss: 6.52970028, Error: 0.8953520059585571\n",
      "Epoch [770/1000], Loss: 6.50929880, Error: 0.8954561352729797\n",
      "Epoch [780/1000], Loss: 6.48917770, Error: 0.8951852917671204\n",
      "Epoch [790/1000], Loss: 6.46933031, Error: 0.8949353098869324\n",
      "Epoch [800/1000], Loss: 6.44975424, Error: 0.8950394988059998\n",
      "Epoch [810/1000], Loss: 6.43044376, Error: 0.8949769735336304\n",
      "Epoch [820/1000], Loss: 6.41139507, Error: 0.8951436281204224\n",
      "Epoch [830/1000], Loss: 6.39260530, Error: 0.8950394988059998\n",
      "Epoch [840/1000], Loss: 6.37406826, Error: 0.894914448261261\n",
      "Epoch [850/1000], Loss: 6.35578108, Error: 0.8948519825935364\n",
      "Epoch [860/1000], Loss: 6.33773994, Error: 0.894622802734375\n",
      "Epoch [870/1000], Loss: 6.31994009, Error: 0.894414484500885\n",
      "Epoch [880/1000], Loss: 6.30238008, Error: 0.8944352865219116\n",
      "Epoch [890/1000], Loss: 6.28505421, Error: 0.8942061066627502\n",
      "Epoch [900/1000], Loss: 6.26795864, Error: 0.8938519358634949\n",
      "Epoch [910/1000], Loss: 6.25109100, Error: 0.8935602903366089\n",
      "Epoch [920/1000], Loss: 6.23444843, Error: 0.8933936357498169\n",
      "Epoch [930/1000], Loss: 6.21802521, Error: 0.8930602669715881\n",
      "Epoch [940/1000], Loss: 6.20181942, Error: 0.8929977416992188\n",
      "Epoch [950/1000], Loss: 6.18582821, Error: 0.8927060961723328\n",
      "Epoch [960/1000], Loss: 6.17004776, Error: 0.8926435708999634\n",
      "Epoch [970/1000], Loss: 6.15447474, Error: 0.892331063747406\n",
      "Epoch [980/1000], Loss: 6.13910723, Error: 0.89203941822052\n",
      "Epoch [990/1000], Loss: 6.12393999, Error: 0.8918935656547546\n",
      "Epoch [1000/1000], Loss: 6.10897255, Error: 0.8920185565948486\n",
      "Trained weights: tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.2573e-05,  1.0165e-05, -3.1509e-06,\n",
      "        -3.1509e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  9.4950e-06,  1.1334e-05,  3.9970e-05,\n",
      "         5.8074e-05,  7.7717e-05,  1.0387e-04,  1.5841e-04,  2.1817e-04,\n",
      "         2.6247e-04,  3.0130e-04,  3.1364e-04,  2.5593e-04,  2.2511e-04,\n",
      "         2.1639e-04,  1.3882e-04,  1.2788e-04,  1.1151e-04,  3.6336e-05,\n",
      "         2.3509e-05,  1.0096e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8868e-06, -1.5658e-05,\n",
      "         2.5308e-05,  2.6503e-05,  7.1796e-05,  2.0210e-04,  3.5799e-04,\n",
      "         6.1810e-04,  1.0999e-03,  1.6470e-03,  2.5423e-03,  3.3659e-03,\n",
      "         4.2805e-03,  4.9039e-03,  5.2811e-03,  5.1230e-03,  4.5389e-03,\n",
      "         3.4274e-03,  2.1146e-03,  1.0720e-03,  5.0072e-04,  1.8182e-04,\n",
      "         5.2440e-05,  4.2314e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  5.3657e-06, -1.1099e-05,  7.7558e-06, -1.3165e-05,\n",
      "         5.6378e-05,  1.5963e-04,  1.9357e-04,  4.0518e-04,  8.8508e-04,\n",
      "         1.4124e-03,  2.1442e-03,  3.2029e-03,  5.3815e-03,  7.2079e-03,\n",
      "         8.5061e-03,  9.6134e-03,  9.3351e-03,  7.3029e-03,  4.6106e-03,\n",
      "         2.7898e-03,  1.4683e-03,  6.4707e-04,  2.6089e-04,  4.5925e-05,\n",
      "         1.4577e-05,  0.0000e+00,  0.0000e+00, -2.5325e-06,  7.5346e-06,\n",
      "        -1.3581e-05, -2.3650e-05, -1.7581e-04, -3.0305e-04, -6.7154e-04,\n",
      "        -1.2784e-03, -1.8558e-03, -2.6148e-03, -4.1085e-03, -6.1521e-03,\n",
      "        -7.8912e-03, -8.5653e-03, -8.7691e-03, -7.0687e-03, -5.0711e-03,\n",
      "        -3.2663e-03, -2.1164e-03, -3.9337e-04,  1.3114e-03,  1.7831e-03,\n",
      "         1.6092e-03,  9.1116e-04,  2.8591e-04,  7.5981e-05,  3.6622e-05,\n",
      "         0.0000e+00,  0.0000e+00,  1.2228e-05, -3.6362e-05, -1.6323e-04,\n",
      "        -6.5349e-04, -1.3726e-03, -2.4012e-03, -3.4049e-03, -4.4448e-03,\n",
      "        -5.9394e-03, -7.0916e-03, -7.8790e-03, -1.0396e-02, -1.2527e-02,\n",
      "        -1.3337e-02, -1.3033e-02, -1.2060e-02, -1.0641e-02, -8.0690e-03,\n",
      "        -3.4652e-03,  1.1263e-03,  3.6547e-03,  3.9286e-03,  2.6663e-03,\n",
      "         1.0710e-03,  1.9916e-04,  3.6934e-05,  0.0000e+00, -2.4864e-06,\n",
      "         6.2534e-05,  4.0497e-06, -1.9815e-04, -6.1702e-04, -1.2103e-03,\n",
      "        -1.6370e-03, -1.1143e-03,  6.3861e-04,  2.8803e-03,  7.1482e-03,\n",
      "         1.2726e-02,  1.7799e-02,  2.0760e-02,  2.0365e-02,  1.5298e-02,\n",
      "         7.5086e-03,  1.4059e-04, -3.4758e-03, -2.4446e-03,  2.1479e-03,\n",
      "         5.7660e-03,  6.8980e-03,  5.1075e-03,  2.3520e-03,  5.0509e-04,\n",
      "         5.9541e-05,  1.1068e-05,  3.2638e-05,  2.3942e-04,  5.6296e-04,\n",
      "         1.0765e-03,  1.7704e-03,  3.4016e-03,  6.9882e-03,  1.2954e-02,\n",
      "         2.0392e-02,  3.0854e-02,  4.3977e-02,  5.6420e-02,  6.4541e-02,\n",
      "         6.5035e-02,  6.0743e-02,  5.0869e-02,  3.6759e-02,  2.3149e-02,\n",
      "         1.2310e-02,  7.0602e-03,  6.0285e-03,  7.0598e-03,  9.3420e-03,\n",
      "         7.4890e-03,  3.4687e-03,  7.5154e-04,  7.1144e-05,  1.1068e-05,\n",
      "         1.3551e-04,  8.0441e-04,  2.0611e-03,  3.9407e-03,  6.7777e-03,\n",
      "         1.1818e-02,  2.1017e-02,  3.3126e-02,  4.7807e-02,  6.6344e-02,\n",
      "         8.2968e-02,  9.1816e-02,  8.7672e-02,  7.4645e-02,  6.2184e-02,\n",
      "         5.5530e-02,  4.9594e-02,  4.0525e-02,  2.7113e-02,  1.4898e-02,\n",
      "         8.8498e-03,  7.1642e-03,  8.7604e-03,  8.1731e-03,  4.0444e-03,\n",
      "         7.6737e-04,  8.3846e-05,  1.1068e-05,  2.2250e-04,  1.2542e-03,\n",
      "         3.2236e-03,  6.3109e-03,  1.1472e-02,  1.9712e-02,  3.2482e-02,\n",
      "         4.9799e-02,  6.9075e-02,  8.7945e-02,  9.8121e-02,  9.2319e-02,\n",
      "         7.0909e-02,  4.7843e-02,  3.5194e-02,  4.1437e-02,  5.3025e-02,\n",
      "         5.3735e-02,  4.0494e-02,  2.1414e-02,  8.0526e-03,  2.8433e-03,\n",
      "         4.2920e-03,  5.8396e-03,  3.2910e-03,  6.7568e-04,  9.2016e-05,\n",
      "         1.9853e-05,  2.5686e-04,  1.1953e-03,  3.1485e-03,  7.1504e-03,\n",
      "         1.3305e-02,  2.2976e-02,  3.6757e-02,  5.5322e-02,  7.4508e-02,\n",
      "         8.8749e-02,  8.8683e-02,  7.0704e-02,  4.1921e-02,  1.7246e-02,\n",
      "         1.3994e-02,  3.6028e-02,  6.2910e-02,  7.0915e-02,  5.4135e-02,\n",
      "         2.4933e-02,  3.4864e-03, -4.9832e-03, -3.2845e-03,  1.8610e-03,\n",
      "         2.2028e-03,  4.8533e-04,  6.1611e-05,  3.1130e-05,  1.7119e-04,\n",
      "         9.2873e-04,  2.6156e-03,  6.5517e-03,  1.2542e-02,  2.1293e-02,\n",
      "         3.4704e-02,  5.2803e-02,  7.0447e-02,  7.8660e-02,  7.2358e-02,\n",
      "         5.3546e-02,  2.6810e-02,  7.1711e-03,  1.4599e-02,  5.0304e-02,\n",
      "         8.5842e-02,  9.1610e-02,  6.3298e-02,  2.4268e-02, -3.7205e-03,\n",
      "        -1.2792e-02, -9.6910e-03, -1.5794e-03,  1.0484e-03,  2.4572e-04,\n",
      "         3.2477e-05,  1.9878e-05,  1.3024e-04,  6.0894e-04,  2.0084e-03,\n",
      "         5.5656e-03,  9.7932e-03,  1.5958e-02,  2.8018e-02,  4.4883e-02,\n",
      "         6.0949e-02,  6.7381e-02,  6.3665e-02,  5.1362e-02,  3.1064e-02,\n",
      "         2.1219e-02,  4.1203e-02,  8.4327e-02,  1.1561e-01,  1.0534e-01,\n",
      "         6.3670e-02,  1.8005e-02, -1.0160e-02, -1.7830e-02, -1.3024e-02,\n",
      "        -3.6794e-03,  3.6018e-04,  1.0818e-04,  2.6085e-05, -3.8287e-06,\n",
      "         5.1586e-05,  3.3026e-04,  1.4823e-03,  4.3490e-03,  5.3930e-03,\n",
      "         8.3236e-03,  1.7812e-02,  3.4435e-02,  5.0868e-02,  6.0663e-02,\n",
      "         6.4666e-02,  6.2225e-02,  4.9923e-02,  5.1773e-02,  7.9430e-02,\n",
      "         1.1950e-01,  1.3436e-01,  1.0486e-01,  5.4031e-02,  8.7454e-03,\n",
      "        -1.4702e-02, -1.9324e-02, -1.3673e-02, -4.1071e-03,  2.5751e-04,\n",
      "         4.5189e-05,  4.3065e-06,  8.3493e-06,  2.3233e-05,  1.4991e-04,\n",
      "         1.0401e-03,  2.3034e-03, -5.3652e-04, -1.2343e-03,  5.2874e-03,\n",
      "         2.1889e-02,  4.1729e-02,  5.8468e-02,  7.1343e-02,  7.4561e-02,\n",
      "         6.9324e-02,  7.8910e-02,  1.0597e-01,  1.3819e-01,  1.3425e-01,\n",
      "         8.9801e-02,  3.7457e-02,  1.6207e-04, -1.6695e-02, -1.8004e-02,\n",
      "        -1.2226e-02, -4.0277e-03,  2.4069e-04, -3.6863e-05, -7.1839e-06,\n",
      "         1.1362e-05,  1.1305e-06,  6.7369e-05,  7.4922e-04, -3.3769e-04,\n",
      "        -7.7098e-03, -1.1870e-02, -7.9716e-03,  9.1085e-03,  3.4514e-02,\n",
      "         5.8397e-02,  7.4882e-02,  7.8600e-02,  7.6142e-02,  8.6964e-02,\n",
      "         1.1396e-01,  1.3673e-01,  1.1795e-01,  6.6971e-02,  2.0531e-02,\n",
      "        -6.3576e-03, -1.6307e-02, -1.5839e-02, -1.0276e-02, -3.4072e-03,\n",
      "         2.0231e-04, -1.0407e-04, -1.0478e-05,  1.5924e-05, -5.3081e-06,\n",
      "        -1.7547e-05,  2.9849e-04, -3.3418e-03, -1.4787e-02, -2.2148e-02,\n",
      "        -2.0050e-02, -2.3144e-03,  2.7031e-02,  5.5379e-02,  7.1023e-02,\n",
      "         7.1200e-02,  6.6994e-02,  8.0546e-02,  1.0931e-01,  1.2216e-01,\n",
      "         9.3043e-02,  4.4559e-02,  7.8602e-03, -1.0553e-02, -1.6319e-02,\n",
      "        -1.3714e-02, -8.3844e-03, -2.8345e-03, -2.5898e-04, -1.2277e-04,\n",
      "        -8.4165e-06,  0.0000e+00, -5.7249e-06, -2.7789e-05, -3.5875e-04,\n",
      "        -6.5437e-03, -2.0607e-02, -2.9857e-02, -2.8923e-02, -1.0938e-02,\n",
      "         1.9754e-02,  4.7187e-02,  5.8392e-02,  5.3332e-02,  5.1495e-02,\n",
      "         7.1690e-02,  9.8090e-02,  1.0009e-01,  6.8290e-02,  2.8720e-02,\n",
      "         1.2357e-03, -1.1989e-02, -1.5211e-02, -1.2201e-02, -6.8890e-03,\n",
      "        -2.7900e-03, -8.5549e-04, -1.7565e-04, -2.1304e-05,  1.9029e-05,\n",
      "         4.8790e-07, -4.5017e-05, -1.0091e-03, -8.7234e-03, -2.3463e-02,\n",
      "        -3.3017e-02, -3.3108e-02, -1.7223e-02,  1.0575e-02,  3.4274e-02,\n",
      "         4.1475e-02,  3.6722e-02,  4.1585e-02,  6.3886e-02,  8.2478e-02,\n",
      "         7.6780e-02,  4.9601e-02,  1.9627e-02, -2.4675e-03, -1.1842e-02,\n",
      "        -1.4015e-02, -1.0830e-02, -6.3271e-03, -3.3644e-03, -1.1331e-03,\n",
      "        -1.2121e-04, -1.4607e-05,  0.0000e+00,  2.9442e-06, -1.2315e-04,\n",
      "        -1.5570e-03, -9.5816e-03, -2.3267e-02, -3.2988e-02, -3.4174e-02,\n",
      "        -2.2554e-02, -1.8490e-03,  1.7883e-02,  2.5361e-02,  2.8495e-02,\n",
      "         3.9138e-02,  5.8082e-02,  6.7401e-02,  5.8263e-02,  3.6649e-02,\n",
      "         1.2997e-02, -3.3652e-03, -1.0582e-02, -1.2198e-02, -9.1513e-03,\n",
      "        -5.7531e-03, -3.1268e-03, -1.0342e-03, -1.2128e-04, -9.6593e-06,\n",
      "         0.0000e+00, -9.7996e-06, -2.1077e-04, -1.8577e-03, -8.9783e-03,\n",
      "        -2.0256e-02, -2.9912e-02, -3.3593e-02, -2.9070e-02, -1.7290e-02,\n",
      "        -1.1728e-03,  1.1885e-02,  2.5549e-02,  4.1128e-02,  5.3402e-02,\n",
      "         5.4896e-02,  4.4805e-02,  2.8603e-02,  1.1405e-02, -9.7675e-04,\n",
      "        -7.2756e-03, -8.4996e-03, -6.4717e-03, -4.3128e-03, -2.4303e-03,\n",
      "        -6.9781e-04, -1.1212e-04,  6.7920e-06, -7.5799e-06, -8.4781e-06,\n",
      "        -2.2569e-04, -1.6750e-03, -6.7328e-03, -1.4977e-02, -2.3145e-02,\n",
      "        -2.9484e-02, -3.1308e-02, -2.5999e-02, -1.2577e-02,  5.7113e-03,\n",
      "         2.5298e-02,  4.0865e-02,  4.7086e-02,  4.4626e-02,  3.7422e-02,\n",
      "         2.6011e-02,  1.3406e-02,  3.7447e-03, -1.2370e-03, -3.1651e-03,\n",
      "        -3.0438e-03, -2.5637e-03, -1.5640e-03, -4.4091e-04, -8.1008e-05,\n",
      "        -1.7535e-05, -7.5799e-06, -6.4434e-07, -1.5439e-04, -1.0223e-03,\n",
      "        -3.6694e-03, -8.3511e-03, -1.3826e-02, -1.9179e-02, -2.1811e-02,\n",
      "        -1.8737e-02, -6.2813e-03,  1.1889e-02,  2.9307e-02,  3.9252e-02,\n",
      "         4.2024e-02,  4.0019e-02,  3.5787e-02,  2.8646e-02,  1.9006e-02,\n",
      "         1.0333e-02,  4.7523e-03,  1.2915e-03, -4.5880e-04, -8.8293e-04,\n",
      "        -6.3149e-04, -1.6760e-04, -3.7022e-05, -1.7535e-05,  0.0000e+00,\n",
      "         0.0000e+00, -5.4144e-05, -3.0488e-04, -1.1302e-03, -2.3737e-03,\n",
      "        -3.1842e-03, -3.4088e-03, -1.7978e-03,  3.7237e-03,  1.5704e-02,\n",
      "         3.1419e-02,  4.4058e-02,  5.0139e-02,  4.9243e-02,  4.5820e-02,\n",
      "         4.1175e-02,  3.3984e-02,  2.3965e-02,  1.4927e-02,  8.3351e-03,\n",
      "         3.7368e-03,  1.4000e-03,  4.1577e-04,  2.5651e-05, -2.8497e-05,\n",
      "        -2.8967e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2002e-06,\n",
      "        -3.1956e-05,  3.3661e-05,  4.8400e-04,  2.4830e-03,  7.3445e-03,\n",
      "         1.4949e-02,  2.5604e-02,  3.8558e-02,  4.9333e-02,  5.5715e-02,\n",
      "         5.6241e-02,  5.2129e-02,  4.6235e-02,  3.9646e-02,  3.1170e-02,\n",
      "         2.1792e-02,  1.4017e-02,  8.3788e-03,  4.3396e-03,  1.9394e-03,\n",
      "         8.6491e-04,  1.3639e-04, -1.5505e-05, -9.3166e-06,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.6578e-05,  4.2257e-05,  2.8856e-04,\n",
      "         1.0422e-03,  3.0230e-03,  6.9351e-03,  1.3237e-02,  2.1125e-02,\n",
      "         2.9421e-02,  3.4832e-02,  3.7497e-02,  3.7092e-02,  3.4252e-02,\n",
      "         3.0581e-02,  2.6407e-02,  2.0823e-02,  1.4688e-02,  9.5190e-03,\n",
      "         5.5116e-03,  2.9383e-03,  1.2933e-03,  4.8512e-04,  8.4251e-05,\n",
      "         1.0446e-05, -7.9782e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.4837e-05,  1.0028e-04,  4.2394e-04,  1.3482e-03,\n",
      "         2.9374e-03,  5.7706e-03,  8.6502e-03,  1.1492e-02,  1.3840e-02,\n",
      "         1.5053e-02,  1.5145e-02,  1.4238e-02,  1.2123e-02,  1.0355e-02,\n",
      "         7.9055e-03,  5.4639e-03,  3.4026e-03,  1.9690e-03,  1.0011e-03,\n",
      "         3.2557e-04,  1.1976e-04,  1.8368e-06,  1.3995e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.3841e-05,  3.5590e-05,  1.0759e-04,  2.3398e-04,  2.8396e-04,\n",
      "         4.2083e-04,  7.4679e-04,  9.4170e-04,  1.3008e-03,  1.3883e-03,\n",
      "         1.7784e-03,  1.6177e-03,  1.3511e-03,  9.9696e-04,  6.3082e-04,\n",
      "         3.0139e-04,  2.0154e-04,  1.0058e-04,  8.5222e-05,  4.6464e-05,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "       requires_grad=True)\n",
      "Trained bias: tensor([0.1554], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.8920)\n",
      "w detach tensor([ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.2573e-05,  1.0165e-05, -3.1509e-06,\n",
      "        -3.1509e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  9.4950e-06,  1.1334e-05,  3.9970e-05,\n",
      "         5.8074e-05,  7.7717e-05,  1.0387e-04,  1.5841e-04,  2.1817e-04,\n",
      "         2.6247e-04,  3.0130e-04,  3.1364e-04,  2.5593e-04,  2.2511e-04,\n",
      "         2.1639e-04,  1.3882e-04,  1.2788e-04,  1.1151e-04,  3.6336e-05,\n",
      "         2.3509e-05,  1.0096e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  5.8868e-06, -1.5658e-05,\n",
      "         2.5308e-05,  2.6503e-05,  7.1796e-05,  2.0210e-04,  3.5799e-04,\n",
      "         6.1810e-04,  1.0999e-03,  1.6470e-03,  2.5423e-03,  3.3659e-03,\n",
      "         4.2805e-03,  4.9039e-03,  5.2811e-03,  5.1230e-03,  4.5389e-03,\n",
      "         3.4274e-03,  2.1146e-03,  1.0720e-03,  5.0072e-04,  1.8182e-04,\n",
      "         5.2440e-05,  4.2314e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  5.3657e-06, -1.1099e-05,  7.7558e-06, -1.3165e-05,\n",
      "         5.6378e-05,  1.5963e-04,  1.9357e-04,  4.0518e-04,  8.8508e-04,\n",
      "         1.4124e-03,  2.1442e-03,  3.2029e-03,  5.3815e-03,  7.2079e-03,\n",
      "         8.5061e-03,  9.6134e-03,  9.3351e-03,  7.3029e-03,  4.6106e-03,\n",
      "         2.7898e-03,  1.4683e-03,  6.4707e-04,  2.6089e-04,  4.5925e-05,\n",
      "         1.4577e-05,  0.0000e+00,  0.0000e+00, -2.5325e-06,  7.5346e-06,\n",
      "        -1.3581e-05, -2.3650e-05, -1.7581e-04, -3.0305e-04, -6.7154e-04,\n",
      "        -1.2784e-03, -1.8558e-03, -2.6148e-03, -4.1085e-03, -6.1521e-03,\n",
      "        -7.8912e-03, -8.5653e-03, -8.7691e-03, -7.0687e-03, -5.0711e-03,\n",
      "        -3.2663e-03, -2.1164e-03, -3.9337e-04,  1.3114e-03,  1.7831e-03,\n",
      "         1.6092e-03,  9.1116e-04,  2.8591e-04,  7.5981e-05,  3.6622e-05,\n",
      "         0.0000e+00,  0.0000e+00,  1.2228e-05, -3.6362e-05, -1.6323e-04,\n",
      "        -6.5349e-04, -1.3726e-03, -2.4012e-03, -3.4049e-03, -4.4448e-03,\n",
      "        -5.9394e-03, -7.0916e-03, -7.8790e-03, -1.0396e-02, -1.2527e-02,\n",
      "        -1.3337e-02, -1.3033e-02, -1.2060e-02, -1.0641e-02, -8.0690e-03,\n",
      "        -3.4652e-03,  1.1263e-03,  3.6547e-03,  3.9286e-03,  2.6663e-03,\n",
      "         1.0710e-03,  1.9916e-04,  3.6934e-05,  0.0000e+00, -2.4864e-06,\n",
      "         6.2534e-05,  4.0497e-06, -1.9815e-04, -6.1702e-04, -1.2103e-03,\n",
      "        -1.6370e-03, -1.1143e-03,  6.3861e-04,  2.8803e-03,  7.1482e-03,\n",
      "         1.2726e-02,  1.7799e-02,  2.0760e-02,  2.0365e-02,  1.5298e-02,\n",
      "         7.5086e-03,  1.4059e-04, -3.4758e-03, -2.4446e-03,  2.1479e-03,\n",
      "         5.7660e-03,  6.8980e-03,  5.1075e-03,  2.3520e-03,  5.0509e-04,\n",
      "         5.9541e-05,  1.1068e-05,  3.2638e-05,  2.3942e-04,  5.6296e-04,\n",
      "         1.0765e-03,  1.7704e-03,  3.4016e-03,  6.9882e-03,  1.2954e-02,\n",
      "         2.0392e-02,  3.0854e-02,  4.3977e-02,  5.6420e-02,  6.4541e-02,\n",
      "         6.5035e-02,  6.0743e-02,  5.0869e-02,  3.6759e-02,  2.3149e-02,\n",
      "         1.2310e-02,  7.0602e-03,  6.0285e-03,  7.0598e-03,  9.3420e-03,\n",
      "         7.4890e-03,  3.4687e-03,  7.5154e-04,  7.1144e-05,  1.1068e-05,\n",
      "         1.3551e-04,  8.0441e-04,  2.0611e-03,  3.9407e-03,  6.7777e-03,\n",
      "         1.1818e-02,  2.1017e-02,  3.3126e-02,  4.7807e-02,  6.6344e-02,\n",
      "         8.2968e-02,  9.1816e-02,  8.7672e-02,  7.4645e-02,  6.2184e-02,\n",
      "         5.5530e-02,  4.9594e-02,  4.0525e-02,  2.7113e-02,  1.4898e-02,\n",
      "         8.8498e-03,  7.1642e-03,  8.7604e-03,  8.1731e-03,  4.0444e-03,\n",
      "         7.6737e-04,  8.3846e-05,  1.1068e-05,  2.2250e-04,  1.2542e-03,\n",
      "         3.2236e-03,  6.3109e-03,  1.1472e-02,  1.9712e-02,  3.2482e-02,\n",
      "         4.9799e-02,  6.9075e-02,  8.7945e-02,  9.8121e-02,  9.2319e-02,\n",
      "         7.0909e-02,  4.7843e-02,  3.5194e-02,  4.1437e-02,  5.3025e-02,\n",
      "         5.3735e-02,  4.0494e-02,  2.1414e-02,  8.0526e-03,  2.8433e-03,\n",
      "         4.2920e-03,  5.8396e-03,  3.2910e-03,  6.7568e-04,  9.2016e-05,\n",
      "         1.9853e-05,  2.5686e-04,  1.1953e-03,  3.1485e-03,  7.1504e-03,\n",
      "         1.3305e-02,  2.2976e-02,  3.6757e-02,  5.5322e-02,  7.4508e-02,\n",
      "         8.8749e-02,  8.8683e-02,  7.0704e-02,  4.1921e-02,  1.7246e-02,\n",
      "         1.3994e-02,  3.6028e-02,  6.2910e-02,  7.0915e-02,  5.4135e-02,\n",
      "         2.4933e-02,  3.4864e-03, -4.9832e-03, -3.2845e-03,  1.8610e-03,\n",
      "         2.2028e-03,  4.8533e-04,  6.1611e-05,  3.1130e-05,  1.7119e-04,\n",
      "         9.2873e-04,  2.6156e-03,  6.5517e-03,  1.2542e-02,  2.1293e-02,\n",
      "         3.4704e-02,  5.2803e-02,  7.0447e-02,  7.8660e-02,  7.2358e-02,\n",
      "         5.3546e-02,  2.6810e-02,  7.1711e-03,  1.4599e-02,  5.0304e-02,\n",
      "         8.5842e-02,  9.1610e-02,  6.3298e-02,  2.4268e-02, -3.7205e-03,\n",
      "        -1.2792e-02, -9.6910e-03, -1.5794e-03,  1.0484e-03,  2.4572e-04,\n",
      "         3.2477e-05,  1.9878e-05,  1.3024e-04,  6.0894e-04,  2.0084e-03,\n",
      "         5.5656e-03,  9.7932e-03,  1.5958e-02,  2.8018e-02,  4.4883e-02,\n",
      "         6.0949e-02,  6.7381e-02,  6.3665e-02,  5.1362e-02,  3.1064e-02,\n",
      "         2.1219e-02,  4.1203e-02,  8.4327e-02,  1.1561e-01,  1.0534e-01,\n",
      "         6.3670e-02,  1.8005e-02, -1.0160e-02, -1.7830e-02, -1.3024e-02,\n",
      "        -3.6794e-03,  3.6018e-04,  1.0818e-04,  2.6085e-05, -3.8287e-06,\n",
      "         5.1586e-05,  3.3026e-04,  1.4823e-03,  4.3490e-03,  5.3930e-03,\n",
      "         8.3236e-03,  1.7812e-02,  3.4435e-02,  5.0868e-02,  6.0663e-02,\n",
      "         6.4666e-02,  6.2225e-02,  4.9923e-02,  5.1773e-02,  7.9430e-02,\n",
      "         1.1950e-01,  1.3436e-01,  1.0486e-01,  5.4031e-02,  8.7454e-03,\n",
      "        -1.4702e-02, -1.9324e-02, -1.3673e-02, -4.1071e-03,  2.5751e-04,\n",
      "         4.5189e-05,  4.3065e-06,  8.3493e-06,  2.3233e-05,  1.4991e-04,\n",
      "         1.0401e-03,  2.3034e-03, -5.3652e-04, -1.2343e-03,  5.2874e-03,\n",
      "         2.1889e-02,  4.1729e-02,  5.8468e-02,  7.1343e-02,  7.4561e-02,\n",
      "         6.9324e-02,  7.8910e-02,  1.0597e-01,  1.3819e-01,  1.3425e-01,\n",
      "         8.9801e-02,  3.7457e-02,  1.6207e-04, -1.6695e-02, -1.8004e-02,\n",
      "        -1.2226e-02, -4.0277e-03,  2.4069e-04, -3.6863e-05, -7.1839e-06,\n",
      "         1.1362e-05,  1.1305e-06,  6.7369e-05,  7.4922e-04, -3.3769e-04,\n",
      "        -7.7098e-03, -1.1870e-02, -7.9716e-03,  9.1085e-03,  3.4514e-02,\n",
      "         5.8397e-02,  7.4882e-02,  7.8600e-02,  7.6142e-02,  8.6964e-02,\n",
      "         1.1396e-01,  1.3673e-01,  1.1795e-01,  6.6971e-02,  2.0531e-02,\n",
      "        -6.3576e-03, -1.6307e-02, -1.5839e-02, -1.0276e-02, -3.4072e-03,\n",
      "         2.0231e-04, -1.0407e-04, -1.0478e-05,  1.5924e-05, -5.3081e-06,\n",
      "        -1.7547e-05,  2.9849e-04, -3.3418e-03, -1.4787e-02, -2.2148e-02,\n",
      "        -2.0050e-02, -2.3144e-03,  2.7031e-02,  5.5379e-02,  7.1023e-02,\n",
      "         7.1200e-02,  6.6994e-02,  8.0546e-02,  1.0931e-01,  1.2216e-01,\n",
      "         9.3043e-02,  4.4559e-02,  7.8602e-03, -1.0553e-02, -1.6319e-02,\n",
      "        -1.3714e-02, -8.3844e-03, -2.8345e-03, -2.5898e-04, -1.2277e-04,\n",
      "        -8.4165e-06,  0.0000e+00, -5.7249e-06, -2.7789e-05, -3.5875e-04,\n",
      "        -6.5437e-03, -2.0607e-02, -2.9857e-02, -2.8923e-02, -1.0938e-02,\n",
      "         1.9754e-02,  4.7187e-02,  5.8392e-02,  5.3332e-02,  5.1495e-02,\n",
      "         7.1690e-02,  9.8090e-02,  1.0009e-01,  6.8290e-02,  2.8720e-02,\n",
      "         1.2357e-03, -1.1989e-02, -1.5211e-02, -1.2201e-02, -6.8890e-03,\n",
      "        -2.7900e-03, -8.5549e-04, -1.7565e-04, -2.1304e-05,  1.9029e-05,\n",
      "         4.8790e-07, -4.5017e-05, -1.0091e-03, -8.7234e-03, -2.3463e-02,\n",
      "        -3.3017e-02, -3.3108e-02, -1.7223e-02,  1.0575e-02,  3.4274e-02,\n",
      "         4.1475e-02,  3.6722e-02,  4.1585e-02,  6.3886e-02,  8.2478e-02,\n",
      "         7.6780e-02,  4.9601e-02,  1.9627e-02, -2.4675e-03, -1.1842e-02,\n",
      "        -1.4015e-02, -1.0830e-02, -6.3271e-03, -3.3644e-03, -1.1331e-03,\n",
      "        -1.2121e-04, -1.4607e-05,  0.0000e+00,  2.9442e-06, -1.2315e-04,\n",
      "        -1.5570e-03, -9.5816e-03, -2.3267e-02, -3.2988e-02, -3.4174e-02,\n",
      "        -2.2554e-02, -1.8490e-03,  1.7883e-02,  2.5361e-02,  2.8495e-02,\n",
      "         3.9138e-02,  5.8082e-02,  6.7401e-02,  5.8263e-02,  3.6649e-02,\n",
      "         1.2997e-02, -3.3652e-03, -1.0582e-02, -1.2198e-02, -9.1513e-03,\n",
      "        -5.7531e-03, -3.1268e-03, -1.0342e-03, -1.2128e-04, -9.6593e-06,\n",
      "         0.0000e+00, -9.7996e-06, -2.1077e-04, -1.8577e-03, -8.9783e-03,\n",
      "        -2.0256e-02, -2.9912e-02, -3.3593e-02, -2.9070e-02, -1.7290e-02,\n",
      "        -1.1728e-03,  1.1885e-02,  2.5549e-02,  4.1128e-02,  5.3402e-02,\n",
      "         5.4896e-02,  4.4805e-02,  2.8603e-02,  1.1405e-02, -9.7675e-04,\n",
      "        -7.2756e-03, -8.4996e-03, -6.4717e-03, -4.3128e-03, -2.4303e-03,\n",
      "        -6.9781e-04, -1.1212e-04,  6.7920e-06, -7.5799e-06, -8.4781e-06,\n",
      "        -2.2569e-04, -1.6750e-03, -6.7328e-03, -1.4977e-02, -2.3145e-02,\n",
      "        -2.9484e-02, -3.1308e-02, -2.5999e-02, -1.2577e-02,  5.7113e-03,\n",
      "         2.5298e-02,  4.0865e-02,  4.7086e-02,  4.4626e-02,  3.7422e-02,\n",
      "         2.6011e-02,  1.3406e-02,  3.7447e-03, -1.2370e-03, -3.1651e-03,\n",
      "        -3.0438e-03, -2.5637e-03, -1.5640e-03, -4.4091e-04, -8.1008e-05,\n",
      "        -1.7535e-05, -7.5799e-06, -6.4434e-07, -1.5439e-04, -1.0223e-03,\n",
      "        -3.6694e-03, -8.3511e-03, -1.3826e-02, -1.9179e-02, -2.1811e-02,\n",
      "        -1.8737e-02, -6.2813e-03,  1.1889e-02,  2.9307e-02,  3.9252e-02,\n",
      "         4.2024e-02,  4.0019e-02,  3.5787e-02,  2.8646e-02,  1.9006e-02,\n",
      "         1.0333e-02,  4.7523e-03,  1.2915e-03, -4.5880e-04, -8.8293e-04,\n",
      "        -6.3149e-04, -1.6760e-04, -3.7022e-05, -1.7535e-05,  0.0000e+00,\n",
      "         0.0000e+00, -5.4144e-05, -3.0488e-04, -1.1302e-03, -2.3737e-03,\n",
      "        -3.1842e-03, -3.4088e-03, -1.7978e-03,  3.7237e-03,  1.5704e-02,\n",
      "         3.1419e-02,  4.4058e-02,  5.0139e-02,  4.9243e-02,  4.5820e-02,\n",
      "         4.1175e-02,  3.3984e-02,  2.3965e-02,  1.4927e-02,  8.3351e-03,\n",
      "         3.7368e-03,  1.4000e-03,  4.1577e-04,  2.5651e-05, -2.8497e-05,\n",
      "        -2.8967e-05,  0.0000e+00,  0.0000e+00,  0.0000e+00, -1.2002e-06,\n",
      "        -3.1956e-05,  3.3661e-05,  4.8400e-04,  2.4830e-03,  7.3445e-03,\n",
      "         1.4949e-02,  2.5604e-02,  3.8558e-02,  4.9333e-02,  5.5715e-02,\n",
      "         5.6241e-02,  5.2129e-02,  4.6235e-02,  3.9646e-02,  3.1170e-02,\n",
      "         2.1792e-02,  1.4017e-02,  8.3788e-03,  4.3396e-03,  1.9394e-03,\n",
      "         8.6491e-04,  1.3639e-04, -1.5505e-05, -9.3166e-06,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  1.6578e-05,  4.2257e-05,  2.8856e-04,\n",
      "         1.0422e-03,  3.0230e-03,  6.9351e-03,  1.3237e-02,  2.1125e-02,\n",
      "         2.9421e-02,  3.4832e-02,  3.7497e-02,  3.7092e-02,  3.4252e-02,\n",
      "         3.0581e-02,  2.6407e-02,  2.0823e-02,  1.4688e-02,  9.5190e-03,\n",
      "         5.5116e-03,  2.9383e-03,  1.2933e-03,  4.8512e-04,  8.4251e-05,\n",
      "         1.0446e-05, -7.9782e-06,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  2.4837e-05,  1.0028e-04,  4.2394e-04,  1.3482e-03,\n",
      "         2.9374e-03,  5.7706e-03,  8.6502e-03,  1.1492e-02,  1.3840e-02,\n",
      "         1.5053e-02,  1.5145e-02,  1.4238e-02,  1.2123e-02,  1.0355e-02,\n",
      "         7.9055e-03,  5.4639e-03,  3.4026e-03,  1.9690e-03,  1.0011e-03,\n",
      "         3.2557e-04,  1.1976e-04,  1.8368e-06,  1.3995e-05,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         1.3841e-05,  3.5590e-05,  1.0759e-04,  2.3398e-04,  2.8396e-04,\n",
      "         4.2083e-04,  7.4679e-04,  9.4170e-04,  1.3008e-03,  1.3883e-03,\n",
      "         1.7784e-03,  1.6177e-03,  1.3511e-03,  9.9696e-04,  6.3082e-04,\n",
      "         3.0139e-04,  2.0154e-04,  1.0058e-04,  8.5222e-05,  4.6464e-05,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00])\n",
      "b detach tensor([0.1554])\n",
      "Test error rate: tensor(0.8892)\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 1000 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "print(\"w detach\", w.detach())\n",
    "print(\"b detach\", b.detach())\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Optimizer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76c6390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features=1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(-0.0785, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/1000], Loss: 29.95688438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_20492\\304550788.py:18: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  param.data.add_(-lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4979, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/1000], Loss: 6.14050770\n",
      "tensor(2.6205, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/1000], Loss: 5.28617573\n",
      "tensor(2.6842, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/1000], Loss: 4.96636009\n",
      "tensor(2.7140, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/1000], Loss: 4.79325199\n",
      "tensor(2.7252, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/1000], Loss: 4.67370129\n",
      "tensor(2.7267, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/1000], Loss: 4.58104849\n",
      "tensor(2.7238, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/1000], Loss: 4.50528526\n",
      "tensor(2.7190, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/1000], Loss: 4.44141483\n",
      "tensor(2.7138, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/1000], Loss: 4.38639736\n",
      "tensor(2.7089, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/1000], Loss: 4.33818531\n",
      "activation_stack.0.weight: tensor([[ 1.3733e-02, -3.0009e-02, -2.4011e-02, -2.3770e-03,  1.0390e-02,\n",
      "         -1.1008e-02,  2.5487e-02, -3.2253e-02, -4.7670e-03,  3.4530e-02,\n",
      "         -2.1759e-02,  6.7775e-03,  1.7395e-02,  1.5530e-02, -1.3260e-02,\n",
      "         -4.1435e-03, -2.6904e-02, -6.1617e-03,  2.4629e-02, -1.8719e-02,\n",
      "         -2.6954e-02,  2.1506e-02, -3.1948e-02,  2.6384e-02, -2.2306e-02,\n",
      "         -2.0686e-02, -2.9630e-02, -1.2115e-02, -1.9484e-02,  1.3978e-02,\n",
      "          3.3085e-02, -2.9635e-02,  7.7709e-03, -3.5397e-02,  2.2608e-02,\n",
      "         -3.4665e-02,  3.5910e-02, -2.3930e-02, -6.0738e-04,  1.9453e-02,\n",
      "         -2.1044e-02, -1.4924e-02, -2.6124e-02,  1.8811e-03, -1.5365e-02,\n",
      "         -2.1542e-02,  3.5869e-02,  5.5104e-04, -2.1415e-02, -1.4899e-02,\n",
      "          7.2687e-03,  2.2719e-02,  2.2002e-02,  2.2209e-02, -2.5726e-02,\n",
      "          2.4639e-02, -3.2519e-02,  1.5675e-02,  3.5271e-02, -2.3324e-02,\n",
      "         -1.1795e-02,  2.9166e-02,  3.5611e-02, -4.9799e-03,  1.5827e-02,\n",
      "         -2.3964e-02,  2.4747e-02, -2.1119e-02,  1.8178e-02,  4.4026e-02,\n",
      "          3.4284e-02,  2.7380e-02,  4.9430e-02,  4.6324e-02, -2.3503e-03,\n",
      "          1.9308e-04,  8.9103e-03,  2.6108e-02, -2.7590e-02,  1.9809e-02,\n",
      "         -3.2186e-02, -1.2756e-02,  3.5661e-02,  2.7835e-02, -1.5009e-02,\n",
      "         -4.8367e-03, -2.5078e-02, -7.7907e-03,  6.4669e-04, -2.7686e-02,\n",
      "          1.7708e-02, -7.2513e-03, -1.3802e-02,  1.1598e-02,  3.7817e-04,\n",
      "          3.9012e-02, -8.7350e-03, -1.0048e-02,  6.9407e-02,  5.3081e-02,\n",
      "          6.7023e-02,  4.0309e-02,  7.5541e-02,  2.7605e-02,  7.1378e-02,\n",
      "          3.7173e-02, -9.1091e-03,  3.0400e-02, -4.5149e-03,  2.5877e-02,\n",
      "          5.4661e-03,  2.2407e-02,  2.4782e-02,  2.2754e-02,  1.2174e-02,\n",
      "         -3.5710e-02, -1.4172e-02, -2.6629e-03, -3.0984e-02,  1.1210e-02,\n",
      "          6.0534e-04,  1.7261e-02, -2.3326e-02, -1.3627e-02, -5.8393e-03,\n",
      "         -9.1095e-04, -1.4667e-02, -3.6479e-02,  5.0807e-03,  3.0302e-02,\n",
      "          2.3110e-02,  2.5321e-02,  1.0771e-02,  4.1085e-02,  4.2963e-02,\n",
      "         -2.3728e-03, -2.0257e-02, -1.5127e-02, -1.4916e-02,  2.4104e-02,\n",
      "         -3.3512e-02,  3.5623e-02,  6.1389e-03,  1.3394e-02,  2.4516e-02,\n",
      "         -3.3565e-02, -1.0840e-02, -2.1065e-02, -3.4832e-02,  1.9848e-03,\n",
      "         -3.2794e-02,  6.1633e-03, -1.5565e-02, -7.1613e-02, -6.5185e-02,\n",
      "         -9.1894e-02, -6.6352e-02, -1.2740e-02, -1.6217e-02, -1.6121e-02,\n",
      "         -4.8874e-02,  1.8257e-02,  2.1101e-02,  1.2941e-02, -1.0117e-02,\n",
      "          3.3525e-02, -1.5326e-03, -2.5381e-03,  1.3236e-02,  2.1318e-02,\n",
      "          1.0561e-02, -1.7352e-02,  2.6794e-02, -6.4631e-03, -3.7179e-04,\n",
      "         -3.1495e-03, -1.1164e-03, -1.3321e-02, -4.8738e-02, -5.0583e-02,\n",
      "         -4.7913e-02, -3.7012e-02, -2.8760e-02,  1.0900e-02, -1.7835e-02,\n",
      "         -4.0758e-02, -2.0891e-02, -7.5912e-02, -1.1572e-02,  1.4312e-02,\n",
      "          3.6238e-03,  4.1985e-03,  1.6566e-02,  2.8509e-02,  1.9629e-02,\n",
      "         -6.5146e-04,  3.0510e-02,  2.2188e-02,  2.2872e-02,  2.3501e-02,\n",
      "          5.2845e-03, -3.7966e-02, -2.7237e-02, -3.5268e-02, -1.9054e-02,\n",
      "          3.1018e-03,  4.4144e-02,  2.1662e-02,  3.6047e-02,  5.3325e-02,\n",
      "          6.7766e-02,  7.2502e-02,  6.7018e-02, -2.8826e-02, -6.4156e-02,\n",
      "         -4.0715e-02, -2.4269e-02, -3.1664e-02, -1.0259e-02,  6.7882e-02,\n",
      "          4.4866e-02,  1.3402e-02, -4.2069e-03,  3.3313e-02, -3.0606e-02,\n",
      "          1.7956e-02,  3.0994e-02,  2.0749e-02, -2.2349e-02,  3.5335e-02,\n",
      "         -3.2098e-03,  5.4488e-02,  4.6402e-02,  6.9947e-02,  1.4008e-01,\n",
      "          1.4024e-01,  1.2278e-01,  1.1630e-01,  7.5376e-02,  6.3060e-02,\n",
      "          3.7598e-02, -3.4317e-02, -2.4766e-02, -2.9198e-02, -4.3343e-02,\n",
      "         -1.3686e-02,  1.6093e-02,  6.0648e-02,  5.2643e-02,  4.2048e-02,\n",
      "          3.5995e-02,  2.5872e-02,  1.9277e-02,  1.7771e-03, -2.2482e-02,\n",
      "         -1.3550e-02, -9.9711e-03,  2.5908e-02,  3.0297e-02,  6.7424e-02,\n",
      "          1.0588e-01,  1.2798e-01,  1.3574e-01,  2.1922e-01,  1.6380e-01,\n",
      "          7.6840e-02,  9.9393e-04, -2.7055e-02, -3.2625e-02, -2.1712e-02,\n",
      "         -6.1701e-03, -5.5172e-02, -5.2604e-03, -9.8744e-04,  5.0451e-03,\n",
      "          5.1339e-02,  7.4451e-02,  1.2662e-02,  3.7024e-02,  3.0040e-02,\n",
      "         -2.0020e-02,  3.5091e-03,  1.0403e-02,  3.3199e-03, -7.4501e-04,\n",
      "          8.1628e-02,  1.0723e-01,  1.0466e-01,  1.5041e-01,  1.4995e-01,\n",
      "          1.4666e-01,  1.6185e-01,  1.3610e-01,  6.8308e-02, -4.1898e-02,\n",
      "         -1.1179e-01, -2.1044e-02,  5.3662e-02,  6.1562e-02,  8.2417e-03,\n",
      "          8.2445e-03,  1.3724e-03,  5.0685e-05, -1.5510e-02,  3.9261e-02,\n",
      "          6.4037e-03,  1.8129e-02,  3.5086e-02, -3.0631e-02, -5.9645e-03,\n",
      "          3.7872e-02,  3.3887e-02,  4.5740e-02,  7.0823e-02,  6.1197e-02,\n",
      "          1.3035e-01,  1.4674e-01,  9.4977e-02,  1.6247e-01,  1.7217e-01,\n",
      "          2.0164e-01,  8.8942e-02, -5.3291e-02, -6.5979e-02,  2.9558e-02,\n",
      "          7.7703e-02,  9.5643e-02,  8.1887e-02,  4.7312e-02, -2.6471e-02,\n",
      "         -2.1335e-02, -3.5930e-02,  8.1473e-03, -2.5580e-02,  2.9933e-02,\n",
      "          1.8348e-03, -3.0001e-02,  1.4053e-02,  2.4427e-02,  3.7940e-02,\n",
      "         -1.0725e-03,  3.5808e-02,  9.4369e-02,  8.4001e-02,  5.4167e-02,\n",
      "          1.0963e-01,  9.1783e-02,  1.7065e-01,  1.6883e-01,  6.5053e-02,\n",
      "         -1.3435e-02,  3.3522e-02,  1.6959e-01,  2.5529e-01,  1.5748e-01,\n",
      "          8.9420e-02,  4.7240e-02, -1.6738e-02, -3.2075e-02, -1.4884e-02,\n",
      "         -3.2676e-02,  2.3490e-03, -1.4681e-02, -3.4566e-02, -7.1081e-04,\n",
      "         -2.8337e-02,  3.6886e-02,  2.0021e-02,  1.2283e-02,  1.1997e-02,\n",
      "          2.0563e-02,  5.7396e-02,  3.9971e-02,  4.6566e-02,  6.1548e-02,\n",
      "          1.3369e-01,  1.8836e-01,  8.3264e-02,  3.2215e-02,  1.2761e-01,\n",
      "          2.2009e-01,  3.1058e-01,  1.9481e-01,  1.1487e-01, -1.9885e-02,\n",
      "         -1.9864e-02, -1.9297e-02, -1.4037e-02,  2.9573e-03,  1.2346e-02,\n",
      "          2.1428e-02, -3.0805e-02, -5.6630e-03,  2.4593e-02,  7.2826e-05,\n",
      "          1.9405e-02,  1.5030e-03, -9.0427e-03,  1.2836e-02, -5.9244e-02,\n",
      "         -6.2701e-02, -1.4318e-02, -3.9402e-03,  1.2438e-01,  1.3424e-01,\n",
      "          7.8641e-02,  1.1521e-01,  1.5467e-01,  2.4708e-01,  2.4394e-01,\n",
      "          1.6248e-01,  3.5135e-02, -5.5666e-02, -5.7895e-02, -2.6543e-02,\n",
      "         -3.2943e-02, -2.8431e-02,  3.0746e-02,  5.3495e-03,  1.7575e-02,\n",
      "         -6.2926e-03, -1.4586e-02,  6.3466e-03, -1.6450e-02, -1.9063e-02,\n",
      "         -4.3304e-02, -6.1855e-02, -9.7300e-02, -1.1946e-01, -4.5413e-02,\n",
      "          1.3947e-02,  7.7909e-02,  1.3434e-01,  2.3176e-02,  5.5935e-02,\n",
      "          1.1420e-01,  2.8143e-01,  1.8776e-01,  2.5296e-02, -6.9315e-03,\n",
      "         -4.7810e-02, -7.7881e-02, -6.5772e-02, -4.0083e-02,  3.7652e-04,\n",
      "          1.4808e-02, -2.7563e-02, -1.1915e-02,  1.8072e-02, -1.9771e-02,\n",
      "          2.6223e-02, -2.2037e-02, -3.9312e-02, -3.1115e-02, -1.1529e-01,\n",
      "         -7.5967e-02, -8.2629e-02, -2.2716e-02,  8.8199e-02,  1.4531e-01,\n",
      "          1.2670e-01,  7.4482e-03,  2.3998e-02,  1.5695e-01,  2.3177e-01,\n",
      "          1.0222e-01, -4.5456e-02, -3.0160e-02, -7.2983e-02, -1.3048e-02,\n",
      "         -3.6760e-02,  1.3030e-02,  3.8288e-04, -7.4225e-04, -3.0896e-02,\n",
      "          2.0637e-03, -3.5156e-02,  1.9161e-02,  6.6044e-04,  1.8764e-02,\n",
      "         -4.5283e-02, -4.4119e-02, -1.0718e-01, -9.8577e-02, -8.1295e-02,\n",
      "          3.8188e-02,  8.4617e-02,  1.5565e-01,  1.0147e-01,  2.6736e-02,\n",
      "          4.7095e-02,  1.0849e-01,  1.0584e-01,  2.8691e-02, -7.1578e-03,\n",
      "         -7.9527e-02, -3.1856e-02, -2.5989e-02, -6.6618e-02, -9.4718e-03,\n",
      "         -4.7250e-02,  1.4988e-02,  2.7181e-02,  1.6140e-03,  1.3059e-02,\n",
      "          2.5452e-02,  9.9598e-03,  7.8917e-03, -5.8041e-02, -8.7318e-02,\n",
      "         -1.1714e-01, -7.7375e-02, -3.8666e-02,  8.0998e-03,  1.1440e-01,\n",
      "          8.2915e-02,  5.1660e-02, -1.4679e-02,  1.9137e-02,  5.6109e-02,\n",
      "          2.4545e-02,  4.1991e-02,  1.3329e-02, -5.4042e-02, -5.9752e-03,\n",
      "         -7.0043e-02, -3.8116e-02, -1.5024e-02, -4.8321e-02,  2.0987e-02,\n",
      "         -1.1982e-02, -3.5180e-02, -6.6614e-03,  7.6688e-03,  1.6129e-02,\n",
      "         -1.3174e-03, -4.8555e-02, -4.8469e-02, -1.0751e-01, -9.7218e-02,\n",
      "         -4.9194e-02, -3.3073e-02,  2.2078e-02,  4.4741e-02,  1.3590e-03,\n",
      "         -1.4341e-02,  1.2017e-02,  6.9136e-02,  5.5477e-02,  1.2054e-02,\n",
      "          1.9422e-02, -4.0467e-02, -3.1528e-02, -2.7279e-02, -9.2417e-03,\n",
      "         -5.4317e-02, -1.2030e-02, -2.2967e-02,  3.3493e-02, -1.6796e-02,\n",
      "          1.2870e-02, -1.3543e-02, -5.1304e-03, -3.7118e-04, -5.2102e-02,\n",
      "         -8.3086e-02, -6.2890e-02, -3.9882e-02, -8.1498e-02, -1.9371e-02,\n",
      "         -9.5361e-03, -2.3673e-02,  1.0560e-02, -9.9176e-03,  6.7001e-03,\n",
      "          4.4048e-03,  2.5975e-02,  1.2078e-02, -1.0362e-03,  1.8445e-02,\n",
      "         -2.3308e-02, -3.9375e-03, -5.0636e-02, -5.7412e-03, -2.5589e-02,\n",
      "          5.3999e-03,  9.4311e-03, -5.8939e-03,  4.5581e-03,  2.3553e-03,\n",
      "         -3.1883e-03, -4.4753e-02, -1.2111e-02, -4.6410e-02, -4.4043e-02,\n",
      "         -5.3887e-02, -5.1542e-02, -1.0529e-01, -4.9259e-02, -1.5594e-02,\n",
      "          5.1661e-02,  1.1551e-02,  1.2419e-02,  7.4010e-03,  8.5296e-03,\n",
      "         -3.1306e-03,  2.0157e-02, -9.0262e-03, -1.8243e-02, -5.0049e-02,\n",
      "         -6.9067e-03, -1.2679e-02, -8.8615e-03, -3.0497e-02, -3.0591e-02,\n",
      "          3.3039e-02, -7.8639e-03, -5.5549e-03, -1.5014e-02, -3.3344e-02,\n",
      "          1.1512e-02, -1.8997e-02, -3.7585e-02, -8.7106e-02, -7.8699e-02,\n",
      "         -9.5978e-02, -7.4943e-02, -2.7195e-02,  7.4536e-04,  7.5874e-03,\n",
      "         -2.2373e-02, -1.3263e-02, -5.7792e-03,  4.7122e-03,  6.1370e-02,\n",
      "          1.6922e-04, -2.2716e-02,  2.8188e-02, -1.7288e-02,  8.0006e-03,\n",
      "         -2.8225e-02,  1.3359e-04,  2.1387e-02,  3.5127e-02, -1.5342e-02,\n",
      "         -6.6680e-03, -1.3964e-02, -6.1090e-04, -1.7114e-02,  2.1044e-03,\n",
      "         -9.2055e-03, -2.7621e-02,  2.8049e-03, -2.0417e-04,  1.4594e-02,\n",
      "          2.8669e-02,  7.6470e-02,  3.1298e-02,  5.9726e-03,  4.6730e-02,\n",
      "          4.9439e-02,  8.2963e-02,  5.2878e-02,  2.8983e-02,  5.5221e-02,\n",
      "          1.2271e-02,  3.6363e-02, -2.7471e-03, -1.0877e-02,  8.8931e-03,\n",
      "         -3.5436e-02, -3.0598e-02, -7.9021e-04, -2.0544e-02,  2.4471e-02,\n",
      "          2.5826e-02,  1.9061e-02,  3.4904e-03,  3.9555e-02,  1.1769e-02,\n",
      "          4.5706e-02,  3.9995e-02,  1.2508e-01,  1.1340e-01,  1.2114e-01,\n",
      "          9.6816e-02,  8.2714e-02,  9.7721e-02,  7.1527e-02,  8.0845e-02,\n",
      "          4.2637e-02,  7.5438e-02,  7.6296e-02,  7.2793e-03, -1.2043e-02,\n",
      "          6.4323e-03,  5.4276e-03,  1.2068e-03,  1.2962e-02, -3.3463e-02,\n",
      "         -5.4954e-03,  3.1713e-02, -4.2568e-04,  1.8514e-02, -1.7410e-02,\n",
      "          3.4705e-02, -1.2558e-02,  5.3658e-02,  5.3429e-02,  6.6927e-02,\n",
      "          1.0915e-01,  1.4184e-01,  1.2294e-01,  8.6840e-02,  1.3971e-01,\n",
      "          1.1564e-01,  8.7450e-02,  1.0647e-01,  9.5713e-02,  6.6713e-02,\n",
      "          1.7340e-02, -1.0160e-03,  3.0936e-02, -2.3344e-02, -2.1102e-02,\n",
      "          7.6386e-03,  1.6575e-02, -3.3516e-03,  2.5966e-02, -2.8236e-02,\n",
      "         -7.9280e-03,  3.5230e-02, -1.2969e-02, -3.2234e-03, -3.0772e-03,\n",
      "          1.4861e-02,  5.0341e-02,  1.9436e-02,  2.6530e-02,  3.4814e-02,\n",
      "          5.7327e-02,  7.5345e-02,  5.6953e-02,  2.2960e-02,  7.3735e-02,\n",
      "          1.8919e-02,  3.8171e-02,  4.8632e-02,  3.3606e-02, -8.0548e-03,\n",
      "         -9.5808e-03, -4.9871e-03, -2.2064e-02, -3.5418e-02, -1.1496e-02,\n",
      "          8.2283e-03,  8.0632e-04,  8.7921e-03, -1.2308e-02,  1.2882e-02,\n",
      "         -2.6998e-02, -4.8403e-03, -9.5471e-03, -4.5068e-03,  3.1036e-02,\n",
      "         -1.3695e-02, -2.1345e-02,  3.3997e-02,  2.2656e-02, -8.9213e-03,\n",
      "         -2.0702e-03,  1.2787e-02,  2.3320e-03, -2.4362e-02, -4.7721e-03,\n",
      "          3.4387e-02,  1.5413e-02, -2.5759e-02,  2.6288e-02,  2.7851e-02,\n",
      "         -7.5050e-03, -5.0516e-03, -2.9172e-02, -7.6582e-03]])\n",
      "activation_stack.0.bias: tensor([0.4745])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            outputs = self.model(self.X_train.float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss = self.criterion(outputs, self.y_train.float())\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Define server wise functions\n",
    "def send_client_weights(server_weights, local_weights):\n",
    "    server_weights.append(local_weights)\n",
    "\n",
    "# Total weight processing functions\n",
    "def Federated_Averaging(client_weights_total):\n",
    "    aggregate_weights = {}\n",
    "    num_clients = len(client_weights_total)\n",
    "    \n",
    "    # Iterate over the parameters of the model\n",
    "    for param_name in client_weights_total[0].keys():\n",
    "        # Initialize the aggregated parameter tensor\n",
    "        aggregated_param = client_weights_total[0][param_name].clone()\n",
    "\n",
    "        # Sum the parameter tensors from all clients\n",
    "        for client_weights in client_weights_total[1:]:\n",
    "            aggregated_param += client_weights[param_name]\n",
    "\n",
    "        # Calculate the average parameter value\n",
    "        aggregated_param /= num_clients\n",
    "\n",
    "        # Assign the averaged parameter to the aggregate_weights dictionary\n",
    "        aggregate_weights[param_name] = aggregated_param\n",
    "\n",
    "    return aggregate_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baec815a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for num_clients is 1 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 0, 1, 2], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[-6.5446e-03, -2.1518e-02, -2.7498e-03, -2.9598e-02,  3.3813e-02,\n",
      "          1.2349e-02,  1.0816e-02,  7.1153e-03,  2.2459e-02, -1.1298e-02,\n",
      "          2.6749e-02, -1.7262e-02,  2.2577e-02,  3.5561e-02, -2.0491e-02,\n",
      "         -1.9130e-02, -2.6891e-02, -7.1190e-03, -1.8836e-03,  2.4361e-02,\n",
      "          2.5456e-03, -1.8451e-02,  3.0681e-02, -1.5387e-03,  1.3231e-03,\n",
      "         -1.5323e-02, -1.7825e-02, -3.0476e-02, -1.2645e-02,  2.6123e-02,\n",
      "          2.6191e-02, -3.2449e-02, -1.9586e-02,  1.3809e-02, -1.6403e-02,\n",
      "          1.4281e-02, -7.7398e-03,  2.5048e-02,  3.3628e-02, -5.2775e-03,\n",
      "         -7.7651e-03,  3.2158e-02,  1.3728e-02, -3.4698e-02, -1.1622e-02,\n",
      "         -2.7644e-02, -1.1686e-02, -1.1248e-02, -2.7135e-02,  2.2430e-02,\n",
      "          2.5643e-02,  5.7089e-03,  1.0510e-02,  3.2244e-02, -1.9635e-02,\n",
      "          2.8050e-02, -8.4912e-03,  2.9170e-02,  2.2513e-02,  2.4695e-02,\n",
      "          1.7117e-02, -2.7167e-02,  1.3714e-03, -2.7182e-02, -7.9021e-03,\n",
      "          3.4793e-02, -1.5851e-02,  8.1603e-03,  8.2290e-03,  1.7083e-02,\n",
      "         -2.8488e-02, -1.0321e-02,  1.7760e-02, -1.0931e-02, -1.4809e-02,\n",
      "          1.7108e-02, -3.5273e-02, -1.7429e-03, -2.3671e-02, -3.5497e-02,\n",
      "          1.1078e-02,  3.3929e-02,  1.1827e-02, -2.5100e-02,  4.6825e-03,\n",
      "          3.4405e-02, -3.2202e-02, -3.5517e-02, -2.2797e-02,  6.8003e-03,\n",
      "         -3.6758e-03, -1.2984e-02, -8.1969e-03,  1.4229e-02,  2.0234e-02,\n",
      "          3.1656e-02,  1.4763e-02,  2.4698e-02,  1.3784e-02,  2.7559e-02,\n",
      "         -1.7330e-02,  3.9887e-03,  2.3286e-03, -2.7030e-02, -5.4325e-03,\n",
      "          8.3961e-03,  2.2221e-02,  5.5252e-03, -1.4557e-02, -3.4666e-03,\n",
      "          1.8669e-03, -2.8924e-02,  2.5851e-02,  2.8337e-02, -1.5057e-03,\n",
      "         -1.8671e-02,  2.7448e-02,  3.4088e-02,  1.0090e-02,  1.6297e-02,\n",
      "         -2.7904e-02, -3.2451e-02,  6.7364e-03, -2.2790e-02, -5.1007e-03,\n",
      "          2.3617e-02, -2.0183e-02, -6.9339e-03,  2.8604e-02,  2.6011e-02,\n",
      "         -2.7939e-02, -2.5093e-02, -2.4301e-03,  1.1471e-03,  3.4533e-02,\n",
      "          1.6160e-02, -6.5478e-03, -2.6961e-02,  5.0138e-03, -2.9275e-02,\n",
      "         -2.9200e-02,  2.2723e-02, -7.9129e-03,  9.2688e-03,  2.3794e-02,\n",
      "          2.4969e-02,  2.1919e-02, -1.9744e-02, -1.1362e-02,  3.2990e-02,\n",
      "         -3.1372e-02, -1.3265e-02, -2.9426e-02, -2.5790e-02, -2.6855e-02,\n",
      "          1.2371e-02,  2.4078e-03,  9.8502e-03,  1.4814e-02,  3.0748e-02,\n",
      "         -7.2930e-03,  1.6320e-03, -1.6434e-02, -5.0343e-03,  6.5783e-03,\n",
      "         -2.8016e-02,  1.9613e-02,  1.0148e-02,  2.8157e-02,  1.1268e-02,\n",
      "          3.1792e-02,  6.9852e-03,  5.9070e-03, -3.2854e-02,  1.4599e-02,\n",
      "         -1.1914e-02,  9.1397e-03, -2.5261e-02, -1.2722e-02,  1.7299e-02,\n",
      "         -2.4303e-02,  2.1165e-03, -3.3164e-02,  3.4269e-02, -3.5244e-03,\n",
      "          3.4399e-02, -1.3879e-03, -2.4135e-02,  2.8429e-02,  8.2612e-03,\n",
      "          2.2689e-02,  1.4472e-02,  2.0797e-02,  9.2825e-03,  5.2652e-03,\n",
      "          2.3805e-02,  1.2006e-03,  3.5349e-02,  2.5543e-02,  2.9358e-02,\n",
      "          1.7151e-02,  1.4071e-02, -3.3998e-02,  5.6295e-03,  3.3939e-03,\n",
      "         -5.8772e-03,  8.1387e-03,  4.5173e-03, -3.1251e-02, -3.4052e-02,\n",
      "         -2.6983e-02,  1.3055e-02,  2.3852e-02, -2.4099e-03,  3.0789e-02,\n",
      "         -3.3448e-02,  1.5453e-02, -2.3590e-02, -3.1437e-02,  3.2011e-02,\n",
      "         -3.1610e-02,  2.5012e-03,  2.2665e-02,  2.4738e-03, -1.7051e-02,\n",
      "         -2.7146e-02,  2.3852e-03, -1.0063e-02,  1.0167e-02, -1.3388e-02,\n",
      "          1.0289e-02,  4.8862e-03, -2.5366e-02,  2.1309e-02,  2.7729e-02,\n",
      "          9.7255e-03,  1.4656e-02,  1.1301e-02,  3.8625e-04,  1.5941e-03,\n",
      "         -2.7879e-02,  2.8384e-02, -1.1137e-02, -1.0033e-02,  3.3313e-02,\n",
      "          5.5119e-03,  3.2291e-03, -1.5387e-02,  1.5380e-02,  9.3942e-03,\n",
      "          3.1240e-02, -2.2957e-02, -1.7874e-02, -2.7812e-02, -2.6728e-02,\n",
      "          6.6386e-03, -1.9028e-02, -2.3572e-02,  3.4904e-02, -6.9895e-03,\n",
      "         -8.7703e-03,  2.1673e-02,  3.3762e-02, -2.3800e-02, -2.0563e-04,\n",
      "          2.4780e-02,  2.3497e-02,  3.5219e-02, -1.3835e-02,  2.3191e-02,\n",
      "         -2.3552e-02, -1.3663e-02, -6.3841e-03, -2.4116e-02, -2.2192e-02,\n",
      "         -1.6095e-02, -2.3122e-02,  1.8141e-02,  1.0030e-02,  1.4279e-03,\n",
      "         -3.1824e-02,  1.3021e-02, -3.1180e-02, -5.6928e-03, -3.2616e-02,\n",
      "          1.7668e-02, -3.0394e-02,  3.4818e-02, -2.7744e-02, -2.8362e-04,\n",
      "         -3.2813e-02, -2.6120e-02, -1.6259e-02, -1.9282e-02,  3.3889e-02,\n",
      "         -3.3557e-02, -2.1925e-02, -9.5104e-03, -2.6540e-03, -7.1128e-03,\n",
      "          8.8429e-03,  2.5276e-02, -2.5203e-03,  3.3279e-02, -4.7220e-03,\n",
      "         -5.9947e-03, -2.5337e-03,  1.4491e-02, -2.0894e-02,  1.9156e-02,\n",
      "          2.4214e-02,  2.5920e-02,  1.9853e-02,  2.7947e-02, -1.0107e-02,\n",
      "         -1.0610e-02, -9.5501e-03, -1.1879e-02, -2.0299e-02,  1.5739e-02,\n",
      "         -2.9465e-02,  2.5109e-02,  3.0678e-02,  5.7795e-03, -3.2393e-03,\n",
      "         -2.2022e-02, -1.5749e-02, -1.2608e-02,  3.8441e-03, -6.7606e-03,\n",
      "         -9.6466e-03, -8.7468e-04,  1.1438e-02, -5.1853e-03,  9.7487e-03,\n",
      "          1.5323e-02,  2.0738e-03, -3.0406e-02,  2.2783e-02, -2.2177e-02,\n",
      "         -2.4244e-02,  1.5244e-02,  1.5138e-02,  1.8369e-02, -2.7058e-02,\n",
      "         -3.1161e-03, -1.2004e-02,  1.6949e-03, -1.1492e-02, -1.7458e-02,\n",
      "          3.0032e-02, -2.7077e-02,  1.3208e-02, -3.6620e-06, -2.2096e-02,\n",
      "          2.4358e-02, -1.5104e-02, -4.2237e-03, -2.9474e-02,  2.3441e-02,\n",
      "         -2.3156e-02,  2.4014e-02, -3.1206e-03,  2.0965e-02, -2.1329e-02,\n",
      "          1.4646e-02,  1.7878e-03,  2.2306e-02, -3.5106e-02, -1.7676e-02,\n",
      "         -2.3861e-03, -7.9818e-03,  8.5117e-03,  1.2856e-03,  6.2061e-03,\n",
      "         -2.4601e-02, -1.3336e-02, -3.3985e-02,  2.8264e-02,  2.2534e-02,\n",
      "          1.0760e-02, -8.8298e-03, -3.3851e-03, -3.5178e-02,  1.0344e-02,\n",
      "          3.4662e-04,  2.9446e-02,  3.3769e-02, -3.2476e-02, -2.9281e-02,\n",
      "         -1.7388e-03, -1.0938e-02,  1.0762e-02, -2.2224e-02,  9.5836e-03,\n",
      "          2.2615e-02, -1.6892e-02,  2.9820e-02, -3.4452e-02,  1.4873e-02,\n",
      "         -2.7239e-04,  3.4742e-02,  1.9148e-02, -3.0687e-02, -1.8247e-03,\n",
      "          3.5663e-02,  2.2764e-02,  2.4806e-02, -5.6234e-03,  3.5293e-02,\n",
      "         -3.3707e-03,  5.5092e-03,  3.0610e-02,  2.7158e-02,  2.7047e-02,\n",
      "          2.7743e-02,  2.2143e-02,  3.3191e-02, -5.4896e-03, -8.2813e-03,\n",
      "         -4.9688e-03, -2.3388e-02, -1.8201e-03, -7.7224e-04,  1.6427e-03,\n",
      "          1.1165e-02,  1.0374e-03,  1.8076e-02, -2.1427e-02, -2.0452e-03,\n",
      "          1.0550e-02, -1.4155e-03,  2.5914e-02, -1.7363e-02,  3.3565e-02,\n",
      "          1.9254e-02, -2.5958e-02,  1.9147e-02, -2.0653e-02, -2.9264e-02,\n",
      "         -5.6681e-04,  9.2701e-03, -2.5676e-02, -3.3000e-02,  1.1492e-02,\n",
      "          4.9690e-03,  2.3585e-02,  1.4031e-02,  3.2339e-02,  6.5066e-03,\n",
      "         -2.5384e-02,  4.6809e-03,  3.4566e-03,  2.1017e-02,  2.1206e-02,\n",
      "         -2.5435e-02,  1.1142e-02, -2.3424e-02, -6.6684e-03,  4.4063e-03,\n",
      "          3.3850e-02, -1.5436e-02, -6.3139e-03, -1.4195e-02, -1.8974e-02,\n",
      "         -2.5215e-02, -3.1790e-02, -8.5741e-03,  2.0979e-02,  5.5648e-03,\n",
      "         -1.6333e-02,  2.9246e-02, -3.8182e-03,  1.0075e-02,  5.5440e-03,\n",
      "          3.7224e-03, -4.8699e-03,  2.0735e-02, -2.8591e-03, -5.1229e-03,\n",
      "         -1.4435e-03, -5.2616e-04, -3.5040e-02, -3.0150e-02,  1.3673e-04,\n",
      "         -1.3938e-02, -6.9271e-03,  1.1500e-02, -3.5616e-02, -1.3296e-02,\n",
      "         -2.8476e-02,  9.7139e-03,  2.2522e-03, -2.3673e-02, -6.6552e-03,\n",
      "          2.1370e-02,  2.1404e-02, -1.0253e-02, -2.5238e-02,  2.6985e-03,\n",
      "         -7.5313e-03,  2.1588e-02,  8.9036e-03,  2.3921e-02, -2.4367e-02,\n",
      "         -9.2637e-03,  3.4995e-02, -8.5663e-04, -3.3372e-02, -6.3856e-03,\n",
      "         -5.5739e-03,  2.2946e-03,  3.5383e-02,  1.9458e-02, -2.0883e-02,\n",
      "          1.0436e-02, -4.3339e-03,  7.7030e-03, -6.7507e-03,  2.7432e-02,\n",
      "          1.0090e-02,  2.6626e-02,  2.5272e-02, -1.9344e-02,  3.5214e-02,\n",
      "         -2.7744e-02,  6.1391e-03, -3.1414e-02, -2.1714e-02, -5.0314e-04,\n",
      "          2.7638e-02,  1.6229e-02, -5.2897e-03, -5.3089e-03,  5.6952e-03,\n",
      "         -2.8567e-02, -2.0308e-02,  3.3875e-02, -3.0773e-02,  2.8275e-02,\n",
      "         -2.5429e-02, -1.5697e-02,  1.2197e-02,  3.2088e-02, -1.1004e-02,\n",
      "         -3.3866e-02, -1.5390e-02,  2.2318e-02,  2.3306e-02,  1.2324e-02,\n",
      "         -2.6133e-02, -7.2300e-03, -9.9842e-03,  2.6252e-02,  1.6247e-02,\n",
      "          3.2536e-02, -1.1282e-02, -4.9304e-03,  1.5702e-02, -6.4819e-03,\n",
      "          2.8661e-02,  1.2233e-02, -1.5079e-02, -2.2014e-04, -2.3960e-02,\n",
      "          1.7387e-02, -2.3921e-02,  1.7763e-02, -2.7720e-02, -1.3412e-02,\n",
      "          2.3977e-02,  2.7160e-02,  3.3736e-02, -3.2655e-02, -2.6811e-02,\n",
      "          2.6018e-02,  3.0527e-03, -2.3896e-02, -2.8194e-03,  1.2792e-02,\n",
      "          1.7415e-03,  1.5429e-02,  1.0545e-02, -3.5061e-02,  1.8487e-02,\n",
      "         -3.3563e-03,  9.3281e-05, -1.8371e-02,  1.5553e-03, -7.6695e-03,\n",
      "         -1.3873e-02,  2.1362e-02, -9.4458e-03, -7.0499e-03, -3.2799e-02,\n",
      "         -1.3349e-02,  1.5420e-02,  1.0431e-02,  1.4616e-02, -1.7576e-02,\n",
      "          2.5770e-02, -2.9455e-03,  3.3277e-02, -2.0848e-02,  1.1132e-02,\n",
      "          1.2953e-02,  1.5005e-02, -1.0473e-02, -6.5568e-03, -5.4902e-03,\n",
      "          1.5944e-02,  2.9554e-02, -2.2967e-02, -3.8919e-03, -8.4503e-03,\n",
      "          1.0848e-02,  3.1536e-03,  9.7426e-03,  1.6538e-02,  2.8799e-02,\n",
      "          1.3028e-02,  3.5225e-02,  2.7806e-02,  5.6014e-04, -2.4850e-02,\n",
      "         -3.4980e-02, -3.0607e-02, -1.0142e-02,  4.5080e-03,  2.5909e-02,\n",
      "         -1.8344e-03, -1.4203e-03, -8.8386e-03, -5.4962e-03, -1.5103e-02,\n",
      "          3.0955e-02,  2.7385e-02, -5.4340e-03,  5.8578e-03,  7.5146e-03,\n",
      "          2.3292e-02,  6.0079e-03, -3.5545e-02,  1.7184e-02,  2.7830e-03,\n",
      "         -2.1274e-02, -3.2491e-02,  6.9204e-03,  8.6892e-05,  1.1501e-03,\n",
      "          2.6366e-02,  1.9671e-02,  2.0233e-02,  2.7559e-02, -1.4431e-02,\n",
      "          1.6443e-02,  1.5993e-02,  2.6892e-02,  1.2417e-02,  4.7202e-03,\n",
      "          1.8692e-02, -1.6154e-02,  2.1249e-02, -2.6253e-02,  3.9100e-03,\n",
      "         -2.5259e-02,  1.7507e-02,  2.7251e-02,  6.2453e-03,  2.2893e-02,\n",
      "          1.1654e-03,  3.1666e-02,  1.3971e-02, -1.6193e-02,  7.9789e-03,\n",
      "         -1.6734e-02,  2.0734e-02,  1.6096e-02,  2.0906e-03, -1.7654e-02,\n",
      "          1.9173e-02, -2.0018e-02,  1.2112e-03, -3.4724e-02,  2.1647e-02,\n",
      "         -1.0252e-02,  2.6085e-02,  1.3959e-02, -2.8560e-02,  8.5864e-03,\n",
      "          7.6155e-03,  2.0895e-02, -2.5685e-02,  2.7677e-02,  1.7302e-02,\n",
      "         -2.4594e-02,  2.1669e-02,  3.3193e-03,  1.3197e-02,  2.2327e-02,\n",
      "          2.3944e-02, -1.1248e-02, -2.8947e-02,  9.1870e-03,  6.2777e-03,\n",
      "         -2.3475e-02, -2.5166e-02, -3.4710e-02,  7.6342e-03, -2.0780e-02,\n",
      "          1.0472e-02, -4.3275e-03,  2.5239e-02, -1.2877e-02, -1.8629e-02,\n",
      "         -1.4125e-02, -2.9361e-02,  1.7833e-02, -5.2834e-03,  1.9620e-02,\n",
      "          1.0685e-03, -1.5461e-02,  7.6083e-03,  8.2809e-04, -9.4974e-03,\n",
      "         -2.3368e-02, -1.4302e-02,  2.7359e-02,  9.0835e-03, -2.6705e-02,\n",
      "         -3.2345e-02,  3.0511e-02,  1.4336e-02, -4.2214e-03, -7.1669e-03,\n",
      "          2.2241e-02, -2.3041e-02,  4.1098e-03, -1.7289e-02,  2.8993e-02,\n",
      "          5.5838e-03, -3.0330e-03,  1.1265e-02, -2.5475e-02, -1.6040e-02,\n",
      "          4.6410e-03, -1.9129e-02,  1.9039e-02, -3.5275e-03,  1.5031e-02,\n",
      "         -2.4129e-02,  1.9951e-03,  2.3047e-02, -2.8010e-03,  9.8897e-03,\n",
      "         -1.0356e-02, -3.2795e-02, -3.1768e-02, -3.4153e-02, -5.4101e-04,\n",
      "          2.0113e-02,  7.7871e-03,  4.9196e-03,  1.4608e-02,  1.1951e-02,\n",
      "         -1.9338e-02,  1.4006e-02, -2.2011e-02,  2.6074e-02,  3.0422e-02,\n",
      "          1.1291e-02,  2.8260e-02, -1.8644e-02,  2.0518e-02,  1.2322e-02,\n",
      "         -2.6717e-02,  3.4513e-03, -1.1828e-02,  8.2386e-03, -2.5822e-02,\n",
      "          3.0727e-02,  3.0997e-03,  2.7669e-03,  1.3365e-02]])), ('activation_stack.0.bias', tensor([0.0152]))])\n",
      "tensor(0.2002, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 25.06418419, Culminative Send Cost: 784\n",
      "tensor(1.3060, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.44649124, Culminative Send Cost: 7840\n",
      "tensor(1.8927, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.03915405, Culminative Send Cost: 15680\n",
      "tensor(2.1696, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.04194355, Culminative Send Cost: 23520\n",
      "tensor(2.3087, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.56057549, Culminative Send Cost: 31360\n",
      "tensor(2.3860, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.21824551, Culminative Send Cost: 39200\n",
      "tensor(2.4352, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 6.93582344, Culminative Send Cost: 47040\n",
      "tensor(2.4711, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.69317579, Culminative Send Cost: 54880\n",
      "tensor(2.5003, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.48238659, Culminative Send Cost: 62720\n",
      "tensor(2.5260, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.29850769, Culminative Send Cost: 70560\n",
      "tensor(2.5493, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.13766575, Culminative Send Cost: 78400\n",
      "tensor(2.5709, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 5.99661732, Culminative Send Cost: 86240\n",
      "tensor(2.5911, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.87259817, Culminative Send Cost: 94080\n",
      "tensor(2.6100, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.76325035, Culminative Send Cost: 101920\n",
      "tensor(2.6279, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.66655159, Culminative Send Cost: 109760\n",
      "tensor(2.6447, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.58077192, Culminative Send Cost: 117600\n",
      "tensor(2.6605, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.50442696, Culminative Send Cost: 125440\n",
      "tensor(2.6753, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.43624544, Culminative Send Cost: 133280\n",
      "tensor(2.6893, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.37513590, Culminative Send Cost: 141120\n",
      "tensor(2.7024, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.32016039, Culminative Send Cost: 148960\n",
      "tensor(2.7147, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.27051544, Culminative Send Cost: 156800\n",
      "tensor(2.7262, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.22550821, Culminative Send Cost: 164640\n",
      "tensor(2.7369, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.18454504, Culminative Send Cost: 172480\n",
      "tensor(2.7470, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.14711237, Culminative Send Cost: 180320\n",
      "tensor(2.7565, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.11277056, Culminative Send Cost: 188160\n",
      "tensor(2.7653, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.08113766, Culminative Send Cost: 196000\n",
      "tensor(2.7735, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.05188704, Culminative Send Cost: 203840\n",
      "tensor(2.7811, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.02473497, Culminative Send Cost: 211680\n",
      "tensor(2.7882, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.99943495, Culminative Send Cost: 219520\n",
      "tensor(2.7949, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.97577667, Culminative Send Cost: 227360\n",
      "tensor(2.8010, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.95357609, Culminative Send Cost: 235200\n",
      "tensor(2.8067, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.93267250, Culminative Send Cost: 243040\n",
      "tensor(2.8120, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.91292858, Culminative Send Cost: 250880\n",
      "tensor(2.8169, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.89422321, Culminative Send Cost: 258720\n",
      "tensor(2.8214, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.87645149, Culminative Send Cost: 266560\n",
      "tensor(2.8255, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.85952139, Culminative Send Cost: 274400\n",
      "tensor(2.8294, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.84335423, Culminative Send Cost: 282240\n",
      "tensor(2.8329, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.82787800, Culminative Send Cost: 290080\n",
      "tensor(2.8361, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.81303215, Culminative Send Cost: 297920\n",
      "tensor(2.8390, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.79876232, Culminative Send Cost: 305760\n",
      "tensor(2.8416, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.78502035, Culminative Send Cost: 313600\n",
      "tensor(2.8441, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.77176428, Culminative Send Cost: 321440\n",
      "tensor(2.8462, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.75895739, Culminative Send Cost: 329280\n",
      "tensor(2.8482, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.74656582, Culminative Send Cost: 337120\n",
      "tensor(2.8499, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.73456001, Culminative Send Cost: 344960\n",
      "tensor(2.8515, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.72291470, Culminative Send Cost: 352800\n",
      "tensor(2.8529, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.71160460, Culminative Send Cost: 360640\n",
      "tensor(2.8541, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.70060968, Culminative Send Cost: 368480\n",
      "tensor(2.8551, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.68991137, Culminative Send Cost: 376320\n",
      "tensor(2.8560, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.67949152, Culminative Send Cost: 384160\n",
      "tensor(2.8568, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.66933537, Culminative Send Cost: 392000\n",
      "tensor(2.8574, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.65942764, Culminative Send Cost: 399840\n",
      "tensor(2.8579, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.64975643, Culminative Send Cost: 407680\n",
      "tensor(2.8583, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.64030933, Culminative Send Cost: 415520\n",
      "tensor(2.8585, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.63107681, Culminative Send Cost: 423360\n",
      "tensor(2.8587, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.62204742, Culminative Send Cost: 431200\n",
      "tensor(2.8588, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.61321306, Culminative Send Cost: 439040\n",
      "tensor(2.8588, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.60456467, Culminative Send Cost: 446880\n",
      "tensor(2.8587, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.59609461, Culminative Send Cost: 454720\n",
      "tensor(2.8585, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.58779621, Culminative Send Cost: 462560\n",
      "tensor(2.8583, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.57966185, Culminative Send Cost: 470400\n",
      "tensor(2.8579, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.57168531, Culminative Send Cost: 478240\n",
      "tensor(2.8576, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.56386089, Culminative Send Cost: 486080\n",
      "tensor(2.8571, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.55618334, Culminative Send Cost: 493920\n",
      "tensor(2.8566, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.54864693, Culminative Send Cost: 501760\n",
      "tensor(2.8561, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.54124594, Culminative Send Cost: 509600\n",
      "tensor(2.8555, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.53397799, Culminative Send Cost: 517440\n",
      "tensor(2.8549, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.52683592, Culminative Send Cost: 525280\n",
      "tensor(2.8542, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.51981735, Culminative Send Cost: 533120\n",
      "tensor(2.8535, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.51291847, Culminative Send Cost: 540960\n",
      "tensor(2.8528, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.50613356, Culminative Send Cost: 548800\n",
      "tensor(2.8520, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.49946165, Culminative Send Cost: 556640\n",
      "tensor(2.8512, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.49289751, Culminative Send Cost: 564480\n",
      "tensor(2.8504, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.48643875, Culminative Send Cost: 572320\n",
      "tensor(2.8496, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.48008108, Culminative Send Cost: 580160\n",
      "tensor(2.8487, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.47382259, Culminative Send Cost: 588000\n",
      "tensor(2.8478, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.46765900, Culminative Send Cost: 595840\n",
      "tensor(2.8469, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.46158981, Culminative Send Cost: 603680\n",
      "tensor(2.8460, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.45561171, Culminative Send Cost: 611520\n",
      "tensor(2.8451, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.44972086, Culminative Send Cost: 619360\n",
      "tensor(2.8442, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.44391537, Culminative Send Cost: 627200\n",
      "tensor(2.8432, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.43819380, Culminative Send Cost: 635040\n",
      "tensor(2.8422, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.43255329, Culminative Send Cost: 642880\n",
      "tensor(2.8413, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.42699099, Culminative Send Cost: 650720\n",
      "tensor(2.8403, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.42150640, Culminative Send Cost: 658560\n",
      "tensor(2.8393, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.41609573, Culminative Send Cost: 666400\n",
      "tensor(2.8383, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.41075850, Culminative Send Cost: 674240\n",
      "tensor(2.8374, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.40549231, Culminative Send Cost: 682080\n",
      "tensor(2.8364, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.40029478, Culminative Send Cost: 689920\n",
      "tensor(2.8354, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.39516497, Culminative Send Cost: 697760\n",
      "tensor(2.8344, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.39010096, Culminative Send Cost: 705600\n",
      "tensor(2.8334, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.38510132, Culminative Send Cost: 713440\n",
      "tensor(2.8324, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.38016367, Culminative Send Cost: 721280\n",
      "tensor(2.8314, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.37528706, Culminative Send Cost: 729120\n",
      "tensor(2.8304, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.37047052, Culminative Send Cost: 736960\n",
      "tensor(2.8294, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.36571217, Culminative Send Cost: 744800\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.36101055, Culminative Send Cost: 752640\n",
      "tensor(2.8275, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.35636473, Culminative Send Cost: 760480\n",
      "tensor(2.8265, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.35177231, Culminative Send Cost: 768320\n",
      "tensor(2.8256, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.34723330, Culminative Send Cost: 776160\n",
      "tensor(2.8246, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.34274530, Culminative Send Cost: 784000\n",
      "tensor(2.8236, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.33830833, Culminative Send Cost: 791840\n",
      "tensor(2.8227, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.33392143, Culminative Send Cost: 799680\n",
      "tensor(2.8218, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.32958221, Culminative Send Cost: 807520\n",
      "tensor(2.8208, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.32529068, Culminative Send Cost: 815360\n",
      "tensor(2.8199, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.32104445, Culminative Send Cost: 823200\n",
      "tensor(2.8190, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.31684399, Culminative Send Cost: 831040\n",
      "tensor(2.8181, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.31268835, Culminative Send Cost: 838880\n",
      "tensor(2.8172, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.30857563, Culminative Send Cost: 846720\n",
      "tensor(2.8163, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.30450439, Culminative Send Cost: 854560\n",
      "tensor(2.8154, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.30047512, Culminative Send Cost: 862400\n",
      "tensor(2.8145, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.29648685, Culminative Send Cost: 870240\n",
      "tensor(2.8137, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.29253864, Culminative Send Cost: 878080\n",
      "tensor(2.8128, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.28862906, Culminative Send Cost: 885920\n",
      "tensor(2.8120, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.28475714, Culminative Send Cost: 893760\n",
      "tensor(2.8112, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.28092337, Culminative Send Cost: 901600\n",
      "tensor(2.8103, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.27712631, Culminative Send Cost: 909440\n",
      "tensor(2.8095, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.27336454, Culminative Send Cost: 917280\n",
      "tensor(2.8087, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.26963758, Culminative Send Cost: 925120\n",
      "tensor(2.8079, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.26594639, Culminative Send Cost: 932960\n",
      "tensor(2.8071, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.26228857, Culminative Send Cost: 940800\n",
      "tensor(2.8064, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.25866413, Culminative Send Cost: 948640\n",
      "tensor(2.8056, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.25507164, Culminative Send Cost: 956480\n",
      "tensor(2.8049, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.25151157, Culminative Send Cost: 964320\n",
      "tensor(2.8041, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.24798298, Culminative Send Cost: 972160\n",
      "tensor(2.8034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.24448538, Culminative Send Cost: 980000\n",
      "tensor(2.8027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.24101734, Culminative Send Cost: 987840\n",
      "tensor(2.8020, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.23757935, Culminative Send Cost: 995680\n",
      "tensor(2.8013, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.23417044, Culminative Send Cost: 1003520\n",
      "tensor(2.8006, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.23079014, Culminative Send Cost: 1011360\n",
      "tensor(2.7999, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.22743797, Culminative Send Cost: 1019200\n",
      "tensor(2.7993, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.22411346, Culminative Send Cost: 1027040\n",
      "tensor(2.7986, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.22081566, Culminative Send Cost: 1034880\n",
      "tensor(2.7980, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.21754551, Culminative Send Cost: 1042720\n",
      "tensor(2.7973, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.21430063, Culminative Send Cost: 1050560\n",
      "tensor(2.7967, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.21108198, Culminative Send Cost: 1058400\n",
      "tensor(2.7961, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.20788765, Culminative Send Cost: 1066240\n",
      "tensor(2.7955, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.20471954, Culminative Send Cost: 1074080\n",
      "tensor(2.7949, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.20157480, Culminative Send Cost: 1081920\n",
      "tensor(2.7944, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.19845486, Culminative Send Cost: 1089760\n",
      "tensor(2.7938, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.19535828, Culminative Send Cost: 1097600\n",
      "tensor(2.7933, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.19228506, Culminative Send Cost: 1105440\n",
      "tensor(2.7927, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.18923426, Culminative Send Cost: 1113280\n",
      "tensor(2.7922, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.18620682, Culminative Send Cost: 1121120\n",
      "tensor(2.7917, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.18320131, Culminative Send Cost: 1128960\n",
      "tensor(2.7912, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.18021727, Culminative Send Cost: 1136800\n",
      "tensor(2.7907, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.17725468, Culminative Send Cost: 1144640\n",
      "tensor(2.7902, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.17431355, Culminative Send Cost: 1152480\n",
      "tensor(2.7897, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.17139339, Culminative Send Cost: 1160320\n",
      "tensor(2.7893, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.16849327, Culminative Send Cost: 1168160\n",
      "tensor(2.7888, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.16561365, Culminative Send Cost: 1176000\n",
      "tensor(2.7884, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.16275358, Culminative Send Cost: 1183840\n",
      "tensor(2.7879, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.15991354, Culminative Send Cost: 1191680\n",
      "tensor(2.7875, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.15709257, Culminative Send Cost: 1199520\n",
      "tensor(2.7871, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.15429020, Culminative Send Cost: 1207360\n",
      "tensor(2.7867, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.15150738, Culminative Send Cost: 1215200\n",
      "tensor(2.7863, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.14874268, Culminative Send Cost: 1223040\n",
      "tensor(2.7860, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.14599657, Culminative Send Cost: 1230880\n",
      "tensor(2.7856, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.14326715, Culminative Send Cost: 1238720\n",
      "tensor(2.7852, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.14055681, Culminative Send Cost: 1246560\n",
      "tensor(2.7849, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.13786364, Culminative Send Cost: 1254400\n",
      "tensor(2.7846, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.13518715, Culminative Send Cost: 1262240\n",
      "tensor(2.7842, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.13252783, Culminative Send Cost: 1270080\n",
      "tensor(2.7839, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.12988567, Culminative Send Cost: 1277920\n",
      "tensor(2.7836, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.12725973, Culminative Send Cost: 1285760\n",
      "tensor(2.7833, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.12465048, Culminative Send Cost: 1293600\n",
      "tensor(2.7831, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.12205648, Culminative Send Cost: 1301440\n",
      "tensor(2.7828, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.11947918, Culminative Send Cost: 1309280\n",
      "tensor(2.7825, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.11691666, Culminative Send Cost: 1317120\n",
      "tensor(2.7823, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.11437082, Culminative Send Cost: 1324960\n",
      "tensor(2.7820, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.11183977, Culminative Send Cost: 1332800\n",
      "tensor(2.7818, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.10932302, Culminative Send Cost: 1340640\n",
      "tensor(2.7816, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.10682201, Culminative Send Cost: 1348480\n",
      "tensor(2.7814, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.10433483, Culminative Send Cost: 1356320\n",
      "tensor(2.7812, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.10186291, Culminative Send Cost: 1364160\n",
      "tensor(2.7810, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.09940529, Culminative Send Cost: 1372000\n",
      "tensor(2.7808, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.09696150, Culminative Send Cost: 1379840\n",
      "tensor(2.7806, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.09453201, Culminative Send Cost: 1387680\n",
      "tensor(2.7804, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.09211636, Culminative Send Cost: 1395520\n",
      "tensor(2.7803, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.08971405, Culminative Send Cost: 1403360\n",
      "tensor(2.7801, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.08732510, Culminative Send Cost: 1411200\n",
      "tensor(2.7800, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.08494949, Culminative Send Cost: 1419040\n",
      "tensor(2.7799, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.08258772, Culminative Send Cost: 1426880\n",
      "tensor(2.7797, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.08023882, Culminative Send Cost: 1434720\n",
      "tensor(2.7796, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.07790184, Culminative Send Cost: 1442560\n",
      "tensor(2.7795, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.07557821, Culminative Send Cost: 1450400\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.07326698, Culminative Send Cost: 1458240\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.07096863, Culminative Send Cost: 1466080\n",
      "tensor(2.7793, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.06868219, Culminative Send Cost: 1473920\n",
      "tensor(2.7792, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.06640816, Culminative Send Cost: 1481760\n",
      "tensor(2.7792, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.06414604, Culminative Send Cost: 1489600\n",
      "tensor(2.7791, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.06189585, Culminative Send Cost: 1497440\n",
      "tensor(2.7791, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.05965757, Culminative Send Cost: 1505280\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.05743074, Culminative Send Cost: 1513120\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.05521536, Culminative Send Cost: 1520960\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.05301142, Culminative Send Cost: 1528800\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.05081892, Culminative Send Cost: 1536640\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.04863691, Culminative Send Cost: 1544480\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.04646730, Culminative Send Cost: 1552320\n",
      "tensor(2.7790, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.04430723, Culminative Send Cost: 1560160\n",
      "tensor(2.7791, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.04215860, Culminative Send Cost: 1568000\n",
      "tensor(2.7791, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.04002047, Culminative Send Cost: 1575840\n",
      "tensor(2.7791, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.03789282, Culminative Send Cost: 1583680\n",
      "tensor(2.7792, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.03577614, Culminative Send Cost: 1591520\n",
      "tensor(2.7793, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.03366995, Culminative Send Cost: 1599360\n",
      "tensor(2.7793, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.03157330, Culminative Send Cost: 1607200\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.02948761, Culminative Send Cost: 1615040\n",
      "tensor(2.7795, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.02741194, Culminative Send Cost: 1622880\n",
      "tensor(2.7796, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.02534580, Culminative Send Cost: 1630720\n",
      "tensor(2.7797, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.02328968, Culminative Send Cost: 1638560\n",
      "tensor(2.7798, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.02124310, Culminative Send Cost: 1646400\n",
      "tensor(2.7799, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.01920700, Culminative Send Cost: 1654240\n",
      "tensor(2.7800, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.01717997, Culminative Send Cost: 1662080\n",
      "tensor(2.7801, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 4.01516247, Culminative Send Cost: 1669920\n",
      "tensor(2.7803, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 4.01315451, Culminative Send Cost: 1677760\n",
      "tensor(2.7804, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 4.01115561, Culminative Send Cost: 1685600\n",
      "tensor(2.7806, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 4.00916672, Culminative Send Cost: 1693440\n",
      "tensor(2.7807, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 4.00718641, Culminative Send Cost: 1701280\n",
      "tensor(2.7809, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 4.00521564, Culminative Send Cost: 1709120\n",
      "tensor(2.7810, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 4.00325346, Culminative Send Cost: 1716960\n",
      "tensor(2.7812, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 4.00130081, Culminative Send Cost: 1724800\n",
      "tensor(2.7814, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.99935603, Culminative Send Cost: 1732640\n",
      "tensor(2.7816, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.99742079, Culminative Send Cost: 1740480\n",
      "tensor(2.7818, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.99549413, Culminative Send Cost: 1748320\n",
      "tensor(2.7820, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.99357605, Culminative Send Cost: 1756160\n",
      "tensor(2.7822, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.99166656, Culminative Send Cost: 1764000\n",
      "tensor(2.7824, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.98976541, Culminative Send Cost: 1771840\n",
      "tensor(2.7827, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.98787284, Culminative Send Cost: 1779680\n",
      "tensor(2.7829, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.98598862, Culminative Send Cost: 1787520\n",
      "tensor(2.7831, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.98411298, Culminative Send Cost: 1795360\n",
      "tensor(2.7834, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.98224497, Culminative Send Cost: 1803200\n",
      "tensor(2.7836, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.98038554, Culminative Send Cost: 1811040\n",
      "tensor(2.7839, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.97853398, Culminative Send Cost: 1818880\n",
      "tensor(2.7841, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.97669101, Culminative Send Cost: 1826720\n",
      "tensor(2.7844, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.97485495, Culminative Send Cost: 1834560\n",
      "tensor(2.7847, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.97302747, Culminative Send Cost: 1842400\n",
      "tensor(2.7850, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.97120714, Culminative Send Cost: 1850240\n",
      "tensor(2.7853, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.96939516, Culminative Send Cost: 1858080\n",
      "tensor(2.7856, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.96759105, Culminative Send Cost: 1865920\n",
      "tensor(2.7859, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.96579385, Culminative Send Cost: 1873760\n",
      "tensor(2.7862, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.96400476, Culminative Send Cost: 1881600\n",
      "tensor(2.7865, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.96222377, Culminative Send Cost: 1889440\n",
      "tensor(2.7868, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.96044850, Culminative Send Cost: 1897280\n",
      "tensor(2.7871, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.95868230, Culminative Send Cost: 1905120\n",
      "tensor(2.7874, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.95692301, Culminative Send Cost: 1912960\n",
      "tensor(2.7878, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.95517063, Culminative Send Cost: 1920800\n",
      "tensor(2.7881, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.95342541, Culminative Send Cost: 1928640\n",
      "tensor(2.7885, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.95168781, Culminative Send Cost: 1936480\n",
      "tensor(2.7888, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.94995737, Culminative Send Cost: 1944320\n",
      "tensor(2.7892, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.94823360, Culminative Send Cost: 1952160\n",
      "tensor(2.7895, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.94651747, Culminative Send Cost: 1960000\n",
      "activation_stack.0.weight: tensor([[-6.5446e-03, -2.1518e-02, -2.7498e-03, -2.9598e-02,  3.3813e-02,\n",
      "          1.2349e-02,  1.0816e-02,  7.1153e-03,  2.2459e-02, -1.1298e-02,\n",
      "          2.6749e-02, -1.7262e-02,  2.2749e-02,  3.5673e-02, -2.0569e-02,\n",
      "         -1.9208e-02, -2.6891e-02, -7.1190e-03, -1.8836e-03,  2.4361e-02,\n",
      "          2.5456e-03, -1.8451e-02,  3.0681e-02, -1.5387e-03,  1.3231e-03,\n",
      "         -1.5323e-02, -1.7825e-02, -3.0476e-02, -1.2645e-02,  2.6123e-02,\n",
      "          2.6191e-02, -3.2449e-02, -1.9409e-02,  1.4017e-02, -1.5704e-02,\n",
      "          1.5251e-02, -6.2655e-03,  2.6960e-02,  3.6381e-02, -1.8967e-03,\n",
      "         -4.6208e-03,  3.5676e-02,  1.8318e-02, -3.0760e-02, -8.5155e-03,\n",
      "         -2.4711e-02, -9.8565e-03, -9.0883e-03, -2.4924e-02,  2.3232e-02,\n",
      "          2.5940e-02,  5.7792e-03,  1.0510e-02,  3.2244e-02, -1.9635e-02,\n",
      "          2.8050e-02, -8.4912e-03,  2.9170e-02,  2.2530e-02,  2.4384e-02,\n",
      "          1.7574e-02, -2.6869e-02,  2.7126e-03, -2.3366e-02, -5.8289e-04,\n",
      "          4.8377e-02,  5.4980e-03,  3.3510e-02,  4.1038e-02,  5.6444e-02,\n",
      "          2.3417e-02,  4.9771e-02,  8.2268e-02,  5.2020e-02,  4.7685e-02,\n",
      "          7.2310e-02,  4.4250e-03,  2.0477e-02, -1.2687e-02, -3.1337e-02,\n",
      "          1.2274e-02,  3.3824e-02,  1.1827e-02, -2.5100e-02,  4.6825e-03,\n",
      "          3.4405e-02, -3.2193e-02, -3.5772e-02, -2.2324e-02,  7.3046e-03,\n",
      "         -3.4064e-04, -6.1130e-03,  4.2264e-03,  3.7745e-02,  5.4105e-02,\n",
      "          6.9054e-02,  5.5384e-02,  7.4164e-02,  8.7441e-02,  1.1971e-01,\n",
      "          9.1196e-02,  1.3570e-01,  1.4804e-01,  1.0604e-01,  9.2625e-02,\n",
      "          7.2506e-02,  5.8054e-02,  2.2204e-02, -7.2095e-03, -2.2348e-03,\n",
      "          2.2886e-03, -2.8924e-02,  2.5851e-02,  2.8395e-02, -1.2224e-03,\n",
      "         -1.8593e-02,  2.7617e-02,  3.3188e-02,  1.2265e-02,  2.4040e-02,\n",
      "         -1.0951e-02,  2.1872e-03,  5.3796e-02,  2.3571e-02,  2.4998e-02,\n",
      "          4.4811e-02,  3.7789e-03,  9.9456e-03,  5.9211e-02,  8.1653e-02,\n",
      "          4.4895e-02,  4.7275e-02,  6.2745e-02,  5.9276e-02,  7.8535e-02,\n",
      "          4.8125e-02,  1.2357e-02, -2.0915e-02,  6.4511e-03, -2.8393e-02,\n",
      "         -2.9200e-02,  2.2723e-02, -7.6933e-03,  8.5382e-03,  2.0800e-02,\n",
      "          1.5812e-02,  7.9838e-03, -3.3207e-02, -1.4936e-02,  3.9660e-02,\n",
      "         -2.2064e-02, -8.5426e-03, -3.8080e-02, -8.2892e-02, -1.1407e-01,\n",
      "         -8.1604e-02, -6.5208e-02, -2.8279e-02, -1.0595e-02,  2.9010e-03,\n",
      "         -2.1486e-02,  9.6709e-03,  3.9850e-03,  2.0087e-02,  3.3872e-02,\n",
      "         -1.2516e-02,  2.2544e-02,  1.1042e-02,  2.8157e-02,  1.1197e-02,\n",
      "          3.2438e-02,  5.6387e-03, -1.9916e-03, -4.9421e-02, -1.5408e-02,\n",
      "         -4.6647e-02, -1.5439e-02, -3.8618e-02, -3.5271e-02, -7.7829e-03,\n",
      "         -6.0949e-02, -3.5508e-02, -5.4409e-02,  4.2026e-02,  3.2413e-03,\n",
      "          1.4379e-02, -5.4002e-02, -9.2960e-02, -3.7012e-02, -2.3585e-02,\n",
      "          2.3746e-02,  4.7076e-02,  6.3671e-02,  3.8368e-02,  1.1294e-02,\n",
      "          2.4770e-02,  1.1746e-03,  3.5428e-02,  2.8303e-02,  3.1055e-02,\n",
      "          1.1156e-02, -5.7108e-03, -6.6718e-02, -1.1438e-02,  1.2098e-02,\n",
      "          3.8961e-03,  2.1134e-02,  2.3206e-02, -5.7517e-04,  2.0377e-02,\n",
      "          5.6588e-02,  1.2843e-01,  1.0865e-01, -1.0300e-02, -3.3494e-02,\n",
      "         -1.0560e-01, -3.3827e-02, -4.5433e-02, -2.3503e-02,  9.6605e-02,\n",
      "          4.9323e-02,  4.7654e-02,  3.2136e-02,  3.4648e-03, -1.7077e-02,\n",
      "         -2.6180e-02,  1.0132e-02,  1.7347e-03,  1.8384e-02, -1.7538e-02,\n",
      "          9.6522e-03,  4.4143e-02,  4.7482e-02,  8.5706e-02,  1.0900e-01,\n",
      "          1.2374e-01,  1.4689e-01,  1.0809e-01,  5.1438e-02,  2.8634e-02,\n",
      "         -1.4562e-02, -1.6118e-02, -7.5590e-02, -7.8491e-02, -1.3355e-02,\n",
      "          3.9014e-03,  4.1959e-02,  7.6713e-02,  1.2079e-01,  6.6941e-02,\n",
      "          4.1625e-02, -2.1825e-02, -1.7901e-02, -2.5743e-02, -1.5722e-02,\n",
      "          2.7341e-02,  4.4352e-03,  5.6275e-03,  8.8316e-02,  9.0924e-02,\n",
      "          1.3052e-01,  1.5714e-01,  1.9748e-01,  1.8148e-01,  1.7156e-01,\n",
      "          7.7210e-02, -3.1683e-02, -7.1465e-02, -6.8449e-02, -8.5858e-03,\n",
      "         -6.3392e-02, -5.4199e-02, -3.2457e-02, -1.0707e-02,  2.0688e-02,\n",
      "          6.2852e-02,  6.4098e-02,  6.3845e-02,  1.8993e-02,  2.6944e-03,\n",
      "         -3.1506e-02,  1.5747e-02, -2.1442e-02,  1.3256e-02,  5.0333e-03,\n",
      "          8.2947e-02,  7.9155e-02,  1.7948e-01,  1.3618e-01,  1.6010e-01,\n",
      "          1.5886e-01,  1.9836e-01,  1.6589e-01,  2.4853e-02, -8.7007e-02,\n",
      "         -1.8338e-01, -5.6606e-02,  1.1110e-02,  1.1921e-02, -5.6382e-03,\n",
      "         -5.1224e-03,  3.0380e-02,  1.5837e-02,  6.2466e-02,  4.0770e-02,\n",
      "          2.4115e-02,  4.6764e-03,  1.5202e-02, -2.0559e-02,  2.0877e-02,\n",
      "          3.1569e-02,  4.6322e-02,  6.9357e-02,  1.2273e-01,  1.2447e-01,\n",
      "          1.4688e-01,  1.4987e-01,  1.4393e-01,  1.4477e-01,  2.2814e-01,\n",
      "          2.1488e-01,  1.3827e-01, -5.2676e-02, -9.1075e-02,  4.3542e-02,\n",
      "          9.9321e-02,  8.4887e-02,  3.3587e-02,  2.7918e-02, -2.8820e-03,\n",
      "         -1.7011e-02, -1.2819e-02,  2.5046e-02,  1.0221e-02,  1.3021e-02,\n",
      "          1.5751e-02,  2.1517e-03, -2.8913e-02,  2.9280e-02, -5.6016e-04,\n",
      "          3.6273e-02,  1.1501e-01,  1.3037e-01,  1.3938e-01,  7.2108e-02,\n",
      "          8.4334e-02,  8.7189e-02,  2.1325e-01,  2.7714e-01,  1.2063e-01,\n",
      "          7.1517e-04,  4.9278e-03,  1.9353e-01,  2.4593e-01,  1.6192e-01,\n",
      "          1.3155e-01,  2.9801e-02, -2.2121e-04, -5.6283e-02, -1.0235e-02,\n",
      "         -3.1282e-02,  2.7928e-02, -2.2089e-03,  2.1196e-02, -2.1405e-02,\n",
      "          1.5361e-02,  5.9172e-03,  4.2296e-02,  2.3752e-02,  5.0190e-02,\n",
      "          5.3682e-02,  3.5246e-02,  3.2033e-02, -7.7803e-03,  2.7743e-02,\n",
      "          1.4697e-01,  2.5536e-01,  6.6458e-02,  4.5977e-02,  1.4859e-01,\n",
      "          2.6492e-01,  2.8896e-01,  2.0565e-01,  8.3630e-02,  3.8012e-02,\n",
      "         -2.0721e-02, -1.8980e-02, -1.5079e-02, -4.4849e-02, -2.6356e-02,\n",
      "         -2.2173e-03, -1.1093e-02,  1.0839e-02, -2.1831e-02,  1.1689e-02,\n",
      "          3.8346e-02,  1.9794e-02,  3.8574e-02, -6.0977e-02, -3.1393e-02,\n",
      "         -6.8280e-02, -6.2597e-02, -2.9310e-02,  9.1942e-02,  1.7230e-01,\n",
      "          5.7597e-02,  5.0298e-02,  1.3222e-01,  2.7979e-01,  3.1538e-01,\n",
      "          1.2157e-01,  4.2593e-02, -2.0995e-03, -3.3016e-02, -2.6629e-02,\n",
      "         -1.8285e-02,  6.7011e-03,  3.6567e-02, -6.8885e-03, -8.6079e-03,\n",
      "         -4.8501e-03, -2.3335e-02, -6.1064e-04,  1.1742e-02,  1.1231e-02,\n",
      "         -3.9590e-02, -8.2492e-02, -8.6286e-02, -1.3694e-01, -1.0048e-01,\n",
      "         -1.3183e-02,  1.0089e-01,  1.2607e-01, -2.5104e-02,  4.4429e-02,\n",
      "          1.3401e-01,  2.6947e-01,  2.2524e-01, -6.3130e-03, -9.0686e-02,\n",
      "         -8.0857e-02, -5.1529e-02, -6.7351e-02, -6.9706e-02, -3.1008e-03,\n",
      "          6.9159e-03,  2.0441e-02,  1.3697e-02,  3.2559e-02,  6.3464e-03,\n",
      "         -2.4919e-02,  1.4992e-02, -6.3017e-03, -5.8613e-02, -8.1645e-02,\n",
      "         -1.3615e-01, -8.8756e-02, -6.3048e-02,  5.9905e-02,  1.6900e-01,\n",
      "          1.5774e-01, -1.6625e-02,  4.8638e-03,  1.3266e-01,  2.3078e-01,\n",
      "          6.9133e-02, -9.5438e-02, -1.0999e-01, -6.5284e-02, -6.1021e-02,\n",
      "         -5.3931e-02, -8.9859e-04, -2.1236e-02,  2.4516e-03,  1.2154e-03,\n",
      "          3.4767e-03, -4.8699e-03,  2.0605e-02, -2.0379e-03,  1.8995e-03,\n",
      "         -2.3875e-02, -8.4795e-02, -1.3924e-01, -1.3922e-01, -6.3046e-02,\n",
      "          2.4840e-02,  1.3792e-01,  2.1487e-01,  7.0995e-02, -2.3174e-02,\n",
      "         -2.4426e-03,  1.3608e-01,  1.3148e-01, -2.0863e-02, -6.4000e-02,\n",
      "         -4.3890e-02, -2.8572e-02, -6.3335e-02, -6.8864e-02, -2.8648e-02,\n",
      "         -3.2601e-02,  5.4178e-03,  3.7925e-03,  2.3133e-02, -2.4158e-02,\n",
      "         -9.1628e-03,  3.5975e-02,  3.3070e-03, -6.1380e-02, -8.4988e-02,\n",
      "         -8.8455e-02, -9.1919e-02, -2.5108e-02,  5.5063e-02,  1.0880e-01,\n",
      "          1.4368e-01,  2.1485e-02, -4.5212e-02, -9.7288e-03,  6.7917e-02,\n",
      "          3.2665e-02,  1.0165e-02,  7.4637e-03, -4.1383e-02,  8.3739e-03,\n",
      "         -8.0526e-02, -4.7049e-02, -7.6032e-02, -6.2069e-02, -1.7463e-02,\n",
      "          2.6029e-02,  1.5851e-02, -5.2897e-03, -5.1899e-03,  6.0855e-03,\n",
      "         -3.0855e-02, -4.9882e-02, -3.1949e-02, -8.8021e-02, -3.1291e-02,\n",
      "         -6.9936e-02, -2.9018e-02,  4.7852e-02,  5.1458e-02, -3.8808e-02,\n",
      "         -9.6789e-02, -3.5971e-02,  1.0905e-02,  1.7554e-02,  9.7277e-03,\n",
      "         -3.6256e-02, -1.9101e-02, -4.3781e-02, -4.0875e-02, -4.7735e-02,\n",
      "         -2.5251e-02, -5.3073e-02, -2.0972e-02,  1.3294e-02, -6.6909e-03,\n",
      "          2.8661e-02,  1.2007e-02, -1.6530e-02, -1.2148e-02, -6.5145e-02,\n",
      "         -3.9561e-02, -6.8171e-02, -1.7700e-02, -7.1308e-02, -7.7665e-02,\n",
      "         -3.0939e-02, -1.4745e-02,  1.7365e-02, -2.5909e-02, -2.4387e-02,\n",
      "          1.7116e-02, -2.3324e-03, -1.9824e-02, -4.6615e-04,  8.8413e-04,\n",
      "         -3.9559e-02, -5.1286e-02, -5.2183e-02, -8.8915e-02, -1.7096e-02,\n",
      "         -1.5424e-02, -2.2622e-03, -1.8340e-02,  1.4076e-03, -7.6993e-03,\n",
      "         -1.6551e-02,  2.5706e-03, -5.5947e-02, -6.8759e-02, -7.6339e-02,\n",
      "         -5.0139e-02, -4.4703e-02, -7.1381e-02, -5.6786e-02, -4.7397e-02,\n",
      "          4.5432e-02,  2.8012e-02,  3.0015e-02, -5.0007e-02,  6.9818e-03,\n",
      "          1.8572e-02,  1.7238e-02, -2.5005e-02, -3.5989e-02, -5.1585e-02,\n",
      "         -2.9089e-02, -9.7699e-03, -4.7383e-02, -1.3013e-02, -1.0314e-02,\n",
      "          1.0552e-02,  3.0059e-03,  9.8134e-03,  1.3961e-02,  1.3767e-02,\n",
      "         -2.4705e-02, -2.5251e-02, -3.7752e-02, -6.6595e-02, -9.7928e-02,\n",
      "         -1.1475e-01, -7.9181e-02, -2.0854e-02,  8.0466e-03,  2.6170e-03,\n",
      "         -5.4920e-02, -6.9176e-02, -4.2348e-02, -9.4595e-03, -4.6146e-03,\n",
      "          3.4761e-02,  2.6211e-02, -1.6217e-02, -1.3102e-02, -9.3943e-03,\n",
      "          1.1669e-02,  1.5084e-03, -3.6505e-02,  1.6888e-02,  2.7830e-03,\n",
      "         -2.1274e-02, -3.3524e-02,  1.4865e-03, -1.8230e-02, -3.2567e-02,\n",
      "         -1.4692e-02, -1.9353e-02, -1.1511e-02, -4.0692e-03, -1.8303e-02,\n",
      "          4.1421e-02,  4.1444e-02,  4.6931e-02,  1.2358e-03, -2.5982e-02,\n",
      "          2.7206e-02,  3.4321e-02,  8.3987e-02,  3.3566e-02,  5.6168e-02,\n",
      "          4.3292e-03,  2.9221e-02,  3.0918e-02,  4.6327e-03,  2.1405e-02,\n",
      "          3.7426e-04,  3.1666e-02,  1.3971e-02, -1.6193e-02,  8.0220e-03,\n",
      "         -1.8231e-02,  1.8874e-02,  1.4198e-02,  1.1017e-02,  2.9996e-02,\n",
      "          1.0112e-01,  9.0987e-02,  1.3515e-01,  8.5819e-02,  1.2389e-01,\n",
      "          8.5615e-02,  1.1216e-01,  1.1400e-01,  1.0571e-01,  1.4869e-01,\n",
      "          1.3403e-01,  1.2930e-01,  6.3425e-02,  8.2053e-02,  4.3767e-02,\n",
      "         -1.1438e-02,  2.2737e-02,  2.2126e-03,  1.2757e-02,  2.2327e-02,\n",
      "          2.3944e-02, -1.1248e-02, -2.8701e-02,  9.6828e-03,  1.0424e-02,\n",
      "         -9.2714e-03,  1.0484e-02,  3.9522e-02,  1.2426e-01,  1.3385e-01,\n",
      "          1.8596e-01,  1.6996e-01,  2.0026e-01,  1.6856e-01,  1.6749e-01,\n",
      "          1.8309e-01,  1.7033e-01,  1.9457e-01,  1.3205e-01,  1.2127e-01,\n",
      "          7.2221e-02,  2.8828e-02,  2.7799e-02,  8.8244e-03, -8.1949e-03,\n",
      "         -2.3492e-02, -1.4699e-02,  2.7359e-02,  9.0835e-03, -2.6705e-02,\n",
      "         -3.2345e-02,  3.0939e-02,  1.5957e-02,  2.5073e-03,  1.0742e-02,\n",
      "          5.5922e-02,  3.4101e-02,  7.7077e-02,  6.2096e-02,  1.1186e-01,\n",
      "          9.5983e-02,  1.0020e-01,  1.1947e-01,  7.5834e-02,  7.8621e-02,\n",
      "          8.0360e-02,  3.7513e-02,  5.6441e-02,  2.1984e-02,  3.0461e-02,\n",
      "         -1.8705e-02,  4.0271e-03,  2.2915e-02, -2.5868e-03,  9.8897e-03,\n",
      "         -1.0356e-02, -3.2795e-02, -3.1768e-02, -3.4153e-02, -5.4101e-04,\n",
      "          2.0262e-02,  8.2871e-03,  6.5280e-03,  1.7585e-02,  1.4802e-02,\n",
      "         -1.5559e-02,  2.0591e-02, -1.4420e-02,  3.7610e-02,  4.3473e-02,\n",
      "          2.9981e-02,  4.5263e-02, -4.0662e-03,  3.1799e-02,  1.9537e-02,\n",
      "         -2.3646e-02,  6.2638e-03, -1.0050e-02,  1.0130e-02, -2.4681e-02,\n",
      "          3.0727e-02,  3.0997e-03,  2.7669e-03,  1.3365e-02]])\n",
      "activation_stack.0.bias: tensor([0.9457])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 1960000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for num_clients is 5 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[ 2.9057e-02,  1.9755e-02,  3.1227e-02,  2.5510e-02, -1.4220e-02,\n",
      "         -2.8423e-02, -1.6518e-03,  8.3696e-03,  9.1939e-04,  5.0871e-03,\n",
      "          1.5797e-02,  2.3204e-02,  3.2200e-03,  1.5971e-02,  3.3545e-02,\n",
      "         -1.2607e-02,  2.8855e-03,  1.1435e-02, -1.8131e-02,  8.4723e-03,\n",
      "          1.9175e-02,  2.2597e-02,  1.3840e-02,  1.7784e-02, -1.4008e-02,\n",
      "          2.0051e-02, -3.3521e-02,  1.7294e-02, -7.0699e-03,  2.2931e-02,\n",
      "          3.2938e-02,  2.1939e-02, -1.6626e-02,  1.7782e-02,  1.2002e-02,\n",
      "         -3.6824e-03, -1.3086e-02,  1.1592e-04, -1.9222e-02, -1.3097e-02,\n",
      "         -3.0604e-02, -3.2357e-02,  1.2384e-02, -5.2202e-03,  1.2590e-02,\n",
      "          2.2525e-02,  1.8023e-02,  1.2608e-02,  2.1586e-02,  1.2243e-02,\n",
      "         -1.9710e-02, -5.5311e-03, -5.9788e-03, -3.2959e-02, -1.7903e-03,\n",
      "          2.0072e-02,  2.4441e-02, -3.5434e-02,  5.8499e-03, -1.2438e-02,\n",
      "         -3.2883e-02, -3.4568e-02, -2.5500e-02, -2.6301e-02,  7.1461e-03,\n",
      "          2.2423e-02, -1.9012e-02, -2.0280e-02, -1.1018e-02,  3.5615e-02,\n",
      "         -2.5163e-02,  2.4091e-02,  1.1600e-02, -2.0047e-02,  3.3826e-03,\n",
      "         -1.6642e-02, -1.3097e-02, -3.3723e-02, -2.4271e-02, -3.5501e-03,\n",
      "          1.4164e-02, -2.5322e-02,  2.3322e-02, -1.9134e-02, -3.1152e-02,\n",
      "         -2.2403e-02, -2.6051e-02,  2.2969e-02, -1.5581e-02,  3.2569e-02,\n",
      "          1.3083e-02,  3.5480e-02, -1.5334e-02, -1.5021e-02, -5.5848e-03,\n",
      "         -9.9745e-03,  5.7703e-03, -1.5943e-02,  1.6958e-02, -3.1826e-02,\n",
      "          4.0319e-04, -9.0475e-03,  3.6898e-03, -3.0094e-02,  3.0813e-02,\n",
      "          3.1689e-02,  3.0696e-02,  2.3232e-02, -2.6021e-03, -2.1680e-02,\n",
      "         -1.7107e-02,  2.7817e-03,  2.3890e-02, -3.3061e-02,  3.7804e-04,\n",
      "         -2.9392e-02,  1.9112e-02, -2.5165e-02,  3.5437e-02, -3.9802e-03,\n",
      "          1.7609e-02,  1.6392e-02, -1.9599e-02, -4.4269e-03,  4.5413e-03,\n",
      "         -1.0101e-02,  1.3371e-02,  4.7983e-03, -5.9700e-03, -3.4045e-02,\n",
      "         -1.1956e-03, -2.1457e-02, -1.1159e-02, -2.6236e-02, -2.2710e-02,\n",
      "         -2.6989e-02,  2.9870e-02,  1.0264e-04, -1.1458e-02, -2.1502e-02,\n",
      "         -2.8180e-02,  3.0185e-02, -1.8000e-02, -1.8610e-02,  2.3662e-02,\n",
      "          1.6082e-02,  3.3569e-02, -2.2378e-02, -2.9433e-02, -1.8914e-02,\n",
      "         -8.3997e-03,  5.4676e-03, -3.0088e-02, -1.4547e-02,  1.7828e-02,\n",
      "          1.0985e-02, -1.4606e-02,  3.1077e-02, -7.3599e-03,  2.8449e-02,\n",
      "          3.2151e-02,  1.9037e-02,  1.5348e-02, -2.2277e-02,  2.1621e-02,\n",
      "          2.8374e-02, -2.1016e-02, -7.7713e-03,  3.5326e-02,  1.0535e-02,\n",
      "         -2.5283e-02, -6.8048e-03, -1.6236e-02, -1.9133e-02, -3.3869e-02,\n",
      "          1.3036e-02, -1.9029e-02,  3.4432e-02,  1.1354e-02,  1.3503e-02,\n",
      "          3.3535e-02, -1.6391e-02, -7.7444e-03,  1.5141e-02,  1.1460e-02,\n",
      "         -1.4758e-02,  3.3247e-02, -1.0649e-02, -2.6678e-02, -1.8139e-02,\n",
      "          1.0534e-03, -1.1676e-02, -1.3614e-02,  3.5089e-02,  2.1439e-03,\n",
      "          1.9407e-02, -5.9690e-04, -2.6931e-02,  1.1924e-03, -2.7253e-02,\n",
      "         -8.5980e-03, -2.6171e-03,  4.8252e-03,  1.0908e-02,  1.0993e-02,\n",
      "          1.8074e-02, -7.3281e-03, -3.2271e-02,  1.6151e-02, -1.7308e-02,\n",
      "          2.0170e-03,  1.8726e-02,  2.8696e-02, -1.7043e-02,  1.5236e-02,\n",
      "         -2.4980e-02, -2.6863e-02,  7.9874e-03,  2.6439e-03, -1.9641e-02,\n",
      "          1.3577e-02, -1.9476e-02,  1.2533e-02, -1.9344e-02, -1.7752e-02,\n",
      "         -2.8342e-02,  3.4600e-02, -1.1475e-02,  8.1230e-03,  2.5418e-02,\n",
      "         -2.1363e-02,  1.2753e-02, -1.4632e-02, -7.6649e-03,  7.3715e-03,\n",
      "         -2.0899e-02,  1.8223e-02,  3.0170e-02, -1.6868e-02, -3.2631e-02,\n",
      "          2.8135e-02, -2.1011e-02, -3.2529e-02,  2.4141e-02, -1.3106e-02,\n",
      "         -1.3795e-02,  3.0953e-02,  3.0092e-02,  7.9020e-03, -3.5300e-02,\n",
      "          2.0342e-02, -3.5215e-02,  7.1325e-04,  9.7278e-03, -2.9068e-02,\n",
      "          3.4296e-02,  8.8494e-03,  1.5225e-02, -2.9298e-02, -3.3828e-02,\n",
      "         -3.0475e-02,  4.4136e-03,  2.2575e-02,  3.1767e-02, -6.1274e-03,\n",
      "         -2.9244e-02, -1.9246e-02, -1.7964e-02,  1.3280e-02, -3.4391e-03,\n",
      "         -2.7907e-02,  1.8439e-02,  2.6463e-02, -3.0637e-02, -1.1873e-02,\n",
      "         -4.9135e-03,  1.8038e-02, -2.5394e-02, -1.8057e-02, -1.2149e-03,\n",
      "         -5.6049e-03,  2.2492e-02,  2.3939e-02, -1.5280e-02,  2.3739e-02,\n",
      "         -3.4437e-02,  2.5055e-02,  3.2434e-02, -1.0795e-02,  3.3662e-02,\n",
      "         -2.1162e-02, -2.5583e-02, -2.9951e-02,  7.3505e-03, -2.6927e-02,\n",
      "          3.2439e-02,  1.5165e-02,  1.8123e-02, -5.2196e-03,  3.1486e-02,\n",
      "          1.9206e-02, -2.6025e-02, -1.4500e-03,  1.0201e-02,  6.6164e-03,\n",
      "          2.4473e-03,  7.5278e-03, -1.8733e-02, -2.1281e-03, -3.4109e-03,\n",
      "         -4.5216e-03,  1.9484e-02, -1.6086e-02,  3.1224e-02,  3.0130e-02,\n",
      "          8.6876e-03,  7.0399e-03, -2.3736e-02, -1.2706e-02, -1.9750e-03,\n",
      "          2.6872e-02,  6.9431e-03,  3.7043e-03, -1.9022e-02,  3.0432e-03,\n",
      "          2.6222e-02, -3.4650e-02, -3.5966e-03, -1.7801e-02,  1.9215e-02,\n",
      "          1.3428e-02,  2.3561e-02,  2.8948e-02, -9.9491e-03,  3.0152e-02,\n",
      "         -1.5389e-02, -9.5149e-03, -3.2272e-02, -1.2377e-02, -3.2840e-02,\n",
      "          2.3233e-02, -1.0798e-02,  1.2486e-02,  3.5641e-02, -8.5122e-03,\n",
      "         -1.7892e-02, -3.3915e-02,  2.6219e-02, -8.8584e-03, -3.1798e-02,\n",
      "          9.9789e-03, -2.7851e-02, -1.3205e-02, -1.6859e-02, -9.5383e-03,\n",
      "         -2.8443e-02, -3.0990e-02,  1.8095e-02,  1.5672e-02,  2.4195e-02,\n",
      "          5.3028e-03,  1.0502e-02, -1.5801e-02,  3.3835e-02,  3.3232e-02,\n",
      "          1.7304e-02,  8.4738e-03, -3.4354e-02,  5.1924e-03, -1.0222e-02,\n",
      "         -1.8914e-02,  2.7452e-02, -2.3827e-02, -3.3414e-02, -1.2657e-02,\n",
      "          9.3027e-03, -2.1539e-02, -1.1614e-02,  2.1090e-02,  1.6344e-02,\n",
      "          1.5198e-02, -1.1964e-02,  1.0405e-02, -1.0667e-03, -2.5188e-02,\n",
      "         -1.8308e-03, -6.7782e-03, -1.0246e-02, -6.5172e-03,  2.9532e-02,\n",
      "          1.2732e-03, -3.1293e-02, -1.4120e-02,  3.4452e-02, -1.3639e-02,\n",
      "          2.6601e-02,  2.2047e-02,  2.3640e-02,  8.9056e-04,  2.8894e-02,\n",
      "          1.5367e-02,  1.2164e-02, -2.3225e-02, -1.9043e-02,  3.4307e-02,\n",
      "         -1.7295e-02,  1.6998e-02,  8.3183e-04,  2.3379e-02,  8.8402e-03,\n",
      "         -1.7597e-02,  1.1270e-02,  1.0922e-02, -2.0056e-02, -5.1699e-03,\n",
      "         -8.5066e-03,  1.4557e-02,  2.2035e-02, -2.7238e-02,  7.6650e-03,\n",
      "          1.9188e-02, -4.6497e-03, -1.1547e-02,  1.4469e-02,  5.7256e-03,\n",
      "          2.8100e-02, -3.2142e-02,  1.1397e-02, -8.1856e-03, -3.3596e-02,\n",
      "          2.7589e-02,  1.5299e-02,  1.9082e-02, -2.1411e-02,  1.9773e-02,\n",
      "          2.9286e-03, -2.9301e-02,  2.8655e-02, -2.4148e-02, -2.6496e-02,\n",
      "          1.5930e-03, -3.0127e-02,  3.4636e-02,  3.6696e-03, -8.0106e-03,\n",
      "         -2.5918e-03, -1.2427e-02, -2.6201e-02,  2.9030e-02, -2.3732e-02,\n",
      "         -1.7496e-02,  3.1867e-02, -3.0122e-02, -3.4759e-02,  2.2080e-02,\n",
      "         -3.5201e-02, -3.7980e-03,  9.7680e-03, -1.8988e-02,  1.7324e-02,\n",
      "         -2.0530e-02, -1.5946e-02, -1.0210e-02,  9.3225e-05,  3.4046e-02,\n",
      "          1.2725e-03, -7.4169e-03,  2.8210e-02,  2.8497e-02,  6.4993e-03,\n",
      "         -5.4530e-03, -1.1761e-02, -3.3295e-02,  2.0105e-02,  2.1814e-02,\n",
      "         -7.5083e-03, -2.3206e-02,  1.8875e-02, -2.8523e-02,  3.4243e-03,\n",
      "          2.5430e-02, -1.7034e-03,  3.1004e-02,  2.1153e-02, -1.7129e-02,\n",
      "         -3.2265e-02,  1.9221e-02, -3.0099e-02,  3.5605e-03,  3.1837e-02,\n",
      "          1.7983e-02,  9.5713e-03, -1.5283e-02, -2.3321e-02, -5.0430e-03,\n",
      "          7.2701e-03, -2.1922e-02, -1.3525e-02, -2.9970e-02,  3.1187e-02,\n",
      "         -2.3542e-02, -2.7256e-02,  2.5910e-02,  1.3007e-02, -3.5485e-02,\n",
      "         -2.3402e-02,  2.4332e-02, -1.3099e-02,  1.3494e-03,  2.7198e-02,\n",
      "         -1.6041e-02,  1.3141e-03, -6.9856e-04,  2.5694e-03, -3.1486e-02,\n",
      "          2.6321e-02, -7.1673e-03,  2.9981e-02, -2.0177e-02, -1.0606e-02,\n",
      "          3.0717e-02, -5.6661e-03, -2.8850e-02, -3.0952e-02,  7.7331e-04,\n",
      "          2.5700e-03,  2.4023e-03, -2.8721e-02,  3.1458e-02,  2.6754e-02,\n",
      "         -8.2940e-03,  1.4420e-03,  2.7356e-02,  2.8986e-02,  3.1953e-02,\n",
      "         -1.8826e-02,  1.4963e-03,  1.2641e-02,  4.7642e-03, -7.2919e-03,\n",
      "         -1.5000e-02,  3.4124e-03,  2.6325e-02,  3.3530e-02, -3.1711e-02,\n",
      "          2.5594e-03,  1.0917e-03,  1.7045e-02,  1.5035e-02,  1.4598e-02,\n",
      "         -2.6861e-02,  2.7406e-02,  5.5983e-03, -3.5243e-02, -2.8650e-02,\n",
      "         -1.5032e-03, -1.0903e-02, -3.2144e-03,  1.7944e-02, -4.1622e-03,\n",
      "          2.8190e-03,  1.6045e-02, -2.5363e-03,  6.7850e-03, -2.4081e-02,\n",
      "         -8.2384e-03, -9.4850e-03,  1.1404e-02,  2.9320e-02, -2.4640e-02,\n",
      "          1.1089e-02,  2.2456e-02,  3.5025e-02, -1.2266e-02, -1.5007e-02,\n",
      "         -1.5129e-02,  1.6493e-02, -3.4143e-02, -3.2130e-02, -2.3030e-02,\n",
      "         -9.7580e-03,  1.8971e-02, -1.0578e-02,  2.6036e-02, -2.3598e-02,\n",
      "          3.5044e-02,  1.2192e-02,  8.3705e-03, -1.6968e-02, -1.9365e-02,\n",
      "         -9.0941e-03, -2.6928e-02,  8.6637e-03, -2.5581e-02, -7.3469e-03,\n",
      "          2.7237e-02,  3.1580e-02,  2.8017e-02,  3.0142e-02, -1.4630e-02,\n",
      "          3.9381e-03, -2.9159e-02,  5.1051e-05,  8.4263e-03, -6.3252e-03,\n",
      "         -1.8608e-02, -2.3197e-02, -1.2928e-02,  3.1423e-02, -3.3121e-02,\n",
      "         -7.5194e-03, -1.1000e-02, -1.2000e-02, -2.8827e-02,  2.7745e-02,\n",
      "         -9.2184e-03, -6.5133e-03,  5.6240e-03, -2.7731e-02,  1.3650e-02,\n",
      "         -3.0542e-02,  2.1526e-02,  5.1441e-03,  3.0557e-02,  1.9639e-03,\n",
      "          1.3634e-02,  1.8021e-02,  7.8055e-03,  1.4856e-03,  2.0357e-02,\n",
      "          2.4325e-02, -2.0168e-02,  1.6694e-02,  7.9307e-03, -2.9890e-02,\n",
      "         -2.2329e-02, -7.1978e-03, -1.0136e-02,  7.6632e-03,  2.8664e-02,\n",
      "          1.2855e-02,  4.2878e-03, -1.8888e-02,  2.3721e-02, -2.9244e-02,\n",
      "         -1.6335e-02, -7.2556e-03,  3.1631e-02,  3.5733e-03,  1.1458e-02,\n",
      "         -5.4049e-03,  1.9662e-02, -3.5691e-02,  1.9761e-02, -2.1142e-02,\n",
      "          1.9751e-03,  4.0142e-03, -2.8451e-02, -2.4614e-02,  2.8285e-02,\n",
      "         -7.1150e-03,  3.7368e-03,  1.9664e-02, -5.2390e-03, -7.5404e-03,\n",
      "         -5.7965e-03,  3.4580e-02,  2.5185e-02,  1.9692e-02, -1.7048e-02,\n",
      "          2.0540e-02, -2.6580e-02, -1.5962e-02, -3.0208e-02, -3.2732e-02,\n",
      "          2.5849e-02,  2.9361e-02, -3.4454e-02, -6.7156e-03,  5.8167e-03,\n",
      "          2.5839e-02,  3.1654e-02,  3.0445e-02, -9.6001e-04, -1.3886e-03,\n",
      "         -1.4971e-02, -9.8005e-03,  1.5716e-02,  2.1728e-03,  7.1030e-03,\n",
      "          1.3546e-02,  1.2762e-03, -1.9268e-02, -2.8579e-02,  1.9176e-02,\n",
      "          2.7626e-02,  1.6970e-03,  3.4677e-02,  2.8397e-02,  1.8350e-02,\n",
      "         -8.7925e-03,  2.0568e-02,  6.4731e-03,  3.3843e-02,  2.4843e-02,\n",
      "         -4.5786e-03,  6.7703e-03, -1.1865e-02,  4.8314e-03, -1.5685e-02,\n",
      "          2.0053e-02,  1.8281e-02, -1.8223e-02, -2.9970e-02, -8.7538e-03,\n",
      "         -2.2831e-02, -1.4772e-02,  1.6463e-02, -5.2219e-03,  2.9273e-02,\n",
      "          4.7677e-03,  4.4461e-03, -5.0490e-03, -4.6346e-03,  9.4343e-03,\n",
      "          2.2229e-02,  3.1103e-02,  1.3589e-02, -1.3592e-02,  2.9583e-02,\n",
      "         -1.0423e-02,  3.4157e-02, -1.1760e-02,  1.6746e-02,  2.4669e-02,\n",
      "          3.0074e-02,  3.4461e-02,  3.1242e-02, -2.9565e-02, -3.3092e-02,\n",
      "         -2.0767e-02, -2.5131e-02, -1.9351e-02,  8.4462e-03,  7.1204e-03,\n",
      "         -2.7725e-02, -4.7676e-04,  3.7633e-03,  3.3171e-02, -2.4137e-03,\n",
      "          2.8889e-02, -2.8155e-02, -2.3977e-02,  1.1285e-02,  1.3143e-02,\n",
      "          1.4372e-02, -3.5362e-02, -1.6213e-02, -3.0135e-03, -1.4180e-02,\n",
      "         -2.8635e-02,  1.0819e-02, -4.2390e-03, -3.5490e-02,  1.7475e-02,\n",
      "          7.9070e-03, -3.4764e-03,  1.2593e-02, -1.5954e-02, -2.0439e-02,\n",
      "         -1.3348e-02,  9.2911e-03, -1.4851e-02,  3.5154e-02,  1.7669e-02,\n",
      "          1.6649e-02,  3.0053e-02, -9.9781e-03, -1.1229e-02, -3.7577e-04,\n",
      "          1.3320e-02,  3.4263e-02, -3.0287e-02,  2.4610e-02]])), ('activation_stack.0.bias', tensor([-0.0163]))])\n",
      "tensor(0.0074, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 25.87866974, Culminative Send Cost: 3920\n",
      "tensor(1.1407, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.66314411, Culminative Send Cost: 39200\n",
      "tensor(1.7424, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.10758972, Culminative Send Cost: 78400\n",
      "tensor(2.0267, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.07724190, Culminative Send Cost: 117600\n",
      "tensor(2.1699, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.58562946, Culminative Send Cost: 156800\n",
      "tensor(2.2497, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.23792028, Culminative Send Cost: 196000\n",
      "tensor(2.3007, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 6.95143127, Culminative Send Cost: 235200\n",
      "tensor(2.3380, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.70530891, Culminative Send Cost: 274400\n",
      "tensor(2.3686, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.49146748, Culminative Send Cost: 313600\n",
      "tensor(2.3954, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.30489445, Culminative Send Cost: 352800\n",
      "tensor(2.4198, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.14167738, Culminative Send Cost: 392000\n",
      "tensor(2.4425, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 5.99853754, Culminative Send Cost: 431200\n",
      "tensor(2.4637, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.87268496, Culminative Send Cost: 470400\n",
      "tensor(2.4837, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.76173067, Culminative Send Cost: 509600\n",
      "tensor(2.5025, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.66363096, Culminative Send Cost: 548800\n",
      "tensor(2.5203, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.57663202, Culminative Send Cost: 588000\n",
      "tensor(2.5370, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.49923038, Culminative Send Cost: 627200\n",
      "tensor(2.5527, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.43013525, Culminative Send Cost: 666400\n",
      "tensor(2.5676, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.36824036, Culminative Send Cost: 705600\n",
      "tensor(2.5815, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.31259298, Culminative Send Cost: 744800\n",
      "tensor(2.5947, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.26237679, Culminative Send Cost: 784000\n",
      "tensor(2.6070, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.21688652, Culminative Send Cost: 823200\n",
      "tensor(2.6186, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.17551804, Culminative Send Cost: 862400\n",
      "tensor(2.6295, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.13775015, Culminative Send Cost: 901600\n",
      "tensor(2.6397, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.10313368, Culminative Send Cost: 940800\n",
      "tensor(2.6493, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.07127953, Culminative Send Cost: 980000\n",
      "tensor(2.6583, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.04185390, Culminative Send Cost: 1019200\n",
      "tensor(2.6667, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.01456738, Culminative Send Cost: 1058400\n",
      "tensor(2.6746, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.98917007, Culminative Send Cost: 1097600\n",
      "tensor(2.6820, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.96544456, Culminative Send Cost: 1136800\n",
      "tensor(2.6889, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.94320440, Culminative Send Cost: 1176000\n",
      "tensor(2.6954, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.92228413, Culminative Send Cost: 1215200\n",
      "tensor(2.7014, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.90254498, Culminative Send Cost: 1254400\n",
      "tensor(2.7070, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.88386106, Culminative Send Cost: 1293600\n",
      "tensor(2.7123, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.86612606, Culminative Send Cost: 1332800\n",
      "tensor(2.7171, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.84924603, Culminative Send Cost: 1372000\n",
      "tensor(2.7217, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.83313894, Culminative Send Cost: 1411200\n",
      "tensor(2.7259, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.81773233, Culminative Send Cost: 1450400\n",
      "tensor(2.7298, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.80296421, Culminative Send Cost: 1489600\n",
      "tensor(2.7335, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.78877831, Culminative Send Cost: 1528800\n",
      "tensor(2.7369, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.77512550, Culminative Send Cost: 1568000\n",
      "tensor(2.7400, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.76196337, Culminative Send Cost: 1607200\n",
      "tensor(2.7429, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.74925327, Culminative Send Cost: 1646400\n",
      "tensor(2.7455, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.73696136, Culminative Send Cost: 1685600\n",
      "tensor(2.7480, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.72505665, Culminative Send Cost: 1724800\n",
      "tensor(2.7503, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.71351433, Culminative Send Cost: 1764000\n",
      "tensor(2.7523, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.70230818, Culminative Send Cost: 1803200\n",
      "tensor(2.7542, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.69141817, Culminative Send Cost: 1842400\n",
      "tensor(2.7560, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.68082333, Culminative Send Cost: 1881600\n",
      "tensor(2.7576, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.67050791, Culminative Send Cost: 1920800\n",
      "tensor(2.7590, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.66045475, Culminative Send Cost: 1960000\n",
      "tensor(2.7603, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.65065002, Culminative Send Cost: 1999200\n",
      "tensor(2.7615, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.64108086, Culminative Send Cost: 2038400\n",
      "tensor(2.7626, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.63173532, Culminative Send Cost: 2077600\n",
      "tensor(2.7635, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.62260199, Culminative Send Cost: 2116800\n",
      "tensor(2.7644, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.61367083, Culminative Send Cost: 2156000\n",
      "tensor(2.7651, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.60493326, Culminative Send Cost: 2195200\n",
      "tensor(2.7658, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.59638023, Culminative Send Cost: 2234400\n",
      "tensor(2.7664, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.58800507, Culminative Send Cost: 2273600\n",
      "tensor(2.7668, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.57979774, Culminative Send Cost: 2312800\n",
      "tensor(2.7673, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.57175446, Culminative Send Cost: 2352000\n",
      "tensor(2.7676, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.56386709, Culminative Send Cost: 2391200\n",
      "tensor(2.7679, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.55612993, Culminative Send Cost: 2430400\n",
      "tensor(2.7681, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.54853821, Culminative Send Cost: 2469600\n",
      "tensor(2.7683, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.54108620, Culminative Send Cost: 2508800\n",
      "tensor(2.7684, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.53376770, Culminative Send Cost: 2548000\n",
      "tensor(2.7685, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.52658081, Culminative Send Cost: 2587200\n",
      "tensor(2.7685, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.51951790, Culminative Send Cost: 2626400\n",
      "tensor(2.7685, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.51257706, Culminative Send Cost: 2665600\n",
      "tensor(2.7684, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.50575399, Culminative Send Cost: 2704800\n",
      "tensor(2.7683, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.49904490, Culminative Send Cost: 2744000\n",
      "tensor(2.7682, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.49244499, Culminative Send Cost: 2783200\n",
      "tensor(2.7680, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.48595285, Culminative Send Cost: 2822400\n",
      "tensor(2.7678, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.47956419, Culminative Send Cost: 2861600\n",
      "tensor(2.7676, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.47327518, Culminative Send Cost: 2900800\n",
      "tensor(2.7674, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.46708441, Culminative Send Cost: 2940000\n",
      "tensor(2.7671, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.46098804, Culminative Send Cost: 2979200\n",
      "tensor(2.7668, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.45498323, Culminative Send Cost: 3018400\n",
      "tensor(2.7665, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.44906855, Culminative Send Cost: 3057600\n",
      "tensor(2.7662, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.44324064, Culminative Send Cost: 3096800\n",
      "tensor(2.7659, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.43749666, Culminative Send Cost: 3136000\n",
      "tensor(2.7655, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.43183517, Culminative Send Cost: 3175200\n",
      "tensor(2.7652, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.42625284, Culminative Send Cost: 3214400\n",
      "tensor(2.7648, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.42074966, Culminative Send Cost: 3253600\n",
      "tensor(2.7644, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.41532135, Culminative Send Cost: 3292800\n",
      "tensor(2.7640, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.40996695, Culminative Send Cost: 3332000\n",
      "tensor(2.7636, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.40468359, Culminative Send Cost: 3371200\n",
      "tensor(2.7632, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.39947176, Culminative Send Cost: 3410400\n",
      "tensor(2.7628, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.39432716, Culminative Send Cost: 3449600\n",
      "tensor(2.7624, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.38924980, Culminative Send Cost: 3488800\n",
      "tensor(2.7620, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.38423634, Culminative Send Cost: 3528000\n",
      "tensor(2.7615, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.37928677, Culminative Send Cost: 3567200\n",
      "tensor(2.7611, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.37439919, Culminative Send Cost: 3606400\n",
      "tensor(2.7607, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.36957169, Culminative Send Cost: 3645600\n",
      "tensor(2.7603, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.36480236, Culminative Send Cost: 3684800\n",
      "tensor(2.7598, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.36009073, Culminative Send Cost: 3724000\n",
      "tensor(2.7594, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.35543585, Culminative Send Cost: 3763200\n",
      "tensor(2.7590, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.35083485, Culminative Send Cost: 3802400\n",
      "tensor(2.7586, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.34628773, Culminative Send Cost: 3841600\n",
      "tensor(2.7581, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.34179306, Culminative Send Cost: 3880800\n",
      "tensor(2.7577, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.33734894, Culminative Send Cost: 3920000\n",
      "tensor(2.7573, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.33295536, Culminative Send Cost: 3959200\n",
      "tensor(2.7569, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.32860994, Culminative Send Cost: 3998400\n",
      "tensor(2.7565, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.32431221, Culminative Send Cost: 4037600\n",
      "tensor(2.7561, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.32006216, Culminative Send Cost: 4076800\n",
      "tensor(2.7557, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.31585693, Culminative Send Cost: 4116000\n",
      "tensor(2.7553, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.31169653, Culminative Send Cost: 4155200\n",
      "tensor(2.7549, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.30757952, Culminative Send Cost: 4194400\n",
      "tensor(2.7545, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.30350590, Culminative Send Cost: 4233600\n",
      "tensor(2.7541, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.29947329, Culminative Send Cost: 4272800\n",
      "tensor(2.7538, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.29548216, Culminative Send Cost: 4312000\n",
      "tensor(2.7534, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.29153156, Culminative Send Cost: 4351200\n",
      "tensor(2.7530, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.28761911, Culminative Send Cost: 4390400\n",
      "tensor(2.7527, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.28374624, Culminative Send Cost: 4429600\n",
      "tensor(2.7523, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.27991056, Culminative Send Cost: 4468800\n",
      "tensor(2.7520, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.27611256, Culminative Send Cost: 4508000\n",
      "tensor(2.7516, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.27235031, Culminative Send Cost: 4547200\n",
      "tensor(2.7513, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.26862335, Culminative Send Cost: 4586400\n",
      "tensor(2.7510, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.26493168, Culminative Send Cost: 4625600\n",
      "tensor(2.7507, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.26127434, Culminative Send Cost: 4664800\n",
      "tensor(2.7504, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.25764990, Culminative Send Cost: 4704000\n",
      "tensor(2.7501, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.25405836, Culminative Send Cost: 4743200\n",
      "tensor(2.7498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.25049973, Culminative Send Cost: 4782400\n",
      "tensor(2.7495, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.24697208, Culminative Send Cost: 4821600\n",
      "tensor(2.7492, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.24347544, Culminative Send Cost: 4860800\n",
      "tensor(2.7489, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.24000978, Culminative Send Cost: 4900000\n",
      "tensor(2.7487, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.23657322, Culminative Send Cost: 4939200\n",
      "tensor(2.7484, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.23316622, Culminative Send Cost: 4978400\n",
      "tensor(2.7482, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.22978878, Culminative Send Cost: 5017600\n",
      "tensor(2.7480, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.22643852, Culminative Send Cost: 5056800\n",
      "tensor(2.7477, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.22311735, Culminative Send Cost: 5096000\n",
      "tensor(2.7475, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.21982241, Culminative Send Cost: 5135200\n",
      "tensor(2.7473, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.21655464, Culminative Send Cost: 5174400\n",
      "tensor(2.7471, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.21331310, Culminative Send Cost: 5213600\n",
      "tensor(2.7469, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.21009731, Culminative Send Cost: 5252800\n",
      "tensor(2.7467, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.20690727, Culminative Send Cost: 5292000\n",
      "tensor(2.7465, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.20374203, Culminative Send Cost: 5331200\n",
      "tensor(2.7464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.20060205, Culminative Send Cost: 5370400\n",
      "tensor(2.7462, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.19748545, Culminative Send Cost: 5409600\n",
      "tensor(2.7461, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.19439316, Culminative Send Cost: 5448800\n",
      "tensor(2.7459, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.19132423, Culminative Send Cost: 5488000\n",
      "tensor(2.7458, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.18827820, Culminative Send Cost: 5527200\n",
      "tensor(2.7456, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.18525505, Culminative Send Cost: 5566400\n",
      "tensor(2.7455, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.18225431, Culminative Send Cost: 5605600\n",
      "tensor(2.7454, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.17927504, Culminative Send Cost: 5644800\n",
      "tensor(2.7453, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.17631769, Culminative Send Cost: 5684000\n",
      "tensor(2.7452, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.17338133, Culminative Send Cost: 5723200\n",
      "tensor(2.7451, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.17046595, Culminative Send Cost: 5762400\n",
      "tensor(2.7451, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.16757107, Culminative Send Cost: 5801600\n",
      "tensor(2.7450, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.16469717, Culminative Send Cost: 5840800\n",
      "tensor(2.7449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.16184282, Culminative Send Cost: 5880000\n",
      "tensor(2.7449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.15900803, Culminative Send Cost: 5919200\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.15619326, Culminative Send Cost: 5958400\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.15339661, Culminative Send Cost: 5997600\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.15061903, Culminative Send Cost: 6036800\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.14786053, Culminative Send Cost: 6076000\n",
      "tensor(2.7447, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.14511967, Culminative Send Cost: 6115200\n",
      "tensor(2.7447, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.14239740, Culminative Send Cost: 6154400\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.13969231, Culminative Send Cost: 6193600\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.13700485, Culminative Send Cost: 6232800\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.13433552, Culminative Send Cost: 6272000\n",
      "tensor(2.7448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.13168240, Culminative Send Cost: 6311200\n",
      "tensor(2.7449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.12904644, Culminative Send Cost: 6350400\n",
      "tensor(2.7449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.12642670, Culminative Send Cost: 6389600\n",
      "tensor(2.7450, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.12382364, Culminative Send Cost: 6428800\n",
      "tensor(2.7450, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.12123632, Culminative Send Cost: 6468000\n",
      "tensor(2.7451, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.11866522, Culminative Send Cost: 6507200\n",
      "tensor(2.7452, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.11611032, Culminative Send Cost: 6546400\n",
      "tensor(2.7453, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.11357021, Culminative Send Cost: 6585600\n",
      "tensor(2.7453, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.11104584, Culminative Send Cost: 6624800\n",
      "tensor(2.7454, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.10853624, Culminative Send Cost: 6664000\n",
      "tensor(2.7456, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.10604143, Culminative Send Cost: 6703200\n",
      "tensor(2.7457, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.10356188, Culminative Send Cost: 6742400\n",
      "tensor(2.7458, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.10109568, Culminative Send Cost: 6781600\n",
      "tensor(2.7459, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.09864521, Culminative Send Cost: 6820800\n",
      "tensor(2.7461, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.09620857, Culminative Send Cost: 6860000\n",
      "tensor(2.7462, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.09378529, Culminative Send Cost: 6899200\n",
      "tensor(2.7464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.09137678, Culminative Send Cost: 6938400\n",
      "tensor(2.7465, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.08898115, Culminative Send Cost: 6977600\n",
      "tensor(2.7467, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.08660030, Culminative Send Cost: 7016800\n",
      "tensor(2.7469, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.08423090, Culminative Send Cost: 7056000\n",
      "tensor(2.7471, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.08187628, Culminative Send Cost: 7095200\n",
      "tensor(2.7472, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.07953405, Culminative Send Cost: 7134400\n",
      "tensor(2.7474, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.07720423, Culminative Send Cost: 7173600\n",
      "tensor(2.7477, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.07488775, Culminative Send Cost: 7212800\n",
      "tensor(2.7479, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.07258368, Culminative Send Cost: 7252000\n",
      "tensor(2.7481, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.07029247, Culminative Send Cost: 7291200\n",
      "tensor(2.7483, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.06801271, Culminative Send Cost: 7330400\n",
      "tensor(2.7485, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.06574631, Culminative Send Cost: 7369600\n",
      "tensor(2.7488, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.06349087, Culminative Send Cost: 7408800\n",
      "tensor(2.7490, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.06124735, Culminative Send Cost: 7448000\n",
      "tensor(2.7493, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.05901623, Culminative Send Cost: 7487200\n",
      "tensor(2.7495, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.05679607, Culminative Send Cost: 7526400\n",
      "tensor(2.7498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.05458832, Culminative Send Cost: 7565600\n",
      "tensor(2.7501, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.05239105, Culminative Send Cost: 7604800\n",
      "tensor(2.7504, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.05020571, Culminative Send Cost: 7644000\n",
      "tensor(2.7506, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.04803085, Culminative Send Cost: 7683200\n",
      "tensor(2.7509, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.04586792, Culminative Send Cost: 7722400\n",
      "tensor(2.7512, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.04371500, Culminative Send Cost: 7761600\n",
      "tensor(2.7515, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.04157400, Culminative Send Cost: 7800800\n",
      "tensor(2.7519, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.03944206, Culminative Send Cost: 7840000\n",
      "tensor(2.7522, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.03732252, Culminative Send Cost: 7879200\n",
      "tensor(2.7525, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.03521204, Culminative Send Cost: 7918400\n",
      "tensor(2.7528, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.03311253, Culminative Send Cost: 7957600\n",
      "tensor(2.7532, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.03102350, Culminative Send Cost: 7996800\n",
      "tensor(2.7535, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.02894449, Culminative Send Cost: 8036000\n",
      "tensor(2.7539, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.02687550, Culminative Send Cost: 8075200\n",
      "tensor(2.7542, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.02481604, Culminative Send Cost: 8114400\n",
      "tensor(2.7546, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.02276754, Culminative Send Cost: 8153600\n",
      "tensor(2.7549, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.02072763, Culminative Send Cost: 8192800\n",
      "tensor(2.7553, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.01869822, Culminative Send Cost: 8232000\n",
      "tensor(2.7557, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.01667786, Culminative Send Cost: 8271200\n",
      "tensor(2.7561, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.01466799, Culminative Send Cost: 8310400\n",
      "tensor(2.7565, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 4.01266670, Culminative Send Cost: 8349600\n",
      "tensor(2.7569, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 4.01067495, Culminative Send Cost: 8388800\n",
      "tensor(2.7573, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 4.00869274, Culminative Send Cost: 8428000\n",
      "tensor(2.7577, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 4.00671911, Culminative Send Cost: 8467200\n",
      "tensor(2.7581, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 4.00475454, Culminative Send Cost: 8506400\n",
      "tensor(2.7585, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 4.00279903, Culminative Send Cost: 8545600\n",
      "tensor(2.7589, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 4.00085306, Culminative Send Cost: 8584800\n",
      "tensor(2.7594, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.99891567, Culminative Send Cost: 8624000\n",
      "tensor(2.7598, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.99698687, Culminative Send Cost: 8663200\n",
      "tensor(2.7602, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.99506736, Culminative Send Cost: 8702400\n",
      "tensor(2.7607, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.99315548, Culminative Send Cost: 8741600\n",
      "tensor(2.7611, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.99125242, Culminative Send Cost: 8780800\n",
      "tensor(2.7616, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.98935843, Culminative Send Cost: 8820000\n",
      "tensor(2.7621, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.98747301, Culminative Send Cost: 8859200\n",
      "tensor(2.7625, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.98559475, Culminative Send Cost: 8898400\n",
      "tensor(2.7630, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.98372555, Culminative Send Cost: 8937600\n",
      "tensor(2.7635, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.98186421, Culminative Send Cost: 8976800\n",
      "tensor(2.7640, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.98001122, Culminative Send Cost: 9016000\n",
      "tensor(2.7644, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.97816586, Culminative Send Cost: 9055200\n",
      "tensor(2.7649, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.97632885, Culminative Send Cost: 9094400\n",
      "tensor(2.7654, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.97450018, Culminative Send Cost: 9133600\n",
      "tensor(2.7659, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.97267842, Culminative Send Cost: 9172800\n",
      "tensor(2.7664, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.97086501, Culminative Send Cost: 9212000\n",
      "tensor(2.7670, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.96905947, Culminative Send Cost: 9251200\n",
      "tensor(2.7675, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.96726179, Culminative Send Cost: 9290400\n",
      "tensor(2.7680, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.96547103, Culminative Send Cost: 9329600\n",
      "tensor(2.7685, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.96368814, Culminative Send Cost: 9368800\n",
      "tensor(2.7690, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.96191263, Culminative Send Cost: 9408000\n",
      "tensor(2.7696, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.96014428, Culminative Send Cost: 9447200\n",
      "tensor(2.7701, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.95838380, Culminative Send Cost: 9486400\n",
      "tensor(2.7707, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.95663071, Culminative Send Cost: 9525600\n",
      "tensor(2.7712, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.95488477, Culminative Send Cost: 9564800\n",
      "tensor(2.7718, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.95314550, Culminative Send Cost: 9604000\n",
      "tensor(2.7723, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.95141435, Culminative Send Cost: 9643200\n",
      "tensor(2.7729, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.94968963, Culminative Send Cost: 9682400\n",
      "tensor(2.7735, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.94797182, Culminative Send Cost: 9721600\n",
      "tensor(2.7740, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.94626117, Culminative Send Cost: 9760800\n",
      "tensor(2.7746, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.94455814, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[ 2.9057e-02,  1.9755e-02,  3.1227e-02,  2.5510e-02, -1.4220e-02,\n",
      "         -2.8423e-02, -1.6518e-03,  8.3696e-03,  9.1939e-04,  5.0871e-03,\n",
      "          1.5797e-02,  2.3204e-02,  3.4209e-03,  1.6121e-02,  3.3481e-02,\n",
      "         -1.2671e-02,  2.8855e-03,  1.1435e-02, -1.8131e-02,  8.4723e-03,\n",
      "          1.9175e-02,  2.2597e-02,  1.3840e-02,  1.7784e-02, -1.4008e-02,\n",
      "          2.0051e-02, -3.3521e-02,  1.7294e-02, -7.0699e-03,  2.2931e-02,\n",
      "          3.2938e-02,  2.1939e-02, -1.6450e-02,  1.7988e-02,  1.2684e-02,\n",
      "         -2.7235e-03, -1.1553e-02,  2.1679e-03, -1.6200e-02, -9.3569e-03,\n",
      "         -2.7025e-02, -2.8452e-02,  1.7132e-02, -1.3262e-03,  1.5538e-02,\n",
      "          2.5333e-02,  1.9801e-02,  1.4734e-02,  2.3784e-02,  1.3050e-02,\n",
      "         -1.9398e-02, -5.4557e-03, -5.9788e-03, -3.2959e-02, -1.7903e-03,\n",
      "          2.0072e-02,  2.4441e-02, -3.5434e-02,  5.8625e-03, -1.2761e-02,\n",
      "         -3.2402e-02, -3.4241e-02, -2.4132e-02, -2.2422e-02,  1.4715e-02,\n",
      "          3.6831e-02,  4.0629e-03,  7.8338e-03,  2.5376e-02,  7.8171e-02,\n",
      "          2.8724e-02,  8.5173e-02,  7.7644e-02,  4.5459e-02,  6.8658e-02,\n",
      "          4.0531e-02,  2.7052e-02, -1.1489e-02, -1.3269e-02,  6.5198e-04,\n",
      "          1.5398e-02, -2.5416e-02,  2.3322e-02, -1.9134e-02, -3.1152e-02,\n",
      "         -2.2403e-02, -2.6048e-02,  2.2680e-02, -1.5120e-02,  3.3042e-02,\n",
      "          1.6361e-02,  4.2486e-02, -2.1757e-03,  1.0724e-02,  3.2724e-02,\n",
      "          3.3948e-02,  5.3889e-02,  4.1077e-02,  9.6230e-02,  6.4840e-02,\n",
      "          1.1316e-01,  1.2718e-01,  1.5171e-01,  1.0241e-01,  1.2664e-01,\n",
      "          9.4340e-02,  6.6045e-02,  3.9828e-02,  4.7034e-03, -2.0451e-02,\n",
      "         -1.6688e-02,  2.7817e-03,  2.3890e-02, -3.3001e-02,  6.5736e-04,\n",
      "         -2.9297e-02,  1.9379e-02, -2.5712e-02,  3.7904e-02,  4.0983e-03,\n",
      "          3.4908e-02,  5.2217e-02,  2.9590e-02,  4.3694e-02,  3.6794e-02,\n",
      "          1.2862e-02,  3.4838e-02,  2.4676e-02,  3.9208e-02,  3.7586e-02,\n",
      "          7.7343e-02,  4.9778e-02,  5.2428e-02,  3.3101e-02,  2.3663e-02,\n",
      "          6.3649e-03,  4.8767e-02,  5.9365e-03, -1.0042e-02, -2.0623e-02,\n",
      "         -2.8180e-02,  3.0185e-02, -1.7749e-02, -1.9220e-02,  2.1112e-02,\n",
      "          7.9140e-03,  2.0971e-02, -3.4436e-02, -3.1561e-02, -1.2666e-02,\n",
      "         -4.6188e-03,  2.8585e-03, -4.6799e-02, -8.3815e-02, -8.3345e-02,\n",
      "         -8.5046e-02, -7.2108e-02,  3.0828e-03, -2.8225e-02,  2.9491e-04,\n",
      "          1.7232e-02,  2.7786e-02,  3.7514e-02,  5.3239e-03,  4.9619e-02,\n",
      "          4.3556e-02, -1.8069e-02, -6.8848e-03,  3.5326e-02,  1.0467e-02,\n",
      "         -2.4530e-02, -7.6160e-03, -2.2937e-02, -3.3889e-02, -6.1987e-02,\n",
      "         -2.1927e-02, -4.7270e-02,  1.0715e-02, -2.1085e-02, -1.9100e-02,\n",
      "         -1.9012e-02, -7.4037e-02, -4.6889e-02,  1.7587e-02,  2.0885e-02,\n",
      "         -2.6322e-02, -1.8351e-02, -7.7140e-02, -8.1616e-02, -4.3610e-02,\n",
      "          5.1290e-03,  2.4422e-02,  3.1220e-02,  6.4232e-02,  8.2921e-03,\n",
      "          2.0392e-02, -6.2409e-04, -2.6836e-02,  4.1450e-03, -2.4596e-02,\n",
      "         -1.3076e-02, -2.0667e-02, -2.6402e-02, -6.2652e-03,  1.6587e-02,\n",
      "          2.5491e-02,  1.2297e-02, -9.8353e-03,  3.1666e-02,  1.6841e-02,\n",
      "          7.3720e-02,  1.3269e-01,  1.1468e-01, -1.3552e-02, -4.0603e-02,\n",
      "         -9.3675e-02, -6.4217e-02, -1.0654e-02,  7.0180e-03,  4.3202e-02,\n",
      "          9.3377e-02,  2.5573e-02,  2.2178e-02, -1.8302e-02, -1.7779e-02,\n",
      "         -2.7430e-02,  4.2256e-02,  6.6911e-04,  1.6815e-02,  2.3050e-02,\n",
      "         -1.6266e-02,  5.8497e-02,  6.5075e-02,  7.0407e-02,  1.0537e-01,\n",
      "          9.7533e-02,  1.4263e-01,  1.2774e-01,  5.2092e-02,  1.2495e-02,\n",
      "          4.3782e-02, -5.2762e-02, -8.8794e-02, -4.8840e-02, -5.6039e-02,\n",
      "         -1.5586e-02,  5.9507e-02,  1.1199e-01,  1.0822e-01,  2.1286e-02,\n",
      "          3.0823e-02, -3.4043e-02,  6.8606e-04,  1.1641e-02, -1.8560e-02,\n",
      "          5.4246e-02,  3.1362e-02,  4.6271e-02,  3.1408e-02,  7.2122e-02,\n",
      "          1.1657e-01,  1.4883e-01,  1.9077e-01,  2.3079e-01,  1.6656e-01,\n",
      "          4.3337e-02, -4.1535e-02, -1.0661e-01, -4.8207e-02, -3.6778e-02,\n",
      "         -7.2478e-02, -3.7634e-02, -7.8006e-03, -2.0106e-02,  2.0102e-02,\n",
      "          6.2043e-02,  9.8665e-02,  1.8785e-02, -9.1574e-03,  3.6309e-05,\n",
      "         -5.2901e-03,  2.5070e-02,  3.3180e-02,  2.7758e-03,  6.0065e-02,\n",
      "          3.1953e-02,  1.3520e-01,  1.7556e-01,  1.4845e-01,  1.9019e-01,\n",
      "          1.6625e-01,  1.9301e-01,  1.5181e-01,  6.2091e-02, -1.2825e-01,\n",
      "         -1.2107e-01, -3.9657e-02,  2.1930e-02, -2.3136e-03,  1.8455e-02,\n",
      "          5.1187e-04, -2.0983e-02,  8.2196e-03,  2.9975e-02,  4.6218e-02,\n",
      "          3.0802e-02,  1.4419e-02, -1.8043e-02, -1.7857e-03, -1.6892e-03,\n",
      "          2.8475e-03,  3.9727e-02,  3.2798e-02,  1.2577e-01,  1.5989e-01,\n",
      "          1.5591e-01,  1.5597e-01,  1.3223e-01,  1.5433e-01,  2.0602e-01,\n",
      "          2.6170e-01,  1.2573e-01, -6.6949e-02, -1.1596e-01,  3.8406e-02,\n",
      "          1.3500e-01,  6.1317e-02,  4.1583e-02,  1.0774e-02,  2.1405e-02,\n",
      "         -2.3080e-03,  4.0869e-03,  3.7966e-02,  4.3685e-03,  3.3217e-02,\n",
      "         -1.4957e-02, -9.4205e-03, -3.0750e-02, -5.6971e-03, -1.0903e-02,\n",
      "          8.3106e-02,  8.8542e-02,  1.2212e-01,  1.4416e-01,  8.6510e-02,\n",
      "          8.4465e-02,  7.7301e-02,  2.2943e-01,  2.6916e-01,  1.1160e-01,\n",
      "         -6.1950e-03,  1.1353e-02,  1.6848e-01,  2.2893e-01,  1.7707e-01,\n",
      "          8.7304e-02,  2.3912e-02,  2.3604e-02, -1.5189e-02, -1.3549e-02,\n",
      "         -5.9845e-03,  1.3831e-02, -1.4983e-02,  3.4045e-02,  3.3151e-02,\n",
      "          1.8039e-02,  1.2697e-02, -1.4111e-02,  6.3112e-02,  5.6684e-02,\n",
      "          3.1050e-02,  5.9762e-02,  5.6181e-03, -1.6846e-02,  2.4503e-02,\n",
      "          1.7099e-01,  2.3622e-01,  9.2996e-02,  5.1543e-02,  1.5215e-01,\n",
      "          2.7250e-01,  2.8801e-01,  2.2107e-01,  1.2156e-01,  1.5017e-02,\n",
      "         -1.1990e-02, -5.0992e-02, -5.9054e-02, -2.0214e-02,  3.2052e-02,\n",
      "          7.7244e-04, -3.1466e-02, -1.4054e-02,  3.4846e-02, -1.1485e-02,\n",
      "          4.2369e-02,  5.7862e-02,  3.1085e-02, -3.1661e-02, -2.4349e-02,\n",
      "         -4.5080e-02, -6.1242e-02, -5.8350e-02,  9.6252e-02,  2.0345e-01,\n",
      "          1.8944e-02,  6.2650e-02,  1.1852e-01,  3.0696e-01,  2.8848e-01,\n",
      "          1.0624e-01,  4.8193e-02, -8.9990e-03, -6.3898e-02, -5.2159e-02,\n",
      "         -5.3074e-02, -3.7048e-04,  2.5350e-02, -2.8636e-02,  7.3255e-03,\n",
      "          1.9295e-02, -4.6076e-03, -1.0316e-02,  2.6997e-02,  1.5027e-02,\n",
      "         -2.3355e-02, -1.1890e-01, -9.6167e-02, -1.1819e-01, -1.1907e-01,\n",
      "          6.6293e-03,  1.1318e-01,  1.2604e-01, -1.2651e-02,  4.3936e-02,\n",
      "          1.1926e-01,  2.5628e-01,  2.2415e-01, -1.9224e-02, -9.3815e-02,\n",
      "         -7.1380e-02, -7.9517e-02, -5.6455e-03, -3.3426e-02, -2.1518e-02,\n",
      "         -6.3652e-04, -1.5547e-02, -2.6543e-02,  2.9243e-02, -2.3902e-02,\n",
      "         -1.6999e-02,  4.2182e-02, -3.9815e-02, -1.1551e-01, -8.6383e-02,\n",
      "         -1.4965e-01, -9.8632e-02, -2.5089e-02,  4.7425e-02,  1.8320e-01,\n",
      "          1.1292e-01, -1.4717e-02, -5.7186e-03,  1.3475e-01,  2.6634e-01,\n",
      "          7.9968e-02, -8.4571e-02, -8.0242e-02, -5.3049e-02, -5.4606e-02,\n",
      "         -4.3921e-02, -4.1749e-02, -4.9467e-02,  1.2559e-02,  1.7487e-02,\n",
      "         -7.7469e-03, -2.3206e-02,  1.8716e-02, -2.7675e-02,  1.0360e-02,\n",
      "          1.8813e-03, -9.1033e-02, -8.4020e-02, -9.3769e-02, -7.2217e-02,\n",
      "          1.5542e-02,  1.6513e-01,  1.7480e-01,  1.0455e-01,  1.8889e-03,\n",
      "          2.3811e-02,  1.2760e-01,  1.0769e-01, -2.4521e-02, -6.2169e-02,\n",
      "         -5.3585e-02, -6.2460e-02, -6.1842e-02, -7.2419e-02,  5.5183e-04,\n",
      "         -4.8273e-02, -4.3707e-02,  2.0735e-02,  1.2269e-02, -3.5275e-02,\n",
      "         -2.3323e-02,  2.5297e-02, -9.2867e-03, -2.8976e-02, -5.8056e-02,\n",
      "         -1.0777e-01, -9.4195e-02, -5.1396e-02,  4.7662e-02,  1.0316e-01,\n",
      "          1.6056e-01,  1.0994e-02, -4.1872e-02, -3.2469e-02,  3.4203e-02,\n",
      "          5.7245e-02, -1.2642e-02, -3.2373e-02, -4.3131e-02, -1.7579e-02,\n",
      "         -4.4368e-02, -4.5725e-02, -7.1536e-02, -9.7928e-03,  8.8565e-03,\n",
      "         -1.0033e-02,  1.0878e-03,  2.7356e-02,  2.9111e-02,  3.2339e-02,\n",
      "         -2.1377e-02, -2.9666e-02, -5.6763e-02, -5.9903e-02, -7.0114e-02,\n",
      "         -6.0735e-02, -8.8955e-03,  6.5632e-02,  5.5599e-02, -6.4076e-02,\n",
      "         -7.2496e-02, -2.0246e-02,  1.3930e-02,  1.4525e-02,  2.1161e-02,\n",
      "         -2.2477e-02,  2.3581e-02, -2.1263e-02, -9.2144e-02, -8.3948e-02,\n",
      "         -5.6266e-02, -5.2926e-02, -1.9877e-02,  1.5495e-02, -4.3559e-03,\n",
      "          2.8190e-03,  1.5860e-02, -3.7704e-03, -4.2322e-03, -6.3997e-02,\n",
      "         -6.4277e-02, -6.0927e-02, -3.6922e-02, -3.1267e-02, -9.7816e-02,\n",
      "         -4.4105e-02, -1.8045e-02,  1.6594e-02, -5.8386e-03, -2.0199e-03,\n",
      "         -1.2245e-02,  1.2567e-02, -2.1202e-02, -1.2625e-02, -2.2471e-02,\n",
      "         -4.3059e-02, -3.8974e-02, -6.6367e-02, -2.5849e-02, -5.8512e-02,\n",
      "          2.2883e-02,  9.7921e-03,  8.4116e-03, -1.7104e-02, -1.9351e-02,\n",
      "         -1.1237e-02, -4.3429e-02, -3.3604e-02, -8.1697e-02, -5.3348e-02,\n",
      "         -2.4868e-02, -5.2014e-02, -7.5107e-02, -5.4697e-02, -4.9726e-02,\n",
      "          2.7841e-02,  1.5204e-02,  1.3661e-02, -1.5596e-02, -1.0632e-02,\n",
      "         -3.8403e-03, -4.7002e-03, -1.6289e-02,  8.2108e-03, -7.3160e-02,\n",
      "         -4.9029e-02, -4.9034e-02, -3.5795e-02, -3.7843e-02,  2.5861e-02,\n",
      "         -9.5115e-03, -6.6493e-03,  5.7301e-03, -2.9802e-02,  7.1124e-04,\n",
      "         -6.2969e-02, -3.0206e-02, -5.3558e-02, -4.0724e-02, -8.6751e-02,\n",
      "         -9.0344e-02, -5.5253e-02, -1.6901e-02,  7.7756e-03,  1.1923e-02,\n",
      "         -1.6248e-02, -8.6831e-02, -2.1786e-02,  6.4073e-03, -9.9488e-03,\n",
      "         -9.3699e-03, -2.6976e-03, -1.8466e-02, -1.0903e-02,  1.1464e-02,\n",
      "          1.1854e-03, -2.3839e-04, -1.9867e-02,  2.3428e-02, -2.9244e-02,\n",
      "         -1.6335e-02, -8.1271e-03,  2.7002e-02, -1.1981e-02, -1.6463e-02,\n",
      "         -3.8180e-02, -1.4168e-02, -6.7431e-02, -2.1671e-02, -3.8600e-02,\n",
      "          2.3541e-02,  4.4572e-02,  1.8223e-02, -1.7435e-02, -1.1251e-03,\n",
      "         -3.3751e-03,  5.2331e-02,  8.5458e-02,  5.8274e-02,  4.6629e-02,\n",
      "          2.3608e-02,  4.5301e-02,  2.8115e-02,  1.7892e-02, -1.8585e-02,\n",
      "          1.9751e-02, -2.6580e-02, -1.5962e-02, -3.0208e-02, -3.2658e-02,\n",
      "          2.4490e-02,  2.8119e-02, -3.5129e-02,  3.1943e-03,  5.1653e-02,\n",
      "          1.0051e-01,  1.2756e-01,  1.4792e-01,  1.1429e-01,  1.1473e-01,\n",
      "          1.0249e-01,  9.0904e-02,  1.1491e-01,  1.2789e-01,  1.4206e-01,\n",
      "          1.3969e-01,  1.1120e-01,  7.0579e-02,  2.6110e-02,  4.5466e-02,\n",
      "          4.0464e-02,  2.7405e-03,  3.3538e-02,  2.7939e-02,  1.8350e-02,\n",
      "         -8.7925e-03,  2.0568e-02,  6.7180e-03,  3.4343e-02,  2.8978e-02,\n",
      "          9.2255e-03,  4.0794e-02,  5.8461e-02,  1.1418e-01,  1.2961e-01,\n",
      "          1.9019e-01,  1.9877e-01,  1.7478e-01,  1.6890e-01,  1.8511e-01,\n",
      "          1.7131e-01,  1.7807e-01,  1.8941e-01,  1.3147e-01,  1.3110e-01,\n",
      "          7.5796e-02,  4.8545e-02,  1.4984e-02,  3.2383e-03,  1.0719e-02,\n",
      "          2.2083e-02,  3.0687e-02,  1.3589e-02, -1.3592e-02,  2.9583e-02,\n",
      "         -1.0423e-02,  3.4593e-02, -1.0159e-02,  2.3240e-02,  4.1429e-02,\n",
      "          6.1361e-02,  8.7366e-02,  1.0049e-01,  5.0957e-02,  5.8169e-02,\n",
      "          8.1773e-02,  8.8064e-02,  9.2947e-02,  1.0786e-01,  9.8518e-02,\n",
      "          4.6672e-02,  5.5871e-02,  4.1088e-02,  5.8463e-02,  1.2877e-02,\n",
      "          3.4243e-02, -2.6163e-02, -2.4119e-02,  1.1493e-02,  1.3143e-02,\n",
      "          1.4372e-02, -3.5362e-02, -1.6213e-02, -3.0135e-03, -1.4180e-02,\n",
      "         -2.8496e-02,  1.1294e-02, -2.7060e-03, -3.2685e-02,  2.0068e-02,\n",
      "          1.1480e-02,  3.2970e-03,  2.0814e-02, -3.1193e-03, -6.1231e-03,\n",
      "          5.9485e-03,  2.5983e-02, -8.1753e-04,  4.6079e-02,  2.4688e-02,\n",
      "          1.9606e-02,  3.2800e-02, -8.2210e-03, -9.3433e-03,  7.5532e-04,\n",
      "          1.3320e-02,  3.4263e-02, -3.0287e-02,  2.4610e-02]])\n",
      "activation_stack.0.bias: tensor([0.9269])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for num_clients is 10 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 8, 4, 7], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([6, 5, 4,  ..., 6, 4, 7], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([1, 2, 8,  ..., 7, 8, 4], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([7, 6, 6,  ..., 8, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([6, 4, 7,  ..., 1, 2, 2], dtype=torch.int32)\n",
      "Client_X_train[5]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[5]: tensor([5, 3, 9,  ..., 1, 1, 1], dtype=torch.int32)\n",
      "Client_X_train[6]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[6]: tensor([2, 3, 6,  ..., 2, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[7]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[7]: tensor([3, 6, 1,  ..., 6, 8, 4], dtype=torch.int32)\n",
      "Client_X_train[8]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[8]: tensor([6, 6, 9,  ..., 5, 7, 2], dtype=torch.int32)\n",
      "Client_X_train[9]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[9]: tensor([3, 9, 8,  ..., 7, 8, 2], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[-0.0264, -0.0003, -0.0314, -0.0324,  0.0016,  0.0068, -0.0333,  0.0305,\n",
      "         -0.0324,  0.0134,  0.0110,  0.0233,  0.0158,  0.0286, -0.0035,  0.0173,\n",
      "          0.0321,  0.0085, -0.0209,  0.0066, -0.0168, -0.0314,  0.0226, -0.0266,\n",
      "         -0.0025,  0.0163,  0.0290, -0.0210,  0.0126, -0.0341, -0.0202,  0.0053,\n",
      "         -0.0307,  0.0050, -0.0263,  0.0330,  0.0173, -0.0289,  0.0087,  0.0075,\n",
      "          0.0316, -0.0342, -0.0121, -0.0213, -0.0346,  0.0075, -0.0335,  0.0287,\n",
      "          0.0034,  0.0336,  0.0268, -0.0096,  0.0119,  0.0146,  0.0183,  0.0254,\n",
      "         -0.0130,  0.0149,  0.0112, -0.0281, -0.0005, -0.0036,  0.0074,  0.0013,\n",
      "         -0.0054,  0.0065,  0.0182, -0.0196, -0.0088, -0.0008,  0.0038, -0.0230,\n",
      "         -0.0193,  0.0165, -0.0344, -0.0034, -0.0165,  0.0298,  0.0354,  0.0015,\n",
      "          0.0193,  0.0227,  0.0013, -0.0225, -0.0221, -0.0182, -0.0052, -0.0117,\n",
      "         -0.0116,  0.0132,  0.0133, -0.0219,  0.0041, -0.0226,  0.0137,  0.0266,\n",
      "         -0.0338, -0.0244, -0.0221,  0.0282,  0.0117, -0.0159, -0.0241, -0.0264,\n",
      "          0.0319,  0.0037, -0.0331, -0.0149, -0.0196,  0.0092,  0.0027,  0.0343,\n",
      "          0.0216, -0.0128, -0.0005,  0.0243,  0.0238,  0.0086,  0.0262,  0.0048,\n",
      "          0.0173, -0.0322,  0.0272, -0.0204,  0.0310, -0.0005, -0.0082,  0.0244,\n",
      "          0.0154, -0.0180, -0.0283,  0.0284,  0.0332,  0.0286,  0.0221,  0.0319,\n",
      "          0.0250, -0.0137,  0.0206, -0.0025, -0.0262,  0.0195,  0.0152,  0.0326,\n",
      "          0.0088, -0.0153, -0.0352,  0.0166,  0.0034,  0.0115,  0.0131,  0.0114,\n",
      "          0.0241,  0.0152,  0.0137,  0.0023,  0.0135, -0.0225, -0.0252, -0.0301,\n",
      "         -0.0225, -0.0095, -0.0192,  0.0285, -0.0190, -0.0015, -0.0167, -0.0008,\n",
      "         -0.0121,  0.0067, -0.0034, -0.0295, -0.0272, -0.0222, -0.0219, -0.0253,\n",
      "         -0.0226,  0.0111, -0.0145, -0.0228,  0.0311,  0.0282,  0.0271, -0.0110,\n",
      "         -0.0339, -0.0108, -0.0279, -0.0283,  0.0114, -0.0032, -0.0160, -0.0293,\n",
      "         -0.0078,  0.0141,  0.0347,  0.0066, -0.0165,  0.0136, -0.0188, -0.0158,\n",
      "         -0.0011,  0.0208, -0.0229,  0.0096,  0.0085,  0.0234,  0.0299,  0.0238,\n",
      "         -0.0071, -0.0327,  0.0329, -0.0188,  0.0218, -0.0075, -0.0279,  0.0120,\n",
      "          0.0350,  0.0178,  0.0313,  0.0117,  0.0154, -0.0044,  0.0065,  0.0192,\n",
      "          0.0337, -0.0227,  0.0180,  0.0218, -0.0338,  0.0028,  0.0170, -0.0320,\n",
      "         -0.0086, -0.0193, -0.0019,  0.0253, -0.0170, -0.0177, -0.0075,  0.0149,\n",
      "         -0.0231,  0.0132,  0.0312, -0.0194,  0.0182, -0.0205,  0.0133,  0.0273,\n",
      "         -0.0132,  0.0300,  0.0243, -0.0233, -0.0182, -0.0092,  0.0094,  0.0307,\n",
      "          0.0201, -0.0015,  0.0264, -0.0246,  0.0041,  0.0113,  0.0314, -0.0109,\n",
      "         -0.0330,  0.0039, -0.0246,  0.0233, -0.0067,  0.0232, -0.0054,  0.0062,\n",
      "          0.0256,  0.0284, -0.0287, -0.0110,  0.0303,  0.0102,  0.0137, -0.0151,\n",
      "         -0.0260, -0.0234,  0.0293, -0.0113,  0.0093, -0.0286, -0.0334,  0.0051,\n",
      "         -0.0125,  0.0352,  0.0066,  0.0166, -0.0276,  0.0114,  0.0195, -0.0288,\n",
      "         -0.0068, -0.0266,  0.0185, -0.0180,  0.0280,  0.0005,  0.0325, -0.0053,\n",
      "          0.0007,  0.0058,  0.0280, -0.0076, -0.0137, -0.0078,  0.0341, -0.0005,\n",
      "          0.0267, -0.0079, -0.0319,  0.0142,  0.0281,  0.0137, -0.0067, -0.0224,\n",
      "         -0.0326, -0.0190, -0.0236, -0.0015, -0.0195, -0.0230,  0.0257, -0.0306,\n",
      "          0.0051, -0.0297, -0.0078,  0.0120,  0.0060, -0.0072,  0.0041,  0.0214,\n",
      "         -0.0304,  0.0030,  0.0236, -0.0225, -0.0059, -0.0127,  0.0077, -0.0204,\n",
      "         -0.0340,  0.0340, -0.0237,  0.0159,  0.0272, -0.0046,  0.0171, -0.0344,\n",
      "          0.0029, -0.0290,  0.0338,  0.0043,  0.0270, -0.0106,  0.0202,  0.0139,\n",
      "         -0.0173,  0.0314, -0.0208, -0.0041, -0.0296,  0.0321,  0.0235,  0.0327,\n",
      "         -0.0020, -0.0201, -0.0322, -0.0337, -0.0243,  0.0186, -0.0328, -0.0110,\n",
      "         -0.0300, -0.0220, -0.0145,  0.0003,  0.0131,  0.0188,  0.0277,  0.0221,\n",
      "         -0.0279,  0.0150,  0.0037, -0.0023, -0.0135,  0.0121, -0.0132, -0.0341,\n",
      "         -0.0234, -0.0031,  0.0298,  0.0258, -0.0035, -0.0329, -0.0216,  0.0162,\n",
      "         -0.0109,  0.0280,  0.0292, -0.0075, -0.0246,  0.0036, -0.0184,  0.0152,\n",
      "         -0.0102,  0.0037, -0.0233,  0.0047, -0.0167,  0.0352, -0.0092,  0.0067,\n",
      "         -0.0266,  0.0215, -0.0228,  0.0183,  0.0159, -0.0031, -0.0302, -0.0153,\n",
      "          0.0160,  0.0004, -0.0349,  0.0076,  0.0039,  0.0118, -0.0109,  0.0355,\n",
      "         -0.0074,  0.0086, -0.0274, -0.0097,  0.0306, -0.0267,  0.0128, -0.0273,\n",
      "         -0.0093,  0.0307, -0.0149,  0.0292,  0.0068,  0.0091, -0.0133, -0.0053,\n",
      "         -0.0344,  0.0180, -0.0177,  0.0202,  0.0292, -0.0043, -0.0131, -0.0211,\n",
      "         -0.0055, -0.0322, -0.0149,  0.0114, -0.0081,  0.0226, -0.0002, -0.0354,\n",
      "         -0.0132, -0.0185,  0.0022, -0.0333, -0.0050, -0.0186,  0.0047,  0.0201,\n",
      "          0.0258,  0.0202, -0.0282,  0.0335, -0.0021, -0.0015,  0.0271, -0.0248,\n",
      "          0.0053,  0.0189,  0.0223,  0.0067, -0.0177,  0.0100, -0.0180, -0.0116,\n",
      "          0.0131,  0.0274, -0.0069,  0.0129,  0.0204, -0.0249, -0.0054,  0.0089,\n",
      "          0.0047,  0.0328, -0.0277, -0.0065,  0.0119,  0.0006,  0.0064, -0.0125,\n",
      "          0.0308, -0.0015, -0.0215, -0.0182, -0.0349, -0.0123,  0.0258,  0.0121,\n",
      "         -0.0236,  0.0162,  0.0002, -0.0134, -0.0096, -0.0316, -0.0010,  0.0334,\n",
      "         -0.0321,  0.0297,  0.0107, -0.0141,  0.0131, -0.0328, -0.0339, -0.0273,\n",
      "          0.0088, -0.0090, -0.0309,  0.0261,  0.0226,  0.0128,  0.0091,  0.0159,\n",
      "         -0.0155, -0.0037, -0.0056, -0.0255, -0.0135, -0.0272, -0.0131, -0.0247,\n",
      "          0.0250, -0.0225,  0.0149, -0.0181,  0.0261, -0.0185, -0.0112,  0.0331,\n",
      "          0.0266,  0.0090, -0.0039, -0.0033, -0.0317,  0.0129, -0.0251,  0.0294,\n",
      "          0.0288,  0.0014,  0.0239, -0.0134,  0.0098,  0.0287,  0.0183, -0.0119,\n",
      "          0.0202, -0.0056,  0.0040,  0.0168,  0.0306,  0.0275,  0.0304, -0.0255,\n",
      "          0.0208,  0.0334,  0.0034,  0.0274, -0.0074,  0.0138,  0.0084,  0.0076,\n",
      "          0.0151, -0.0268, -0.0354,  0.0341, -0.0216, -0.0153,  0.0332, -0.0198,\n",
      "          0.0310, -0.0179,  0.0029,  0.0260, -0.0192,  0.0296,  0.0129, -0.0228,\n",
      "         -0.0228, -0.0257, -0.0158, -0.0086,  0.0173, -0.0020, -0.0339,  0.0295,\n",
      "          0.0106, -0.0048,  0.0089, -0.0280,  0.0031, -0.0019, -0.0147,  0.0338,\n",
      "         -0.0152,  0.0294,  0.0223, -0.0214, -0.0060,  0.0065,  0.0122, -0.0296,\n",
      "          0.0199,  0.0172, -0.0198, -0.0277,  0.0210, -0.0122, -0.0148,  0.0325,\n",
      "         -0.0322, -0.0043,  0.0054,  0.0246,  0.0045,  0.0093, -0.0098, -0.0149,\n",
      "         -0.0286, -0.0176,  0.0258, -0.0318,  0.0334,  0.0313,  0.0169, -0.0266,\n",
      "          0.0157, -0.0356, -0.0288,  0.0228, -0.0283, -0.0067,  0.0216,  0.0031,\n",
      "         -0.0012, -0.0231, -0.0344, -0.0257, -0.0348,  0.0201, -0.0070,  0.0342,\n",
      "          0.0071, -0.0032,  0.0104,  0.0108, -0.0018, -0.0146, -0.0059,  0.0173,\n",
      "          0.0236, -0.0353, -0.0034,  0.0336, -0.0269,  0.0259,  0.0186, -0.0248,\n",
      "         -0.0118,  0.0141,  0.0172, -0.0040,  0.0253, -0.0264, -0.0288,  0.0265,\n",
      "         -0.0102, -0.0265, -0.0076, -0.0044,  0.0030, -0.0334,  0.0187, -0.0189,\n",
      "          0.0163,  0.0342, -0.0356, -0.0300,  0.0035, -0.0119, -0.0292,  0.0137,\n",
      "          0.0156, -0.0022,  0.0071, -0.0013, -0.0207, -0.0137, -0.0252, -0.0159,\n",
      "          0.0026,  0.0115, -0.0263,  0.0213, -0.0036,  0.0020,  0.0180, -0.0028,\n",
      "          0.0069, -0.0311, -0.0054,  0.0218,  0.0006, -0.0071,  0.0338,  0.0081,\n",
      "          0.0103,  0.0287, -0.0225,  0.0177, -0.0020, -0.0293,  0.0205,  0.0221,\n",
      "          0.0251,  0.0198,  0.0226,  0.0222,  0.0138, -0.0258,  0.0331, -0.0196,\n",
      "          0.0152, -0.0105,  0.0061, -0.0245,  0.0264,  0.0184, -0.0119,  0.0035,\n",
      "         -0.0128, -0.0280, -0.0322, -0.0194,  0.0266,  0.0143,  0.0066,  0.0182,\n",
      "         -0.0128,  0.0169, -0.0238,  0.0217, -0.0179,  0.0028, -0.0219,  0.0182,\n",
      "          0.0118, -0.0276, -0.0186,  0.0110, -0.0269,  0.0265,  0.0011, -0.0275,\n",
      "          0.0244,  0.0276, -0.0160, -0.0008,  0.0015, -0.0327, -0.0198, -0.0263]])), ('activation_stack.0.bias', tensor([0.0247]))])\n",
      "tensor(0.2721, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 25.95913887, Culminative Send Cost: 7840\n",
      "tensor(1.4004, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.65230560, Culminative Send Cost: 78400\n",
      "tensor(1.9947, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.07667351, Culminative Send Cost: 156800\n",
      "tensor(2.2714, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.04449081, Culminative Send Cost: 235200\n",
      "tensor(2.4070, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.55475903, Culminative Send Cost: 313600\n",
      "tensor(2.4797, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.20973873, Culminative Send Cost: 392000\n",
      "tensor(2.5240, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 6.92610598, Culminative Send Cost: 470400\n",
      "tensor(2.5551, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.68282652, Culminative Send Cost: 548800\n",
      "tensor(2.5800, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.47173786, Culminative Send Cost: 627200\n",
      "tensor(2.6015, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.28778315, Culminative Send Cost: 705600\n",
      "tensor(2.6210, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.12702370, Culminative Send Cost: 784000\n",
      "tensor(2.6391, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 5.98616362, Culminative Send Cost: 862400\n",
      "tensor(2.6562, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.86240578, Culminative Send Cost: 940800\n",
      "tensor(2.6723, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.75336075, Culminative Send Cost: 1019200\n",
      "tensor(2.6875, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.65698481, Culminative Send Cost: 1097600\n",
      "tensor(2.7019, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.57153130, Culminative Send Cost: 1176000\n",
      "tensor(2.7156, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.49550581, Culminative Send Cost: 1254400\n",
      "tensor(2.7285, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.42762661, Culminative Send Cost: 1332800\n",
      "tensor(2.7407, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.36679649, Culminative Send Cost: 1411200\n",
      "tensor(2.7521, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.31207657, Culminative Send Cost: 1489600\n",
      "tensor(2.7630, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.26265764, Culminative Send Cost: 1568000\n",
      "tensor(2.7732, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.21784878, Culminative Send Cost: 1646400\n",
      "tensor(2.7828, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.17705441, Culminative Send Cost: 1724800\n",
      "tensor(2.7918, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.13976336, Culminative Send Cost: 1803200\n",
      "tensor(2.8003, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.10553503, Culminative Send Cost: 1881600\n",
      "tensor(2.8083, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.07399130, Culminative Send Cost: 1960000\n",
      "tensor(2.8158, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.04480410, Culminative Send Cost: 2038400\n",
      "tensor(2.8228, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.01769304, Culminative Send Cost: 2116800\n",
      "tensor(2.8293, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.99241400, Culminative Send Cost: 2195200\n",
      "tensor(2.8354, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.96875668, Culminative Send Cost: 2273600\n",
      "tensor(2.8411, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.94653988, Culminative Send Cost: 2352000\n",
      "tensor(2.8464, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.92560530, Culminative Send Cost: 2430400\n",
      "tensor(2.8514, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.90581465, Culminative Send Cost: 2508800\n",
      "tensor(2.8560, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.88705111, Culminative Send Cost: 2587200\n",
      "tensor(2.8603, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.86920881, Culminative Send Cost: 2665600\n",
      "tensor(2.8642, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.85219860, Culminative Send Cost: 2744000\n",
      "tensor(2.8679, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.83594179, Culminative Send Cost: 2822400\n",
      "tensor(2.8713, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.82036829, Culminative Send Cost: 2900800\n",
      "tensor(2.8744, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.80541801, Culminative Send Cost: 2979200\n",
      "tensor(2.8773, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.79103804, Culminative Send Cost: 3057600\n",
      "tensor(2.8799, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.77718019, Culminative Send Cost: 3136000\n",
      "tensor(2.8823, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.76380444, Culminative Send Cost: 3214400\n",
      "tensor(2.8845, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.75087357, Culminative Send Cost: 3292800\n",
      "tensor(2.8865, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.73835516, Culminative Send Cost: 3371200\n",
      "tensor(2.8883, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.72622108, Culminative Send Cost: 3449600\n",
      "tensor(2.8899, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.71444368, Culminative Send Cost: 3528000\n",
      "tensor(2.8914, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.70300150, Culminative Send Cost: 3606400\n",
      "tensor(2.8927, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.69187260, Culminative Send Cost: 3684800\n",
      "tensor(2.8938, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.68103981, Culminative Send Cost: 3763200\n",
      "tensor(2.8948, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.67048550, Culminative Send Cost: 3841600\n",
      "tensor(2.8957, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.66019392, Culminative Send Cost: 3920000\n",
      "tensor(2.8965, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.65015173, Culminative Send Cost: 3998400\n",
      "tensor(2.8971, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.64034653, Culminative Send Cost: 4076800\n",
      "tensor(2.8976, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.63076639, Culminative Send Cost: 4155200\n",
      "tensor(2.8980, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.62140083, Culminative Send Cost: 4233600\n",
      "tensor(2.8983, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.61224079, Culminative Send Cost: 4312000\n",
      "tensor(2.8986, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.60327625, Culminative Send Cost: 4390400\n",
      "tensor(2.8987, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.59450006, Culminative Send Cost: 4468800\n",
      "tensor(2.8988, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.58590269, Culminative Send Cost: 4547200\n",
      "tensor(2.8988, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.57747936, Culminative Send Cost: 4625600\n",
      "tensor(2.8987, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.56922150, Culminative Send Cost: 4704000\n",
      "tensor(2.8985, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.56112385, Culminative Send Cost: 4782400\n",
      "tensor(2.8983, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.55318022, Culminative Send Cost: 4860800\n",
      "tensor(2.8980, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.54538488, Culminative Send Cost: 4939200\n",
      "tensor(2.8977, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.53773260, Culminative Send Cost: 5017600\n",
      "tensor(2.8973, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.53021955, Culminative Send Cost: 5096000\n",
      "tensor(2.8969, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.52284002, Culminative Send Cost: 5174400\n",
      "tensor(2.8964, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.51558971, Culminative Send Cost: 5252800\n",
      "tensor(2.8959, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.50846529, Culminative Send Cost: 5331200\n",
      "tensor(2.8954, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.50146246, Culminative Send Cost: 5409600\n",
      "tensor(2.8948, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.49457598, Culminative Send Cost: 5488000\n",
      "tensor(2.8942, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.48780489, Culminative Send Cost: 5566400\n",
      "tensor(2.8936, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.48114395, Culminative Send Cost: 5644800\n",
      "tensor(2.8929, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.47458982, Culminative Send Cost: 5723200\n",
      "tensor(2.8922, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.46814108, Culminative Send Cost: 5801600\n",
      "tensor(2.8915, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.46179295, Culminative Send Cost: 5880000\n",
      "tensor(2.8908, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.45554304, Culminative Send Cost: 5958400\n",
      "tensor(2.8900, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.44938898, Culminative Send Cost: 6036800\n",
      "tensor(2.8892, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.44332790, Culminative Send Cost: 6115200\n",
      "tensor(2.8884, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.43735743, Culminative Send Cost: 6193600\n",
      "tensor(2.8876, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.43147469, Culminative Send Cost: 6272000\n",
      "tensor(2.8868, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.42567730, Culminative Send Cost: 6350400\n",
      "tensor(2.8860, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.41996384, Culminative Send Cost: 6428800\n",
      "tensor(2.8851, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.41433144, Culminative Send Cost: 6507200\n",
      "tensor(2.8843, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.40877819, Culminative Send Cost: 6585600\n",
      "tensor(2.8834, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.40330219, Culminative Send Cost: 6664000\n",
      "tensor(2.8826, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.39790106, Culminative Send Cost: 6742400\n",
      "tensor(2.8817, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.39257240, Culminative Send Cost: 6820800\n",
      "tensor(2.8808, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.38731623, Culminative Send Cost: 6899200\n",
      "tensor(2.8800, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.38212919, Culminative Send Cost: 6977600\n",
      "tensor(2.8791, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.37701035, Culminative Send Cost: 7056000\n",
      "tensor(2.8782, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.37195778, Culminative Send Cost: 7134400\n",
      "tensor(2.8773, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.36697006, Culminative Send Cost: 7212800\n",
      "tensor(2.8764, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.36204481, Culminative Send Cost: 7291200\n",
      "tensor(2.8756, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.35718107, Culminative Send Cost: 7369600\n",
      "tensor(2.8747, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.35237837, Culminative Send Cost: 7448000\n",
      "tensor(2.8738, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.34763384, Culminative Send Cost: 7526400\n",
      "tensor(2.8729, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.34294748, Culminative Send Cost: 7604800\n",
      "tensor(2.8720, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.33831596, Culminative Send Cost: 7683200\n",
      "tensor(2.8712, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.33374023, Culminative Send Cost: 7761600\n",
      "tensor(2.8703, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.32921696, Culminative Send Cost: 7840000\n",
      "tensor(2.8694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.32474709, Culminative Send Cost: 7918400\n",
      "tensor(2.8686, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.32032824, Culminative Send Cost: 7996800\n",
      "tensor(2.8677, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.31595898, Culminative Send Cost: 8075200\n",
      "tensor(2.8669, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.31163931, Culminative Send Cost: 8153600\n",
      "tensor(2.8660, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.30736780, Culminative Send Cost: 8232000\n",
      "tensor(2.8652, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.30314255, Culminative Send Cost: 8310400\n",
      "tensor(2.8644, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.29896307, Culminative Send Cost: 8388800\n",
      "tensor(2.8635, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.29482841, Culminative Send Cost: 8467200\n",
      "tensor(2.8627, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.29073763, Culminative Send Cost: 8545600\n",
      "tensor(2.8619, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.28669071, Culminative Send Cost: 8624000\n",
      "tensor(2.8611, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.28268480, Culminative Send Cost: 8702400\n",
      "tensor(2.8603, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.27872086, Culminative Send Cost: 8780800\n",
      "tensor(2.8595, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.27479696, Culminative Send Cost: 8859200\n",
      "tensor(2.8588, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.27091265, Culminative Send Cost: 8937600\n",
      "tensor(2.8580, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.26706743, Culminative Send Cost: 9016000\n",
      "tensor(2.8572, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.26326036, Culminative Send Cost: 9094400\n",
      "tensor(2.8565, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.25949001, Culminative Send Cost: 9172800\n",
      "tensor(2.8558, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.25575686, Culminative Send Cost: 9251200\n",
      "tensor(2.8550, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.25205851, Culminative Send Cost: 9329600\n",
      "tensor(2.8543, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.24839592, Culminative Send Cost: 9408000\n",
      "tensor(2.8536, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.24476767, Culminative Send Cost: 9486400\n",
      "tensor(2.8529, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.24117279, Culminative Send Cost: 9564800\n",
      "tensor(2.8522, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.23761129, Culminative Send Cost: 9643200\n",
      "tensor(2.8515, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.23408222, Culminative Send Cost: 9721600\n",
      "tensor(2.8508, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.23058510, Culminative Send Cost: 9800000\n",
      "tensor(2.8502, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.22711945, Culminative Send Cost: 9878400\n",
      "tensor(2.8495, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.22368383, Culminative Send Cost: 9956800\n",
      "tensor(2.8489, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.22027922, Culminative Send Cost: 10035200\n",
      "tensor(2.8482, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.21690369, Culminative Send Cost: 10113600\n",
      "tensor(2.8476, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.21355724, Culminative Send Cost: 10192000\n",
      "tensor(2.8470, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.21023941, Culminative Send Cost: 10270400\n",
      "tensor(2.8464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.20694923, Culminative Send Cost: 10348800\n",
      "tensor(2.8458, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.20368671, Culminative Send Cost: 10427200\n",
      "tensor(2.8452, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.20045137, Culminative Send Cost: 10505600\n",
      "tensor(2.8446, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.19724321, Culminative Send Cost: 10584000\n",
      "tensor(2.8441, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.19405985, Culminative Send Cost: 10662400\n",
      "tensor(2.8435, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.19090319, Culminative Send Cost: 10740800\n",
      "tensor(2.8430, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.18777132, Culminative Send Cost: 10819200\n",
      "tensor(2.8424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.18466473, Culminative Send Cost: 10897600\n",
      "tensor(2.8419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.18158245, Culminative Send Cost: 10976000\n",
      "tensor(2.8414, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.17852354, Culminative Send Cost: 11054400\n",
      "tensor(2.8409, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.17548847, Culminative Send Cost: 11132800\n",
      "tensor(2.8404, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.17247677, Culminative Send Cost: 11211200\n",
      "tensor(2.8399, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.16948795, Culminative Send Cost: 11289600\n",
      "tensor(2.8395, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.16652155, Culminative Send Cost: 11368000\n",
      "tensor(2.8390, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.16357708, Culminative Send Cost: 11446400\n",
      "tensor(2.8385, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.16065454, Culminative Send Cost: 11524800\n",
      "tensor(2.8381, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.15775347, Culminative Send Cost: 11603200\n",
      "tensor(2.8377, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.15487289, Culminative Send Cost: 11681600\n",
      "tensor(2.8373, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.15201378, Culminative Send Cost: 11760000\n",
      "tensor(2.8369, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.14917421, Culminative Send Cost: 11838400\n",
      "tensor(2.8365, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.14635611, Culminative Send Cost: 11916800\n",
      "tensor(2.8361, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.14355707, Culminative Send Cost: 11995200\n",
      "tensor(2.8357, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.14077711, Culminative Send Cost: 12073600\n",
      "tensor(2.8353, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.13801670, Culminative Send Cost: 12152000\n",
      "tensor(2.8350, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.13527489, Culminative Send Cost: 12230400\n",
      "tensor(2.8346, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.13255215, Culminative Send Cost: 12308800\n",
      "tensor(2.8343, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.12984753, Culminative Send Cost: 12387200\n",
      "tensor(2.8340, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.12716150, Culminative Send Cost: 12465600\n",
      "tensor(2.8336, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.12449360, Culminative Send Cost: 12544000\n",
      "tensor(2.8333, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.12184238, Culminative Send Cost: 12622400\n",
      "tensor(2.8330, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.11920834, Culminative Send Cost: 12700800\n",
      "tensor(2.8327, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.11659193, Culminative Send Cost: 12779200\n",
      "tensor(2.8325, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.11399269, Culminative Send Cost: 12857600\n",
      "tensor(2.8322, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.11140919, Culminative Send Cost: 12936000\n",
      "tensor(2.8319, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.10884237, Culminative Send Cost: 13014400\n",
      "tensor(2.8317, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.10629272, Culminative Send Cost: 13092800\n",
      "tensor(2.8315, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.10375881, Culminative Send Cost: 13171200\n",
      "tensor(2.8312, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.10123920, Culminative Send Cost: 13249600\n",
      "tensor(2.8310, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.09873629, Culminative Send Cost: 13328000\n",
      "tensor(2.8308, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.09624815, Culminative Send Cost: 13406400\n",
      "tensor(2.8306, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.09377527, Culminative Send Cost: 13484800\n",
      "tensor(2.8304, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.09131765, Culminative Send Cost: 13563200\n",
      "tensor(2.8302, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.08887434, Culminative Send Cost: 13641600\n",
      "tensor(2.8301, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.08644581, Culminative Send Cost: 13720000\n",
      "tensor(2.8299, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.08403158, Culminative Send Cost: 13798400\n",
      "tensor(2.8297, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.08163214, Culminative Send Cost: 13876800\n",
      "tensor(2.8296, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.07924652, Culminative Send Cost: 13955200\n",
      "tensor(2.8295, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.07687378, Culminative Send Cost: 14033600\n",
      "tensor(2.8293, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.07451534, Culminative Send Cost: 14112000\n",
      "tensor(2.8292, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.07217073, Culminative Send Cost: 14190400\n",
      "tensor(2.8291, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.06983852, Culminative Send Cost: 14268800\n",
      "tensor(2.8290, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.06752014, Culminative Send Cost: 14347200\n",
      "tensor(2.8289, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.06521511, Culminative Send Cost: 14425600\n",
      "tensor(2.8288, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.06292248, Culminative Send Cost: 14504000\n",
      "tensor(2.8288, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.06064272, Culminative Send Cost: 14582400\n",
      "tensor(2.8287, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.05837488, Culminative Send Cost: 14660800\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.05612040, Culminative Send Cost: 14739200\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.05387735, Culminative Send Cost: 14817600\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.05164671, Culminative Send Cost: 14896000\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.04942894, Culminative Send Cost: 14974400\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.04722166, Culminative Send Cost: 15052800\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.04502726, Culminative Send Cost: 15131200\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.04284382, Culminative Send Cost: 15209600\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.04067183, Culminative Send Cost: 15288000\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.03851128, Culminative Send Cost: 15366400\n",
      "tensor(2.8285, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.03636217, Culminative Send Cost: 15444800\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.03422403, Culminative Send Cost: 15523200\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.03209686, Culminative Send Cost: 15601600\n",
      "tensor(2.8286, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.02998066, Culminative Send Cost: 15680000\n",
      "tensor(2.8287, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.02787495, Culminative Send Cost: 15758400\n",
      "tensor(2.8287, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.02577972, Culminative Send Cost: 15836800\n",
      "tensor(2.8288, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.02369499, Culminative Send Cost: 15915200\n",
      "tensor(2.8289, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.02162170, Culminative Send Cost: 15993600\n",
      "tensor(2.8290, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.01955748, Culminative Send Cost: 16072000\n",
      "tensor(2.8291, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.01750374, Culminative Send Cost: 16150400\n",
      "tensor(2.8292, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.01546049, Culminative Send Cost: 16228800\n",
      "tensor(2.8293, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.01342726, Culminative Send Cost: 16307200\n",
      "tensor(2.8294, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.01140404, Culminative Send Cost: 16385600\n",
      "tensor(2.8295, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.00939035, Culminative Send Cost: 16464000\n",
      "tensor(2.8296, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.00738621, Culminative Send Cost: 16542400\n",
      "tensor(2.8298, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.00539207, Culminative Send Cost: 16620800\n",
      "tensor(2.8299, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 4.00340748, Culminative Send Cost: 16699200\n",
      "tensor(2.8301, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 4.00143194, Culminative Send Cost: 16777600\n",
      "tensor(2.8302, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.99946618, Culminative Send Cost: 16856000\n",
      "tensor(2.8304, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.99750900, Culminative Send Cost: 16934400\n",
      "tensor(2.8305, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.99556184, Culminative Send Cost: 17012800\n",
      "tensor(2.8307, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.99362350, Culminative Send Cost: 17091200\n",
      "tensor(2.8309, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.99169350, Culminative Send Cost: 17169600\n",
      "tensor(2.8311, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.98977351, Culminative Send Cost: 17248000\n",
      "tensor(2.8313, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.98786163, Culminative Send Cost: 17326400\n",
      "tensor(2.8315, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.98595858, Culminative Send Cost: 17404800\n",
      "tensor(2.8317, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.98406410, Culminative Send Cost: 17483200\n",
      "tensor(2.8319, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.98217845, Culminative Send Cost: 17561600\n",
      "tensor(2.8322, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.98030138, Culminative Send Cost: 17640000\n",
      "tensor(2.8324, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.97843242, Culminative Send Cost: 17718400\n",
      "tensor(2.8326, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.97657251, Culminative Send Cost: 17796800\n",
      "tensor(2.8329, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.97472024, Culminative Send Cost: 17875200\n",
      "tensor(2.8331, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.97287631, Culminative Send Cost: 17953600\n",
      "tensor(2.8334, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.97104073, Culminative Send Cost: 18032000\n",
      "tensor(2.8336, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.96921325, Culminative Send Cost: 18110400\n",
      "tensor(2.8339, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.96739388, Culminative Send Cost: 18188800\n",
      "tensor(2.8342, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.96558237, Culminative Send Cost: 18267200\n",
      "tensor(2.8345, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.96377850, Culminative Send Cost: 18345600\n",
      "tensor(2.8348, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.96198297, Culminative Send Cost: 18424000\n",
      "tensor(2.8351, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.96019459, Culminative Send Cost: 18502400\n",
      "tensor(2.8354, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.95841455, Culminative Send Cost: 18580800\n",
      "tensor(2.8357, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.95664167, Culminative Send Cost: 18659200\n",
      "tensor(2.8360, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.95487642, Culminative Send Cost: 18737600\n",
      "tensor(2.8363, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.95311880, Culminative Send Cost: 18816000\n",
      "tensor(2.8366, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.95136881, Culminative Send Cost: 18894400\n",
      "tensor(2.8369, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.94962597, Culminative Send Cost: 18972800\n",
      "tensor(2.8373, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.94789076, Culminative Send Cost: 19051200\n",
      "tensor(2.8376, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.94616199, Culminative Send Cost: 19129600\n",
      "tensor(2.8380, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.94444132, Culminative Send Cost: 19208000\n",
      "tensor(2.8383, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.94272733, Culminative Send Cost: 19286400\n",
      "tensor(2.8387, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.94102097, Culminative Send Cost: 19364800\n",
      "tensor(2.8390, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.93932128, Culminative Send Cost: 19443200\n",
      "tensor(2.8394, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.93762898, Culminative Send Cost: 19521600\n",
      "tensor(2.8398, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.93594337, Culminative Send Cost: 19600000\n",
      "activation_stack.0.weight: tensor([[-2.6384e-02, -2.9160e-04, -3.1369e-02, -3.2404e-02,  1.5587e-03,\n",
      "          6.8069e-03, -3.3342e-02,  3.0474e-02, -3.2352e-02,  1.3386e-02,\n",
      "          1.1004e-02,  2.3321e-02,  1.5999e-02,  2.8763e-02, -3.5429e-03,\n",
      "          1.7281e-02,  3.2099e-02,  8.5405e-03, -2.0918e-02,  6.6375e-03,\n",
      "         -1.6839e-02, -3.1404e-02,  2.2614e-02, -2.6640e-02, -2.5129e-03,\n",
      "          1.6262e-02,  2.8975e-02, -2.1023e-02,  1.2618e-02, -3.4126e-02,\n",
      "         -2.0156e-02,  5.2508e-03, -3.0470e-02,  5.2343e-03, -2.5706e-02,\n",
      "          3.3853e-02,  1.8764e-02, -2.6985e-02,  1.1459e-02,  1.0852e-02,\n",
      "          3.4801e-02, -3.0482e-02, -7.3356e-03, -1.7347e-02, -3.1479e-02,\n",
      "          1.0377e-02, -3.1652e-02,  3.0903e-02,  5.6361e-03,  3.4327e-02,\n",
      "          2.7055e-02, -9.4973e-03,  1.1855e-02,  1.4631e-02,  1.8346e-02,\n",
      "          2.5444e-02, -1.3014e-02,  1.4882e-02,  1.1255e-02, -2.8429e-02,\n",
      "         -6.9355e-06, -3.3089e-03,  8.7171e-03,  5.0187e-03,  2.0075e-03,\n",
      "          2.0381e-02,  3.9812e-02,  6.7235e-03,  2.6367e-02,  4.1334e-02,\n",
      "          5.7230e-02,  3.7287e-02,  4.5557e-02,  8.0937e-02,  3.1372e-02,\n",
      "          5.4502e-02,  2.3719e-02,  5.1764e-02,  4.6226e-02,  5.6743e-03,\n",
      "          2.0515e-02,  2.2665e-02,  1.2679e-03, -2.2469e-02, -2.2117e-02,\n",
      "         -1.8215e-02, -5.1976e-03, -1.2005e-02, -1.1081e-02,  1.3705e-02,\n",
      "          1.6502e-02, -1.5053e-02,  1.6502e-02,  1.1659e-03,  4.8373e-02,\n",
      "          6.6701e-02,  1.2296e-02,  3.1019e-02,  5.4120e-02,  1.1844e-01,\n",
      "          1.1853e-01,  1.1855e-01,  1.2567e-01,  1.0720e-01,  1.2776e-01,\n",
      "          6.6422e-02,  2.5885e-03,  1.8236e-03, -1.2235e-02,  1.0405e-02,\n",
      "          3.1134e-03,  3.4293e-02,  2.1625e-02, -1.2766e-02, -2.0994e-04,\n",
      "          2.4336e-02,  2.4109e-02,  8.0507e-03,  2.8670e-02,  1.2080e-02,\n",
      "          3.1946e-02, -1.2273e-03,  6.8276e-02,  1.8913e-02,  5.2483e-02,\n",
      "          1.0907e-02,  2.7695e-03,  3.2819e-02,  5.1855e-02,  5.3880e-02,\n",
      "          5.6092e-02,  1.0033e-01,  9.3138e-02,  8.4292e-02,  6.6301e-02,\n",
      "          6.4033e-02,  4.3930e-02, -7.6822e-03,  2.2020e-02, -1.6856e-03,\n",
      "         -2.6152e-02,  1.9465e-02,  1.5441e-02,  3.2040e-02,  6.5632e-03,\n",
      "         -2.2617e-02, -4.6911e-02,  3.1486e-03, -2.8814e-03,  1.0736e-02,\n",
      "          7.9631e-03, -3.0940e-03, -1.1741e-02, -7.4843e-02, -1.0138e-01,\n",
      "         -9.6870e-02, -3.5901e-02, -2.7848e-02, -1.8385e-02, -3.6880e-02,\n",
      "         -2.7215e-02,  4.4971e-03,  6.1363e-03,  5.6690e-02,  9.9148e-03,\n",
      "          1.4140e-02, -1.3705e-02,  7.7357e-05, -1.2144e-02,  6.6665e-03,\n",
      "         -2.7269e-03, -3.0325e-02, -3.3317e-02, -3.5720e-02, -4.8167e-02,\n",
      "         -5.8083e-02, -4.9370e-02, -1.1862e-02, -4.9340e-02, -6.3532e-02,\n",
      "         -3.4292e-02, -4.2735e-02, -1.7719e-02,  3.5474e-03,  1.6801e-03,\n",
      "          8.2242e-03, -4.6536e-02, -7.8128e-02, -4.4300e-02, -2.9279e-02,\n",
      "         -9.5677e-03,  7.6994e-03,  3.7050e-02,  4.3011e-02,  4.0522e-02,\n",
      "          7.4777e-03, -1.6494e-02,  1.3647e-02, -1.6022e-02, -1.3455e-02,\n",
      "         -5.3460e-03,  3.6038e-03, -5.1987e-02, -3.3785e-03,  1.7462e-02,\n",
      "          2.9170e-02,  3.7985e-02,  3.1876e-02,  8.3138e-03,  9.4937e-03,\n",
      "          1.0932e-01,  1.0513e-01,  1.2127e-01,  3.5792e-03, -7.6035e-02,\n",
      "         -6.0615e-02, -2.1383e-02, -1.3698e-02,  3.1714e-02,  7.2822e-02,\n",
      "          9.3354e-02,  3.8590e-02,  1.5528e-02,  2.0131e-02,  3.3701e-02,\n",
      "         -2.1856e-02,  2.5280e-02,  3.2878e-02, -2.5659e-02, -7.3933e-05,\n",
      "          2.1713e-02,  1.7071e-02,  7.0946e-02,  5.2258e-02,  8.3065e-02,\n",
      "          1.3787e-01,  1.2143e-01,  9.2717e-02,  5.6919e-02,  4.9646e-02,\n",
      "         -9.1895e-03, -3.1645e-02, -4.0594e-02, -9.8785e-02, -4.0924e-02,\n",
      "         -3.5806e-02,  3.9432e-02,  1.1009e-01,  8.6166e-02,  8.4177e-02,\n",
      "          3.4172e-02, -2.2207e-02, -1.8259e-02, -7.3715e-03,  1.9165e-02,\n",
      "          4.9309e-02,  4.1806e-02,  2.9577e-02,  8.8635e-02,  8.5357e-02,\n",
      "          1.4748e-01,  1.4446e-01,  1.8651e-01,  1.8979e-01,  1.5033e-01,\n",
      "          7.3631e-02, -6.2336e-02, -7.5608e-02, -6.3426e-02, -1.4107e-02,\n",
      "         -5.9247e-02, -4.8307e-02, -1.4314e-02,  2.8781e-02,  4.2764e-03,\n",
      "          6.0516e-02,  1.1213e-01,  5.3306e-02,  2.2278e-02, -1.3893e-02,\n",
      "         -2.5690e-02, -2.0948e-02,  3.7949e-02,  5.7439e-03,  4.5685e-02,\n",
      "          4.1776e-02,  8.8243e-02,  1.5970e-01,  1.4535e-01,  1.7536e-01,\n",
      "          1.7709e-01,  2.3361e-01,  1.6106e-01,  6.3854e-02, -8.8342e-02,\n",
      "         -1.7213e-01, -3.9843e-02, -6.8021e-03,  2.0720e-02, -2.6024e-02,\n",
      "          5.9807e-03,  1.2584e-03,  4.6656e-02,  2.1501e-02,  4.3837e-02,\n",
      "          3.4544e-02,  3.4909e-02, -6.9183e-03, -1.3361e-02, -6.1567e-03,\n",
      "          4.1016e-02,  1.9059e-02,  7.6612e-02,  9.5406e-02,  1.1719e-01,\n",
      "          1.7941e-01,  1.7667e-01,  1.4577e-01,  1.4319e-01,  1.9170e-01,\n",
      "          2.1657e-01,  1.0438e-01, -8.9865e-02, -8.5669e-02,  3.8051e-02,\n",
      "          9.8409e-02,  1.1017e-01,  7.2428e-03,  2.5501e-02, -2.4638e-02,\n",
      "         -1.5879e-02,  4.2842e-05,  1.9222e-02,  7.7519e-03,  7.3030e-03,\n",
      "          2.1835e-02, -3.0303e-02,  4.4221e-03,  2.9886e-02, -1.1046e-03,\n",
      "          5.5920e-02,  9.7536e-02,  1.3939e-01,  1.1187e-01,  6.0246e-02,\n",
      "          1.0668e-01,  7.2830e-02,  2.2658e-01,  3.1258e-01,  1.3865e-01,\n",
      "          3.1612e-03,  1.4222e-02,  1.9460e-01,  2.1054e-01,  1.9538e-01,\n",
      "          9.9281e-02,  6.8816e-02, -3.8284e-03, -5.5677e-03, -1.7807e-02,\n",
      "         -2.4454e-02,  3.5135e-02, -1.9944e-02, -3.8829e-03, -2.9640e-02,\n",
      "          3.2808e-02,  2.7466e-02,  5.2520e-02,  5.8468e-02,  5.8975e-02,\n",
      "          4.0508e-02,  2.2865e-02,  1.9207e-03,  6.2318e-03, -8.2118e-03,\n",
      "          1.5820e-01,  2.3914e-01,  9.0502e-02,  2.6857e-02,  1.4836e-01,\n",
      "          2.7487e-01,  3.0619e-01,  2.1794e-01,  1.2865e-01,  4.0892e-03,\n",
      "         -9.4699e-04, -4.0556e-02, -4.7155e-02, -2.3712e-02,  1.4955e-02,\n",
      "         -1.3656e-02, -3.4260e-02, -2.3341e-02, -2.7693e-03,  3.1791e-02,\n",
      "          4.1374e-02,  3.4960e-02, -1.2474e-02, -3.3281e-02, -1.9159e-02,\n",
      "         -7.4822e-02, -6.7750e-02, -1.8413e-02,  1.1356e-01,  1.6029e-01,\n",
      "          5.0254e-02,  4.0181e-02,  1.4534e-01,  2.7986e-01,  2.8220e-01,\n",
      "          9.6214e-02,  4.1747e-02, -3.7144e-02, -1.8474e-02, -5.8909e-02,\n",
      "         -3.6018e-02, -3.9368e-02,  2.4892e-02, -2.4192e-02,  1.7933e-02,\n",
      "          1.6018e-02, -3.0169e-03, -2.9068e-02, -2.7548e-03,  2.6761e-02,\n",
      "         -4.1261e-02, -1.0704e-01, -9.0647e-02, -1.1081e-01, -8.6841e-02,\n",
      "         -3.5818e-02,  1.3558e-01,  1.0389e-01,  1.6482e-02,  4.6766e-03,\n",
      "          1.1746e-01,  3.2131e-01,  1.7758e-01,  2.2266e-02, -8.1783e-02,\n",
      "         -7.6080e-02, -2.6021e-02, -5.6969e-02, -7.9699e-03, -6.6955e-03,\n",
      "          1.0516e-02, -1.6561e-02, -5.6598e-03, -3.4187e-02,  1.7838e-02,\n",
      "         -1.7222e-02,  3.0412e-02,  1.9155e-02, -8.0838e-02, -1.1401e-01,\n",
      "         -1.3262e-01, -1.0388e-01, -7.0609e-02,  5.1094e-02,  1.7586e-01,\n",
      "          1.1838e-01,  1.5191e-02,  7.0000e-03,  1.0881e-01,  2.2849e-01,\n",
      "          6.8473e-02, -6.8309e-02, -1.2513e-01, -7.8464e-02, -8.2575e-02,\n",
      "         -3.5337e-02, -1.0635e-02,  8.1916e-03,  1.1902e-02, -3.2611e-02,\n",
      "          3.3232e-02, -2.0595e-03, -1.7021e-03,  2.7849e-02, -1.8300e-02,\n",
      "         -1.9389e-02, -7.0430e-02, -9.1935e-02, -1.0800e-01, -7.5697e-02,\n",
      "          5.2866e-02,  1.2862e-01,  1.9336e-01,  1.1295e-01,  6.2867e-04,\n",
      "          4.7603e-03,  1.3259e-01,  1.4829e-01, -2.3756e-02, -6.3596e-02,\n",
      "         -4.9430e-02, -3.7281e-02, -1.9505e-02, -7.0207e-02, -3.6127e-02,\n",
      "         -1.3262e-02, -1.6159e-02,  1.3520e-03, -1.3234e-02,  3.0997e-02,\n",
      "         -1.4689e-03, -2.0680e-02, -1.5599e-02, -6.7312e-02, -9.9984e-02,\n",
      "         -6.9966e-02, -8.3784e-02, -7.3257e-02,  6.1398e-02,  1.4014e-01,\n",
      "          1.3162e-01,  1.6606e-02, -9.4242e-02, -1.5376e-02,  7.5729e-02,\n",
      "          3.3851e-03,  1.9555e-02, -7.3476e-03, -3.9757e-02, -1.4576e-02,\n",
      "         -8.3168e-02, -8.0250e-02, -6.8055e-02, -3.1220e-02, -2.6207e-02,\n",
      "         -3.2381e-02,  2.5785e-02,  2.2617e-02,  1.2852e-02,  9.1330e-03,\n",
      "          1.0947e-02, -5.1140e-02, -7.9170e-02, -7.4802e-02, -8.5640e-02,\n",
      "         -5.3447e-02, -3.1232e-02,  4.0386e-02,  1.1627e-02, -5.8661e-03,\n",
      "         -1.0177e-01, -1.7781e-02, -1.9401e-02,  3.1550e-02, -2.1289e-02,\n",
      "         -3.2301e-02,  5.4795e-03, -1.4890e-02, -5.4169e-02, -6.0030e-02,\n",
      "         -5.6882e-02, -7.2559e-02, -3.1189e-03, -2.7340e-02,  2.9206e-02,\n",
      "          2.8823e-02,  1.1727e-03,  2.2176e-02, -2.7437e-02, -3.7289e-02,\n",
      "         -3.7419e-02, -3.9645e-02, -5.6081e-02, -3.3354e-02, -7.3011e-02,\n",
      "         -3.9378e-02, -1.4272e-02,  8.9271e-03,  1.7538e-02,  2.8317e-02,\n",
      "         -2.2469e-02,  1.3801e-02,  2.4467e-02, -7.4068e-03,  2.0628e-03,\n",
      "         -5.4354e-02, -4.9769e-02, -4.9253e-02, -4.3760e-02, -1.9604e-02,\n",
      "         -3.8667e-02, -3.7646e-02,  3.4168e-02, -2.1748e-02, -1.5304e-02,\n",
      "          3.0564e-02, -3.8699e-02, -1.7729e-02, -8.2700e-02, -4.9377e-02,\n",
      "         -2.3607e-02, -9.5345e-02, -6.6700e-02, -5.8801e-02, -4.3916e-02,\n",
      "          5.0459e-03,  1.3322e-02, -2.9162e-03, -3.1296e-02,  7.6872e-03,\n",
      "         -1.5008e-03, -3.4336e-02,  7.3155e-03, -2.2085e-02, -4.9147e-02,\n",
      "         -3.2851e-02, -6.4906e-02, -2.0377e-02, -1.0790e-02, -1.6521e-02,\n",
      "          3.3554e-02, -1.5295e-02,  2.9454e-02,  2.0015e-02, -3.5262e-02,\n",
      "         -4.1230e-02, -4.9040e-02, -4.9185e-02, -9.7855e-02, -6.4473e-02,\n",
      "         -8.0317e-02, -7.9830e-02, -3.6992e-02,  3.4658e-02, -1.4805e-02,\n",
      "         -4.7257e-02, -2.8128e-02, -6.1588e-02, -5.2194e-03,  1.5284e-02,\n",
      "          2.4396e-02,  1.3673e-03, -1.3953e-03, -2.6884e-02, -3.0030e-02,\n",
      "         -3.9374e-02, -2.1939e-02,  2.4875e-02, -3.2059e-02,  3.3429e-02,\n",
      "          3.1291e-02,  1.5997e-02, -3.1132e-02,  4.0243e-05, -6.2670e-02,\n",
      "         -5.8732e-02, -5.4115e-03, -5.2989e-02, -4.0844e-02,  8.4266e-03,\n",
      "          2.7440e-02,  3.8284e-02,  2.5509e-02, -1.7157e-02, -3.9851e-02,\n",
      "         -1.7447e-02,  7.2975e-02,  5.5760e-02,  9.0298e-02,  5.6737e-02,\n",
      "          2.5733e-02,  2.2746e-02,  1.5315e-02, -2.8900e-03, -1.5979e-02,\n",
      "         -6.6432e-03,  1.7350e-02,  2.3608e-02, -3.5257e-02, -3.3586e-03,\n",
      "          3.2266e-02, -2.8032e-02,  2.5630e-02,  3.0589e-02,  2.7328e-02,\n",
      "          7.3663e-02,  1.2124e-01,  1.4185e-01,  1.1128e-01,  1.3563e-01,\n",
      "          8.9320e-02,  7.6516e-02,  1.3173e-01,  1.2262e-01,  1.1328e-01,\n",
      "          1.1882e-01,  1.0287e-01,  9.1254e-02,  2.1589e-02,  4.5899e-02,\n",
      "         -5.1776e-03,  1.7582e-02,  3.3121e-02, -3.6026e-02, -2.9981e-02,\n",
      "          3.4963e-03, -1.1868e-02, -2.8969e-02,  1.4177e-02,  1.9656e-02,\n",
      "          1.1504e-02,  4.0841e-02,  6.9404e-02,  9.0281e-02,  1.3259e-01,\n",
      "          1.4347e-01,  1.5739e-01,  1.8328e-01,  2.0002e-01,  1.6241e-01,\n",
      "          2.1272e-01,  1.8943e-01,  1.7634e-01,  1.5461e-01,  9.8415e-02,\n",
      "          7.7677e-02,  1.3197e-02,  1.4996e-02,  2.9956e-02,  1.9678e-03,\n",
      "         -7.1804e-03,  3.3370e-02,  8.0880e-03,  1.0253e-02,  2.8739e-02,\n",
      "         -2.2515e-02,  1.8175e-02, -3.8765e-04, -2.2853e-02,  3.7186e-02,\n",
      "          5.3359e-02,  7.7873e-02,  8.7493e-02,  9.7849e-02,  1.0475e-01,\n",
      "          1.0748e-01,  8.1135e-02,  1.4162e-01,  7.8457e-02,  1.0648e-01,\n",
      "          6.4122e-02,  6.2789e-02,  1.3106e-02,  5.1730e-02,  3.3768e-02,\n",
      "         -6.4513e-03,  5.4856e-03, -1.2963e-02, -2.7820e-02, -3.2227e-02,\n",
      "         -1.9366e-02,  2.6646e-02,  1.4338e-02,  6.6218e-03,  1.8162e-02,\n",
      "         -1.2656e-02,  1.7336e-02, -2.2279e-02,  2.4602e-02, -1.5161e-02,\n",
      "          6.5214e-03, -1.5330e-02,  2.5957e-02,  2.3968e-02, -1.3726e-02,\n",
      "          4.0123e-04,  2.7485e-02, -1.2859e-02,  3.7474e-02,  8.1971e-03,\n",
      "         -2.4474e-02,  2.7126e-02,  2.9361e-02, -1.4071e-02,  3.9184e-04,\n",
      "          1.5075e-03, -3.2662e-02, -1.9838e-02, -2.6251e-02]])\n",
      "activation_stack.0.bias: tensor([0.9596])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 19600000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for num_clients is 15 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 1, 0, 2], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 3, 9,  ..., 6, 7, 3], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([5, 0, 5,  ..., 4, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([7, 1, 2,  ..., 7, 4, 0], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([2, 1, 0,  ..., 0, 2, 6], dtype=torch.int32)\n",
      "Client_X_train[5]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[5]: tensor([5, 7, 7,  ..., 4, 7, 8], dtype=torch.int32)\n",
      "Client_X_train[6]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[6]: tensor([3, 6, 6,  ..., 2, 7, 5], dtype=torch.int32)\n",
      "Client_X_train[7]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[7]: tensor([4, 2, 5,  ..., 3, 2, 8], dtype=torch.int32)\n",
      "Client_X_train[8]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[8]: tensor([6, 7, 1,  ..., 9, 7, 7], dtype=torch.int32)\n",
      "Client_X_train[9]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[9]: tensor([1, 1, 1,  ..., 3, 3, 1], dtype=torch.int32)\n",
      "Client_X_train[10]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[10]: tensor([7, 7, 1,  ..., 5, 9, 9], dtype=torch.int32)\n",
      "Client_X_train[11]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[11]: tensor([3, 9, 0,  ..., 8, 8, 1], dtype=torch.int32)\n",
      "Client_X_train[12]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[12]: tensor([7, 6, 8,  ..., 1, 6, 5], dtype=torch.int32)\n",
      "Client_X_train[13]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[13]: tensor([0, 9, 3,  ..., 2, 1, 1], dtype=torch.int32)\n",
      "Client_X_train[14]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[14]: tensor([7, 0, 8,  ..., 5, 2, 1], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[ 7.3726e-03, -1.6370e-02,  1.6249e-02, -4.7326e-03,  3.5360e-02,\n",
      "         -3.0594e-02, -3.8635e-05,  2.6320e-02, -2.9766e-02, -3.2403e-02,\n",
      "          3.4962e-02,  8.8612e-03, -2.9183e-02, -1.1166e-02,  1.5661e-02,\n",
      "          2.5624e-02, -3.4839e-02,  2.5568e-02, -1.0033e-02,  3.3612e-03,\n",
      "         -2.8900e-02, -2.4515e-02,  2.7405e-02,  1.8571e-02,  1.7184e-02,\n",
      "          1.3589e-02, -1.2101e-02, -1.0455e-02,  9.1147e-03,  1.2074e-02,\n",
      "          1.5997e-02,  3.3635e-02,  4.3617e-03, -3.0811e-02,  5.4378e-03,\n",
      "          1.3837e-02, -8.3326e-03,  3.3879e-02,  1.2879e-03, -2.7445e-03,\n",
      "          1.2693e-02, -1.0894e-02,  3.1151e-02, -3.2539e-02, -7.5965e-03,\n",
      "         -1.1729e-02, -2.1778e-02,  1.2340e-02,  1.4123e-02,  3.5665e-02,\n",
      "          3.0312e-02,  1.5644e-02,  1.4946e-02,  1.4588e-02,  2.5679e-02,\n",
      "         -2.1712e-02,  3.1492e-02,  1.5405e-02, -1.2939e-02,  7.9051e-04,\n",
      "          1.7478e-03,  3.5530e-02,  5.3991e-03,  1.8372e-02, -3.1752e-02,\n",
      "         -1.6371e-02, -2.4343e-03,  2.1687e-02,  9.9803e-03,  3.2923e-02,\n",
      "         -3.5590e-02, -3.4232e-02, -3.3657e-02, -1.3302e-02, -2.9038e-02,\n",
      "          1.6548e-03, -3.1916e-02,  8.2258e-04, -6.4104e-03, -1.9746e-02,\n",
      "         -1.6114e-02, -9.3570e-03, -1.7457e-02, -2.5022e-04,  3.1825e-02,\n",
      "         -1.7062e-04, -5.6402e-03, -3.4137e-02,  8.1741e-03, -3.9503e-04,\n",
      "          1.7920e-02,  8.0034e-03,  3.1439e-03,  1.9475e-02,  1.1707e-02,\n",
      "          2.8307e-02,  2.4624e-02,  1.0548e-02, -1.9608e-02, -6.8900e-03,\n",
      "          2.7413e-02,  2.4359e-02,  3.5575e-02, -2.3650e-02, -1.4613e-02,\n",
      "          1.0144e-02,  1.6223e-02,  3.0312e-02, -9.2044e-03,  1.9577e-02,\n",
      "          2.5740e-02, -1.2563e-02,  2.4460e-02, -7.7763e-03, -2.8267e-02,\n",
      "         -1.3315e-02,  3.0195e-02, -8.5090e-03, -1.1764e-02, -1.1361e-02,\n",
      "          1.5939e-02, -2.6891e-02, -3.7040e-03,  2.7466e-02,  2.1191e-02,\n",
      "         -6.0745e-03,  3.2373e-02, -2.4274e-02, -6.9623e-03, -9.6900e-03,\n",
      "          3.5144e-02,  3.2573e-02, -2.9972e-02, -1.2392e-02, -8.5188e-03,\n",
      "         -8.0121e-03, -3.4098e-02,  2.7342e-02,  1.0512e-02, -1.2798e-04,\n",
      "         -2.3804e-02,  2.6845e-02,  9.3467e-03,  1.1444e-02, -3.8169e-03,\n",
      "          2.2033e-02,  1.8153e-02,  3.1769e-02,  5.0455e-03, -1.1980e-02,\n",
      "          1.3338e-02, -2.1253e-02,  8.1329e-03,  3.3234e-02, -7.4379e-03,\n",
      "          3.3647e-02, -1.4876e-02,  1.3174e-02,  3.0725e-02, -2.9892e-04,\n",
      "         -9.5722e-03, -8.7446e-03, -2.5361e-02,  3.0920e-02, -3.0508e-02,\n",
      "         -3.1403e-02,  3.3251e-02,  1.3195e-02, -2.9458e-02, -1.8624e-02,\n",
      "         -1.7605e-02, -2.6957e-02, -1.0611e-02, -2.2394e-02, -3.1993e-03,\n",
      "          2.9386e-02, -2.1354e-02,  1.9128e-02,  1.2755e-02, -1.4477e-02,\n",
      "          2.6782e-02,  1.3021e-02,  1.1973e-02,  2.9690e-02,  1.1595e-03,\n",
      "         -3.5631e-02,  1.0459e-03,  1.6996e-03,  9.3558e-03,  9.4781e-03,\n",
      "         -7.6024e-03,  1.1881e-02,  3.1063e-02, -3.0086e-02, -5.3309e-03,\n",
      "         -1.2062e-02, -1.0162e-02, -1.3464e-02,  2.4891e-02,  3.3259e-02,\n",
      "         -1.6973e-03,  1.7581e-02, -3.2535e-02,  2.1538e-02,  2.7934e-02,\n",
      "          1.6645e-02, -1.6777e-02, -6.1994e-03, -3.3661e-02,  2.7046e-02,\n",
      "          1.1336e-02, -1.3011e-02,  1.8405e-02, -3.2375e-02,  2.4825e-02,\n",
      "          1.4829e-02,  1.6691e-02, -6.8085e-03, -1.7176e-02,  3.4928e-02,\n",
      "          2.2587e-02, -1.8776e-02,  1.4309e-02,  3.3267e-02, -3.1062e-02,\n",
      "          2.6968e-02,  1.4196e-02, -7.5566e-04, -3.8333e-03,  4.6290e-03,\n",
      "          3.2883e-03,  1.2749e-02,  1.9838e-02,  3.3401e-02, -1.6355e-03,\n",
      "         -2.5533e-03, -1.2002e-02, -1.4253e-02,  1.7532e-02, -2.0658e-03,\n",
      "          2.7877e-02, -4.6801e-03, -3.3495e-02, -2.5268e-03,  1.3174e-03,\n",
      "          2.4040e-02,  2.0073e-02,  9.9752e-04, -1.4245e-02, -4.0513e-03,\n",
      "          1.1674e-02,  2.1040e-02, -3.3005e-02,  9.3573e-04,  1.0947e-02,\n",
      "         -2.3357e-02,  1.8545e-02, -2.6162e-02, -2.3883e-02, -2.9833e-02,\n",
      "         -8.3488e-04, -2.0127e-02,  1.5453e-02,  1.3620e-02, -1.4079e-02,\n",
      "         -2.8573e-02, -1.7203e-02, -1.6395e-02,  2.3427e-02,  1.4659e-02,\n",
      "          2.0890e-02, -1.1937e-02, -2.7887e-02,  1.8347e-02,  3.0245e-02,\n",
      "         -2.7750e-02,  2.1819e-02,  2.8192e-02,  3.4349e-02, -1.0515e-02,\n",
      "          2.6927e-02,  3.2169e-02, -9.5958e-03, -6.0180e-03,  1.2813e-02,\n",
      "         -3.6988e-03,  3.0021e-03,  2.1003e-02, -2.8234e-02,  1.4166e-02,\n",
      "          1.1799e-02, -1.6848e-02, -1.1327e-02,  1.4819e-02, -1.2877e-02,\n",
      "          2.9528e-02,  2.1220e-02,  3.4037e-02, -1.2342e-02,  1.6672e-02,\n",
      "          1.9868e-02,  1.8942e-02,  3.1075e-02, -3.3972e-02,  2.8598e-02,\n",
      "         -8.1462e-03, -1.7141e-02, -3.5167e-02, -2.8047e-02,  1.4688e-02,\n",
      "         -6.3344e-03, -3.0088e-02,  1.0217e-02, -3.2513e-02,  9.2009e-03,\n",
      "         -3.1479e-02,  3.0201e-02, -2.9248e-02, -3.4251e-02,  3.1991e-02,\n",
      "         -1.3243e-02, -6.3490e-03,  7.7087e-03,  2.0285e-02,  3.0590e-03,\n",
      "         -3.1791e-02,  2.1822e-02, -2.2471e-02,  2.5770e-02,  1.1303e-02,\n",
      "          2.6977e-02,  2.6342e-02, -9.8757e-03, -1.2735e-02,  7.0231e-03,\n",
      "         -3.2897e-02,  3.8499e-03, -1.6635e-03, -1.9575e-02, -3.5257e-02,\n",
      "          2.1868e-02,  7.2702e-03,  2.8636e-02, -1.9808e-02,  3.2308e-02,\n",
      "          9.7175e-03, -1.3864e-02, -2.8769e-03,  3.3117e-02, -2.6197e-02,\n",
      "         -2.7975e-02, -2.6318e-02, -1.1983e-02,  2.6009e-02,  3.4547e-02,\n",
      "         -1.7974e-02, -3.5124e-02, -2.8260e-03, -4.2868e-03, -3.4629e-02,\n",
      "         -4.0909e-03,  1.9009e-02, -2.9701e-02,  1.7784e-02, -1.2128e-02,\n",
      "         -5.7681e-03,  1.2711e-02,  1.8430e-02,  7.8864e-03,  1.4906e-02,\n",
      "          1.0928e-02,  1.3113e-02, -1.5180e-02,  1.8523e-02, -5.8441e-03,\n",
      "         -3.2664e-02,  1.6834e-02, -2.8726e-02, -2.3571e-02,  2.4755e-02,\n",
      "          8.5788e-03,  3.0784e-03,  1.5031e-02,  1.7474e-03,  1.0606e-02,\n",
      "          2.1767e-02, -1.1355e-02,  7.0314e-03, -3.0747e-02, -1.6659e-03,\n",
      "          1.1108e-02,  5.5669e-04, -6.6943e-03, -1.7095e-02, -2.9153e-02,\n",
      "          3.6100e-03,  4.1750e-03,  1.1149e-02,  1.4142e-02, -6.6488e-03,\n",
      "         -1.6230e-02,  4.2142e-03, -3.1680e-02,  2.7384e-02, -2.5925e-02,\n",
      "         -3.8489e-03, -2.5383e-02,  7.3597e-03, -2.7184e-02, -1.1014e-02,\n",
      "          1.7735e-02, -2.0450e-02,  4.0425e-03,  2.1159e-02,  2.9726e-02,\n",
      "          2.8574e-02, -2.4235e-02, -3.5421e-02, -1.7786e-02,  2.5307e-02,\n",
      "         -2.0595e-02,  1.9085e-02, -1.0278e-02,  2.3882e-02, -1.2561e-02,\n",
      "          1.1310e-02,  1.8285e-02,  2.0671e-02, -3.2821e-02, -2.8558e-02,\n",
      "         -3.3441e-02, -5.3904e-03,  2.8926e-02, -1.6542e-02,  8.6926e-05,\n",
      "          2.6744e-02,  1.0734e-02,  6.1839e-03,  2.1612e-02,  1.7418e-03,\n",
      "          7.6128e-03, -2.7469e-02, -6.2298e-03,  9.1626e-03, -3.4226e-02,\n",
      "         -1.4465e-02, -9.6055e-03, -2.3617e-02, -2.2415e-02, -7.0944e-03,\n",
      "          1.7664e-02,  8.1974e-03, -1.8501e-02, -6.9174e-03, -5.2433e-03,\n",
      "          8.5156e-04,  2.1229e-02,  2.8844e-02, -3.4074e-03, -2.5587e-02,\n",
      "          5.4530e-03, -2.4902e-02,  1.6391e-02, -2.0767e-02,  2.3433e-02,\n",
      "         -2.7034e-02,  4.9427e-03,  2.0333e-02, -1.7620e-02,  7.8198e-03,\n",
      "          5.8765e-03, -2.8431e-02,  3.8319e-03,  8.8460e-03,  1.7433e-02,\n",
      "         -2.0832e-02, -5.6113e-03,  3.4976e-02, -3.7866e-03,  3.6517e-03,\n",
      "          2.8880e-02,  2.7362e-02,  4.4316e-03,  3.3824e-02,  1.5750e-02,\n",
      "         -1.0826e-03, -3.6475e-04,  2.5360e-02,  2.4171e-02, -1.7382e-02,\n",
      "         -7.9016e-03,  6.0076e-03,  6.6568e-03, -9.0100e-03,  1.3558e-02,\n",
      "          1.4937e-02, -1.3337e-02, -3.4245e-02, -2.0160e-02, -3.2060e-02,\n",
      "         -2.6224e-02,  1.0892e-02, -2.2594e-02,  2.6455e-02,  1.8976e-02,\n",
      "          1.5008e-02,  1.9386e-02, -3.5388e-02,  2.5792e-02,  5.8605e-03,\n",
      "          1.6308e-02,  3.8721e-03, -1.1037e-03, -1.8213e-02,  1.2311e-03,\n",
      "          2.3883e-02, -2.8112e-02, -4.3414e-03,  3.4659e-02,  1.5349e-02,\n",
      "          7.8501e-03,  3.2740e-02, -2.1164e-02,  2.4337e-02,  2.1006e-02,\n",
      "         -1.6425e-04, -2.6121e-02, -1.7210e-02,  3.1931e-02,  3.5450e-02,\n",
      "         -2.7992e-02, -3.7451e-03, -2.4560e-02, -2.4860e-02, -1.8786e-02,\n",
      "          1.3302e-04, -3.4968e-03,  4.2833e-03, -2.8249e-02, -9.5201e-04,\n",
      "         -3.1661e-02,  1.6457e-02, -2.5735e-02, -2.2506e-02, -2.9747e-02,\n",
      "          1.3769e-02,  2.9612e-02,  2.0801e-02, -1.9516e-02, -1.7338e-02,\n",
      "          2.2835e-03, -3.2772e-02, -1.1303e-02, -4.4409e-03, -2.4853e-02,\n",
      "         -2.2701e-02, -3.3686e-02,  1.9717e-02,  7.2143e-03,  1.0305e-02,\n",
      "         -2.4387e-02, -2.0375e-02, -2.6402e-02,  1.7258e-02, -3.1393e-02,\n",
      "          2.5563e-03,  1.8503e-02, -1.3619e-02, -3.4118e-02, -3.2978e-02,\n",
      "          5.1801e-03,  9.3824e-03,  2.8738e-02,  2.5892e-04, -1.1073e-02,\n",
      "          1.1472e-02, -1.6032e-02,  1.0689e-02, -6.7415e-03,  1.0183e-02,\n",
      "          4.3926e-03, -2.9986e-02,  2.1900e-02,  2.9539e-02,  7.1668e-03,\n",
      "         -3.4465e-02,  3.0848e-02, -2.5239e-02, -2.8705e-04, -3.4844e-02,\n",
      "         -1.1825e-02,  2.0994e-02,  2.6820e-02,  1.7154e-02,  1.8026e-02,\n",
      "         -1.2331e-02,  1.9411e-02, -1.5542e-02,  1.4494e-02,  9.0040e-03,\n",
      "          2.6246e-02,  1.6204e-02, -1.5852e-03, -2.5430e-02,  1.3912e-02,\n",
      "          1.2160e-02, -3.3686e-03,  3.4654e-02,  2.6072e-02,  1.2795e-03,\n",
      "          8.6608e-03, -2.7745e-02, -1.7028e-02, -3.2817e-02,  2.1683e-02,\n",
      "         -2.7346e-02,  3.5059e-02,  1.9552e-03, -3.1209e-02,  2.8074e-02,\n",
      "         -3.2229e-03, -1.9681e-03,  1.8356e-02,  2.3373e-02, -2.3444e-02,\n",
      "         -2.3563e-02, -2.3515e-02,  8.3380e-03, -5.2961e-03,  2.0080e-02,\n",
      "         -1.8073e-02, -3.3385e-02,  1.1976e-02,  2.1674e-02,  3.8822e-03,\n",
      "         -3.0827e-02,  1.9110e-02, -2.2066e-02, -2.7801e-03,  1.4320e-02,\n",
      "         -1.2513e-02, -2.0556e-02,  1.4316e-02,  1.5670e-03,  3.1062e-02,\n",
      "          1.7608e-02, -1.6688e-02, -5.0084e-03,  3.1075e-02,  3.4434e-02,\n",
      "          8.1706e-03,  1.4513e-03, -2.2661e-02,  2.3029e-03, -9.7075e-03,\n",
      "          3.2027e-02, -1.1957e-02,  5.4092e-03,  1.6367e-02,  3.4385e-02,\n",
      "         -1.1961e-02, -3.4093e-02,  3.2390e-03, -1.6279e-02,  1.7590e-02,\n",
      "          2.9142e-02, -2.4339e-02, -3.3565e-02,  2.0146e-02,  3.0084e-02,\n",
      "          1.6706e-02,  3.4192e-02, -2.1138e-02, -9.0215e-03, -2.4381e-04,\n",
      "          3.0579e-02, -1.9264e-02,  7.7904e-04, -2.3741e-02,  1.7845e-02,\n",
      "         -2.5607e-02, -3.0574e-02,  1.1216e-02,  2.0789e-02, -3.2364e-02,\n",
      "         -1.5361e-02,  6.5796e-03, -1.4187e-02, -2.2704e-02,  2.6275e-02,\n",
      "         -2.9387e-02, -1.9607e-02, -3.1699e-02,  3.1055e-02, -2.1777e-02,\n",
      "          2.5647e-03,  1.8341e-02, -1.4563e-02,  3.2054e-02,  5.2788e-04,\n",
      "          2.4715e-02, -1.1054e-02,  1.0666e-02,  2.0470e-02, -1.7748e-03,\n",
      "          1.5924e-02, -8.4313e-04,  1.2980e-02,  4.3984e-03, -1.8796e-02,\n",
      "         -1.2363e-02, -1.8458e-02,  2.4722e-02, -2.7543e-03, -1.7757e-02,\n",
      "         -1.0849e-02,  1.0207e-02, -3.4444e-02, -1.9284e-02, -2.9025e-02,\n",
      "          1.6617e-03, -4.8352e-03,  2.0763e-02, -3.4776e-02,  8.5017e-03,\n",
      "         -4.3326e-03,  1.3233e-04,  1.3169e-02, -2.9158e-02,  5.3223e-03,\n",
      "         -1.6744e-02, -1.0436e-02, -7.8124e-03,  3.0751e-02,  3.1879e-02,\n",
      "         -1.5615e-02,  1.8862e-02,  1.1555e-02,  2.8834e-02,  3.3760e-03,\n",
      "         -4.6458e-04, -4.7443e-03, -2.6732e-02,  2.3260e-02, -9.9271e-03,\n",
      "          7.0793e-03, -1.1392e-03, -2.5752e-02,  1.6039e-02, -2.0545e-02,\n",
      "          1.5186e-02, -3.5059e-02,  2.5777e-03,  8.2190e-04, -1.2031e-02,\n",
      "         -2.4986e-02,  3.0517e-02,  4.2994e-03,  6.9406e-04, -2.4706e-02,\n",
      "          2.6575e-02,  2.7292e-02,  2.6913e-02,  2.8338e-02,  1.6345e-02,\n",
      "          2.7832e-02,  3.3702e-03,  3.5615e-02,  1.7653e-02, -3.5348e-02,\n",
      "         -7.5851e-03,  3.1067e-02, -1.2914e-02,  5.2088e-04,  1.4586e-02,\n",
      "          2.2976e-02, -3.3296e-02, -2.6569e-02, -1.4139e-02, -2.3603e-02,\n",
      "         -1.4640e-02, -1.0716e-03,  3.2883e-03,  1.0921e-03]])), ('activation_stack.0.bias', tensor([-0.0297]))])\n",
      "tensor(0.1248, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 24.92434502, Culminative Send Cost: 11760\n",
      "tensor(1.2179, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.46065140, Culminative Send Cost: 117600\n",
      "tensor(1.7950, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.08680344, Culminative Send Cost: 235200\n",
      "tensor(2.0649, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.09287930, Culminative Send Cost: 352800\n",
      "tensor(2.1982, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.60929537, Culminative Send Cost: 470400\n",
      "tensor(2.2705, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.26403618, Culminative Send Cost: 588000\n",
      "tensor(2.3152, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 6.97885132, Culminative Send Cost: 705600\n",
      "tensor(2.3470, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.73372746, Culminative Send Cost: 823200\n",
      "tensor(2.3725, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.52073336, Culminative Send Cost: 940800\n",
      "tensor(2.3947, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.33488083, Culminative Send Cost: 1058400\n",
      "tensor(2.4148, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.17225361, Culminative Send Cost: 1176000\n",
      "tensor(2.4334, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 6.02958250, Culminative Send Cost: 1293600\n",
      "tensor(2.4509, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.90407610, Culminative Send Cost: 1411200\n",
      "tensor(2.4673, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.79335642, Culminative Send Cost: 1528800\n",
      "tensor(2.4828, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.69538546, Culminative Send Cost: 1646400\n",
      "tensor(2.4974, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.60841751, Culminative Send Cost: 1764000\n",
      "tensor(2.5112, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.53095913, Culminative Send Cost: 1881600\n",
      "tensor(2.5241, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.46173048, Culminative Send Cost: 1999200\n",
      "tensor(2.5364, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.39963055, Culminative Send Cost: 2116800\n",
      "tensor(2.5478, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.34371948, Culminative Send Cost: 2234400\n",
      "tensor(2.5586, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.29318523, Culminative Send Cost: 2352000\n",
      "tensor(2.5688, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.24733400, Culminative Send Cost: 2469600\n",
      "tensor(2.5783, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.20556593, Culminative Send Cost: 2587200\n",
      "tensor(2.5872, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.16736698, Culminative Send Cost: 2704800\n",
      "tensor(2.5955, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.13229370, Culminative Send Cost: 2822400\n",
      "tensor(2.6033, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.09996319, Culminative Send Cost: 2940000\n",
      "tensor(2.6106, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.07004595, Culminative Send Cost: 3057600\n",
      "tensor(2.6173, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.04225588, Culminative Send Cost: 3175200\n",
      "tensor(2.6237, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 5.01634741, Culminative Send Cost: 3292800\n",
      "tensor(2.6296, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.99210644, Culminative Send Cost: 3410400\n",
      "tensor(2.6350, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.96934748, Culminative Send Cost: 3528000\n",
      "tensor(2.6401, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.94791174, Culminative Send Cost: 3645600\n",
      "tensor(2.6448, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.92765808, Culminative Send Cost: 3763200\n",
      "tensor(2.6492, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.90846443, Culminative Send Cost: 3880800\n",
      "tensor(2.6532, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.89022636, Culminative Send Cost: 3998400\n",
      "tensor(2.6569, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.87285042, Culminative Send Cost: 4116000\n",
      "tensor(2.6603, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.85625553, Culminative Send Cost: 4233600\n",
      "tensor(2.6634, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.84037066, Culminative Send Cost: 4351200\n",
      "tensor(2.6663, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.82513332, Culminative Send Cost: 4468800\n",
      "tensor(2.6689, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.81048965, Culminative Send Cost: 4586400\n",
      "tensor(2.6713, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.79638910, Culminative Send Cost: 4704000\n",
      "tensor(2.6735, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.78278971, Culminative Send Cost: 4821600\n",
      "tensor(2.6754, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.76965523, Culminative Send Cost: 4939200\n",
      "tensor(2.6772, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.75694990, Culminative Send Cost: 5056800\n",
      "tensor(2.6788, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.74464321, Culminative Send Cost: 5174400\n",
      "tensor(2.6802, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.73270988, Culminative Send Cost: 5292000\n",
      "tensor(2.6814, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.72112465, Culminative Send Cost: 5409600\n",
      "tensor(2.6825, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.70986557, Culminative Send Cost: 5527200\n",
      "tensor(2.6834, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.69891453, Culminative Send Cost: 5644800\n",
      "tensor(2.6843, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.68825245, Culminative Send Cost: 5762400\n",
      "tensor(2.6849, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.67786455, Culminative Send Cost: 5880000\n",
      "tensor(2.6855, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.66773510, Culminative Send Cost: 5997600\n",
      "tensor(2.6860, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.65785170, Culminative Send Cost: 6115200\n",
      "tensor(2.6863, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.64820099, Culminative Send Cost: 6232800\n",
      "tensor(2.6866, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.63877296, Culminative Send Cost: 6350400\n",
      "tensor(2.6868, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.62955713, Culminative Send Cost: 6468000\n",
      "tensor(2.6869, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.62054300, Culminative Send Cost: 6585600\n",
      "tensor(2.6869, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.61172342, Culminative Send Cost: 6703200\n",
      "tensor(2.6868, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.60308886, Culminative Send Cost: 6820800\n",
      "tensor(2.6867, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.59463215, Culminative Send Cost: 6938400\n",
      "tensor(2.6865, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.58634663, Culminative Send Cost: 7056000\n",
      "tensor(2.6863, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.57822561, Culminative Send Cost: 7173600\n",
      "tensor(2.6860, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.57026243, Culminative Send Cost: 7291200\n",
      "tensor(2.6856, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.56245184, Culminative Send Cost: 7408800\n",
      "tensor(2.6852, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.55478764, Culminative Send Cost: 7526400\n",
      "tensor(2.6848, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.54726505, Culminative Send Cost: 7644000\n",
      "tensor(2.6843, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.53987932, Culminative Send Cost: 7761600\n",
      "tensor(2.6838, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.53262520, Culminative Send Cost: 7879200\n",
      "tensor(2.6832, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.52549934, Culminative Send Cost: 7996800\n",
      "tensor(2.6827, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.51849699, Culminative Send Cost: 8114400\n",
      "tensor(2.6821, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.51161432, Culminative Send Cost: 8232000\n",
      "tensor(2.6814, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.50484657, Culminative Send Cost: 8349600\n",
      "tensor(2.6808, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.49819136, Culminative Send Cost: 8467200\n",
      "tensor(2.6801, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.49164534, Culminative Send Cost: 8584800\n",
      "tensor(2.6794, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.48520422, Culminative Send Cost: 8702400\n",
      "tensor(2.6787, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.47886610, Culminative Send Cost: 8820000\n",
      "tensor(2.6780, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.47262716, Culminative Send Cost: 8937600\n",
      "tensor(2.6773, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.46648455, Culminative Send Cost: 9055200\n",
      "tensor(2.6765, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.46043587, Culminative Send Cost: 9172800\n",
      "tensor(2.6758, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.45447779, Culminative Send Cost: 9290400\n",
      "tensor(2.6750, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.44860840, Culminative Send Cost: 9408000\n",
      "tensor(2.6742, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.44282484, Culminative Send Cost: 9525600\n",
      "tensor(2.6735, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.43712568, Culminative Send Cost: 9643200\n",
      "tensor(2.6727, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.43150759, Culminative Send Cost: 9760800\n",
      "tensor(2.6719, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.42596865, Culminative Send Cost: 9878400\n",
      "tensor(2.6711, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.42050695, Culminative Send Cost: 9996000\n",
      "tensor(2.6703, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.41512012, Culminative Send Cost: 10113600\n",
      "tensor(2.6696, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.40980673, Culminative Send Cost: 10231200\n",
      "tensor(2.6688, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.40456486, Culminative Send Cost: 10348800\n",
      "tensor(2.6680, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.39939165, Culminative Send Cost: 10466400\n",
      "tensor(2.6672, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.39428663, Culminative Send Cost: 10584000\n",
      "tensor(2.6665, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.38924789, Culminative Send Cost: 10701600\n",
      "tensor(2.6657, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.38427305, Culminative Send Cost: 10819200\n",
      "tensor(2.6649, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.37936163, Culminative Send Cost: 10936800\n",
      "tensor(2.6642, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.37451029, Culminative Send Cost: 11054400\n",
      "tensor(2.6634, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.36971998, Culminative Send Cost: 11172000\n",
      "tensor(2.6626, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.36498737, Culminative Send Cost: 11289600\n",
      "tensor(2.6619, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.36031151, Culminative Send Cost: 11407200\n",
      "tensor(2.6612, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.35569143, Culminative Send Cost: 11524800\n",
      "tensor(2.6604, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.35112572, Culminative Send Cost: 11642400\n",
      "tensor(2.6597, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.34661388, Culminative Send Cost: 11760000\n",
      "tensor(2.6590, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.34215212, Culminative Send Cost: 11877600\n",
      "tensor(2.6583, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.33774185, Culminative Send Cost: 11995200\n",
      "tensor(2.6576, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.33338165, Culminative Send Cost: 12112800\n",
      "tensor(2.6569, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.32906866, Culminative Send Cost: 12230400\n",
      "tensor(2.6562, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.32480383, Culminative Send Cost: 12348000\n",
      "tensor(2.6556, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.32058477, Culminative Send Cost: 12465600\n",
      "tensor(2.6549, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.31641150, Culminative Send Cost: 12583200\n",
      "tensor(2.6543, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.31228209, Culminative Send Cost: 12700800\n",
      "tensor(2.6536, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.30819607, Culminative Send Cost: 12818400\n",
      "tensor(2.6530, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.30415249, Culminative Send Cost: 12936000\n",
      "tensor(2.6524, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.30014992, Culminative Send Cost: 13053600\n",
      "tensor(2.6518, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.29618835, Culminative Send Cost: 13171200\n",
      "tensor(2.6512, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.29226637, Culminative Send Cost: 13288800\n",
      "tensor(2.6506, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.28838348, Culminative Send Cost: 13406400\n",
      "tensor(2.6500, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.28453779, Culminative Send Cost: 13524000\n",
      "tensor(2.6495, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.28073120, Culminative Send Cost: 13641600\n",
      "tensor(2.6489, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.27695942, Culminative Send Cost: 13759200\n",
      "tensor(2.6484, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.27322435, Culminative Send Cost: 13876800\n",
      "tensor(2.6479, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.26952457, Culminative Send Cost: 13994400\n",
      "tensor(2.6474, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.26585913, Culminative Send Cost: 14112000\n",
      "tensor(2.6469, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.26222706, Culminative Send Cost: 14229600\n",
      "tensor(2.6464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.25862837, Culminative Send Cost: 14347200\n",
      "tensor(2.6459, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.25506210, Culminative Send Cost: 14464800\n",
      "tensor(2.6454, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.25152826, Culminative Send Cost: 14582400\n",
      "tensor(2.6450, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.24802494, Culminative Send Cost: 14700000\n",
      "tensor(2.6445, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.24455214, Culminative Send Cost: 14817600\n",
      "tensor(2.6441, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.24111032, Culminative Send Cost: 14935200\n",
      "tensor(2.6437, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.23769712, Culminative Send Cost: 15052800\n",
      "tensor(2.6433, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.23431301, Culminative Send Cost: 15170400\n",
      "tensor(2.6429, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.23095798, Culminative Send Cost: 15288000\n",
      "tensor(2.6425, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.22763062, Culminative Send Cost: 15405600\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.22433090, Culminative Send Cost: 15523200\n",
      "tensor(2.6418, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.22105742, Culminative Send Cost: 15640800\n",
      "tensor(2.6414, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.21781063, Culminative Send Cost: 15758400\n",
      "tensor(2.6411, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.21459007, Culminative Send Cost: 15876000\n",
      "tensor(2.6407, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.21139526, Culminative Send Cost: 15993600\n",
      "tensor(2.6404, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.20822525, Culminative Send Cost: 16111200\n",
      "tensor(2.6401, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.20507956, Culminative Send Cost: 16228800\n",
      "tensor(2.6398, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.20195913, Culminative Send Cost: 16346400\n",
      "tensor(2.6396, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.19886160, Culminative Send Cost: 16464000\n",
      "tensor(2.6393, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.19578838, Culminative Send Cost: 16581600\n",
      "tensor(2.6391, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.19273758, Culminative Send Cost: 16699200\n",
      "tensor(2.6388, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.18970966, Culminative Send Cost: 16816800\n",
      "tensor(2.6386, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.18670464, Culminative Send Cost: 16934400\n",
      "tensor(2.6384, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.18372059, Culminative Send Cost: 17052000\n",
      "tensor(2.6381, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.18075895, Culminative Send Cost: 17169600\n",
      "tensor(2.6380, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.17781830, Culminative Send Cost: 17287200\n",
      "tensor(2.6378, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.17489862, Culminative Send Cost: 17404800\n",
      "tensor(2.6376, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.17199945, Culminative Send Cost: 17522400\n",
      "tensor(2.6374, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.16912127, Culminative Send Cost: 17640000\n",
      "tensor(2.6373, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.16626215, Culminative Send Cost: 17757600\n",
      "tensor(2.6371, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.16342306, Culminative Send Cost: 17875200\n",
      "tensor(2.6370, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.16060352, Culminative Send Cost: 17992800\n",
      "tensor(2.6369, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.15780258, Culminative Send Cost: 18110400\n",
      "tensor(2.6368, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.15502167, Culminative Send Cost: 18228000\n",
      "tensor(2.6367, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.15225792, Culminative Send Cost: 18345600\n",
      "tensor(2.6366, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.14951324, Culminative Send Cost: 18463200\n",
      "tensor(2.6365, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.14678621, Culminative Send Cost: 18580800\n",
      "tensor(2.6365, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.14407730, Culminative Send Cost: 18698400\n",
      "tensor(2.6364, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.14138508, Culminative Send Cost: 18816000\n",
      "tensor(2.6364, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.13871145, Culminative Send Cost: 18933600\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.13605309, Culminative Send Cost: 19051200\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.13341284, Culminative Send Cost: 19168800\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.13078833, Culminative Send Cost: 19286400\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.12818098, Culminative Send Cost: 19404000\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.12558937, Culminative Send Cost: 19521600\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.12301350, Culminative Send Cost: 19639200\n",
      "tensor(2.6363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.12045336, Culminative Send Cost: 19756800\n",
      "tensor(2.6364, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.11790848, Culminative Send Cost: 19874400\n",
      "tensor(2.6364, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.11537886, Culminative Send Cost: 19992000\n",
      "tensor(2.6365, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.11286497, Culminative Send Cost: 20109600\n",
      "tensor(2.6365, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.11036539, Culminative Send Cost: 20227200\n",
      "tensor(2.6366, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.10787964, Culminative Send Cost: 20344800\n",
      "tensor(2.6367, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.10540962, Culminative Send Cost: 20462400\n",
      "tensor(2.6368, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.10295391, Culminative Send Cost: 20580000\n",
      "tensor(2.6369, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.10051155, Culminative Send Cost: 20697600\n",
      "tensor(2.6370, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.09808397, Culminative Send Cost: 20815200\n",
      "tensor(2.6371, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.09566975, Culminative Send Cost: 20932800\n",
      "tensor(2.6373, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.09326887, Culminative Send Cost: 21050400\n",
      "tensor(2.6374, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.09088182, Culminative Send Cost: 21168000\n",
      "tensor(2.6376, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.08850765, Culminative Send Cost: 21285600\n",
      "tensor(2.6377, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.08614683, Culminative Send Cost: 21403200\n",
      "tensor(2.6379, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.08379936, Culminative Send Cost: 21520800\n",
      "tensor(2.6381, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.08146477, Culminative Send Cost: 21638400\n",
      "tensor(2.6383, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.07914209, Culminative Send Cost: 21756000\n",
      "tensor(2.6385, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.07683229, Culminative Send Cost: 21873600\n",
      "tensor(2.6387, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.07453489, Culminative Send Cost: 21991200\n",
      "tensor(2.6389, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.07224941, Culminative Send Cost: 22108800\n",
      "tensor(2.6391, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.06997681, Culminative Send Cost: 22226400\n",
      "tensor(2.6393, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.06771564, Culminative Send Cost: 22344000\n",
      "tensor(2.6396, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.06546640, Culminative Send Cost: 22461600\n",
      "tensor(2.6398, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.06322908, Culminative Send Cost: 22579200\n",
      "tensor(2.6401, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.06100273, Culminative Send Cost: 22696800\n",
      "tensor(2.6403, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.05878830, Culminative Send Cost: 22814400\n",
      "tensor(2.6406, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.05658531, Culminative Send Cost: 22932000\n",
      "tensor(2.6409, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.05439329, Culminative Send Cost: 23049600\n",
      "tensor(2.6412, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.05221272, Culminative Send Cost: 23167200\n",
      "tensor(2.6415, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.05004263, Culminative Send Cost: 23284800\n",
      "tensor(2.6418, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.04788399, Culminative Send Cost: 23402400\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.04573536, Culminative Send Cost: 23520000\n",
      "tensor(2.6424, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.04359818, Culminative Send Cost: 23637600\n",
      "tensor(2.6427, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.04147100, Culminative Send Cost: 23755200\n",
      "tensor(2.6431, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.03935432, Culminative Send Cost: 23872800\n",
      "tensor(2.6434, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.03724766, Culminative Send Cost: 23990400\n",
      "tensor(2.6438, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.03515196, Culminative Send Cost: 24108000\n",
      "tensor(2.6441, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.03306580, Culminative Send Cost: 24225600\n",
      "tensor(2.6445, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.03098965, Culminative Send Cost: 24343200\n",
      "tensor(2.6449, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.02892399, Culminative Send Cost: 24460800\n",
      "tensor(2.6452, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.02686787, Culminative Send Cost: 24578400\n",
      "tensor(2.6456, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.02482080, Culminative Send Cost: 24696000\n",
      "tensor(2.6460, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.02278471, Culminative Send Cost: 24813600\n",
      "tensor(2.6464, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.02075672, Culminative Send Cost: 24931200\n",
      "tensor(2.6468, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 4.01873922, Culminative Send Cost: 25048800\n",
      "tensor(2.6472, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 4.01673079, Culminative Send Cost: 25166400\n",
      "tensor(2.6477, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 4.01473188, Culminative Send Cost: 25284000\n",
      "tensor(2.6481, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 4.01274252, Culminative Send Cost: 25401600\n",
      "tensor(2.6485, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 4.01076126, Culminative Send Cost: 25519200\n",
      "tensor(2.6490, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 4.00878954, Culminative Send Cost: 25636800\n",
      "tensor(2.6494, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 4.00682688, Culminative Send Cost: 25754400\n",
      "tensor(2.6499, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 4.00487328, Culminative Send Cost: 25872000\n",
      "tensor(2.6503, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 4.00292778, Culminative Send Cost: 25989600\n",
      "tensor(2.6508, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 4.00099134, Culminative Send Cost: 26107200\n",
      "tensor(2.6513, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.99906373, Culminative Send Cost: 26224800\n",
      "tensor(2.6518, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.99714446, Culminative Send Cost: 26342400\n",
      "tensor(2.6522, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.99523354, Culminative Send Cost: 26460000\n",
      "tensor(2.6527, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.99333191, Culminative Send Cost: 26577600\n",
      "tensor(2.6532, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.99143767, Culminative Send Cost: 26695200\n",
      "tensor(2.6537, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.98955250, Culminative Send Cost: 26812800\n",
      "tensor(2.6543, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.98767495, Culminative Send Cost: 26930400\n",
      "tensor(2.6548, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.98580599, Culminative Send Cost: 27048000\n",
      "tensor(2.6553, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.98394465, Culminative Send Cost: 27165600\n",
      "tensor(2.6558, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.98209143, Culminative Send Cost: 27283200\n",
      "tensor(2.6564, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.98024631, Culminative Send Cost: 27400800\n",
      "tensor(2.6569, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.97840929, Culminative Send Cost: 27518400\n",
      "tensor(2.6575, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.97657967, Culminative Send Cost: 27636000\n",
      "tensor(2.6580, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.97475791, Culminative Send Cost: 27753600\n",
      "tensor(2.6586, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.97294402, Culminative Send Cost: 27871200\n",
      "tensor(2.6591, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.97113776, Culminative Send Cost: 27988800\n",
      "tensor(2.6597, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.96933913, Culminative Send Cost: 28106400\n",
      "tensor(2.6603, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.96754789, Culminative Send Cost: 28224000\n",
      "tensor(2.6609, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.96576405, Culminative Send Cost: 28341600\n",
      "tensor(2.6614, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.96398759, Culminative Send Cost: 28459200\n",
      "tensor(2.6620, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.96221828, Culminative Send Cost: 28576800\n",
      "tensor(2.6626, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.96045685, Culminative Send Cost: 28694400\n",
      "tensor(2.6632, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.95870233, Culminative Send Cost: 28812000\n",
      "tensor(2.6638, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.95695448, Culminative Send Cost: 28929600\n",
      "tensor(2.6644, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.95521450, Culminative Send Cost: 29047200\n",
      "tensor(2.6651, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.95348144, Culminative Send Cost: 29164800\n",
      "tensor(2.6657, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.95175552, Culminative Send Cost: 29282400\n",
      "tensor(2.6663, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.95003629, Culminative Send Cost: 29400000\n",
      "activation_stack.0.weight: tensor([[ 7.3726e-03, -1.6370e-02,  1.6249e-02, -4.7326e-03,  3.5360e-02,\n",
      "         -3.0594e-02, -3.8635e-05,  2.6320e-02, -2.9766e-02, -3.2403e-02,\n",
      "          3.4962e-02,  8.8612e-03, -2.8981e-02, -1.1012e-02,  1.5599e-02,\n",
      "          2.5561e-02, -3.4839e-02,  2.5568e-02, -1.0033e-02,  3.3612e-03,\n",
      "         -2.8900e-02, -2.4515e-02,  2.7405e-02,  1.8571e-02,  1.7184e-02,\n",
      "          1.3589e-02, -1.2101e-02, -1.0455e-02,  9.1147e-03,  1.2074e-02,\n",
      "          1.5997e-02,  3.3635e-02,  4.5401e-03, -3.0603e-02,  6.1028e-03,\n",
      "          1.4781e-02, -6.7962e-03,  3.5902e-02,  4.1722e-03,  7.2152e-04,\n",
      "          1.5947e-02, -7.1180e-03,  3.6077e-02, -2.8266e-02, -4.2639e-03,\n",
      "         -8.6828e-03, -1.9897e-02,  1.4496e-02,  1.6319e-02,  3.6467e-02,\n",
      "          3.0620e-02,  1.5723e-02,  1.4946e-02,  1.4588e-02,  2.5679e-02,\n",
      "         -2.1712e-02,  3.1492e-02,  1.5405e-02, -1.2929e-02,  4.7258e-04,\n",
      "          2.2168e-03,  3.5798e-02,  6.7196e-03,  2.2196e-02, -2.4123e-02,\n",
      "         -2.1247e-03,  1.9793e-02,  4.7924e-02,  4.4074e-02,  7.4586e-02,\n",
      "          2.0465e-02,  3.0702e-02,  3.4832e-02,  5.1712e-02,  3.4440e-02,\n",
      "          5.7500e-02,  8.2811e-03,  2.3459e-02,  4.7934e-03, -1.5433e-02,\n",
      "         -1.4779e-02, -9.3878e-03, -1.7457e-02, -2.5022e-04,  3.1825e-02,\n",
      "         -1.7062e-04, -5.6402e-03, -3.4410e-02,  8.6284e-03,  9.0251e-05,\n",
      "          2.1197e-02,  1.4770e-02,  1.5396e-02,  4.3134e-02,  4.6098e-02,\n",
      "          6.6417e-02,  6.7465e-02,  6.5712e-02,  6.1968e-02,  9.1713e-02,\n",
      "          1.3717e-01,  1.5320e-01,  1.7692e-01,  1.0690e-01,  8.3775e-02,\n",
      "          7.5672e-02,  5.3157e-02,  4.7569e-02, -1.5421e-03,  2.0894e-02,\n",
      "          2.6155e-02, -1.2563e-02,  2.4460e-02, -7.7243e-03, -2.8002e-02,\n",
      "         -1.3217e-02,  3.0401e-02, -9.2753e-03, -9.7385e-03, -5.2387e-03,\n",
      "          2.9165e-02,  3.2925e-03,  3.6584e-02,  6.4612e-02,  4.2720e-02,\n",
      "          8.3062e-03,  4.9323e-02, -6.4089e-03,  3.0869e-02,  4.6628e-02,\n",
      "          9.6256e-02,  9.4859e-02,  3.5351e-02,  5.1469e-02,  4.0683e-02,\n",
      "          2.6648e-02, -1.3953e-02,  3.3633e-02,  1.1982e-02,  7.5145e-04,\n",
      "         -2.3804e-02,  2.6845e-02,  9.5768e-03,  1.0778e-02, -6.5833e-03,\n",
      "          1.2954e-02,  2.8678e-03,  1.3305e-02, -5.4726e-03, -1.3997e-02,\n",
      "          9.0576e-03, -3.0504e-02, -1.9273e-02, -4.9621e-02, -1.1763e-01,\n",
      "         -6.7552e-02, -7.1689e-02, -1.3356e-02,  5.3900e-03, -2.9738e-02,\n",
      "         -1.9029e-02,  8.4992e-03,  2.7775e-03,  5.9875e-02, -9.0834e-04,\n",
      "         -1.5136e-02,  3.6347e-02,  1.4094e-02, -2.9458e-02, -1.8689e-02,\n",
      "         -1.6886e-02, -2.7979e-02, -1.7867e-02, -3.8490e-02, -3.5004e-02,\n",
      "         -1.2021e-02, -5.6030e-02, -8.6487e-03, -2.1068e-02, -4.6163e-02,\n",
      "         -2.7156e-02, -5.1709e-02, -3.5456e-02,  3.1125e-02,  2.2403e-02,\n",
      "         -2.9785e-02, -4.0820e-02, -6.8892e-02, -5.3012e-02, -1.7031e-02,\n",
      "         -1.8445e-03,  4.6007e-02,  7.4870e-02, -8.9876e-06,  1.1162e-03,\n",
      "         -1.1079e-02, -1.0181e-02, -1.3372e-02,  2.7767e-02,  3.5513e-02,\n",
      "         -6.5221e-03, -8.0653e-04, -6.5612e-02, -1.2051e-03,  2.4421e-02,\n",
      "          1.7532e-02,  1.9562e-03,  1.9421e-02, -9.6005e-03,  6.3445e-02,\n",
      "          8.1700e-02,  1.0414e-01,  1.1429e-01, -1.7486e-02, -2.7514e-02,\n",
      "         -6.2212e-02, -3.6188e-02, -3.5417e-02, -1.5673e-02,  9.5386e-02,\n",
      "          1.0165e-01,  2.6627e-02,  2.4129e-02,  3.4262e-02, -3.1082e-02,\n",
      "          2.7937e-02,  2.2111e-02,  1.1839e-02,  5.7910e-03,  3.5059e-03,\n",
      "          7.1086e-03,  5.2597e-02,  8.8183e-02,  1.0382e-01,  9.7866e-02,\n",
      "          1.2284e-01,  1.2541e-01,  9.1729e-02,  8.1806e-02,  3.4839e-02,\n",
      "          3.8550e-02, -4.1895e-02, -9.3203e-02, -7.1990e-02, -4.7405e-02,\n",
      "          7.4523e-03,  4.2103e-02,  8.4717e-02,  8.8416e-02,  5.2791e-02,\n",
      "          2.2142e-02,  2.2137e-02, -3.3024e-02,  2.9801e-03,  2.2111e-02,\n",
      "         -1.7255e-03,  4.3710e-02,  8.5289e-03,  3.9282e-02,  7.5869e-02,\n",
      "          1.4318e-01,  1.2687e-01,  1.8836e-01,  2.1911e-01,  1.6519e-01,\n",
      "          4.4203e-02, -4.5966e-02, -1.1319e-01, -4.6235e-02, -3.0250e-02,\n",
      "         -2.9587e-02, -5.6030e-02, -5.7646e-02,  1.2341e-02,  5.2235e-02,\n",
      "          4.3116e-02,  1.0600e-01,  7.2563e-02,  4.3202e-02, -9.2453e-03,\n",
      "          2.7238e-02,  3.4933e-02,  5.3660e-04,  1.3848e-02,  5.2146e-02,\n",
      "          6.7471e-02,  1.2051e-01,  1.7269e-01,  1.3700e-01,  1.7325e-01,\n",
      "          1.9639e-01,  1.9974e-01,  1.6980e-01,  6.7402e-02, -1.1874e-01,\n",
      "         -1.2869e-01, -3.8472e-02,  3.1695e-02, -1.2875e-02,  1.1279e-02,\n",
      "         -1.2108e-03,  7.3613e-03,  3.3009e-02, -9.0121e-03,  7.3050e-02,\n",
      "          2.1210e-02, -1.0040e-02, -3.4441e-02, -2.7696e-02,  1.6497e-02,\n",
      "          1.6720e-03, -8.5895e-03,  6.1042e-02,  6.6960e-02,  1.4800e-01,\n",
      "          1.2826e-01,  1.8148e-01,  1.1929e-01,  1.2422e-01,  2.3588e-01,\n",
      "          2.2409e-01,  1.1316e-01, -6.3352e-02, -8.0229e-02,  3.7339e-02,\n",
      "          7.4567e-02,  1.0525e-01,  1.8101e-02,  4.3752e-02,  3.0879e-03,\n",
      "          9.7264e-03,  1.3976e-02,  5.3097e-03,  2.7985e-03,  1.0351e-02,\n",
      "         -3.2430e-02,  3.9579e-03, -1.1420e-04, -1.2714e-02, -1.3221e-02,\n",
      "          8.2110e-02,  1.0859e-01,  1.4429e-01,  9.9366e-02,  1.2166e-01,\n",
      "          9.3730e-02,  8.5130e-02,  2.0063e-01,  3.1044e-01,  1.2000e-01,\n",
      "         -3.5126e-02,  1.7682e-02,  1.6914e-01,  2.5897e-01,  2.0023e-01,\n",
      "          8.5375e-02,  8.8824e-03, -3.4853e-03, -3.3708e-02, -6.4621e-02,\n",
      "         -8.4491e-03,  2.3352e-02, -2.8738e-02,  1.8013e-02, -1.2201e-02,\n",
      "         -5.0443e-03,  1.6915e-02,  3.8394e-02,  6.5395e-02,  8.1387e-02,\n",
      "          6.2732e-02,  5.2570e-02,  1.0002e-02,  1.9255e-02,  2.5517e-02,\n",
      "          1.3311e-01,  2.7658e-01,  8.7794e-02,  2.5762e-02,  1.7267e-01,\n",
      "          2.7019e-01,  2.9501e-01,  2.1176e-01,  1.1494e-01,  4.0578e-02,\n",
      "          2.8467e-03, -5.6724e-02, -3.6267e-02, -3.7469e-02,  2.1233e-03,\n",
      "          1.0694e-02,  3.8725e-04, -6.6185e-03, -1.6705e-02, -2.7022e-02,\n",
      "          1.9257e-02,  3.9325e-02,  1.7271e-02, -2.0132e-02, -5.5424e-02,\n",
      "         -7.3396e-02, -6.7685e-02, -5.8284e-02,  1.5095e-01,  1.5481e-01,\n",
      "          4.7074e-02,  3.7985e-02,  1.3620e-01,  2.6542e-01,  2.7023e-01,\n",
      "          1.3248e-01,  1.0913e-02, -2.2916e-02, -3.2322e-02, -2.0718e-02,\n",
      "         -1.2505e-02, -3.2662e-02, -3.0477e-02, -1.9087e-02,  2.4971e-02,\n",
      "         -2.0478e-02,  1.9128e-02, -9.0642e-03,  3.6342e-02, -4.7492e-03,\n",
      "         -4.3291e-02, -7.4531e-02, -9.0324e-02, -1.4241e-01, -1.0748e-01,\n",
      "         -3.7833e-02,  1.0612e-01,  1.4221e-01, -1.7203e-03,  3.0534e-02,\n",
      "          1.4159e-01,  2.9573e-01,  2.0360e-01,  1.5527e-02, -7.2746e-02,\n",
      "         -6.7265e-02, -7.9343e-02, -4.5560e-02, -2.2896e-02, -4.1862e-02,\n",
      "         -1.0831e-02, -1.2571e-02, -2.3936e-02, -2.2188e-02, -7.2604e-03,\n",
      "          1.8130e-02,  1.8281e-02, -3.1410e-02, -9.2709e-02, -1.2005e-01,\n",
      "         -1.2527e-01, -8.8298e-02, -1.6349e-02,  6.1268e-02,  1.4016e-01,\n",
      "          1.3463e-01, -1.9915e-02,  2.2909e-02,  1.1042e-01,  2.5392e-01,\n",
      "          4.9567e-02, -8.4171e-02, -9.4807e-02, -9.5655e-02, -4.8940e-02,\n",
      "         -2.6764e-02, -5.0860e-02, -7.6328e-03,  2.5259e-03,  1.3229e-02,\n",
      "         -2.1055e-02, -5.6113e-03,  3.4814e-02, -2.9792e-03,  9.8394e-03,\n",
      "          2.9465e-04, -6.6998e-02, -1.1520e-01, -9.2437e-02, -5.7421e-02,\n",
      "          2.9410e-02,  1.3397e-01,  2.1893e-01,  1.2405e-01, -3.4379e-02,\n",
      "         -3.8546e-04,  1.1653e-01,  1.2640e-01, -1.7785e-02, -5.8914e-02,\n",
      "         -5.6090e-02, -5.3507e-02, -7.6487e-02, -5.4669e-02, -5.4727e-02,\n",
      "         -4.7775e-02, -5.1471e-03, -2.7631e-02,  2.5717e-02,  1.9178e-02,\n",
      "          1.5068e-02,  2.0303e-02, -3.3089e-02, -1.1140e-02, -8.4018e-02,\n",
      "         -7.8326e-02, -9.5327e-02, -5.6480e-02,  2.3368e-02,  1.3425e-01,\n",
      "          1.6261e-01,  2.8793e-03, -6.8601e-02,  7.1249e-03,  4.6844e-02,\n",
      "          3.6668e-02,  2.0263e-02, -4.0507e-02, -1.3350e-03,  7.6961e-04,\n",
      "         -4.3109e-02, -6.7821e-02, -5.5208e-02, -7.7572e-03,  1.7597e-02,\n",
      "         -2.9678e-02, -4.0942e-03, -2.4560e-02, -2.4773e-02, -1.8578e-02,\n",
      "         -5.0901e-03, -4.2547e-02, -7.1997e-02, -9.3242e-02, -5.6661e-02,\n",
      "         -6.5252e-02,  1.5302e-02,  2.8385e-02,  1.7867e-02, -5.1778e-02,\n",
      "         -7.0773e-02, -1.4135e-02,  1.1640e-02, -7.1051e-03, -1.0928e-02,\n",
      "         -1.0674e-02, -4.7650e-02, -4.2213e-02, -6.1160e-02, -7.7710e-02,\n",
      "         -7.5624e-02, -7.5055e-02,  3.1016e-03,  4.7611e-03,  1.0106e-02,\n",
      "         -2.4387e-02, -2.0600e-02, -2.8094e-02,  2.4048e-03, -8.0833e-02,\n",
      "         -6.5736e-02, -3.6803e-02, -4.7839e-02, -6.9403e-02, -8.5489e-02,\n",
      "         -3.3645e-02, -2.0987e-02,  6.1836e-03, -1.3742e-02, -1.5250e-02,\n",
      "          1.7736e-02, -7.3964e-03,  1.2911e-02, -1.0753e-02, -7.2088e-03,\n",
      "         -3.7799e-02, -9.0245e-02, -3.5369e-02, -2.2615e-02, -2.7993e-02,\n",
      "         -4.6491e-02,  2.8480e-02, -2.5201e-02, -4.3343e-04, -3.4873e-02,\n",
      "         -1.4636e-02,  4.9249e-04, -2.5106e-02, -5.4242e-02, -3.8140e-02,\n",
      "         -5.5449e-02, -4.0339e-02, -9.2751e-02, -5.4831e-02, -2.7777e-02,\n",
      "          3.3340e-02,  3.7979e-02,  3.9377e-03, -4.0680e-02,  1.1274e-02,\n",
      "          8.1972e-03, -9.6405e-03,  1.3004e-02, -6.1463e-03, -4.3675e-02,\n",
      "         -3.4688e-02, -6.5781e-02, -4.0743e-02, -4.1670e-02,  1.9846e-02,\n",
      "         -2.7634e-02,  3.4905e-02,  2.0257e-03, -3.3709e-02,  1.2519e-02,\n",
      "         -4.2555e-02, -6.5438e-02, -5.1757e-02, -4.7227e-02, -9.5892e-02,\n",
      "         -1.0314e-01, -7.8929e-02, -1.5417e-02, -8.1397e-03,  5.3062e-04,\n",
      "         -5.9900e-02, -9.6373e-02, -3.0344e-02,  6.9644e-03,  1.1340e-02,\n",
      "         -2.5358e-02,  1.7718e-02, -3.3656e-02, -2.1326e-02, -1.7059e-03,\n",
      "         -2.3581e-02, -2.4937e-02,  1.3349e-02,  1.2807e-03,  3.1062e-02,\n",
      "          1.7608e-02, -1.7649e-02, -1.0418e-02,  1.2769e-02,  1.3778e-03,\n",
      "         -2.9566e-02, -3.0380e-02, -4.2784e-02, -2.1107e-02, -1.6392e-02,\n",
      "          5.1300e-02,  2.0301e-02,  3.8031e-02,  1.2357e-02,  2.7004e-03,\n",
      "         -6.8369e-03,  1.7414e-02,  7.2132e-02,  4.8102e-02,  6.9603e-02,\n",
      "          5.7001e-02, -1.2821e-02, -2.9525e-02,  1.8795e-02,  2.8639e-02,\n",
      "          1.5905e-02,  3.4192e-02, -2.1138e-02, -9.0215e-03, -1.9124e-04,\n",
      "          2.9119e-02, -2.0910e-02, -5.8583e-04, -1.3255e-02,  6.9508e-02,\n",
      "          6.3466e-02,  8.4092e-02,  1.4007e-01,  1.3588e-01,  7.7568e-02,\n",
      "          9.1592e-02,  1.0153e-01,  8.8998e-02,  1.1096e-01,  1.6911e-01,\n",
      "          1.0403e-01,  9.4439e-02,  5.8465e-02,  8.4837e-02,  4.6117e-03,\n",
      "          1.5872e-02,  1.9551e-02, -1.5665e-02,  3.1617e-02,  5.2788e-04,\n",
      "          2.4715e-02, -1.1054e-02,  1.0912e-02,  2.0977e-02,  2.3678e-03,\n",
      "          2.9816e-02,  3.3982e-02,  8.5794e-02,  1.1838e-01,  1.3052e-01,\n",
      "          1.5668e-01,  1.5428e-01,  2.0442e-01,  1.8390e-01,  1.7115e-01,\n",
      "          1.8357e-01,  2.0608e-01,  1.4410e-01,  1.2314e-01,  7.6780e-02,\n",
      "          7.3922e-02,  3.9020e-02,  4.0821e-02, -2.6744e-02,  9.8208e-03,\n",
      "         -4.4630e-03, -2.6926e-04,  1.3169e-02, -2.9158e-02,  5.3223e-03,\n",
      "         -1.6744e-02, -9.9871e-03, -6.2142e-03,  3.7208e-02,  4.8973e-02,\n",
      "          1.6710e-02,  7.3010e-02,  8.0241e-02,  1.0458e-01,  8.6888e-02,\n",
      "          9.4169e-02,  1.0292e-01,  8.3587e-02,  1.2273e-01,  8.2592e-02,\n",
      "          8.3292e-02,  5.7590e-02,  1.3374e-02,  4.2074e-02, -5.1259e-03,\n",
      "          2.0604e-02, -3.3044e-02,  2.4256e-03,  1.0313e-03, -1.2031e-02,\n",
      "         -2.4986e-02,  3.0517e-02,  4.2994e-03,  6.9406e-04, -2.4706e-02,\n",
      "          2.6704e-02,  2.7739e-02,  2.8427e-02,  3.1146e-02,  1.8933e-02,\n",
      "          3.1313e-02,  9.8413e-03,  4.3265e-02,  2.9827e-02, -2.1305e-02,\n",
      "          1.1680e-02,  4.7844e-02,  1.3293e-03,  1.1837e-02,  2.1993e-02,\n",
      "          2.6217e-02, -3.0375e-02, -2.4772e-02, -1.2242e-02, -2.2462e-02,\n",
      "         -1.4640e-02, -1.0716e-03,  3.2883e-03,  1.0921e-03]])\n",
      "activation_stack.0.bias: tensor([0.9164])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 29400000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for num_clients is 20 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([8, 5, 2,  ..., 4, 8, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 6, 5,  ..., 6, 2, 7], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([9, 8, 9,  ..., 8, 4, 6], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([4, 7, 1,  ..., 0, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[5]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[5]: tensor([6, 2, 8,  ..., 2, 0, 3], dtype=torch.int32)\n",
      "Client_X_train[6]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[6]: tensor([7, 8, 4,  ..., 2, 6, 3], dtype=torch.int32)\n",
      "Client_X_train[7]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[7]: tensor([2, 7, 6,  ..., 9, 2, 4], dtype=torch.int32)\n",
      "Client_X_train[8]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[8]: tensor([7, 8, 3,  ..., 8, 4, 9], dtype=torch.int32)\n",
      "Client_X_train[9]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[9]: tensor([9, 8, 1,  ..., 0, 9, 9], dtype=torch.int32)\n",
      "Client_X_train[10]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[10]: tensor([0, 4, 1,  ..., 5, 1, 0], dtype=torch.int32)\n",
      "Client_X_train[11]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[11]: tensor([1, 3, 6,  ..., 2, 2, 8], dtype=torch.int32)\n",
      "Client_X_train[12]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[12]: tensor([9, 7, 7,  ..., 2, 5, 7], dtype=torch.int32)\n",
      "Client_X_train[13]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[13]: tensor([2, 7, 5,  ..., 6, 6, 1], dtype=torch.int32)\n",
      "Client_X_train[14]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[14]: tensor([3, 6, 3,  ..., 8, 3, 1], dtype=torch.int32)\n",
      "Client_X_train[15]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[15]: tensor([3, 8, 6,  ..., 8, 3, 1], dtype=torch.int32)\n",
      "Client_X_train[16]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[16]: tensor([4, 8, 8,  ..., 9, 2, 7], dtype=torch.int32)\n",
      "Client_X_train[17]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[17]: tensor([7, 6, 9,  ..., 3, 8, 8], dtype=torch.int32)\n",
      "Client_X_train[18]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[18]: tensor([9, 2, 9,  ..., 1, 6, 6], dtype=torch.int32)\n",
      "Client_X_train[19]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[19]: tensor([5, 7, 2,  ..., 7, 3, 6], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[-3.7134e-04,  4.0768e-03,  2.3317e-02, -2.2343e-02,  1.6495e-02,\n",
      "          3.2540e-02, -2.8453e-02,  2.8620e-02,  2.6597e-02,  7.5245e-03,\n",
      "          2.3157e-02,  2.0873e-02, -1.4685e-03, -8.1146e-03,  3.1931e-02,\n",
      "         -3.1646e-02,  2.0223e-02, -3.2978e-02,  3.2972e-02, -6.7634e-03,\n",
      "          2.0926e-02,  2.8112e-02,  2.4477e-02,  3.5391e-02,  2.2725e-02,\n",
      "          2.3132e-02,  2.1101e-02,  2.5530e-02, -3.6340e-04,  3.4465e-02,\n",
      "         -3.3373e-02, -2.5531e-02, -9.4548e-03,  3.5415e-02, -3.5625e-02,\n",
      "         -2.8957e-02,  1.4791e-02,  2.5903e-02,  1.5470e-02,  2.3139e-02,\n",
      "          1.6609e-02,  7.3494e-03, -2.1686e-02, -3.5665e-02,  2.9426e-02,\n",
      "          1.7517e-02, -2.3666e-02, -2.0840e-02,  1.6622e-02, -6.6746e-05,\n",
      "          9.0414e-03, -1.9256e-02,  9.4995e-03,  1.4765e-02,  2.3025e-02,\n",
      "          4.7662e-03,  2.8537e-02, -2.9543e-02, -2.1509e-02, -3.5771e-03,\n",
      "          3.3643e-02,  1.7672e-02, -2.4880e-02, -2.0985e-02,  1.6464e-02,\n",
      "          2.0354e-02, -3.4631e-02, -2.8504e-02,  3.0554e-02, -1.5022e-02,\n",
      "          2.4020e-02,  1.5569e-02,  5.2445e-03, -3.4146e-03, -4.6100e-03,\n",
      "          2.9689e-03, -1.8945e-02,  1.1313e-02, -2.6059e-02,  3.5575e-03,\n",
      "         -2.0792e-02, -2.9543e-02,  2.6959e-03, -2.7931e-02,  8.7709e-03,\n",
      "          3.3612e-02,  1.1622e-02, -2.8303e-02,  3.3582e-02, -1.9790e-02,\n",
      "          1.8743e-02,  2.0993e-02,  1.3979e-02,  1.0331e-02, -8.1728e-03,\n",
      "          2.2038e-02, -1.7297e-02, -1.1347e-02,  3.5497e-03,  2.5457e-02,\n",
      "         -1.3307e-03,  1.6991e-03, -1.6929e-02, -1.4014e-02, -1.3914e-02,\n",
      "         -2.0926e-02, -2.7481e-03,  1.7084e-02, -2.5705e-02,  1.6405e-02,\n",
      "         -7.7286e-03,  2.8226e-02,  5.2052e-03,  2.4074e-02, -1.7168e-02,\n",
      "          1.9909e-02, -2.0492e-02, -7.3069e-03, -1.9398e-02, -4.9330e-03,\n",
      "         -8.7642e-03,  5.8999e-03,  9.8910e-03,  2.3172e-02,  2.1906e-02,\n",
      "         -9.1806e-04, -1.8163e-02, -1.0653e-02,  1.1155e-02, -3.1968e-03,\n",
      "         -1.8353e-04, -1.9669e-02,  3.0444e-02,  3.2067e-02, -8.1388e-03,\n",
      "          1.6596e-02,  2.4938e-02,  3.5638e-02,  7.8818e-03,  3.0854e-02,\n",
      "         -2.6825e-02, -3.4479e-02,  2.4562e-02,  4.8375e-03,  1.8915e-03,\n",
      "         -1.0256e-02,  2.0236e-02, -1.9450e-02,  2.2498e-02, -2.0637e-02,\n",
      "          2.3016e-02, -4.7136e-03, -3.4772e-02,  5.6735e-03, -7.0614e-03,\n",
      "         -2.2859e-02, -7.8373e-03, -2.6880e-02, -1.4555e-02,  3.0703e-02,\n",
      "          1.2322e-02,  2.0927e-02,  2.9548e-02,  2.3777e-02, -2.8087e-02,\n",
      "          9.3316e-03, -1.9858e-02, -5.7620e-03,  3.4113e-02,  6.3834e-03,\n",
      "          2.6850e-02, -1.1653e-02, -2.3810e-02,  1.9922e-04,  3.4536e-02,\n",
      "         -9.6880e-03,  2.1890e-02, -1.5837e-02,  2.5739e-02,  5.7086e-03,\n",
      "          2.5573e-02, -2.6758e-02,  1.3084e-02, -3.3692e-02, -2.9183e-02,\n",
      "          2.2910e-02, -3.4141e-02,  2.2079e-02,  3.7723e-03,  2.1508e-02,\n",
      "         -3.2537e-02,  3.2011e-02,  2.3983e-02,  3.3109e-02, -2.2901e-02,\n",
      "         -3.2561e-02, -1.5945e-02,  3.2977e-02,  3.5223e-02, -2.8837e-02,\n",
      "         -1.6984e-02,  2.8060e-02,  2.1254e-03,  1.8897e-02,  2.7191e-02,\n",
      "          3.6023e-03, -2.2760e-02,  2.9058e-02,  1.4185e-02, -1.2199e-02,\n",
      "         -9.6121e-03,  3.1130e-02, -1.8999e-02,  2.1028e-02, -2.2546e-02,\n",
      "         -3.2686e-02, -1.7391e-02,  3.2202e-02,  2.7413e-02,  3.1698e-02,\n",
      "          1.1494e-02, -1.3106e-02,  1.3678e-03,  3.4339e-02,  2.6362e-02,\n",
      "          2.4056e-02,  2.1632e-03,  1.9646e-03,  4.0735e-03,  1.9976e-02,\n",
      "         -6.2883e-03, -3.0488e-02,  2.1530e-03,  1.5693e-02,  2.6880e-02,\n",
      "         -1.6206e-02, -1.0924e-02,  1.5990e-02, -2.7153e-03, -8.8034e-03,\n",
      "          3.1071e-02, -2.7219e-02, -1.6437e-04, -1.1886e-02, -2.0505e-02,\n",
      "          1.5408e-02,  1.0225e-02, -1.7709e-02,  2.5261e-02,  1.0069e-02,\n",
      "         -1.6007e-02, -1.0882e-02,  1.3103e-02, -1.8645e-03,  1.5262e-02,\n",
      "          1.9954e-02,  1.8142e-02,  1.9734e-02, -8.6268e-03, -1.7800e-02,\n",
      "          2.8768e-03, -1.7035e-02,  4.4250e-03, -2.2087e-02,  2.0030e-02,\n",
      "          2.0408e-02, -3.1675e-02,  3.0864e-02,  1.9916e-02, -9.1605e-05,\n",
      "          2.2678e-02,  1.8655e-02,  1.0766e-02,  1.8579e-02, -2.8378e-02,\n",
      "         -2.9808e-02,  2.1437e-02, -1.5578e-02, -1.1882e-02, -2.1108e-02,\n",
      "         -1.6214e-02, -1.7607e-02,  2.4544e-02,  1.0126e-02, -1.9119e-02,\n",
      "          3.0186e-02,  3.1276e-02, -1.1542e-02, -1.2315e-02, -2.9544e-02,\n",
      "          3.2351e-02, -1.9696e-02,  7.9023e-03, -2.0381e-02, -1.4557e-03,\n",
      "          2.8100e-02,  4.9222e-03, -2.2212e-02,  1.1702e-02, -1.9127e-02,\n",
      "         -2.6410e-03,  2.5780e-02, -2.7458e-04,  3.1103e-02,  4.1765e-03,\n",
      "          2.6062e-02,  2.2166e-02,  3.3375e-03, -2.3920e-02, -2.6068e-02,\n",
      "          3.3364e-03, -3.4838e-02, -3.6532e-04,  2.0241e-02,  2.7741e-02,\n",
      "          3.4123e-02,  2.4701e-02,  6.1611e-03,  3.4687e-02, -1.3478e-02,\n",
      "         -1.9978e-02,  1.8726e-02, -2.4443e-02,  7.5341e-03, -3.1989e-02,\n",
      "          2.8553e-02, -1.3488e-02,  3.2071e-02, -2.0725e-02, -2.4316e-02,\n",
      "          1.8276e-02, -1.9906e-02,  5.8983e-03,  6.6449e-03, -8.4428e-03,\n",
      "          1.8772e-02,  1.4239e-02,  3.5443e-02,  2.2157e-02, -1.5191e-02,\n",
      "          5.2605e-05,  2.7517e-02, -3.5374e-02,  1.3402e-02, -2.2198e-02,\n",
      "          1.9420e-03, -3.2046e-02, -3.5063e-02, -3.1533e-02,  6.3966e-03,\n",
      "         -1.9715e-02,  2.6583e-02,  2.8018e-02,  9.9888e-03, -2.7284e-02,\n",
      "         -1.3348e-02,  2.4434e-02,  3.4085e-02,  1.3245e-02, -1.3433e-02,\n",
      "          1.6864e-02,  1.5530e-02, -2.2811e-02,  1.0203e-03,  6.0389e-03,\n",
      "         -1.2099e-02,  3.5020e-02, -3.5030e-02,  1.7392e-02,  1.8820e-02,\n",
      "          1.8421e-02,  1.4193e-02,  2.7585e-02,  2.5247e-02,  1.4381e-02,\n",
      "          3.4752e-02,  1.3430e-02,  7.6776e-03, -3.2646e-02, -3.0091e-02,\n",
      "          8.7438e-03, -2.0382e-02, -5.8449e-03, -2.6331e-03,  1.2923e-02,\n",
      "          2.8685e-02,  6.9789e-03, -2.5873e-02, -3.3134e-02, -3.0585e-02,\n",
      "         -2.4420e-02, -1.6357e-02, -5.1475e-03, -7.7622e-03, -1.5827e-02,\n",
      "         -2.3600e-02, -2.2234e-02, -3.4931e-02, -3.5120e-02, -6.2014e-03,\n",
      "          2.8497e-02,  2.2161e-02, -2.9949e-02,  3.4609e-02,  2.7963e-02,\n",
      "          2.3656e-02,  2.0675e-03, -1.8725e-02, -1.9013e-03, -2.5723e-02,\n",
      "          2.1562e-02, -2.3013e-02,  2.1510e-02,  1.6751e-02, -9.7953e-03,\n",
      "          2.3980e-02,  1.4990e-02, -2.0927e-02, -3.3343e-02, -6.3707e-03,\n",
      "          2.9998e-02, -3.2792e-02, -2.0996e-02, -1.2881e-02, -2.0665e-02,\n",
      "          7.5277e-04, -1.8710e-02,  2.3506e-02, -1.0822e-02,  1.5527e-02,\n",
      "          4.9267e-03,  8.9456e-03, -2.6366e-02, -3.9094e-03,  2.0893e-02,\n",
      "          1.5437e-02, -1.7502e-02,  6.0672e-03, -1.5675e-02, -1.9039e-02,\n",
      "          6.5551e-03, -5.0842e-03, -2.2252e-02,  3.3672e-02,  7.8194e-03,\n",
      "          3.4989e-02,  2.0883e-02,  1.7432e-02,  2.5853e-03,  2.6046e-02,\n",
      "         -2.9063e-02,  1.0353e-02,  4.5288e-03, -6.6047e-03, -2.2706e-03,\n",
      "          3.1128e-02, -2.3910e-02,  1.2461e-03, -2.2752e-02, -3.1437e-02,\n",
      "         -2.5859e-02,  1.5590e-02, -6.9082e-03,  1.6320e-03, -3.3264e-02,\n",
      "          1.7518e-02,  3.3351e-03,  7.3492e-03,  1.6026e-02,  3.2989e-02,\n",
      "          1.7603e-02,  2.8670e-02,  3.1367e-02, -1.5615e-03,  1.2816e-02,\n",
      "         -2.5519e-03, -3.1740e-02, -1.3862e-02,  5.4378e-03,  1.2748e-02,\n",
      "          7.5827e-03,  2.6813e-02, -2.5253e-02,  2.9494e-02,  2.2711e-03,\n",
      "         -4.7641e-03, -2.1265e-02, -3.1591e-02, -1.4798e-02,  9.1502e-03,\n",
      "          3.0098e-02,  8.3396e-03, -2.5717e-02, -1.6801e-02, -2.3431e-02,\n",
      "          1.3421e-02,  8.1299e-03, -2.0744e-02,  9.3791e-03, -1.3916e-02,\n",
      "          1.8419e-02,  1.7118e-04, -1.3908e-02, -7.9755e-03, -2.3131e-02,\n",
      "          2.5728e-02, -9.2284e-03,  2.2069e-02, -7.5216e-03, -1.7868e-02,\n",
      "         -2.2965e-02,  2.7185e-02,  1.6612e-02,  7.7586e-03, -4.6639e-03,\n",
      "          6.4644e-03,  2.3648e-02, -2.7218e-02,  2.8439e-02, -1.3437e-02,\n",
      "          3.1629e-02,  6.8957e-03,  1.8896e-02,  2.1706e-02,  1.5984e-02,\n",
      "         -1.8785e-03,  1.9427e-02,  1.1918e-02, -2.5275e-02,  2.9507e-02,\n",
      "         -1.2220e-02,  2.6955e-02,  7.5893e-03, -5.3268e-04,  1.1855e-02,\n",
      "          3.5394e-02, -1.3169e-04,  1.3245e-03,  2.8413e-02, -2.3754e-02,\n",
      "          2.0173e-02, -2.8163e-02,  2.5038e-02,  1.6294e-02,  2.9447e-02,\n",
      "          1.8880e-02,  2.4561e-02, -9.7800e-03,  3.2940e-02, -1.5812e-02,\n",
      "         -4.1023e-03, -5.6017e-03, -3.1790e-02,  3.4578e-02,  2.2736e-02,\n",
      "          1.8100e-02, -3.5672e-02,  1.1076e-02,  1.8874e-02, -1.3433e-02,\n",
      "         -5.5038e-03, -3.3553e-02, -2.6231e-02,  8.8663e-03, -2.6075e-02,\n",
      "         -2.2768e-02, -3.0576e-02,  1.5645e-02, -3.2368e-02, -2.9934e-02,\n",
      "          2.6482e-02, -6.3153e-03, -3.4384e-02,  1.6626e-02,  1.1496e-02,\n",
      "         -2.5199e-02, -1.1598e-02, -2.1871e-02,  2.8157e-02,  3.5120e-02,\n",
      "         -1.6377e-02, -2.6852e-02, -2.2475e-02, -2.9050e-02,  2.0800e-02,\n",
      "          1.2196e-02, -2.6540e-02, -8.4378e-05, -1.2573e-02,  3.0326e-03,\n",
      "         -1.5195e-02,  7.0076e-03,  7.1545e-03,  1.4630e-03,  7.0987e-03,\n",
      "          1.8430e-02,  3.0589e-02, -1.0573e-02, -3.1050e-02, -1.8363e-02,\n",
      "          2.1907e-02, -2.3882e-02, -4.8139e-03,  6.8992e-03,  1.6615e-02,\n",
      "          6.2213e-03, -1.0757e-02, -5.1451e-03, -2.6566e-02, -8.0190e-03,\n",
      "          1.2053e-02,  2.8513e-02, -2.0581e-02, -2.1369e-03, -2.8836e-02,\n",
      "         -9.3541e-03, -1.8974e-02,  1.0495e-02, -5.6132e-03, -2.7226e-02,\n",
      "         -3.4116e-02,  3.1884e-02, -2.9031e-02,  2.5878e-02, -2.5086e-02,\n",
      "         -3.6427e-03, -2.1518e-02, -3.5148e-02, -1.4432e-02, -7.7208e-03,\n",
      "         -1.2454e-02,  2.3947e-02, -6.2003e-03, -7.4691e-04,  2.1669e-03,\n",
      "         -2.7067e-02,  2.5745e-02, -3.2888e-03,  9.6647e-03, -3.0589e-02,\n",
      "         -3.2568e-02,  1.4469e-02, -2.9817e-02, -1.3403e-02, -6.6300e-03,\n",
      "         -3.0431e-02,  3.0452e-02,  1.3513e-02, -2.7877e-02,  1.3825e-02,\n",
      "          1.6909e-02, -1.0342e-02, -1.2377e-02, -1.5413e-02,  3.5175e-02,\n",
      "          6.6090e-04,  1.9571e-02,  2.0834e-03, -2.8141e-02,  1.1524e-03,\n",
      "         -2.8007e-02,  8.3412e-03, -8.1812e-03, -2.1900e-02,  3.5588e-02,\n",
      "         -2.0571e-02,  8.9161e-03,  1.7265e-02, -2.5530e-02,  2.0266e-02,\n",
      "         -9.3826e-03, -2.3172e-02, -8.9248e-03,  3.0582e-03,  1.0625e-03,\n",
      "         -2.5908e-02, -1.8020e-02, -2.9516e-02, -1.1222e-02,  1.2739e-02,\n",
      "         -7.6522e-03,  1.0554e-02,  3.1949e-02,  3.0601e-02, -2.8927e-03,\n",
      "         -2.9549e-02, -2.5770e-02, -1.2658e-02,  1.9944e-02,  2.3383e-02,\n",
      "          3.5002e-02,  2.8578e-02, -1.9250e-02, -1.7491e-02, -2.0521e-02,\n",
      "          1.6507e-02, -3.5613e-02,  3.0624e-02,  1.9343e-02, -3.3946e-03,\n",
      "          7.9454e-03, -2.5319e-02, -2.1401e-02, -2.3172e-02,  2.2338e-03,\n",
      "         -2.5118e-02,  1.5971e-02,  7.3953e-03,  1.7731e-02,  1.8903e-02,\n",
      "         -2.6590e-02, -2.9044e-02,  1.4073e-02, -1.2635e-02, -1.5699e-02,\n",
      "          3.4185e-02,  3.7090e-03, -2.6676e-02, -3.5473e-02,  1.1623e-02,\n",
      "          3.3679e-02,  1.9626e-02,  3.0627e-02, -2.3768e-02, -2.1100e-02,\n",
      "          7.6600e-03,  5.2688e-03,  3.4710e-02, -1.2882e-03, -5.4475e-03,\n",
      "          7.2281e-03,  1.4400e-02, -3.0968e-02,  2.8166e-02,  5.5055e-03,\n",
      "         -2.3082e-02, -2.2102e-02,  7.6855e-03,  2.2778e-02,  1.5912e-03,\n",
      "         -2.1035e-02,  2.7213e-04, -5.2235e-03,  2.5938e-02,  1.5020e-02,\n",
      "          2.6314e-02, -1.0712e-02, -1.2214e-02, -2.6620e-02, -2.1072e-02,\n",
      "         -1.5336e-02,  1.2878e-02, -1.7126e-02, -2.1656e-02,  7.7359e-04,\n",
      "          1.3039e-04,  2.0988e-03, -1.8096e-02, -1.7744e-02, -1.0787e-02,\n",
      "          1.9308e-02, -9.8488e-03,  1.6543e-02,  2.8214e-02,  1.9595e-02,\n",
      "         -3.4491e-02, -1.8448e-02,  2.5000e-02,  2.9117e-02,  1.5212e-02,\n",
      "          1.2660e-02, -2.8175e-02, -2.8809e-02,  3.1109e-02, -3.4963e-02,\n",
      "         -9.7692e-03, -7.2804e-03, -2.3362e-02,  3.7395e-03, -1.3071e-02,\n",
      "          3.2060e-02,  5.8400e-03, -2.2093e-02, -1.1623e-02]])), ('activation_stack.0.bias', tensor([0.0226]))])\n",
      "tensor(0.2498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 25.18761253, Culminative Send Cost: 15680\n",
      "tensor(1.3526, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.45533466, Culminative Send Cost: 156800\n",
      "tensor(1.9334, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.02299690, Culminative Send Cost: 313600\n",
      "tensor(2.2036, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.02312183, Culminative Send Cost: 470400\n",
      "tensor(2.3359, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.54323149, Culminative Send Cost: 627200\n",
      "tensor(2.4067, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.20301247, Culminative Send Cost: 784000\n",
      "tensor(2.4497, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 6.92265511, Culminative Send Cost: 940800\n",
      "tensor(2.4799, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.68189430, Culminative Send Cost: 1097600\n",
      "tensor(2.5039, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.47279358, Culminative Send Cost: 1254400\n",
      "tensor(2.5246, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.29040337, Culminative Send Cost: 1411200\n",
      "tensor(2.5434, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.13085842, Culminative Send Cost: 1568000\n",
      "tensor(2.5608, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 5.99092293, Culminative Send Cost: 1724800\n",
      "tensor(2.5772, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.86785030, Culminative Send Cost: 1881600\n",
      "tensor(2.5926, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.75929070, Culminative Send Cost: 2038400\n",
      "tensor(2.6072, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.66323757, Culminative Send Cost: 2195200\n",
      "tensor(2.6209, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.57797384, Culminative Send Cost: 2352000\n",
      "tensor(2.6339, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.50202847, Culminative Send Cost: 2508800\n",
      "tensor(2.6462, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.43414259, Culminative Send Cost: 2665600\n",
      "tensor(2.6577, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.37323570, Culminative Send Cost: 2822400\n",
      "tensor(2.6686, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.31838226, Culminative Send Cost: 2979200\n",
      "tensor(2.6788, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.26878834, Culminative Send Cost: 3136000\n",
      "tensor(2.6885, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.22377110, Culminative Send Cost: 3292800\n",
      "tensor(2.6975, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.18274260, Culminative Send Cost: 3449600\n",
      "tensor(2.7060, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.14520025, Culminative Send Cost: 3606400\n",
      "tensor(2.7139, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.11070871, Culminative Send Cost: 3763200\n",
      "tensor(2.7213, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.07889366, Culminative Send Cost: 3920000\n",
      "tensor(2.7283, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.04943275, Culminative Send Cost: 4076800\n",
      "tensor(2.7347, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.02204657, Culminative Send Cost: 4233600\n",
      "tensor(2.7408, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.99649525, Culminative Send Cost: 4390400\n",
      "tensor(2.7464, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.97256899, Culminative Send Cost: 4547200\n",
      "tensor(2.7516, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.95008898, Culminative Send Cost: 4704000\n",
      "tensor(2.7564, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.92889690, Culminative Send Cost: 4860800\n",
      "tensor(2.7609, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.90885687, Culminative Send Cost: 5017600\n",
      "tensor(2.7651, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.88985062, Culminative Send Cost: 5174400\n",
      "tensor(2.7689, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.87177610, Culminative Send Cost: 5331200\n",
      "tensor(2.7724, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.85454082, Culminative Send Cost: 5488000\n",
      "tensor(2.7757, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.83806801, Culminative Send Cost: 5644800\n",
      "tensor(2.7786, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.82228708, Culminative Send Cost: 5801600\n",
      "tensor(2.7813, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.80713844, Culminative Send Cost: 5958400\n",
      "tensor(2.7838, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.79256868, Culminative Send Cost: 6115200\n",
      "tensor(2.7861, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.77853012, Culminative Send Cost: 6272000\n",
      "tensor(2.7881, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.76498127, Culminative Send Cost: 6428800\n",
      "tensor(2.7899, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.75188494, Culminative Send Cost: 6585600\n",
      "tensor(2.7915, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.73920918, Culminative Send Cost: 6742400\n",
      "tensor(2.7930, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.72692394, Culminative Send Cost: 6899200\n",
      "tensor(2.7943, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.71500349, Culminative Send Cost: 7056000\n",
      "tensor(2.7954, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.70342398, Culminative Send Cost: 7212800\n",
      "tensor(2.7963, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.69216633, Culminative Send Cost: 7369600\n",
      "tensor(2.7972, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.68120861, Culminative Send Cost: 7526400\n",
      "tensor(2.7979, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.67053509, Culminative Send Cost: 7683200\n",
      "tensor(2.7984, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.66013098, Culminative Send Cost: 7840000\n",
      "tensor(2.7989, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.64998102, Culminative Send Cost: 7996800\n",
      "tensor(2.7992, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.64007235, Culminative Send Cost: 8153600\n",
      "tensor(2.7995, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.63039398, Culminative Send Cost: 8310400\n",
      "tensor(2.7996, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.62093496, Culminative Send Cost: 8467200\n",
      "tensor(2.7996, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.61168528, Culminative Send Cost: 8624000\n",
      "tensor(2.7996, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.60263491, Culminative Send Cost: 8780800\n",
      "tensor(2.7995, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.59377575, Culminative Send Cost: 8937600\n",
      "tensor(2.7993, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.58510065, Culminative Send Cost: 9094400\n",
      "tensor(2.7991, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.57660103, Culminative Send Cost: 9251200\n",
      "tensor(2.7987, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.56827116, Culminative Send Cost: 9408000\n",
      "tensor(2.7984, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.56010389, Culminative Send Cost: 9564800\n",
      "tensor(2.7979, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.55209351, Culminative Send Cost: 9721600\n",
      "tensor(2.7974, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.54423380, Culminative Send Cost: 9878400\n",
      "tensor(2.7969, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.53652048, Culminative Send Cost: 10035200\n",
      "tensor(2.7963, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.52894783, Culminative Send Cost: 10192000\n",
      "tensor(2.7957, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.52151108, Culminative Send Cost: 10348800\n",
      "tensor(2.7951, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.51420593, Culminative Send Cost: 10505600\n",
      "tensor(2.7944, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.50702810, Culminative Send Cost: 10662400\n",
      "tensor(2.7936, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.49997330, Culminative Send Cost: 10819200\n",
      "tensor(2.7929, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.49303818, Culminative Send Cost: 10976000\n",
      "tensor(2.7921, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.48621845, Culminative Send Cost: 11132800\n",
      "tensor(2.7913, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.47951078, Culminative Send Cost: 11289600\n",
      "tensor(2.7905, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.47291231, Culminative Send Cost: 11446400\n",
      "tensor(2.7896, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.46641874, Culminative Send Cost: 11603200\n",
      "tensor(2.7888, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.46002865, Culminative Send Cost: 11760000\n",
      "tensor(2.7879, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.45373726, Culminative Send Cost: 11916800\n",
      "tensor(2.7870, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.44754314, Culminative Send Cost: 12073600\n",
      "tensor(2.7861, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.44144344, Culminative Send Cost: 12230400\n",
      "tensor(2.7851, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.43543482, Culminative Send Cost: 12387200\n",
      "tensor(2.7842, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.42951488, Culminative Send Cost: 12544000\n",
      "tensor(2.7833, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.42368174, Culminative Send Cost: 12700800\n",
      "tensor(2.7823, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.41793251, Culminative Send Cost: 12857600\n",
      "tensor(2.7814, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.41226625, Culminative Send Cost: 13014400\n",
      "tensor(2.7804, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.40667915, Culminative Send Cost: 13171200\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.40117025, Culminative Send Cost: 13328000\n",
      "tensor(2.7785, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.39573669, Culminative Send Cost: 13484800\n",
      "tensor(2.7775, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.39037848, Culminative Send Cost: 13641600\n",
      "tensor(2.7765, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.38509083, Culminative Send Cost: 13798400\n",
      "tensor(2.7756, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.37987375, Culminative Send Cost: 13955200\n",
      "tensor(2.7746, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.37472534, Culminative Send Cost: 14112000\n",
      "tensor(2.7736, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.36964369, Culminative Send Cost: 14268800\n",
      "tensor(2.7727, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.36462736, Culminative Send Cost: 14425600\n",
      "tensor(2.7717, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.35967493, Culminative Send Cost: 14582400\n",
      "tensor(2.7707, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.35478497, Culminative Send Cost: 14739200\n",
      "tensor(2.7698, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.34995461, Culminative Send Cost: 14896000\n",
      "tensor(2.7688, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.34518433, Culminative Send Cost: 15052800\n",
      "tensor(2.7679, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.34047127, Culminative Send Cost: 15209600\n",
      "tensor(2.7669, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.33581543, Culminative Send Cost: 15366400\n",
      "tensor(2.7660, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.33121443, Culminative Send Cost: 15523200\n",
      "tensor(2.7651, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.32666779, Culminative Send Cost: 15680000\n",
      "tensor(2.7642, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.32217360, Culminative Send Cost: 15836800\n",
      "tensor(2.7632, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.31773138, Culminative Send Cost: 15993600\n",
      "tensor(2.7623, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.31333923, Culminative Send Cost: 16150400\n",
      "tensor(2.7614, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.30899620, Culminative Send Cost: 16307200\n",
      "tensor(2.7605, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.30470228, Culminative Send Cost: 16464000\n",
      "tensor(2.7597, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.30045557, Culminative Send Cost: 16620800\n",
      "tensor(2.7588, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.29625416, Culminative Send Cost: 16777600\n",
      "tensor(2.7579, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.29209852, Culminative Send Cost: 16934400\n",
      "tensor(2.7571, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.28798771, Culminative Send Cost: 17091200\n",
      "tensor(2.7562, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.28391933, Culminative Send Cost: 17248000\n",
      "tensor(2.7554, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.27989340, Culminative Send Cost: 17404800\n",
      "tensor(2.7545, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.27590990, Culminative Send Cost: 17561600\n",
      "tensor(2.7537, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.27196646, Culminative Send Cost: 17718400\n",
      "tensor(2.7529, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.26806355, Culminative Send Cost: 17875200\n",
      "tensor(2.7521, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.26419926, Culminative Send Cost: 18032000\n",
      "tensor(2.7513, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.26037312, Culminative Send Cost: 18188800\n",
      "tensor(2.7506, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.25658464, Culminative Send Cost: 18345600\n",
      "tensor(2.7498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.25283289, Culminative Send Cost: 18502400\n",
      "tensor(2.7490, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.24911737, Culminative Send Cost: 18659200\n",
      "tensor(2.7483, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.24543762, Culminative Send Cost: 18816000\n",
      "tensor(2.7475, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.24179220, Culminative Send Cost: 18972800\n",
      "tensor(2.7468, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.23818064, Culminative Send Cost: 19129600\n",
      "tensor(2.7461, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.23460293, Culminative Send Cost: 19286400\n",
      "tensor(2.7454, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.23105764, Culminative Send Cost: 19443200\n",
      "tensor(2.7447, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.22754526, Culminative Send Cost: 19600000\n",
      "tensor(2.7440, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.22406340, Culminative Send Cost: 19756800\n",
      "tensor(2.7434, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.22061253, Culminative Send Cost: 19913600\n",
      "tensor(2.7427, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.21719265, Culminative Send Cost: 20070400\n",
      "tensor(2.7421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.21380281, Culminative Send Cost: 20227200\n",
      "tensor(2.7414, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.21044159, Culminative Send Cost: 20384000\n",
      "tensor(2.7408, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.20710945, Culminative Send Cost: 20540800\n",
      "tensor(2.7402, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.20380592, Culminative Send Cost: 20697600\n",
      "tensor(2.7396, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.20053005, Culminative Send Cost: 20854400\n",
      "tensor(2.7390, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.19728088, Culminative Send Cost: 21011200\n",
      "tensor(2.7384, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.19405937, Culminative Send Cost: 21168000\n",
      "tensor(2.7378, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.19086361, Culminative Send Cost: 21324800\n",
      "tensor(2.7373, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.18769455, Culminative Send Cost: 21481600\n",
      "tensor(2.7367, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.18455029, Culminative Send Cost: 21638400\n",
      "tensor(2.7362, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.18143082, Culminative Send Cost: 21795200\n",
      "tensor(2.7357, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.17833662, Culminative Send Cost: 21952000\n",
      "tensor(2.7352, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.17526674, Culminative Send Cost: 22108800\n",
      "tensor(2.7347, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.17222023, Culminative Send Cost: 22265600\n",
      "tensor(2.7342, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.16919708, Culminative Send Cost: 22422400\n",
      "tensor(2.7337, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.16619730, Culminative Send Cost: 22579200\n",
      "tensor(2.7332, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.16322041, Culminative Send Cost: 22736000\n",
      "tensor(2.7328, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.16026592, Culminative Send Cost: 22892800\n",
      "tensor(2.7323, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.15733290, Culminative Send Cost: 23049600\n",
      "tensor(2.7319, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.15442133, Culminative Send Cost: 23206400\n",
      "tensor(2.7315, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.15153170, Culminative Send Cost: 23363200\n",
      "tensor(2.7310, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.14866257, Culminative Send Cost: 23520000\n",
      "tensor(2.7306, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.14581442, Culminative Send Cost: 23676800\n",
      "tensor(2.7302, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.14298582, Culminative Send Cost: 23833600\n",
      "tensor(2.7299, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.14017820, Culminative Send Cost: 23990400\n",
      "tensor(2.7295, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.13739014, Culminative Send Cost: 24147200\n",
      "tensor(2.7291, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.13462114, Culminative Send Cost: 24304000\n",
      "tensor(2.7288, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.13187170, Culminative Send Cost: 24460800\n",
      "tensor(2.7284, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.12914085, Culminative Send Cost: 24617600\n",
      "tensor(2.7281, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.12642908, Culminative Send Cost: 24774400\n",
      "tensor(2.7278, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.12373495, Culminative Send Cost: 24931200\n",
      "tensor(2.7275, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.12105894, Culminative Send Cost: 25088000\n",
      "tensor(2.7272, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.11840105, Culminative Send Cost: 25244800\n",
      "tensor(2.7269, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.11576080, Culminative Send Cost: 25401600\n",
      "tensor(2.7266, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.11313772, Culminative Send Cost: 25558400\n",
      "tensor(2.7264, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.11053181, Culminative Send Cost: 25715200\n",
      "tensor(2.7261, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.10794258, Culminative Send Cost: 25872000\n",
      "tensor(2.7259, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.10537004, Culminative Send Cost: 26028800\n",
      "tensor(2.7256, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.10281324, Culminative Send Cost: 26185600\n",
      "tensor(2.7254, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.10027313, Culminative Send Cost: 26342400\n",
      "tensor(2.7252, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.09774923, Culminative Send Cost: 26499200\n",
      "tensor(2.7250, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.09524107, Culminative Send Cost: 26656000\n",
      "tensor(2.7248, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.09274817, Culminative Send Cost: 26812800\n",
      "tensor(2.7246, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.09027052, Culminative Send Cost: 26969600\n",
      "tensor(2.7244, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.08780766, Culminative Send Cost: 27126400\n",
      "tensor(2.7242, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.08536053, Culminative Send Cost: 27283200\n",
      "tensor(2.7241, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.08292770, Culminative Send Cost: 27440000\n",
      "tensor(2.7239, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.08050919, Culminative Send Cost: 27596800\n",
      "tensor(2.7238, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.07810593, Culminative Send Cost: 27753600\n",
      "tensor(2.7237, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.07571602, Culminative Send Cost: 27910400\n",
      "tensor(2.7235, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.07334089, Culminative Send Cost: 28067200\n",
      "tensor(2.7234, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.07097816, Culminative Send Cost: 28224000\n",
      "tensor(2.7233, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.06863022, Culminative Send Cost: 28380800\n",
      "tensor(2.7232, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.06629562, Culminative Send Cost: 28537600\n",
      "tensor(2.7231, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.06397438, Culminative Send Cost: 28694400\n",
      "tensor(2.7231, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.06166649, Culminative Send Cost: 28851200\n",
      "tensor(2.7230, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.05937147, Culminative Send Cost: 29008000\n",
      "tensor(2.7229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.05708885, Culminative Send Cost: 29164800\n",
      "tensor(2.7229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.05481958, Culminative Send Cost: 29321600\n",
      "tensor(2.7229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.05256271, Culminative Send Cost: 29478400\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.05031824, Culminative Send Cost: 29635200\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.04808569, Culminative Send Cost: 29792000\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.04586601, Culminative Send Cost: 29948800\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.04365826, Culminative Send Cost: 30105600\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.04146194, Culminative Send Cost: 30262400\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.03927755, Culminative Send Cost: 30419200\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.03710461, Culminative Send Cost: 30576000\n",
      "tensor(2.7228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.03494358, Culminative Send Cost: 30732800\n",
      "tensor(2.7229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.03279352, Culminative Send Cost: 30889600\n",
      "tensor(2.7229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.03065491, Culminative Send Cost: 31046400\n",
      "tensor(2.7230, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.02852726, Culminative Send Cost: 31203200\n",
      "tensor(2.7230, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.02641153, Culminative Send Cost: 31360000\n",
      "tensor(2.7231, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.02430534, Culminative Send Cost: 31516800\n",
      "tensor(2.7232, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.02221012, Culminative Send Cost: 31673600\n",
      "tensor(2.7233, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.02012634, Culminative Send Cost: 31830400\n",
      "tensor(2.7234, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.01805258, Culminative Send Cost: 31987200\n",
      "tensor(2.7235, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.01598978, Culminative Send Cost: 32144000\n",
      "tensor(2.7236, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.01393700, Culminative Send Cost: 32300800\n",
      "tensor(2.7237, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.01189423, Culminative Send Cost: 32457600\n",
      "tensor(2.7238, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.00986147, Culminative Send Cost: 32614400\n",
      "tensor(2.7239, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.00783920, Culminative Send Cost: 32771200\n",
      "tensor(2.7241, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.00582647, Culminative Send Cost: 32928000\n",
      "tensor(2.7242, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.00382423, Culminative Send Cost: 33084800\n",
      "tensor(2.7244, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.00183153, Culminative Send Cost: 33241600\n",
      "tensor(2.7245, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 3.99984789, Culminative Send Cost: 33398400\n",
      "tensor(2.7247, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 3.99787426, Culminative Send Cost: 33555200\n",
      "tensor(2.7249, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.99591017, Culminative Send Cost: 33712000\n",
      "tensor(2.7251, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.99395561, Culminative Send Cost: 33868800\n",
      "tensor(2.7252, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.99200964, Culminative Send Cost: 34025600\n",
      "tensor(2.7254, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.99007297, Culminative Send Cost: 34182400\n",
      "tensor(2.7256, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.98814631, Culminative Send Cost: 34339200\n",
      "tensor(2.7258, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.98622823, Culminative Send Cost: 34496000\n",
      "tensor(2.7261, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.98431873, Culminative Send Cost: 34652800\n",
      "tensor(2.7263, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.98241830, Culminative Send Cost: 34809600\n",
      "tensor(2.7265, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.98052669, Culminative Send Cost: 34966400\n",
      "tensor(2.7267, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.97864366, Culminative Send Cost: 35123200\n",
      "tensor(2.7270, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.97676969, Culminative Send Cost: 35280000\n",
      "tensor(2.7272, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.97490406, Culminative Send Cost: 35436800\n",
      "tensor(2.7275, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.97304630, Culminative Send Cost: 35593600\n",
      "tensor(2.7278, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.97119737, Culminative Send Cost: 35750400\n",
      "tensor(2.7280, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.96935749, Culminative Send Cost: 35907200\n",
      "tensor(2.7283, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.96752477, Culminative Send Cost: 36064000\n",
      "tensor(2.7286, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.96570110, Culminative Send Cost: 36220800\n",
      "tensor(2.7289, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.96388531, Culminative Send Cost: 36377600\n",
      "tensor(2.7292, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.96207738, Culminative Send Cost: 36534400\n",
      "tensor(2.7295, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.96027756, Culminative Send Cost: 36691200\n",
      "tensor(2.7298, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.95848536, Culminative Send Cost: 36848000\n",
      "tensor(2.7301, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.95670128, Culminative Send Cost: 37004800\n",
      "tensor(2.7304, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.95492482, Culminative Send Cost: 37161600\n",
      "tensor(2.7307, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.95315623, Culminative Send Cost: 37318400\n",
      "tensor(2.7311, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.95139551, Culminative Send Cost: 37475200\n",
      "tensor(2.7314, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.94964194, Culminative Send Cost: 37632000\n",
      "tensor(2.7317, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.94789577, Culminative Send Cost: 37788800\n",
      "tensor(2.7321, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.94615769, Culminative Send Cost: 37945600\n",
      "tensor(2.7324, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.94442701, Culminative Send Cost: 38102400\n",
      "tensor(2.7328, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.94270325, Culminative Send Cost: 38259200\n",
      "tensor(2.7332, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.94098711, Culminative Send Cost: 38416000\n",
      "tensor(2.7335, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.93927765, Culminative Send Cost: 38572800\n",
      "tensor(2.7339, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.93757558, Culminative Send Cost: 38729600\n",
      "tensor(2.7343, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.93588114, Culminative Send Cost: 38886400\n",
      "tensor(2.7347, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.93419361, Culminative Send Cost: 39043200\n",
      "tensor(2.7351, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.93251300, Culminative Send Cost: 39200000\n",
      "activation_stack.0.weight: tensor([[-3.7134e-04,  4.0768e-03,  2.3317e-02, -2.2343e-02,  1.6495e-02,\n",
      "          3.2540e-02, -2.8453e-02,  2.8620e-02,  2.6597e-02,  7.5245e-03,\n",
      "          2.3157e-02,  2.0873e-02, -1.2846e-03, -7.9913e-03,  3.1852e-02,\n",
      "         -3.1725e-02,  2.0223e-02, -3.2978e-02,  3.2972e-02, -6.7634e-03,\n",
      "          2.0926e-02,  2.8112e-02,  2.4477e-02,  3.5391e-02,  2.2725e-02,\n",
      "          2.3132e-02,  2.1101e-02,  2.5530e-02, -3.6340e-04,  3.4465e-02,\n",
      "         -3.3373e-02, -2.5531e-02, -9.2829e-03,  3.5615e-02, -3.4944e-02,\n",
      "         -2.8026e-02,  1.6193e-02,  2.7764e-02,  1.8210e-02,  2.6469e-02,\n",
      "          1.9722e-02,  1.0926e-02, -1.7089e-02, -3.1874e-02,  3.2217e-02,\n",
      "          2.0210e-02, -2.1870e-02, -1.8686e-02,  1.8815e-02,  7.4449e-04,\n",
      "          9.3559e-03, -1.9189e-02,  9.4995e-03,  1.4765e-02,  2.3025e-02,\n",
      "          4.7662e-03,  2.8537e-02, -2.9543e-02, -2.1502e-02, -3.8978e-03,\n",
      "          3.4096e-02,  1.7953e-02, -2.3569e-02, -1.7337e-02,  2.3436e-02,\n",
      "          3.3733e-02, -1.3178e-02, -2.8761e-03,  6.3863e-02,  2.4689e-02,\n",
      "          7.4450e-02,  7.3423e-02,  6.8797e-02,  6.0163e-02,  5.9703e-02,\n",
      "          5.9771e-02,  2.1387e-02,  3.3696e-02, -1.5007e-02,  7.7945e-03,\n",
      "         -1.9532e-02, -2.9611e-02,  2.6959e-03, -2.7931e-02,  8.7709e-03,\n",
      "          3.3612e-02,  1.1618e-02, -2.8585e-02,  3.4044e-02, -1.9316e-02,\n",
      "          2.1919e-02,  2.7370e-02,  2.5576e-02,  3.3328e-02,  2.6158e-02,\n",
      "          6.0852e-02,  2.6380e-02,  4.1504e-02,  7.8391e-02,  1.1708e-01,\n",
      "          1.0796e-01,  1.3703e-01,  1.3237e-01,  1.2021e-01,  8.3880e-02,\n",
      "          4.3174e-02,  3.3188e-02,  3.3707e-02, -1.8405e-02,  1.7633e-02,\n",
      "         -7.3151e-03,  2.8226e-02,  5.2052e-03,  2.4124e-02, -1.6891e-02,\n",
      "          1.9988e-02, -2.0187e-02, -7.8677e-03, -1.7211e-02,  1.6679e-03,\n",
      "          5.2498e-03,  3.6365e-02,  5.1581e-02,  6.3230e-02,  4.8127e-02,\n",
      "          1.9893e-02,  7.7640e-03,  1.5492e-02,  5.8323e-02,  6.8905e-02,\n",
      "          7.9166e-02,  4.9493e-02,  8.8291e-02,  8.5537e-02,  3.4342e-02,\n",
      "          4.7649e-02,  4.3057e-02,  4.1288e-02,  9.2471e-03,  3.1717e-02,\n",
      "         -2.6825e-02, -3.4479e-02,  2.4781e-02,  4.1749e-03, -7.9634e-04,\n",
      "         -1.9429e-02,  4.5891e-03, -3.7128e-02,  1.2316e-02, -2.1308e-02,\n",
      "          2.1071e-02, -1.0820e-02, -5.2660e-02, -5.9991e-02, -9.3622e-02,\n",
      "         -9.6285e-02, -4.3965e-02, -3.5152e-02, -2.3026e-02,  2.7264e-03,\n",
      "         -1.0345e-02,  1.9441e-02,  4.2732e-02,  4.4702e-02, -2.7487e-03,\n",
      "          2.3897e-02, -1.6992e-02, -4.8828e-03,  3.4113e-02,  6.3117e-03,\n",
      "          2.7528e-02, -1.2686e-02, -3.1520e-02, -1.8513e-02, -7.4766e-04,\n",
      "         -5.2653e-02, -1.3710e-02, -4.1324e-02, -8.3196e-03, -2.9482e-02,\n",
      "         -2.6782e-02, -7.8044e-02, -1.1385e-02, -5.7860e-03,  7.1254e-03,\n",
      "          3.1699e-02, -6.5564e-02, -4.1569e-02, -6.3873e-02, -2.0012e-02,\n",
      "         -4.0441e-02,  5.8274e-02,  6.3441e-02,  6.0561e-02, -1.6987e-02,\n",
      "         -3.1592e-02, -1.5991e-02,  3.3025e-02,  3.7985e-02, -2.6805e-02,\n",
      "         -2.3663e-02,  4.1060e-03, -3.7157e-02, -5.5028e-03,  2.6736e-02,\n",
      "          7.7380e-03, -8.3855e-03,  4.2689e-02,  2.9308e-02,  2.6539e-02,\n",
      "          6.6919e-02,  1.5157e-01,  7.8404e-02,  3.0648e-02, -6.6213e-02,\n",
      "         -9.2930e-02, -6.1630e-02, -1.0243e-03,  2.1887e-02,  8.7850e-02,\n",
      "          8.7121e-02,  2.9961e-02,  1.0816e-02,  3.5333e-02,  2.6315e-02,\n",
      "          2.4861e-02,  9.4226e-03,  1.2740e-02,  9.3787e-03,  1.0582e-02,\n",
      "         -9.7370e-03,  9.2129e-03,  7.5164e-02,  8.8816e-02,  1.2101e-01,\n",
      "          1.0242e-01,  1.1811e-01,  1.1082e-01,  5.4799e-02,  2.1679e-02,\n",
      "          3.9248e-02, -6.4405e-02, -5.9122e-02, -7.5363e-02, -6.1618e-02,\n",
      "          4.4200e-03,  3.7642e-02,  6.7994e-02,  1.2625e-01,  6.5941e-02,\n",
      "         -5.5391e-03, -9.7528e-03,  1.3057e-02, -7.2600e-05,  2.5045e-02,\n",
      "          3.8232e-02,  3.6864e-02,  4.2967e-02,  4.3343e-02,  8.3166e-02,\n",
      "          1.4697e-01,  1.3126e-01,  1.7549e-01,  1.8000e-01,  1.8733e-01,\n",
      "          7.9606e-02, -6.9906e-02, -7.5983e-02, -4.8404e-02, -3.7233e-02,\n",
      "         -2.9245e-02, -3.0187e-02, -1.7813e-02,  2.3328e-02,  8.2209e-03,\n",
      "          4.6503e-02,  1.0579e-01,  2.8890e-02, -2.8749e-03, -1.9815e-02,\n",
      "         -1.5902e-02, -1.5104e-02,  3.3214e-02,  2.6820e-02,  1.4034e-02,\n",
      "          8.9131e-02,  1.3547e-01,  1.3210e-01,  1.4957e-01,  1.3007e-01,\n",
      "          2.1024e-01,  1.9383e-01,  1.8651e-01,  3.2507e-02, -1.0994e-01,\n",
      "         -1.3004e-01, -4.4808e-02, -1.3698e-02,  1.1226e-02, -2.5980e-02,\n",
      "         -1.8815e-02,  2.5924e-02,  1.4817e-02,  6.0810e-02,  4.8535e-02,\n",
      "          5.5004e-02,  2.9142e-02,  4.0608e-03, -2.3579e-02, -2.4417e-02,\n",
      "          1.0282e-02, -1.5397e-02,  4.6507e-02,  1.1038e-01,  1.5683e-01,\n",
      "          1.8471e-01,  1.7313e-01,  1.4770e-01,  1.8441e-01,  1.9945e-01,\n",
      "          2.3022e-01,  1.4287e-01, -9.5493e-02, -9.1975e-02,  6.2644e-03,\n",
      "          1.3532e-01,  7.7089e-02,  7.2314e-02,  1.7402e-04, -2.6999e-02,\n",
      "          7.4485e-03, -2.8729e-02,  2.0169e-02,  2.1672e-02, -5.1990e-03,\n",
      "          1.9206e-02,  1.4326e-02,  3.6894e-02,  2.8495e-02,  6.1669e-03,\n",
      "          5.9058e-02,  1.2544e-01,  7.9114e-02,  1.2838e-01,  6.3819e-02,\n",
      "          7.6044e-02,  6.3052e-02,  1.7838e-01,  2.5757e-01,  1.4861e-01,\n",
      "         -3.6977e-02,  6.1339e-02,  2.0496e-01,  2.4996e-01,  1.5482e-01,\n",
      "          9.0901e-02,  5.9705e-02,  2.6941e-02, -1.6556e-02, -4.2624e-02,\n",
      "          9.9139e-03,  1.9444e-02, -2.1837e-02,  1.2495e-03,  5.9603e-03,\n",
      "         -1.1382e-02,  3.9070e-02, -1.4946e-02,  7.5858e-02,  8.8513e-02,\n",
      "          7.6024e-02,  5.1447e-02,  3.7603e-02,  6.3794e-03,  2.9831e-02,\n",
      "          1.8730e-01,  2.6186e-01,  1.0539e-01,  3.3037e-03,  1.1671e-01,\n",
      "          2.7590e-01,  2.8663e-01,  2.0625e-01,  1.1138e-01,  3.1600e-02,\n",
      "          3.6966e-04, -4.0823e-02, -7.0169e-02, -4.4469e-02, -2.7633e-02,\n",
      "         -2.4804e-02, -1.6514e-02, -5.0748e-03, -7.3584e-03, -1.3683e-02,\n",
      "         -7.4888e-03,  1.6009e-02, -2.0069e-02, -5.8281e-02, -5.7883e-02,\n",
      "         -5.1654e-02, -7.8503e-02, -8.2068e-02,  1.3511e-01,  1.8607e-01,\n",
      "          5.1026e-02,  5.0303e-02,  1.1067e-01,  2.9871e-01,  2.6939e-01,\n",
      "          1.4744e-01,  1.1876e-02, -1.2798e-02, -4.3712e-02, -6.1831e-02,\n",
      "         -2.1091e-02, -1.1226e-03, -1.7981e-02, -3.4862e-02, -6.7281e-03,\n",
      "          3.0113e-02, -3.2744e-02, -1.9725e-02,  1.1803e-04, -8.4950e-03,\n",
      "         -4.2759e-02, -9.9046e-02, -8.9070e-02, -1.3633e-01, -8.4095e-02,\n",
      "         -1.4243e-02,  1.1629e-01,  8.9188e-02,  2.0164e-03,  4.1783e-02,\n",
      "          1.3329e-01,  2.8010e-01,  2.1319e-01, -9.3074e-03, -8.5392e-02,\n",
      "         -7.2574e-02, -6.6104e-02, -6.7950e-02, -8.9379e-03, -1.0693e-02,\n",
      "          3.5249e-02,  1.7395e-02,  1.7046e-02,  2.8116e-03,  2.5878e-02,\n",
      "         -2.8589e-02,  2.0638e-02, -3.5539e-03, -8.1585e-02, -1.0701e-01,\n",
      "         -9.4110e-02, -1.3275e-01, -3.8453e-02,  5.7484e-02,  1.5830e-01,\n",
      "          1.2203e-01,  1.6636e-02, -7.4570e-03,  1.3712e-01,  2.1334e-01,\n",
      "          1.0120e-01, -7.4979e-02, -1.0104e-01, -6.9762e-02, -3.7933e-02,\n",
      "         -3.0746e-02, -1.1409e-02,  7.6716e-03, -1.1463e-02,  8.1181e-03,\n",
      "         -2.8101e-03, -3.1740e-02, -1.4021e-02,  6.2399e-03,  1.8556e-02,\n",
      "         -1.5369e-02, -5.7535e-02, -1.3626e-01, -9.5953e-02, -7.0105e-02,\n",
      "          3.4101e-02,  1.3435e-01,  1.8828e-01,  9.5186e-02, -1.8864e-02,\n",
      "          2.8341e-02,  1.2557e-01,  1.0642e-01, -1.7707e-02, -8.8281e-02,\n",
      "         -5.6390e-02, -4.1287e-02, -7.9672e-02, -4.6801e-02, -5.5286e-02,\n",
      "         -1.2877e-02, -1.8229e-02, -1.9235e-02, -8.7326e-03, -2.2926e-02,\n",
      "          2.5807e-02, -8.3807e-03,  2.3742e-02, -3.8089e-02, -9.7656e-02,\n",
      "         -1.1241e-01, -7.7921e-02, -4.9686e-02,  4.3467e-02,  1.2916e-01,\n",
      "          1.4259e-01,  3.7934e-02, -1.0572e-01, -1.3749e-03,  2.8801e-02,\n",
      "          6.2723e-02, -4.7952e-03, -3.9539e-03, -8.8633e-03, -1.1943e-02,\n",
      "         -6.0703e-02, -4.3319e-02, -4.0233e-02, -6.9662e-02,  1.0910e-02,\n",
      "         -1.3957e-02,  2.6604e-02,  7.5893e-03, -4.0740e-04,  1.2042e-02,\n",
      "          2.9941e-02, -3.2670e-02, -6.4377e-02, -3.4782e-02, -8.8891e-02,\n",
      "         -2.9518e-02, -3.9379e-02,  6.5958e-02,  3.7829e-02, -1.2477e-02,\n",
      "         -7.2866e-02, -1.3987e-02, -1.1044e-02,  3.7898e-02, -1.5486e-02,\n",
      "         -2.4567e-02, -2.6367e-02, -6.4867e-02, -3.1749e-02, -4.2902e-02,\n",
      "         -4.1561e-02, -7.8517e-02, -5.6103e-03,  1.6449e-02, -1.3624e-02,\n",
      "         -5.5038e-03, -3.3733e-02, -2.7689e-02, -4.8677e-03, -6.8637e-02,\n",
      "         -7.8590e-02, -8.0203e-02, -2.7555e-02, -8.0181e-02, -8.7352e-02,\n",
      "         -1.2809e-02, -3.3620e-02, -4.9660e-02,  9.9656e-03,  1.4085e-02,\n",
      "         -1.8987e-02, -1.2225e-02, -2.4502e-02,  1.8426e-02,  1.9441e-02,\n",
      "         -5.0920e-02, -8.5044e-02, -8.0511e-02, -8.1309e-02, -1.4111e-02,\n",
      "          1.1485e-04, -2.8879e-02, -5.1945e-05, -1.2719e-02,  3.0343e-03,\n",
      "         -1.7516e-02, -1.1253e-02, -3.8002e-02, -5.7733e-02, -3.9567e-02,\n",
      "         -2.8127e-02, -3.6744e-02, -8.6325e-02, -8.3053e-02, -2.8761e-02,\n",
      "          5.3165e-02,  1.5953e-02,  7.3653e-03, -1.7034e-02,  5.5956e-03,\n",
      "          3.1565e-03, -1.0743e-02, -1.4004e-02, -4.5633e-02, -4.7019e-02,\n",
      "         -2.9624e-02, -8.7928e-03, -4.3679e-02, -1.1032e-02, -3.0680e-02,\n",
      "         -9.6449e-03, -1.9120e-02,  1.0585e-02, -7.7441e-03, -4.0543e-02,\n",
      "         -6.7972e-02, -2.2947e-02, -8.9333e-02, -4.2955e-02, -1.0374e-01,\n",
      "         -8.8383e-02, -6.9432e-02, -3.7098e-02,  2.3726e-03, -1.1813e-02,\n",
      "         -4.8492e-02, -4.2237e-02, -4.4397e-02, -6.6615e-03,  1.6844e-02,\n",
      "         -1.5050e-02,  2.9495e-02, -1.2455e-02, -8.0186e-03, -4.5981e-02,\n",
      "         -4.3118e-02,  1.0147e-02, -3.0770e-02, -1.3694e-02, -6.6300e-03,\n",
      "         -3.0431e-02,  2.9616e-02,  9.1390e-03, -4.3122e-02, -1.4867e-02,\n",
      "         -1.7260e-02, -4.3301e-02, -4.1763e-02, -5.3130e-02,  1.5598e-02,\n",
      "          1.7454e-02,  5.0941e-02,  4.2323e-02, -1.7950e-02, -2.0014e-02,\n",
      "         -1.6659e-02,  5.9167e-02,  5.6511e-02,  3.8240e-02,  8.6464e-02,\n",
      "          8.5399e-03,  2.1069e-02,  2.1589e-02, -2.6666e-02,  1.8855e-02,\n",
      "         -1.0168e-02, -2.3172e-02, -8.9248e-03,  3.0582e-03,  1.1584e-03,\n",
      "         -2.7066e-02, -1.8727e-02, -2.9568e-02, -6.8270e-04,  6.0585e-02,\n",
      "          7.1076e-02,  1.0922e-01,  1.4593e-01,  1.3708e-01,  1.0552e-01,\n",
      "          8.7016e-02,  8.0787e-02,  9.1902e-02,  1.4652e-01,  1.5632e-01,\n",
      "          1.5775e-01,  1.3460e-01,  6.8481e-02,  3.7034e-02,  6.4023e-03,\n",
      "          2.9925e-02, -3.4371e-02,  2.9529e-02,  1.8904e-02, -3.3946e-03,\n",
      "          7.9454e-03, -2.5319e-02, -2.1149e-02, -2.2637e-02,  6.4855e-03,\n",
      "         -1.1009e-02,  5.0537e-02,  7.8979e-02,  1.2858e-01,  1.6447e-01,\n",
      "          1.4231e-01,  1.4697e-01,  1.9875e-01,  1.7940e-01,  1.7356e-01,\n",
      "          2.2216e-01,  1.9164e-01,  1.4479e-01,  1.0075e-01,  1.1268e-01,\n",
      "          1.0428e-01,  6.3682e-02,  5.0792e-02, -1.5746e-02, -1.9763e-02,\n",
      "          7.5236e-03,  4.8714e-03,  3.4710e-02, -1.2882e-03, -5.4475e-03,\n",
      "          7.2281e-03,  1.4847e-02, -2.9319e-02,  3.4804e-02,  2.3010e-02,\n",
      "          9.7782e-03,  3.2727e-02,  7.7062e-02,  1.0024e-01,  8.7922e-02,\n",
      "          7.6173e-02,  1.0882e-01,  1.0321e-01,  1.2166e-01,  1.0349e-01,\n",
      "          9.9145e-02,  4.5590e-02,  2.5493e-02, -9.0774e-04, -5.5793e-03,\n",
      "         -9.9079e-03,  1.4875e-02, -1.7268e-02, -2.1440e-02,  7.7359e-04,\n",
      "          1.3039e-04,  2.0988e-03, -1.8096e-02, -1.7744e-02, -1.0787e-02,\n",
      "          1.9448e-02, -9.3832e-03,  1.8089e-02,  3.1080e-02,  2.2306e-02,\n",
      "         -3.0754e-02, -1.1779e-02,  3.2729e-02,  4.1062e-02,  2.8634e-02,\n",
      "          3.1159e-02, -1.2075e-02, -1.5243e-02,  4.1791e-02, -2.7941e-02,\n",
      "         -6.6922e-03, -4.4155e-03, -2.1579e-02,  5.6356e-03, -1.1934e-02,\n",
      "          3.2060e-02,  5.8400e-03, -2.2093e-02, -1.1623e-02]])\n",
      "activation_stack.0.bias: tensor([0.9516])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 39200000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The Experiment Result ===\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Experiment for num_clients\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2500\n",
    "batch_size = 1\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [1]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        print(f'Client_X_train[{client}]: {X_train_client[client]}')\n",
    "        print(f'Client_y_train[{client}]: {y_train_client[client]}')\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "        print(client_device[client].model)\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = client_weights[client]['activation_stack.0.weight']\n",
    "            send_cost = send_cost + client_weights_size.numel()\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record the loss\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot the cost history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot send cost history\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Record the history of loss and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Loss Error Rate\")\n",
    "plt.title(\"Loss Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for local_update_epochs is 1 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[ 3.4813e-02, -1.2070e-02,  2.7103e-02, -2.0954e-02, -2.9180e-03,\n",
      "          1.7424e-03,  7.9385e-04, -1.6866e-02,  5.1630e-03, -1.5287e-02,\n",
      "         -1.0681e-02, -1.9872e-02, -3.1724e-02,  4.0415e-03,  1.6608e-02,\n",
      "          2.2944e-03, -7.3472e-03, -2.6391e-03, -2.7997e-02, -2.3973e-03,\n",
      "         -2.3801e-02, -1.4203e-02,  1.5831e-02,  1.4611e-03,  1.7395e-02,\n",
      "         -5.5745e-03, -2.1995e-02,  2.3595e-02, -1.6931e-02,  2.0373e-02,\n",
      "          1.6462e-02,  6.5476e-04,  1.4350e-02, -3.2670e-02,  3.3129e-02,\n",
      "         -2.7808e-02, -1.0834e-02, -1.1754e-02,  2.9730e-02, -2.0450e-02,\n",
      "         -2.4143e-02,  4.0272e-03,  8.5327e-03, -2.0549e-03, -2.2741e-02,\n",
      "         -3.2230e-02, -7.4984e-04, -9.2642e-03,  2.6139e-02, -2.9780e-02,\n",
      "         -1.6145e-02, -3.3527e-02,  1.4942e-02, -5.2641e-03, -3.4192e-02,\n",
      "         -1.4756e-02, -2.3073e-02, -2.2974e-02,  1.8328e-02, -8.1352e-03,\n",
      "         -1.2385e-02, -3.4741e-02, -3.2484e-02, -3.7224e-03, -1.0493e-02,\n",
      "         -6.9448e-03, -1.6837e-02, -7.9526e-03, -3.2740e-02,  1.9394e-02,\n",
      "         -2.8536e-02,  3.4944e-02, -3.3448e-02, -2.1431e-02, -3.1180e-02,\n",
      "          1.1977e-02,  3.4828e-03, -2.0784e-02, -2.0673e-02,  2.6465e-02,\n",
      "         -2.7967e-03,  2.0195e-02,  2.4499e-02,  7.1801e-03,  1.7700e-02,\n",
      "         -6.6616e-03, -3.3046e-02, -2.8277e-02,  2.0918e-02, -6.1441e-03,\n",
      "         -3.2985e-02,  6.5549e-03, -2.2172e-02,  2.5602e-02, -2.0613e-02,\n",
      "          1.1507e-02, -1.4733e-02, -2.7263e-03,  1.8076e-02, -2.6588e-02,\n",
      "          2.6423e-02, -7.2952e-05, -2.3283e-02, -2.8231e-02, -3.2872e-02,\n",
      "          2.9248e-02,  2.9317e-02, -2.9167e-02, -2.7608e-02,  1.0098e-02,\n",
      "         -4.5684e-03,  4.9287e-04, -2.0186e-02, -1.4465e-02, -1.6780e-02,\n",
      "         -2.0806e-02, -3.3061e-02, -1.6690e-02,  1.4820e-02,  2.1615e-02,\n",
      "          1.5706e-03,  1.8943e-02, -1.3370e-03, -6.1346e-03,  3.3881e-02,\n",
      "          2.0155e-03, -1.0509e-02, -3.0791e-02,  9.8955e-03,  1.9876e-02,\n",
      "          1.7946e-02,  3.1987e-02, -5.8756e-03,  2.4044e-02, -1.5817e-02,\n",
      "          3.1272e-02, -3.3206e-02, -2.6155e-02,  5.2350e-03,  5.8508e-03,\n",
      "          5.9542e-03,  6.1119e-03,  3.4339e-02,  1.4339e-02, -1.5458e-02,\n",
      "          2.9322e-02,  3.2712e-02, -1.5104e-02, -1.1657e-02,  2.6080e-02,\n",
      "         -6.8131e-03,  1.3603e-02,  1.7525e-03,  1.7196e-02,  1.8071e-02,\n",
      "         -1.0032e-03,  2.1033e-02, -5.1449e-03,  2.8746e-03,  1.8180e-02,\n",
      "          2.1266e-03, -2.0798e-02,  3.5339e-02, -1.4501e-02,  8.4265e-03,\n",
      "         -7.0102e-03, -8.6359e-03,  2.9875e-02,  1.3041e-02, -1.5137e-02,\n",
      "         -2.8942e-02,  1.8395e-02,  1.3492e-02, -1.5626e-02, -2.2562e-03,\n",
      "          9.5546e-03, -4.7109e-03, -2.9925e-02,  3.4639e-02, -2.6203e-02,\n",
      "          2.3390e-02, -1.1222e-02, -2.2171e-02,  1.3169e-02,  3.0295e-02,\n",
      "         -9.7220e-03, -2.9823e-02, -8.0816e-03,  2.0275e-02, -5.5926e-03,\n",
      "          1.9853e-02,  6.2440e-04, -2.6464e-02, -1.0985e-02, -8.7225e-03,\n",
      "          1.0579e-02, -4.5295e-03, -1.1722e-02, -3.2059e-02,  2.4299e-02,\n",
      "          2.0701e-02, -4.4329e-03, -2.5304e-02,  1.4151e-03,  3.0717e-02,\n",
      "         -2.1695e-02,  1.7245e-02,  1.1304e-02, -3.9664e-04,  2.3472e-02,\n",
      "          1.2338e-02,  3.5631e-02, -1.3564e-02,  3.5114e-02,  9.2792e-03,\n",
      "         -1.2420e-02,  1.8238e-03,  2.9534e-02,  1.8858e-02,  1.4065e-02,\n",
      "          2.8133e-02,  6.0570e-03,  1.2401e-02,  1.6445e-02,  1.2926e-02,\n",
      "          2.9969e-02,  9.9567e-03, -4.0652e-03,  3.2663e-02,  1.0751e-02,\n",
      "          2.2244e-03, -3.3048e-02, -1.1232e-02,  2.5205e-02, -3.3673e-02,\n",
      "         -2.3883e-02, -1.1941e-02, -3.1689e-02,  1.7746e-02,  1.1525e-02,\n",
      "          2.0307e-02,  2.3318e-02, -9.3297e-03,  2.2940e-02, -3.1645e-02,\n",
      "         -1.2994e-02,  2.1013e-02, -2.5759e-03,  2.7874e-02, -3.1928e-02,\n",
      "         -9.8187e-03,  3.2171e-02,  1.5950e-02, -6.0802e-03, -2.5019e-02,\n",
      "          1.9126e-02, -1.6919e-02, -2.7283e-02, -7.1548e-03,  2.2451e-02,\n",
      "          2.9361e-02,  1.3708e-02,  4.4237e-03,  6.2493e-03,  2.2740e-02,\n",
      "          2.5045e-04, -1.8642e-02, -8.9809e-03, -2.8836e-02,  3.1780e-02,\n",
      "          2.8652e-02,  1.4068e-02, -2.5320e-02, -3.4598e-02, -3.2778e-02,\n",
      "          3.2014e-02,  2.0063e-02, -1.1337e-02, -1.6727e-02, -2.2359e-02,\n",
      "          2.7551e-02, -1.8105e-02, -2.7617e-02,  1.0142e-02, -9.4326e-03,\n",
      "          2.7933e-02,  1.4891e-02, -9.8825e-03,  1.1230e-02, -1.3340e-02,\n",
      "         -8.4802e-04, -2.2964e-02, -2.0564e-02, -2.7364e-02,  2.9614e-02,\n",
      "         -2.2919e-02,  3.4638e-02, -2.1570e-02,  2.1213e-02,  3.2237e-02,\n",
      "          2.8041e-03,  1.8956e-02,  8.2385e-03, -1.8266e-02, -1.5269e-02,\n",
      "         -3.2668e-02,  2.3244e-03,  3.3616e-02, -2.5255e-02, -9.6496e-03,\n",
      "          2.5417e-02, -2.5554e-02, -3.3938e-02,  3.6714e-03,  2.9251e-02,\n",
      "         -3.3959e-02,  2.1557e-02,  2.9975e-02,  1.9771e-02, -2.6797e-02,\n",
      "          1.0459e-02,  3.1721e-02, -2.8080e-02,  2.1612e-02,  1.0386e-02,\n",
      "         -2.4983e-02, -2.7556e-02,  2.9488e-02,  2.8769e-02, -2.5328e-02,\n",
      "          1.1131e-02,  2.8959e-02,  3.0828e-02, -2.7607e-02, -2.3083e-02,\n",
      "          1.8149e-02, -8.4503e-04,  1.0518e-02,  3.0134e-02, -3.1816e-02,\n",
      "         -3.2255e-02,  1.3354e-02, -2.8908e-02,  6.7730e-05, -2.1680e-02,\n",
      "          1.3187e-02,  2.0111e-02,  3.3393e-02, -3.3105e-02, -2.8163e-02,\n",
      "          2.7866e-02,  1.3749e-02, -8.3275e-03,  1.7551e-02,  2.3763e-02,\n",
      "         -1.0138e-02, -3.5549e-02, -7.9541e-03,  1.1921e-02, -3.0944e-02,\n",
      "          1.1888e-02, -3.2478e-02, -2.0086e-02, -2.0790e-04,  3.1835e-02,\n",
      "          1.6072e-02, -1.4957e-02, -1.9677e-02, -1.0526e-02,  6.0216e-03,\n",
      "         -2.8069e-02, -2.1260e-02, -1.9830e-02,  3.0085e-04, -2.5002e-02,\n",
      "         -3.2349e-02,  1.8325e-02,  9.2478e-03, -1.9365e-03,  9.7911e-03,\n",
      "          3.5410e-02,  1.6717e-02,  2.5944e-02,  1.7259e-02,  3.3024e-02,\n",
      "         -1.8690e-02,  1.7374e-02,  3.1622e-02, -3.0208e-03,  1.0755e-02,\n",
      "         -1.8962e-02, -2.6809e-02, -2.0419e-02,  1.2730e-02, -3.1685e-03,\n",
      "          2.0037e-02, -2.9475e-02,  5.4905e-04,  6.6883e-03, -1.3193e-02,\n",
      "         -3.5313e-02,  1.4371e-02,  2.4015e-02, -1.5698e-02,  8.4504e-03,\n",
      "          1.7193e-02, -3.2643e-04, -3.4019e-02,  3.8198e-04,  4.9449e-03,\n",
      "          2.6251e-02, -1.4793e-02, -1.9545e-02,  2.4516e-02, -2.7726e-02,\n",
      "         -1.5151e-02,  2.8896e-02,  3.5166e-02,  2.2527e-02,  2.1886e-02,\n",
      "          3.1788e-02,  2.6872e-02, -2.2026e-02, -1.8988e-02, -2.7976e-02,\n",
      "         -1.9602e-02, -1.6378e-03,  2.0395e-02,  1.5220e-02, -1.1655e-02,\n",
      "         -6.0864e-04, -8.0378e-03, -2.1824e-02, -1.6160e-02, -1.8239e-03,\n",
      "          1.8011e-02, -1.8677e-02, -3.6320e-03,  1.2158e-02,  2.4481e-02,\n",
      "          1.9782e-02,  1.6772e-02,  8.6536e-03,  3.4423e-02,  2.3676e-02,\n",
      "         -1.6553e-02,  2.0381e-02,  2.4424e-02,  2.3277e-02, -4.7972e-03,\n",
      "         -2.9128e-03, -1.9552e-02,  6.3628e-03, -1.1142e-02,  3.3852e-02,\n",
      "         -5.1074e-03, -8.8821e-03,  2.6178e-02, -3.2762e-02, -9.4779e-03,\n",
      "         -1.4135e-02, -8.3377e-03,  2.0493e-03, -2.8365e-02,  2.7761e-02,\n",
      "         -1.6547e-02, -3.4779e-02, -2.3583e-02, -8.1850e-03,  2.6302e-02,\n",
      "          1.5093e-02,  9.7901e-04, -1.9298e-02, -8.3337e-04, -3.1081e-02,\n",
      "          2.2018e-02, -2.2399e-02,  2.3237e-02,  2.1887e-03, -8.8831e-03,\n",
      "         -1.8499e-02,  1.8081e-02,  3.1077e-03,  8.5513e-03,  4.2027e-04,\n",
      "         -2.8064e-02,  1.7684e-02, -3.2794e-02,  2.5649e-02, -4.2453e-03,\n",
      "         -9.3961e-03, -3.1937e-02, -2.8464e-02,  2.8752e-02,  1.5938e-02,\n",
      "          2.3521e-02,  3.0382e-02,  3.1154e-02, -2.9030e-02,  1.8959e-02,\n",
      "          9.5070e-03,  8.4093e-03,  3.3517e-02,  8.2889e-03,  1.7419e-02,\n",
      "         -3.2178e-02, -2.7134e-02, -1.7497e-02, -4.4006e-03,  8.3203e-03,\n",
      "          1.7354e-02,  1.5736e-02,  2.1651e-02, -9.8758e-03,  1.1891e-02,\n",
      "         -1.5546e-02, -2.5447e-02,  2.4059e-03, -2.7698e-03,  8.4480e-03,\n",
      "         -1.0468e-02,  6.7626e-03,  1.2444e-03,  1.4957e-02, -2.8190e-02,\n",
      "          1.1170e-02, -6.2816e-03, -2.9070e-02, -2.3227e-02,  3.3348e-02,\n",
      "          1.7525e-02, -1.4889e-02,  2.3663e-02, -6.8757e-03,  6.7446e-03,\n",
      "         -8.1824e-03, -3.4621e-02, -6.6025e-03, -1.2420e-02,  1.2321e-02,\n",
      "          2.9726e-02, -2.8729e-02, -9.1256e-04,  2.0146e-02,  1.3341e-02,\n",
      "         -6.7785e-03,  2.9258e-02, -2.7546e-02, -2.9048e-02, -5.5332e-03,\n",
      "          1.7659e-02,  3.0365e-03, -2.2997e-02, -7.4359e-03,  4.1042e-03,\n",
      "          2.1750e-02, -1.3393e-02, -1.7691e-02, -2.7031e-02,  1.8283e-02,\n",
      "          4.5534e-03, -1.1726e-02, -3.1457e-02,  1.6499e-02,  1.3513e-02,\n",
      "         -1.5437e-02,  1.3784e-02,  1.6264e-02, -5.9536e-03,  1.7263e-02,\n",
      "         -8.8286e-03,  3.3354e-02, -6.6648e-03,  3.0997e-02, -9.8226e-03,\n",
      "          2.1263e-02, -2.1805e-02,  8.3765e-03,  2.5235e-02,  7.1708e-03,\n",
      "         -9.1492e-03,  9.1000e-04, -2.2390e-02,  3.0775e-02,  3.3807e-02,\n",
      "         -2.9985e-02, -7.6882e-03, -2.4684e-02, -2.6507e-02,  3.5198e-02,\n",
      "          2.6671e-02, -2.0702e-03,  4.1077e-03, -1.8718e-02, -1.2998e-02,\n",
      "          2.3273e-02, -3.0699e-02, -2.1101e-03,  1.7139e-03, -3.6752e-04,\n",
      "         -2.5433e-02, -2.8820e-02, -2.9653e-02, -5.9185e-03, -2.7259e-02,\n",
      "         -1.5386e-02,  2.7379e-02, -2.4303e-02, -1.3072e-02, -2.9362e-02,\n",
      "         -4.7753e-03, -4.7385e-03,  3.3663e-02, -2.7963e-02,  3.2312e-02,\n",
      "          3.4087e-02,  2.4714e-02, -2.6782e-03, -2.0363e-02,  1.8798e-02,\n",
      "          1.5901e-02,  1.5857e-02,  2.4602e-04,  3.2866e-02, -2.6825e-02,\n",
      "         -3.6066e-03,  2.0602e-02, -1.2077e-02, -2.5959e-02,  1.6239e-03,\n",
      "         -2.4667e-03, -3.6224e-03, -3.0739e-02, -1.8866e-02, -4.0819e-03,\n",
      "          1.3331e-02, -7.8019e-03, -2.1028e-02,  3.3322e-02,  3.0262e-02,\n",
      "          3.3942e-02,  3.1067e-02,  1.1115e-02, -3.5476e-02,  2.8123e-02,\n",
      "          1.0511e-02,  3.2240e-02,  2.8502e-02, -5.4314e-03, -2.1109e-02,\n",
      "          3.0699e-02, -1.4220e-02, -2.6618e-03,  1.4678e-02,  3.2062e-02,\n",
      "         -2.7287e-02, -2.4039e-02,  2.6014e-02,  2.0671e-02, -5.4934e-03,\n",
      "         -1.9697e-02,  2.3827e-03, -3.5477e-02, -3.1744e-03,  2.3816e-02,\n",
      "          9.8759e-03,  1.9811e-02, -3.5480e-02, -1.7981e-02, -1.1152e-02,\n",
      "         -1.0819e-02, -1.8655e-02,  3.2633e-03, -2.3363e-02, -7.0780e-03,\n",
      "         -2.7582e-02,  5.8081e-03,  2.7079e-03, -7.8626e-03,  2.3543e-02,\n",
      "          1.8189e-02, -2.5969e-03, -2.4152e-02,  2.4306e-02,  1.0086e-02,\n",
      "          1.9405e-02, -9.4420e-03, -1.6832e-02,  5.9958e-03, -2.6583e-02,\n",
      "         -8.8366e-03, -1.6074e-02,  3.4768e-02,  3.4406e-02, -5.8525e-03,\n",
      "          2.4977e-02, -6.3456e-03,  9.7668e-03, -1.1738e-02,  2.0772e-02,\n",
      "          1.5845e-02,  3.5105e-02, -2.5524e-02,  3.1602e-02,  1.4939e-02,\n",
      "          8.9530e-03, -6.5434e-03,  1.9342e-02,  3.0700e-03,  1.2144e-02,\n",
      "          1.6746e-02, -1.1873e-02, -2.2448e-02,  2.3250e-02,  3.6882e-03,\n",
      "         -3.3583e-02,  1.3798e-03, -2.9617e-02, -7.3383e-03,  1.0212e-03,\n",
      "          1.3944e-02,  7.4319e-03,  1.6491e-02,  1.9425e-02,  8.0343e-03,\n",
      "          2.1043e-02,  2.9184e-02, -9.0173e-03,  3.3174e-02,  8.2652e-03,\n",
      "          1.2418e-02, -3.0745e-02, -2.9852e-02,  1.7728e-02,  1.0772e-02,\n",
      "         -1.5923e-04, -3.5006e-02, -1.6268e-02, -2.8662e-02, -1.1619e-02,\n",
      "          8.0293e-03, -2.8925e-02, -1.0674e-02, -3.1765e-02, -2.7701e-02,\n",
      "          1.7134e-04,  3.0311e-04,  2.6321e-02,  1.3528e-02, -3.1344e-02,\n",
      "          1.9222e-02, -1.2535e-02,  2.7305e-02,  2.5253e-03,  6.3117e-03,\n",
      "         -1.8497e-02, -1.7664e-02,  1.6916e-03,  2.6351e-02, -2.7523e-02,\n",
      "          2.3572e-02, -2.5993e-02, -3.2255e-02,  2.7272e-02,  2.6686e-02,\n",
      "         -3.4060e-02,  2.4467e-02, -1.3300e-02, -2.0746e-02,  1.6549e-02,\n",
      "         -7.3885e-03,  2.9425e-02,  2.8277e-02,  3.3154e-02, -1.5183e-02,\n",
      "          7.0093e-03, -8.7752e-03,  3.3658e-02,  3.4346e-02, -1.2466e-02,\n",
      "         -2.1450e-02,  2.4592e-02,  7.8287e-03, -1.0718e-02]])), ('activation_stack.0.bias', tensor([0.0082]))])\n",
      "tensor(0.1066, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 24.99023247, Culminative Send Cost: 3920\n",
      "tensor(1.1998, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 12.53748035, Culminative Send Cost: 39200\n",
      "tensor(1.7778, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 9.15357494, Culminative Send Cost: 78400\n",
      "tensor(2.0487, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 8.14670658, Culminative Send Cost: 117600\n",
      "tensor(2.1833, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 7.65135860, Culminative Send Cost: 156800\n",
      "tensor(2.2567, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 7.29604959, Culminative Send Cost: 196000\n",
      "tensor(2.3025, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 7.00240755, Culminative Send Cost: 235200\n",
      "tensor(2.3354, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 6.75021029, Culminative Send Cost: 274400\n",
      "tensor(2.3619, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 6.53132010, Culminative Send Cost: 313600\n",
      "tensor(2.3851, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 6.34057331, Culminative Send Cost: 352800\n",
      "tensor(2.4060, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 6.17390585, Culminative Send Cost: 392000\n",
      "tensor(2.4255, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 6.02791071, Culminative Send Cost: 431200\n",
      "tensor(2.4438, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.89969397, Culminative Send Cost: 470400\n",
      "tensor(2.4610, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.78677702, Culminative Send Cost: 509600\n",
      "tensor(2.4772, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 5.68704176, Culminative Send Cost: 548800\n",
      "tensor(2.4925, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 5.59867716, Culminative Send Cost: 588000\n",
      "tensor(2.5069, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 5.52012920, Culminative Send Cost: 627200\n",
      "tensor(2.5204, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 5.45006800, Culminative Send Cost: 666400\n",
      "tensor(2.5332, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 5.38735199, Culminative Send Cost: 705600\n",
      "tensor(2.5452, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 5.33100367, Culminative Send Cost: 744800\n",
      "tensor(2.5566, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 5.28018141, Culminative Send Cost: 784000\n",
      "tensor(2.5672, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 5.23416519, Culminative Send Cost: 823200\n",
      "tensor(2.5771, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 5.19233418, Culminative Send Cost: 862400\n",
      "tensor(2.5865, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 5.15415382, Culminative Send Cost: 901600\n",
      "tensor(2.5952, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 5.11916637, Culminative Send Cost: 940800\n",
      "tensor(2.6034, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 5.08697557, Culminative Send Cost: 980000\n",
      "tensor(2.6111, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 5.05724049, Culminative Send Cost: 1019200\n",
      "tensor(2.6183, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 5.02966595, Culminative Send Cost: 1058400\n",
      "tensor(2.6250, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 5.00399828, Culminative Send Cost: 1097600\n",
      "tensor(2.6312, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.98001766, Culminative Send Cost: 1136800\n",
      "tensor(2.6370, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.95753241, Culminative Send Cost: 1176000\n",
      "tensor(2.6424, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.93637800, Culminative Send Cost: 1215200\n",
      "tensor(2.6474, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.91641092, Culminative Send Cost: 1254400\n",
      "tensor(2.6521, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.89750719, Culminative Send Cost: 1293600\n",
      "tensor(2.6564, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.87955809, Culminative Send Cost: 1332800\n",
      "tensor(2.6604, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.86246729, Culminative Send Cost: 1372000\n",
      "tensor(2.6640, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.84615421, Culminative Send Cost: 1411200\n",
      "tensor(2.6674, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.83054590, Culminative Send Cost: 1450400\n",
      "tensor(2.6706, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.81557846, Culminative Send Cost: 1489600\n",
      "tensor(2.6734, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.80119658, Culminative Send Cost: 1528800\n",
      "tensor(2.6761, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.78735018, Culminative Send Cost: 1568000\n",
      "tensor(2.6785, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.77399778, Culminative Send Cost: 1607200\n",
      "tensor(2.6806, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.76110077, Culminative Send Cost: 1646400\n",
      "tensor(2.6826, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.74862385, Culminative Send Cost: 1685600\n",
      "tensor(2.6844, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.73653793, Culminative Send Cost: 1724800\n",
      "tensor(2.6860, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.72481585, Culminative Send Cost: 1764000\n",
      "tensor(2.6875, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.71343231, Culminative Send Cost: 1803200\n",
      "tensor(2.6888, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.70236826, Culminative Send Cost: 1842400\n",
      "tensor(2.6899, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.69160318, Culminative Send Cost: 1881600\n",
      "tensor(2.6909, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.68111801, Culminative Send Cost: 1920800\n",
      "tensor(2.6918, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.67089939, Culminative Send Cost: 1960000\n",
      "tensor(2.6925, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.66093159, Culminative Send Cost: 1999200\n",
      "tensor(2.6932, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.65120220, Culminative Send Cost: 2038400\n",
      "tensor(2.6937, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.64169836, Culminative Send Cost: 2077600\n",
      "tensor(2.6941, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.63241053, Culminative Send Cost: 2116800\n",
      "tensor(2.6945, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.62332726, Culminative Send Cost: 2156000\n",
      "tensor(2.6947, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.61444044, Culminative Send Cost: 2195200\n",
      "tensor(2.6949, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.60574102, Culminative Send Cost: 2234400\n",
      "tensor(2.6950, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.59722137, Culminative Send Cost: 2273600\n",
      "tensor(2.6950, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.58887434, Culminative Send Cost: 2312800\n",
      "tensor(2.6949, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.58069229, Culminative Send Cost: 2352000\n",
      "tensor(2.6948, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.57266951, Culminative Send Cost: 2391200\n",
      "tensor(2.6947, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.56479979, Culminative Send Cost: 2430400\n",
      "tensor(2.6944, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.55707788, Culminative Send Cost: 2469600\n",
      "tensor(2.6942, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.54949808, Culminative Send Cost: 2508800\n",
      "tensor(2.6938, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.54205561, Culminative Send Cost: 2548000\n",
      "tensor(2.6935, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.53474617, Culminative Send Cost: 2587200\n",
      "tensor(2.6931, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.52756453, Culminative Send Cost: 2626400\n",
      "tensor(2.6926, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.52050686, Culminative Send Cost: 2665600\n",
      "tensor(2.6922, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.51356983, Culminative Send Cost: 2704800\n",
      "tensor(2.6917, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.50674820, Culminative Send Cost: 2744000\n",
      "tensor(2.6911, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.50003958, Culminative Send Cost: 2783200\n",
      "tensor(2.6906, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.49344015, Culminative Send Cost: 2822400\n",
      "tensor(2.6900, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.48694706, Culminative Send Cost: 2861600\n",
      "tensor(2.6894, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.48055601, Culminative Send Cost: 2900800\n",
      "tensor(2.6888, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.47426558, Culminative Send Cost: 2940000\n",
      "tensor(2.6881, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.46807146, Culminative Send Cost: 2979200\n",
      "tensor(2.6875, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.46197176, Culminative Send Cost: 3018400\n",
      "tensor(2.6868, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.45596409, Culminative Send Cost: 3057600\n",
      "tensor(2.6861, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.45004463, Culminative Send Cost: 3096800\n",
      "tensor(2.6854, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.44421244, Culminative Send Cost: 3136000\n",
      "tensor(2.6847, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.43846369, Culminative Send Cost: 3175200\n",
      "tensor(2.6840, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.43279743, Culminative Send Cost: 3214400\n",
      "tensor(2.6833, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.42721081, Culminative Send Cost: 3253600\n",
      "tensor(2.6826, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.42170191, Culminative Send Cost: 3292800\n",
      "tensor(2.6818, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.41626883, Culminative Send Cost: 3332000\n",
      "tensor(2.6811, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.41090918, Culminative Send Cost: 3371200\n",
      "tensor(2.6804, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.40562153, Culminative Send Cost: 3410400\n",
      "tensor(2.6796, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.40040398, Culminative Send Cost: 3449600\n",
      "tensor(2.6789, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.39525509, Culminative Send Cost: 3488800\n",
      "tensor(2.6781, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.39017200, Culminative Send Cost: 3528000\n",
      "tensor(2.6774, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.38515472, Culminative Send Cost: 3567200\n",
      "tensor(2.6767, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.38020086, Culminative Send Cost: 3606400\n",
      "tensor(2.6759, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.37530804, Culminative Send Cost: 3645600\n",
      "tensor(2.6752, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.37047625, Culminative Send Cost: 3684800\n",
      "tensor(2.6744, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.36570311, Culminative Send Cost: 3724000\n",
      "tensor(2.6737, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.36098814, Culminative Send Cost: 3763200\n",
      "tensor(2.6730, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.35632896, Culminative Send Cost: 3802400\n",
      "tensor(2.6722, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.35172462, Culminative Send Cost: 3841600\n",
      "tensor(2.6715, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.34717369, Culminative Send Cost: 3880800\n",
      "tensor(2.6708, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.34267569, Culminative Send Cost: 3920000\n",
      "tensor(2.6701, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.33822918, Culminative Send Cost: 3959200\n",
      "tensor(2.6694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.33383226, Culminative Send Cost: 3998400\n",
      "tensor(2.6687, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.32948494, Culminative Send Cost: 4037600\n",
      "tensor(2.6680, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.32518530, Culminative Send Cost: 4076800\n",
      "tensor(2.6673, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.32093191, Culminative Send Cost: 4116000\n",
      "tensor(2.6666, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.31672525, Culminative Send Cost: 4155200\n",
      "tensor(2.6659, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.31256294, Culminative Send Cost: 4194400\n",
      "tensor(2.6653, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.30844450, Culminative Send Cost: 4233600\n",
      "tensor(2.6646, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 4.30436945, Culminative Send Cost: 4272800\n",
      "tensor(2.6640, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 4.30033684, Culminative Send Cost: 4312000\n",
      "tensor(2.6633, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 4.29634476, Culminative Send Cost: 4351200\n",
      "tensor(2.6627, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 4.29239321, Culminative Send Cost: 4390400\n",
      "tensor(2.6621, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 4.28848124, Culminative Send Cost: 4429600\n",
      "tensor(2.6614, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 4.28460836, Culminative Send Cost: 4468800\n",
      "tensor(2.6608, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 4.28077269, Culminative Send Cost: 4508000\n",
      "tensor(2.6602, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 4.27697468, Culminative Send Cost: 4547200\n",
      "tensor(2.6596, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 4.27321291, Culminative Send Cost: 4586400\n",
      "tensor(2.6591, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 4.26948738, Culminative Send Cost: 4625600\n",
      "tensor(2.6585, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 4.26579618, Culminative Send Cost: 4664800\n",
      "tensor(2.6579, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 4.26213980, Culminative Send Cost: 4704000\n",
      "tensor(2.6574, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 4.25851727, Culminative Send Cost: 4743200\n",
      "tensor(2.6568, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 4.25492716, Culminative Send Cost: 4782400\n",
      "tensor(2.6563, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 4.25136995, Culminative Send Cost: 4821600\n",
      "tensor(2.6558, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 4.24784422, Culminative Send Cost: 4860800\n",
      "tensor(2.6553, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 4.24434996, Culminative Send Cost: 4900000\n",
      "tensor(2.6547, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 4.24088621, Culminative Send Cost: 4939200\n",
      "tensor(2.6543, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 4.23745298, Culminative Send Cost: 4978400\n",
      "tensor(2.6538, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 4.23404837, Culminative Send Cost: 5017600\n",
      "tensor(2.6533, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 4.23067331, Culminative Send Cost: 5056800\n",
      "tensor(2.6528, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 4.22732639, Culminative Send Cost: 5096000\n",
      "tensor(2.6524, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 4.22400808, Culminative Send Cost: 5135200\n",
      "tensor(2.6519, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 4.22071600, Culminative Send Cost: 5174400\n",
      "tensor(2.6515, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 4.21745253, Culminative Send Cost: 5213600\n",
      "tensor(2.6511, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 4.21421432, Culminative Send Cost: 5252800\n",
      "tensor(2.6506, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 4.21100330, Culminative Send Cost: 5292000\n",
      "tensor(2.6502, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 4.20781660, Culminative Send Cost: 5331200\n",
      "tensor(2.6498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 4.20465565, Culminative Send Cost: 5370400\n",
      "tensor(2.6495, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 4.20151997, Culminative Send Cost: 5409600\n",
      "tensor(2.6491, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 4.19840765, Culminative Send Cost: 5448800\n",
      "tensor(2.6487, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 4.19531965, Culminative Send Cost: 5488000\n",
      "tensor(2.6484, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 4.19225550, Culminative Send Cost: 5527200\n",
      "tensor(2.6480, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 4.18921375, Culminative Send Cost: 5566400\n",
      "tensor(2.6477, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 4.18619537, Culminative Send Cost: 5605600\n",
      "tensor(2.6473, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 4.18319941, Culminative Send Cost: 5644800\n",
      "tensor(2.6470, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 4.18022490, Culminative Send Cost: 5684000\n",
      "tensor(2.6467, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 4.17727280, Culminative Send Cost: 5723200\n",
      "tensor(2.6464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 4.17434168, Culminative Send Cost: 5762400\n",
      "tensor(2.6461, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 4.17143202, Culminative Send Cost: 5801600\n",
      "tensor(2.6459, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 4.16854286, Culminative Send Cost: 5840800\n",
      "tensor(2.6456, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 4.16567421, Culminative Send Cost: 5880000\n",
      "tensor(2.6453, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 4.16282463, Culminative Send Cost: 5919200\n",
      "tensor(2.6451, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 4.15999603, Culminative Send Cost: 5958400\n",
      "tensor(2.6448, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 4.15718603, Culminative Send Cost: 5997600\n",
      "tensor(2.6446, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 4.15439558, Culminative Send Cost: 6036800\n",
      "tensor(2.6444, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 4.15162468, Culminative Send Cost: 6076000\n",
      "tensor(2.6442, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 4.14887142, Culminative Send Cost: 6115200\n",
      "tensor(2.6440, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 4.14613676, Culminative Send Cost: 6154400\n",
      "tensor(2.6438, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 4.14342070, Culminative Send Cost: 6193600\n",
      "tensor(2.6436, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 4.14072227, Culminative Send Cost: 6232800\n",
      "tensor(2.6434, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 4.13804102, Culminative Send Cost: 6272000\n",
      "tensor(2.6433, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 4.13537741, Culminative Send Cost: 6311200\n",
      "tensor(2.6431, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 4.13273048, Culminative Send Cost: 6350400\n",
      "tensor(2.6430, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 4.13010120, Culminative Send Cost: 6389600\n",
      "tensor(2.6428, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 4.12748814, Culminative Send Cost: 6428800\n",
      "tensor(2.6427, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 4.12489080, Culminative Send Cost: 6468000\n",
      "tensor(2.6426, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 4.12231064, Culminative Send Cost: 6507200\n",
      "tensor(2.6425, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 4.11974621, Culminative Send Cost: 6546400\n",
      "tensor(2.6424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 4.11719751, Culminative Send Cost: 6585600\n",
      "tensor(2.6423, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 4.11466408, Culminative Send Cost: 6624800\n",
      "tensor(2.6422, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 4.11214542, Culminative Send Cost: 6664000\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 4.10964346, Culminative Send Cost: 6703200\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 4.10715532, Culminative Send Cost: 6742400\n",
      "tensor(2.6420, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 4.10468197, Culminative Send Cost: 6781600\n",
      "tensor(2.6420, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 4.10222340, Culminative Send Cost: 6820800\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 4.09977961, Culminative Send Cost: 6860000\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 4.09734917, Culminative Send Cost: 6899200\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 4.09493303, Culminative Send Cost: 6938400\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 4.09253120, Culminative Send Cost: 6977600\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 4.09014273, Culminative Send Cost: 7016800\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 4.08776760, Culminative Send Cost: 7056000\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 4.08540583, Culminative Send Cost: 7095200\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 4.08305788, Culminative Send Cost: 7134400\n",
      "tensor(2.6419, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 4.08072186, Culminative Send Cost: 7173600\n",
      "tensor(2.6420, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 4.07839918, Culminative Send Cost: 7212800\n",
      "tensor(2.6420, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 4.07608986, Culminative Send Cost: 7252000\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 4.07379246, Culminative Send Cost: 7291200\n",
      "tensor(2.6421, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 4.07150793, Culminative Send Cost: 7330400\n",
      "tensor(2.6422, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 4.06923580, Culminative Send Cost: 7369600\n",
      "tensor(2.6423, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 4.06697512, Culminative Send Cost: 7408800\n",
      "tensor(2.6424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 4.06472683, Culminative Send Cost: 7448000\n",
      "tensor(2.6424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 4.06249046, Culminative Send Cost: 7487200\n",
      "tensor(2.6425, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 4.06026602, Culminative Send Cost: 7526400\n",
      "tensor(2.6427, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 4.05805302, Culminative Send Cost: 7565600\n",
      "tensor(2.6428, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 4.05585194, Culminative Send Cost: 7604800\n",
      "tensor(2.6429, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 4.05366182, Culminative Send Cost: 7644000\n",
      "tensor(2.6430, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 4.05148268, Culminative Send Cost: 7683200\n",
      "tensor(2.6432, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 4.04931545, Culminative Send Cost: 7722400\n",
      "tensor(2.6433, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 4.04715872, Culminative Send Cost: 7761600\n",
      "tensor(2.6435, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 4.04501295, Culminative Send Cost: 7800800\n",
      "tensor(2.6436, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 4.04287767, Culminative Send Cost: 7840000\n",
      "tensor(2.6438, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 4.04075432, Culminative Send Cost: 7879200\n",
      "tensor(2.6440, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 4.03864050, Culminative Send Cost: 7918400\n",
      "tensor(2.6442, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 4.03653717, Culminative Send Cost: 7957600\n",
      "tensor(2.6443, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 4.03444386, Culminative Send Cost: 7996800\n",
      "tensor(2.6445, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 4.03236151, Culminative Send Cost: 8036000\n",
      "tensor(2.6447, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 4.03028917, Culminative Send Cost: 8075200\n",
      "tensor(2.6449, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 4.02822733, Culminative Send Cost: 8114400\n",
      "tensor(2.6452, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 4.02617502, Culminative Send Cost: 8153600\n",
      "tensor(2.6454, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 4.02413177, Culminative Send Cost: 8192800\n",
      "tensor(2.6456, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 4.02209902, Culminative Send Cost: 8232000\n",
      "tensor(2.6459, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 4.02007627, Culminative Send Cost: 8271200\n",
      "tensor(2.6461, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 4.01806259, Culminative Send Cost: 8310400\n",
      "tensor(2.6464, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 4.01605892, Culminative Send Cost: 8349600\n",
      "tensor(2.6466, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 4.01406431, Culminative Send Cost: 8388800\n",
      "tensor(2.6469, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 4.01207924, Culminative Send Cost: 8428000\n",
      "tensor(2.6472, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 4.01010227, Culminative Send Cost: 8467200\n",
      "tensor(2.6474, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 4.00813580, Culminative Send Cost: 8506400\n",
      "tensor(2.6477, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 4.00617790, Culminative Send Cost: 8545600\n",
      "tensor(2.6480, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 4.00422859, Culminative Send Cost: 8584800\n",
      "tensor(2.6483, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 4.00228930, Culminative Send Cost: 8624000\n",
      "tensor(2.6486, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 4.00035763, Culminative Send Cost: 8663200\n",
      "tensor(2.6489, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.99843526, Culminative Send Cost: 8702400\n",
      "tensor(2.6492, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.99652147, Culminative Send Cost: 8741600\n",
      "tensor(2.6495, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.99461603, Culminative Send Cost: 8780800\n",
      "tensor(2.6499, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.99271917, Culminative Send Cost: 8820000\n",
      "tensor(2.6502, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.99083114, Culminative Send Cost: 8859200\n",
      "tensor(2.6505, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.98895168, Culminative Send Cost: 8898400\n",
      "tensor(2.6509, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.98707986, Culminative Send Cost: 8937600\n",
      "tensor(2.6512, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.98521614, Culminative Send Cost: 8976800\n",
      "tensor(2.6516, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.98336101, Culminative Send Cost: 9016000\n",
      "tensor(2.6520, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.98151398, Culminative Send Cost: 9055200\n",
      "tensor(2.6523, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.97967482, Culminative Send Cost: 9094400\n",
      "tensor(2.6527, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.97784352, Culminative Send Cost: 9133600\n",
      "tensor(2.6531, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.97602034, Culminative Send Cost: 9172800\n",
      "tensor(2.6535, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.97420526, Culminative Send Cost: 9212000\n",
      "tensor(2.6539, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.97239733, Culminative Send Cost: 9251200\n",
      "tensor(2.6543, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.97059727, Culminative Send Cost: 9290400\n",
      "tensor(2.6547, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.96880507, Culminative Send Cost: 9329600\n",
      "tensor(2.6551, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.96702003, Culminative Send Cost: 9368800\n",
      "tensor(2.6555, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.96524310, Culminative Send Cost: 9408000\n",
      "tensor(2.6559, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.96347260, Culminative Send Cost: 9447200\n",
      "tensor(2.6563, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.96171045, Culminative Send Cost: 9486400\n",
      "tensor(2.6568, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.95995593, Culminative Send Cost: 9525600\n",
      "tensor(2.6572, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.95820785, Culminative Send Cost: 9564800\n",
      "tensor(2.6576, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.95646715, Culminative Send Cost: 9604000\n",
      "tensor(2.6581, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.95473409, Culminative Send Cost: 9643200\n",
      "tensor(2.6585, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.95300746, Culminative Send Cost: 9682400\n",
      "tensor(2.6590, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.95128798, Culminative Send Cost: 9721600\n",
      "tensor(2.6594, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.94957566, Culminative Send Cost: 9760800\n",
      "tensor(2.6599, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.94787097, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[ 3.4813e-02, -1.2070e-02,  2.7103e-02, -2.0954e-02, -2.9180e-03,\n",
      "          1.7424e-03,  7.9385e-04, -1.6866e-02,  5.1630e-03, -1.5287e-02,\n",
      "         -1.0681e-02, -1.9872e-02, -3.1522e-02,  4.1823e-03,  1.6526e-02,\n",
      "          2.2145e-03, -7.3472e-03, -2.6391e-03, -2.7997e-02, -2.3973e-03,\n",
      "         -2.3801e-02, -1.4203e-02,  1.5831e-02,  1.4611e-03,  1.7395e-02,\n",
      "         -5.5745e-03, -2.1995e-02,  2.3595e-02, -1.6931e-02,  2.0373e-02,\n",
      "          1.6462e-02,  6.5476e-04,  1.4515e-02, -3.2474e-02,  3.3776e-02,\n",
      "         -2.6898e-02, -9.3641e-03, -9.7773e-03,  3.2546e-02, -1.6914e-02,\n",
      "         -2.0744e-02,  7.6723e-03,  1.3016e-02,  1.7162e-03, -1.9708e-02,\n",
      "         -2.9289e-02,  1.0606e-03, -7.1781e-03,  2.8269e-02, -2.8977e-02,\n",
      "         -1.5852e-02, -3.3473e-02,  1.4942e-02, -5.2641e-03, -3.4192e-02,\n",
      "         -1.4756e-02, -2.3073e-02, -2.2974e-02,  1.8327e-02, -8.4563e-03,\n",
      "         -1.1918e-02, -3.4446e-02, -3.1150e-02,  1.6469e-05, -3.1940e-03,\n",
      "          6.9902e-03,  5.4189e-03,  1.9284e-02,  2.7549e-03,  6.0732e-02,\n",
      "          2.4166e-02,  9.5797e-02,  3.3221e-02,  4.3776e-02,  3.3034e-02,\n",
      "          6.7875e-02,  4.3096e-02,  1.3793e-03, -9.6640e-03,  3.0678e-02,\n",
      "         -1.5289e-03,  2.0126e-02,  2.4499e-02,  7.1801e-03,  1.7700e-02,\n",
      "         -6.6616e-03, -3.3056e-02, -2.8546e-02,  2.1412e-02, -5.6281e-03,\n",
      "         -2.9665e-02,  1.3277e-02, -1.0004e-02,  4.9490e-02,  1.5395e-02,\n",
      "          5.3100e-02,  3.1112e-02,  5.1856e-02,  9.6448e-02,  7.0581e-02,\n",
      "          1.3816e-01,  1.3375e-01,  1.2334e-01,  1.0461e-01,  6.4911e-02,\n",
      "          9.3258e-02,  6.5347e-02, -1.2138e-02, -2.0004e-02,  1.1394e-02,\n",
      "         -4.1546e-03,  4.9287e-04, -2.0186e-02, -1.4410e-02, -1.6497e-02,\n",
      "         -2.0702e-02, -3.2770e-02, -1.7422e-02,  1.6649e-02,  2.7852e-02,\n",
      "          1.5396e-02,  4.9670e-02,  4.1978e-02,  3.5763e-02,  5.8656e-02,\n",
      "          2.0594e-02,  1.4766e-02, -5.2402e-03,  5.0833e-02,  8.0007e-02,\n",
      "          8.6211e-02,  9.6261e-02,  5.4916e-02,  8.1683e-02,  2.9598e-02,\n",
      "          6.4451e-02, -1.3560e-02, -1.9915e-02,  6.7432e-03,  6.7220e-03,\n",
      "          5.9542e-03,  6.1119e-03,  3.4560e-02,  1.3644e-02, -1.8300e-02,\n",
      "          1.9984e-02,  1.7625e-02, -3.1324e-02, -1.9913e-02,  2.6025e-02,\n",
      "         -8.3398e-03,  4.1171e-03, -2.4509e-02, -5.9044e-02, -8.2375e-02,\n",
      "         -9.4669e-02, -3.7077e-02, -2.7825e-02, -1.1530e-02, -7.0672e-03,\n",
      "         -1.2039e-02, -1.1918e-02,  5.6124e-02,  1.2793e-02,  3.7791e-02,\n",
      "          9.1188e-03, -5.5255e-03,  3.0760e-02,  1.3041e-02, -1.5198e-02,\n",
      "         -2.8250e-02,  1.7231e-02,  5.9459e-03, -3.1972e-02, -3.2431e-02,\n",
      "         -2.7477e-02, -3.4342e-02, -5.1498e-02,  9.6914e-04, -6.0723e-02,\n",
      "         -2.9924e-02, -6.7942e-02, -5.9707e-02,  1.4968e-02,  3.9783e-02,\n",
      "         -1.4857e-02, -6.6951e-02, -6.9808e-02, -4.1543e-02, -3.8104e-02,\n",
      "          1.8447e-02,  3.3982e-02,  1.8153e-02,  1.8860e-02, -2.3862e-03,\n",
      "          1.1556e-02, -4.5565e-03, -1.1633e-02, -2.9218e-02,  2.6317e-02,\n",
      "          1.5565e-02, -2.2201e-02, -5.5062e-02, -1.3927e-02,  3.6625e-02,\n",
      "         -1.4216e-02,  3.3073e-02,  3.1938e-02,  2.0500e-02,  6.0297e-02,\n",
      "          7.8915e-02,  1.4051e-01,  6.7790e-02,  2.6918e-02, -5.1892e-02,\n",
      "         -8.2668e-02, -4.1059e-02,  3.5460e-03,  1.7739e-02,  7.3384e-02,\n",
      "          1.0645e-01,  5.0895e-02,  2.2200e-02,  1.7455e-02,  1.2899e-02,\n",
      "          3.0857e-02,  1.7525e-02,  7.5918e-03,  4.1461e-02,  9.0775e-03,\n",
      "          7.0138e-03,  1.0839e-02,  6.1454e-02,  9.6548e-02,  6.7610e-02,\n",
      "          1.0406e-01,  1.2440e-01,  7.0303e-02,  7.4489e-02,  4.1464e-02,\n",
      "          2.7864e-02, -2.9706e-02, -8.6592e-02, -5.2374e-02, -6.6081e-02,\n",
      "         -1.0225e-02,  5.2269e-02,  8.1028e-02,  1.2890e-01,  2.5369e-02,\n",
      "          9.7172e-04,  3.3297e-02,  1.5924e-02, -4.1593e-03, -1.4406e-02,\n",
      "          3.9237e-02,  6.7086e-03,  4.0151e-03,  5.0757e-02,  1.2221e-01,\n",
      "          1.6542e-01,  1.5468e-01,  1.7796e-01,  2.1132e-01,  1.9670e-01,\n",
      "          6.7651e-02, -5.1966e-02, -1.0005e-01, -8.5250e-02, -1.1992e-02,\n",
      "         -3.4878e-02, -4.2698e-02, -4.4172e-02, -1.3246e-02,  6.9676e-03,\n",
      "          1.0351e-01,  1.0315e-01,  3.4448e-02, -7.3338e-03, -2.1057e-02,\n",
      "          2.7860e-02, -1.5428e-02, -1.7970e-02,  2.9017e-02,  2.8974e-02,\n",
      "          9.4608e-02,  1.2616e-01,  1.3686e-01,  1.6908e-01,  1.4076e-01,\n",
      "          1.7884e-01,  1.9288e-01,  1.6354e-01,  2.8180e-02, -7.6017e-02,\n",
      "         -1.6965e-01, -1.1201e-02, -1.2321e-02,  1.4503e-02,  1.4277e-02,\n",
      "         -1.0704e-02,  3.0773e-02,  2.6018e-02,  7.5161e-03,  2.7482e-02,\n",
      "         -2.2922e-03,  9.7721e-03,  3.4339e-02, -2.4912e-02, -7.8819e-03,\n",
      "          3.3125e-02, -4.1233e-03,  1.8092e-02,  1.0151e-01,  1.6758e-01,\n",
      "          1.2887e-01,  1.7461e-01,  1.6884e-01,  1.6476e-01,  1.7849e-01,\n",
      "          2.5359e-01,  1.5155e-01, -1.0300e-01, -8.1290e-02,  4.8005e-02,\n",
      "          8.8531e-02,  5.6044e-02,  5.8600e-02,  5.0476e-02, -1.6169e-02,\n",
      "          3.9235e-03,  1.4974e-02,  4.1604e-02, -1.2048e-02, -1.9635e-02,\n",
      "          1.8581e-02, -7.4559e-04,  1.2033e-02,  3.6917e-02, -8.9152e-03,\n",
      "          3.1096e-02,  1.1746e-01,  9.5127e-02,  1.3084e-01,  7.7227e-02,\n",
      "          8.9462e-02,  1.0557e-01,  2.3543e-01,  2.5169e-01,  1.1001e-01,\n",
      "         -2.8797e-03,  3.8908e-02,  1.6751e-01,  2.5032e-01,  1.8570e-01,\n",
      "          8.3302e-02,  1.0787e-02,  3.1086e-03, -1.2897e-02, -6.5003e-02,\n",
      "          6.6471e-04, -2.8601e-02, -1.9106e-02,  1.7833e-05,  3.1755e-02,\n",
      "          1.6791e-02, -1.0700e-02,  1.3235e-03,  5.1166e-02,  7.8990e-02,\n",
      "          3.6645e-02,  3.2745e-02,  1.2878e-02, -5.7779e-03, -4.2071e-03,\n",
      "          1.3194e-01,  2.7547e-01,  1.0414e-01,  2.0396e-02,  1.4444e-01,\n",
      "          2.8975e-01,  2.9964e-01,  2.1050e-01,  1.1998e-01,  6.3326e-02,\n",
      "         -3.0170e-02, -2.5677e-02, -1.6712e-02, -1.8291e-02,  1.3353e-02,\n",
      "         -1.9447e-02, -2.6976e-02, -2.0352e-02,  1.3124e-02, -1.0015e-03,\n",
      "          3.6528e-02,  1.0776e-02,  1.5426e-02, -1.5754e-02, -5.2539e-02,\n",
      "         -9.2712e-02, -7.1660e-02, -1.8141e-02,  1.0645e-01,  1.8407e-01,\n",
      "          5.1601e-02,  5.0208e-02,  9.7201e-02,  2.9320e-01,  2.7978e-01,\n",
      "          1.3192e-01,  1.0524e-02, -4.6747e-02, -2.8162e-02, -7.7185e-02,\n",
      "         -6.1634e-02,  1.0829e-02,  3.8026e-02,  2.1012e-02,  2.1537e-02,\n",
      "          3.1893e-02,  2.6921e-02, -2.0749e-02, -5.5428e-03, -1.4158e-02,\n",
      "         -6.5322e-02, -8.6688e-02, -8.7474e-02, -1.0112e-01, -1.0472e-01,\n",
      "         -1.4818e-02,  1.0832e-01,  1.0018e-01, -1.5506e-03,  3.0495e-02,\n",
      "          1.4404e-01,  2.7672e-01,  1.9712e-01,  1.1316e-02, -4.7179e-02,\n",
      "         -5.9709e-02, -4.5519e-02, -3.7366e-02, -6.7126e-03,  6.3152e-03,\n",
      "         -1.5254e-02,  1.7104e-02,  2.4064e-02,  2.3487e-02, -4.9687e-03,\n",
      "         -2.3852e-03, -8.3745e-03,  3.5003e-04, -9.0374e-02, -7.8737e-02,\n",
      "         -1.2938e-01, -1.1705e-01, -1.7585e-02,  3.9386e-02,  1.7000e-01,\n",
      "          1.2655e-01, -3.2814e-03,  1.4194e-02,  1.1888e-01,  2.7074e-01,\n",
      "          6.5179e-02, -1.1193e-01, -1.3025e-01, -9.4877e-02, -4.8454e-02,\n",
      "         -3.1718e-02, -3.3806e-02, -3.8915e-02, -9.0491e-03, -3.5473e-02,\n",
      "          2.1774e-02, -2.2399e-02,  2.3080e-02,  3.1090e-03, -1.5013e-03,\n",
      "         -3.8989e-02, -7.1971e-02, -1.1777e-01, -1.1865e-01, -7.1387e-02,\n",
      "          9.5512e-03,  1.6456e-01,  1.7848e-01,  1.3137e-01, -2.5495e-02,\n",
      "          7.9637e-03,  1.0127e-01,  1.0813e-01,  2.2400e-02, -5.4546e-02,\n",
      "         -4.7589e-02, -2.0380e-02, -2.9763e-02, -7.9141e-02, -1.5313e-02,\n",
      "         -1.7568e-02, -8.7785e-03,  2.8267e-02,  7.5287e-03,  1.7611e-02,\n",
      "         -3.2099e-02, -2.6049e-02, -1.3648e-02, -3.2604e-02, -7.7735e-02,\n",
      "         -8.2312e-02, -9.5017e-02, -4.6080e-02,  2.8457e-02,  1.4784e-01,\n",
      "          1.2605e-01, -2.3512e-03, -6.5894e-02, -1.6389e-02,  6.2684e-02,\n",
      "          3.5396e-02, -4.5413e-03, -2.8162e-02, -1.7925e-02, -5.2977e-02,\n",
      "         -4.2027e-02, -5.9037e-02, -7.4275e-02, -6.4964e-02,  1.5390e-02,\n",
      "          1.5865e-02, -1.5236e-02,  2.3663e-02, -6.7839e-03,  7.1281e-03,\n",
      "         -1.1698e-02, -6.5205e-02, -7.6889e-02, -8.3180e-02, -6.2034e-02,\n",
      "         -2.6924e-02, -4.1424e-02,  4.3537e-02,  4.5050e-02, -2.3301e-02,\n",
      "         -9.0571e-02, -3.9152e-03, -2.0206e-02, -5.3336e-03, -3.0051e-03,\n",
      "         -1.1021e-02, -2.0440e-02, -5.1363e-02, -6.5419e-02, -5.3958e-02,\n",
      "         -3.5593e-02, -5.6447e-02, -3.4233e-02, -2.9363e-02,  1.8091e-02,\n",
      "          4.5534e-03, -1.1975e-02, -3.3010e-02,  3.3561e-03, -2.9043e-02,\n",
      "         -7.4200e-02, -4.1931e-02, -3.2783e-02, -6.1356e-02, -5.1272e-02,\n",
      "         -5.8132e-02, -5.5550e-03, -2.4666e-02,  2.6682e-02, -6.6015e-03,\n",
      "          3.2218e-02, -5.9267e-03,  9.9758e-03,  1.1376e-02, -9.1262e-03,\n",
      "         -4.0822e-02, -5.3561e-02, -7.8721e-02, -2.3608e-02, -3.2074e-03,\n",
      "         -4.2354e-02, -1.0050e-02, -2.4644e-02, -2.6653e-02,  3.5141e-02,\n",
      "          2.3922e-02, -2.1341e-02, -4.2801e-02, -8.0195e-02, -6.2855e-02,\n",
      "         -2.4080e-02, -9.6803e-02, -8.9962e-02, -7.2330e-02, -2.4999e-02,\n",
      "          1.1721e-02,  1.8116e-02, -1.4193e-02, -1.7993e-02, -1.5257e-02,\n",
      "         -7.8276e-03,  2.5225e-02, -3.4877e-02, -3.1859e-02, -6.6471e-02,\n",
      "         -4.6853e-02, -4.4694e-02,  7.8826e-03, -3.7449e-02,  3.0410e-02,\n",
      "          3.3799e-02,  2.4569e-02, -2.6112e-03, -2.2921e-02,  3.8136e-03,\n",
      "         -2.1013e-02, -4.2527e-02, -6.4949e-02, -3.8984e-02, -1.0680e-01,\n",
      "         -9.7849e-02, -4.4486e-02, -2.0755e-02, -3.2219e-03, -5.0091e-03,\n",
      "         -4.5929e-02, -6.1531e-02, -5.1363e-02, -1.5651e-02,  1.1287e-02,\n",
      "          2.3706e-02, -3.3258e-03, -2.9902e-02,  1.3944e-02,  1.2326e-02,\n",
      "          2.1494e-02,  2.6356e-02,  1.0135e-02, -3.5764e-02,  2.8123e-02,\n",
      "          1.0511e-02,  3.1189e-02,  2.3224e-02, -2.2876e-02, -5.2748e-02,\n",
      "         -8.0604e-03, -5.1911e-02, -3.6504e-02, -2.7428e-02,  1.4110e-02,\n",
      "          4.9333e-04,  1.5398e-02,  5.1529e-02,  9.7308e-03, -2.8024e-02,\n",
      "          2.3404e-03,  6.3901e-02,  3.6057e-02,  5.9254e-02,  7.4720e-02,\n",
      "          3.7164e-02,  3.0200e-02, -3.2234e-02, -1.9705e-02, -1.2634e-02,\n",
      "         -1.1616e-02, -1.8655e-02,  3.2633e-03, -2.3363e-02, -7.0476e-03,\n",
      "         -2.9028e-02,  4.1483e-03,  9.1014e-04, -2.0179e-04,  6.7246e-02,\n",
      "          9.3887e-02,  9.9611e-02,  1.0238e-01,  1.4173e-01,  1.1367e-01,\n",
      "          1.1219e-01,  7.8531e-02,  9.1647e-02,  1.4894e-01,  1.2230e-01,\n",
      "          1.2289e-01,  9.2770e-02,  1.2128e-01,  8.6946e-02,  1.9972e-02,\n",
      "          3.7968e-02, -5.2910e-03,  8.6624e-03, -1.2184e-02,  2.0772e-02,\n",
      "          1.5845e-02,  3.5105e-02, -2.5284e-02,  3.2081e-02,  1.8979e-02,\n",
      "          2.2370e-02,  2.6380e-02,  8.7823e-02,  1.1135e-01,  1.5855e-01,\n",
      "          1.8936e-01,  1.6588e-01,  1.5635e-01,  2.0237e-01,  1.8871e-01,\n",
      "          1.6541e-01,  2.0367e-01,  1.5120e-01,  1.3237e-01,  1.0252e-01,\n",
      "          8.3804e-02,  5.0634e-02,  3.6175e-02,  2.7251e-02,  9.3185e-03,\n",
      "          2.0919e-02,  2.8781e-02, -9.0173e-03,  3.3174e-02,  8.2652e-03,\n",
      "          1.2418e-02, -3.0298e-02, -2.8236e-02,  2.4116e-02,  2.7402e-02,\n",
      "          3.1416e-02,  1.9745e-02,  5.5748e-02,  5.2944e-02,  7.5860e-02,\n",
      "          1.0135e-01,  7.4966e-02,  9.7986e-02,  7.0063e-02,  6.7419e-02,\n",
      "          7.6701e-02,  5.7371e-02,  6.3421e-02,  3.8523e-02, -1.6217e-02,\n",
      "          2.4521e-02, -1.0551e-02,  2.7166e-02,  2.7350e-03,  6.3117e-03,\n",
      "         -1.8497e-02, -1.7664e-02,  1.6916e-03,  2.6351e-02, -2.7523e-02,\n",
      "          2.3714e-02, -2.5510e-02, -3.0704e-02,  3.0152e-02,  2.9509e-02,\n",
      "         -3.0152e-02,  3.1264e-02, -5.3433e-03, -8.6728e-03,  2.9989e-02,\n",
      "          1.1460e-02,  4.6055e-02,  4.2339e-02,  4.4149e-02, -8.0805e-03,\n",
      "          1.0020e-02, -6.0055e-03,  3.5385e-02,  3.6187e-02, -1.1363e-02,\n",
      "         -2.1450e-02,  2.4592e-02,  7.8287e-03, -1.0718e-02]])\n",
      "activation_stack.0.bias: tensor([0.9435])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for local_update_epochs is 2 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[ 2.0256e-02,  2.1094e-02,  9.7596e-04,  4.6496e-03, -6.6251e-03,\n",
      "          2.1825e-02,  5.0315e-03, -3.5072e-02,  2.8665e-03, -1.7183e-02,\n",
      "         -2.1101e-02, -2.3860e-02, -1.9961e-02,  3.2538e-02, -2.4640e-02,\n",
      "         -1.0180e-02,  1.5129e-02, -9.7309e-03, -2.8402e-02,  3.1308e-02,\n",
      "          4.4725e-03,  1.2328e-02, -1.7217e-02, -1.9245e-02, -3.5145e-02,\n",
      "          2.5789e-02, -1.8652e-02,  1.4428e-02, -3.4297e-02, -6.3828e-03,\n",
      "          2.0537e-03,  2.9865e-02, -2.1172e-02, -3.2473e-02,  1.4511e-03,\n",
      "         -3.2600e-02,  2.0195e-02,  9.0330e-03, -2.7921e-02,  2.0683e-02,\n",
      "         -2.5111e-02,  2.8585e-02, -1.5144e-02, -2.4945e-02,  2.0655e-02,\n",
      "         -2.2821e-02, -1.7513e-03, -9.2347e-03,  1.2044e-02,  1.7186e-02,\n",
      "          2.0187e-02, -1.5308e-02, -2.6509e-02, -1.9871e-02,  5.0023e-03,\n",
      "         -6.5236e-03,  1.0710e-02,  2.3447e-03, -1.7611e-02,  6.0821e-03,\n",
      "         -3.4613e-02,  6.9339e-03,  2.0554e-02, -9.6164e-03, -1.7822e-02,\n",
      "          1.5009e-02,  2.9981e-02,  4.3843e-03,  5.3465e-03,  1.1346e-03,\n",
      "         -2.7532e-02,  1.9828e-02, -3.2778e-02, -1.4917e-02,  2.6950e-02,\n",
      "          1.0476e-02,  2.2888e-02, -1.6369e-02, -1.0831e-02,  1.8010e-02,\n",
      "          2.7330e-02,  1.1942e-02,  9.3211e-03,  3.0728e-02, -8.0789e-03,\n",
      "          9.9559e-03,  4.5195e-03, -2.9498e-02, -3.1577e-02,  1.2731e-02,\n",
      "         -2.6024e-02,  4.1624e-03,  2.5755e-02,  2.9233e-02,  2.1866e-02,\n",
      "          5.3348e-03, -5.3768e-03,  1.4733e-02, -1.1556e-02, -2.1494e-02,\n",
      "          1.7051e-02,  3.3193e-02,  2.1374e-02, -2.4747e-02, -7.3103e-03,\n",
      "          1.7898e-02,  7.8929e-03,  2.9197e-02,  3.1475e-02, -1.4469e-02,\n",
      "         -9.4096e-03, -3.3792e-02, -8.7817e-03,  3.3977e-02, -4.0825e-03,\n",
      "         -2.2858e-02,  1.5190e-02,  6.2934e-03,  3.5351e-02,  3.4426e-02,\n",
      "         -2.5496e-03, -2.7590e-02, -2.2286e-02,  3.1920e-03,  5.7199e-03,\n",
      "          2.9530e-02,  1.7910e-02,  2.7723e-02, -2.0349e-02, -2.8362e-02,\n",
      "         -8.4809e-04,  3.3452e-02, -2.6769e-02, -1.5608e-03, -1.0895e-02,\n",
      "          4.3228e-04, -3.3140e-02,  4.6418e-03,  3.1546e-02,  2.1629e-03,\n",
      "         -3.0927e-02, -2.9759e-02, -2.4278e-02,  1.3549e-02, -7.6140e-03,\n",
      "         -1.9769e-02,  3.2302e-02,  7.7469e-03,  9.8477e-04,  7.4511e-03,\n",
      "         -5.4027e-03,  8.3464e-03, -2.9547e-04, -5.6933e-03,  2.3479e-02,\n",
      "         -2.3748e-02,  2.3997e-02,  2.8238e-02, -2.2564e-02, -1.2964e-02,\n",
      "          2.0642e-02,  1.0062e-02, -2.3542e-02, -1.8600e-02,  1.1094e-02,\n",
      "          5.6665e-03,  2.4504e-02, -2.6416e-02,  1.7121e-02,  3.2327e-02,\n",
      "          2.0914e-02, -5.6400e-03, -2.5767e-02, -2.2633e-02, -3.0237e-02,\n",
      "         -3.2658e-02,  2.5193e-02, -6.4237e-03,  5.6624e-05, -9.7023e-03,\n",
      "          2.3186e-02,  3.3939e-02, -2.8354e-02,  1.2400e-02,  3.1060e-02,\n",
      "          1.8964e-02,  1.3940e-02, -2.8375e-03, -4.3840e-03,  3.3366e-02,\n",
      "          6.4091e-03,  2.2664e-02,  7.8449e-03,  1.5659e-02, -2.2754e-02,\n",
      "          2.8664e-02, -4.3263e-04,  9.5666e-03, -9.7401e-03,  1.9400e-02,\n",
      "         -1.5819e-02, -1.5975e-02, -8.1533e-03,  2.7718e-02,  1.2098e-02,\n",
      "          1.6861e-02,  2.5651e-02, -3.2001e-02, -1.9413e-04, -4.5317e-03,\n",
      "         -1.5873e-02,  1.0348e-02,  2.3742e-02,  1.1012e-02,  2.4183e-02,\n",
      "          1.3317e-03,  2.5330e-02,  1.4304e-02, -3.5096e-02,  1.0979e-02,\n",
      "          2.3932e-03, -1.2984e-02,  2.0753e-02,  2.9563e-02,  5.4670e-03,\n",
      "         -8.5836e-03, -1.0821e-02, -2.7776e-02,  1.9163e-02, -1.7446e-02,\n",
      "         -1.5932e-02, -1.0794e-02,  1.9381e-02,  3.3904e-02, -1.4234e-02,\n",
      "         -3.0366e-02, -1.0533e-02,  3.2315e-02, -1.2868e-02,  3.2031e-02,\n",
      "         -2.4140e-02, -5.9771e-03, -1.8048e-02, -1.6095e-02, -2.0966e-03,\n",
      "         -5.4149e-04,  1.0850e-03, -1.1973e-02, -1.0651e-02, -1.8428e-02,\n",
      "         -6.7199e-03,  2.5459e-02, -3.2284e-02, -3.1623e-02, -1.3283e-02,\n",
      "         -1.2307e-02, -3.2234e-02,  3.3340e-03, -1.0687e-02,  2.7777e-02,\n",
      "          9.9243e-03, -1.5316e-02,  3.4060e-02, -2.5557e-02, -9.2485e-03,\n",
      "         -1.2921e-02, -3.2857e-02, -3.3766e-02,  9.7635e-03,  2.9882e-02,\n",
      "          3.3961e-02, -3.3395e-02, -1.1574e-02,  3.3526e-03,  1.5888e-02,\n",
      "         -2.5243e-02, -3.5691e-02,  1.7448e-02,  9.8496e-03,  1.5647e-02,\n",
      "          8.6907e-03,  1.0796e-03,  1.0821e-02, -1.8360e-02,  1.6660e-02,\n",
      "         -1.1950e-02,  2.0982e-02, -1.1626e-02,  1.8126e-02,  2.2767e-02,\n",
      "         -3.3351e-02, -2.2800e-02, -4.7192e-03,  3.1662e-02, -1.6262e-02,\n",
      "          2.8320e-02, -5.0092e-03, -2.9004e-02,  1.5208e-02, -3.2146e-02,\n",
      "         -1.7576e-02,  6.0268e-03,  1.3171e-02,  1.8811e-02, -3.5556e-02,\n",
      "          1.5008e-03, -2.7285e-02, -3.0852e-02,  1.5603e-02,  1.8731e-03,\n",
      "         -1.1186e-02, -1.1542e-02, -1.8113e-02, -2.0053e-02, -6.9751e-03,\n",
      "          1.0246e-02,  1.3227e-02, -2.2714e-02, -2.9913e-02,  1.9415e-02,\n",
      "         -8.1584e-04,  2.0653e-02, -2.1844e-03, -8.8202e-03,  2.3631e-02,\n",
      "         -2.9961e-02, -2.3788e-02, -2.5393e-02, -1.2123e-02,  9.6035e-03,\n",
      "          5.0937e-03,  2.2422e-02,  3.8739e-03,  8.3382e-03,  3.4034e-02,\n",
      "          1.6171e-02,  2.6923e-02,  3.4652e-02,  2.8907e-02, -3.4993e-02,\n",
      "         -8.2911e-03, -3.5671e-02,  9.5132e-03, -7.3024e-03, -1.6128e-02,\n",
      "          1.3503e-02,  2.1596e-02,  2.7966e-02,  4.7837e-03,  9.4108e-03,\n",
      "          2.3498e-02,  3.1263e-04,  2.1396e-02, -4.2196e-03,  2.4822e-02,\n",
      "          1.8569e-02,  2.0632e-02,  1.3674e-02,  2.1588e-02, -6.3166e-03,\n",
      "          2.9050e-02, -1.7834e-02,  3.3059e-02,  1.4467e-02, -1.3438e-02,\n",
      "          2.2753e-02, -1.3098e-02,  3.3077e-02, -2.2026e-02,  6.0005e-03,\n",
      "         -3.4037e-02,  8.6002e-03, -2.2840e-02,  1.3186e-02,  3.1017e-02,\n",
      "          2.3202e-02,  1.8467e-02,  2.7665e-02,  7.1023e-03,  7.3309e-03,\n",
      "         -1.8651e-02,  3.1074e-02,  8.5080e-03, -2.0154e-03, -1.9778e-02,\n",
      "         -1.5569e-02,  2.0983e-02,  1.8429e-02,  3.0186e-02,  1.9757e-02,\n",
      "          5.5875e-03, -1.4943e-02, -1.9212e-02, -1.5085e-02,  4.9725e-03,\n",
      "         -2.4832e-02,  3.3842e-02,  1.0286e-02, -2.7082e-02, -2.8239e-02,\n",
      "          3.1505e-02,  1.2924e-02,  2.3720e-02,  1.6550e-03,  1.3231e-02,\n",
      "         -2.2149e-02, -1.3138e-02, -1.9087e-02,  2.4502e-02, -3.2823e-02,\n",
      "          3.5426e-02, -3.4282e-02, -3.0251e-02, -3.5362e-02,  2.9919e-02,\n",
      "          2.5044e-02, -2.2050e-02,  7.2226e-03,  1.8623e-02,  1.0610e-03,\n",
      "         -2.9967e-03, -3.3069e-02,  1.4145e-02,  1.7815e-02,  3.5005e-02,\n",
      "         -2.6926e-02,  1.7184e-02,  1.6893e-02, -2.9984e-02,  1.9133e-02,\n",
      "         -1.6467e-02,  1.7527e-02, -3.1279e-02,  2.7065e-03, -5.0342e-03,\n",
      "          1.5065e-02,  1.9435e-02, -5.8466e-03,  3.4987e-02, -2.5599e-02,\n",
      "         -2.5526e-02,  2.2723e-02,  6.7950e-03, -2.4324e-02,  3.3020e-02,\n",
      "         -1.9988e-02, -2.9343e-02, -8.6115e-03,  2.3929e-02, -1.8921e-02,\n",
      "         -5.5959e-03, -1.4846e-02, -2.9067e-03,  5.7280e-03,  3.3360e-02,\n",
      "         -2.3594e-02,  2.0424e-03,  1.0844e-02,  3.4069e-02, -1.8220e-02,\n",
      "         -8.7543e-03, -6.7892e-04, -6.8940e-05, -1.8513e-02,  1.4712e-02,\n",
      "          2.7681e-02, -6.5775e-03, -3.0537e-02, -2.4797e-02,  6.8665e-03,\n",
      "          3.2874e-02,  4.2713e-04, -2.2404e-02,  2.3792e-02, -3.2491e-02,\n",
      "         -5.0687e-03,  3.2919e-02,  1.5268e-02,  1.2032e-02, -3.2502e-02,\n",
      "         -9.5978e-03, -1.9527e-02,  4.2256e-03, -9.7483e-03,  3.1076e-02,\n",
      "          3.5061e-02, -6.7355e-03,  1.2259e-02,  9.8873e-03,  6.6502e-03,\n",
      "          3.4751e-02, -2.3716e-02, -1.8621e-02,  3.0465e-02,  2.9109e-03,\n",
      "          3.5036e-02, -3.4797e-02, -1.6756e-02, -1.5256e-02,  5.3746e-03,\n",
      "          3.3363e-02, -2.5488e-02,  1.9601e-02, -1.2684e-02,  1.9002e-02,\n",
      "         -9.7825e-03, -1.7105e-02, -2.7801e-02,  2.9145e-02,  3.9520e-03,\n",
      "          2.7440e-02, -2.2839e-02,  3.3515e-02,  3.0206e-02, -1.6660e-03,\n",
      "         -1.2225e-02,  2.1232e-02, -1.4281e-02, -2.2833e-03, -3.0987e-02,\n",
      "         -2.7008e-02,  6.2144e-04, -1.6155e-02,  5.5653e-03,  6.7908e-03,\n",
      "         -2.5408e-02,  5.8253e-03,  1.2003e-02,  3.5351e-02,  1.2639e-02,\n",
      "         -2.0312e-02,  1.6960e-02, -3.5616e-02, -7.2640e-03,  1.2859e-02,\n",
      "          1.9291e-02,  3.2985e-02,  2.9631e-03, -1.7332e-02, -1.4234e-02,\n",
      "          2.4625e-02,  3.5271e-02, -1.2240e-02,  2.5827e-02, -1.7681e-02,\n",
      "          2.1266e-03, -3.9367e-03, -6.4023e-03,  3.4874e-02,  2.9225e-02,\n",
      "          7.9038e-04,  1.3744e-03, -1.7082e-02, -5.0445e-03,  6.7752e-03,\n",
      "         -1.6732e-02, -3.0693e-02,  2.9591e-02, -2.2651e-02,  1.4085e-02,\n",
      "         -2.8013e-02, -2.0698e-02, -2.3922e-02, -1.7206e-02,  8.5452e-03,\n",
      "          3.3839e-02,  3.3542e-02,  1.8170e-02, -1.0362e-02,  3.4348e-02,\n",
      "          3.4133e-02, -1.4786e-02,  9.2577e-03,  2.4120e-02, -1.8213e-02,\n",
      "         -1.7246e-02,  3.2815e-02,  3.5428e-03, -2.6270e-02, -2.9954e-02,\n",
      "          3.1572e-02, -1.4043e-02,  1.1776e-02,  2.4817e-02, -1.9910e-02,\n",
      "         -3.3967e-02,  2.6466e-02,  3.0003e-02, -1.7773e-02, -2.8083e-02,\n",
      "          1.2821e-02,  2.2076e-02, -1.5442e-02, -9.5836e-03,  1.0208e-02,\n",
      "         -2.2414e-03, -2.4787e-02, -3.4791e-02,  2.1877e-02,  6.7864e-03,\n",
      "         -6.7470e-03, -2.2322e-02,  3.3929e-02,  2.3519e-02,  2.7767e-02,\n",
      "          2.8391e-03,  3.3061e-02, -1.8059e-02, -1.7179e-02, -2.7769e-02,\n",
      "          4.2885e-03, -3.3166e-02, -2.5884e-02, -6.1756e-03, -1.2290e-02,\n",
      "         -3.0253e-02, -1.4824e-02,  2.9204e-02, -3.3229e-02, -7.1663e-03,\n",
      "          1.4428e-02,  2.2279e-02,  5.8254e-03,  2.7965e-02,  7.4975e-03,\n",
      "         -3.4758e-02,  2.4911e-02,  3.3359e-02,  3.3585e-02,  1.4607e-02,\n",
      "         -1.0316e-02, -3.2760e-02,  2.4093e-02, -3.0517e-02,  1.2092e-02,\n",
      "         -3.2566e-02,  3.3306e-02,  1.3586e-02, -1.2512e-02,  3.0580e-02,\n",
      "         -3.3696e-02,  8.5384e-03, -1.5558e-02,  2.5185e-02, -3.8156e-03,\n",
      "          1.9977e-02,  2.1218e-03, -3.5260e-02, -3.9946e-04, -2.9861e-02,\n",
      "         -4.7172e-03,  2.7698e-02, -2.5239e-02,  6.6783e-03, -4.6800e-03,\n",
      "          2.3862e-02,  3.0467e-02, -1.2346e-02, -2.9447e-02,  3.4747e-02,\n",
      "          1.5303e-02, -1.3828e-02, -8.3732e-03, -1.6963e-02,  1.6024e-02,\n",
      "          1.5946e-03, -5.3038e-04,  1.3825e-02,  5.6078e-03, -2.2692e-02,\n",
      "          2.8960e-02, -2.4932e-03, -2.0933e-02,  1.4564e-02,  7.6706e-03,\n",
      "          2.2930e-02, -2.1219e-02,  1.6320e-02, -7.8556e-03, -3.6461e-03,\n",
      "          2.8435e-02,  1.4163e-02, -7.9444e-03,  2.4853e-02,  3.4575e-02,\n",
      "         -6.3027e-03, -2.5255e-02, -7.8700e-03, -3.0309e-02, -1.9794e-02,\n",
      "         -6.3197e-03, -2.1512e-02, -4.0191e-03, -1.0372e-02,  9.9546e-03,\n",
      "          1.6645e-02, -1.5816e-02, -3.8744e-03, -2.1854e-02,  6.9896e-03,\n",
      "          1.9871e-02,  1.4336e-02, -2.7602e-03, -8.6426e-03, -1.2692e-02,\n",
      "         -1.9858e-02,  2.4669e-02,  1.7127e-02, -3.1628e-02,  1.0646e-02,\n",
      "         -2.5027e-02,  4.8542e-03,  1.2517e-02,  2.3559e-02,  3.5287e-02,\n",
      "         -3.0784e-02, -4.0433e-03, -1.5833e-02, -1.9485e-02, -1.3507e-02,\n",
      "         -6.5257e-03, -1.5864e-02, -1.1629e-02, -8.5797e-03,  1.9999e-02,\n",
      "          2.1555e-02, -2.6065e-02,  4.6417e-03,  2.2478e-02,  3.3599e-02,\n",
      "          3.1547e-02, -1.9112e-02,  3.0904e-02, -3.0371e-03, -2.8710e-02,\n",
      "          3.0254e-02,  8.8636e-03, -1.1150e-02,  2.3454e-02,  1.9010e-02,\n",
      "         -2.5523e-02,  2.8442e-02, -1.8618e-02,  2.4359e-02,  1.1418e-02,\n",
      "         -1.0317e-02,  1.1975e-02,  2.3664e-02,  2.4251e-02, -1.9655e-03,\n",
      "          2.7699e-02,  8.2512e-03,  1.1226e-02,  3.5099e-02,  3.2822e-02,\n",
      "         -3.3113e-02, -1.2207e-02,  8.9842e-03, -3.2644e-02, -3.3672e-02,\n",
      "          8.6144e-03,  4.0881e-03,  3.4905e-02,  3.2400e-02, -2.8284e-02,\n",
      "         -5.7448e-03, -2.6482e-02, -2.9622e-02,  2.7974e-02, -2.6626e-02,\n",
      "         -1.1696e-02,  2.6189e-02,  1.4939e-02,  2.9699e-02,  1.9558e-02,\n",
      "          2.2971e-02,  3.3216e-02, -3.5704e-02,  2.5966e-02,  2.1536e-02,\n",
      "         -2.1201e-02,  1.0422e-02, -1.8044e-03,  2.1717e-02]])), ('activation_stack.0.bias', tensor([0.0155]))])\n",
      "tensor(0.1979, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 21.35892105, Culminative Send Cost: 3920\n",
      "tensor(1.6436, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 9.09907722, Culminative Send Cost: 39200\n",
      "tensor(2.0417, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 7.64648342, Culminative Send Cost: 78400\n",
      "tensor(2.1665, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 6.98932791, Culminative Send Cost: 117600\n",
      "tensor(2.2332, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 6.51086760, Culminative Send Cost: 156800\n",
      "tensor(2.2843, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 6.14837503, Culminative Send Cost: 196000\n",
      "tensor(2.3285, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 5.87084484, Culminative Send Cost: 235200\n",
      "tensor(2.3679, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 5.65616655, Culminative Send Cost: 274400\n",
      "tensor(2.4030, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 5.48816490, Culminative Send Cost: 313600\n",
      "tensor(2.4344, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 5.35496998, Culminative Send Cost: 352800\n",
      "tensor(2.4624, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 5.24785614, Culminative Send Cost: 392000\n",
      "tensor(2.4874, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 5.16039324, Culminative Send Cost: 431200\n",
      "tensor(2.5096, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 5.08783388, Culminative Send Cost: 470400\n",
      "tensor(2.5293, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 5.02665854, Culminative Send Cost: 509600\n",
      "tensor(2.5468, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 4.97425652, Culminative Send Cost: 548800\n",
      "tensor(2.5624, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 4.92867804, Culminative Send Cost: 588000\n",
      "tensor(2.5761, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 4.88846445, Culminative Send Cost: 627200\n",
      "tensor(2.5883, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 4.85251808, Culminative Send Cost: 666400\n",
      "tensor(2.5990, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 4.82000875, Culminative Send Cost: 705600\n",
      "tensor(2.6085, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 4.79030561, Culminative Send Cost: 744800\n",
      "tensor(2.6169, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 4.76292324, Culminative Send Cost: 784000\n",
      "tensor(2.6242, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 4.73748779, Culminative Send Cost: 823200\n",
      "tensor(2.6307, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 4.71370745, Culminative Send Cost: 862400\n",
      "tensor(2.6364, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 4.69135284, Culminative Send Cost: 901600\n",
      "tensor(2.6414, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 4.67024040, Culminative Send Cost: 940800\n",
      "tensor(2.6458, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 4.65022373, Culminative Send Cost: 980000\n",
      "tensor(2.6496, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 4.63118219, Culminative Send Cost: 1019200\n",
      "tensor(2.6530, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 4.61301851, Culminative Send Cost: 1058400\n",
      "tensor(2.6559, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.59564781, Culminative Send Cost: 1097600\n",
      "tensor(2.6585, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.57900143, Culminative Send Cost: 1136800\n",
      "tensor(2.6607, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.56301975, Culminative Send Cost: 1176000\n",
      "tensor(2.6627, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.54764986, Culminative Send Cost: 1215200\n",
      "tensor(2.6644, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.53284836, Culminative Send Cost: 1254400\n",
      "tensor(2.6659, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.51857471, Culminative Send Cost: 1293600\n",
      "tensor(2.6671, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.50479221, Culminative Send Cost: 1332800\n",
      "tensor(2.6683, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.49147224, Culminative Send Cost: 1372000\n",
      "tensor(2.6692, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.47858238, Culminative Send Cost: 1411200\n",
      "tensor(2.6701, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.46609974, Culminative Send Cost: 1450400\n",
      "tensor(2.6708, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.45399857, Culminative Send Cost: 1489600\n",
      "tensor(2.6714, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.44225788, Culminative Send Cost: 1528800\n",
      "tensor(2.6720, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.43085718, Culminative Send Cost: 1568000\n",
      "tensor(2.6725, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.41977882, Culminative Send Cost: 1607200\n",
      "tensor(2.6729, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.40900469, Culminative Send Cost: 1646400\n",
      "tensor(2.6733, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.39852047, Culminative Send Cost: 1685600\n",
      "tensor(2.6736, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.38830996, Culminative Send Cost: 1724800\n",
      "tensor(2.6739, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.37835979, Culminative Send Cost: 1764000\n",
      "tensor(2.6742, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.36865759, Culminative Send Cost: 1803200\n",
      "tensor(2.6744, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.35919094, Culminative Send Cost: 1842400\n",
      "tensor(2.6747, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.34994936, Culminative Send Cost: 1881600\n",
      "tensor(2.6749, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.34092236, Culminative Send Cost: 1920800\n",
      "tensor(2.6751, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.33209896, Culminative Send Cost: 1960000\n",
      "tensor(2.6754, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.32347155, Culminative Send Cost: 1999200\n",
      "tensor(2.6756, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.31502914, Culminative Send Cost: 2038400\n",
      "tensor(2.6758, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.30676699, Culminative Send Cost: 2077600\n",
      "tensor(2.6760, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.29867506, Culminative Send Cost: 2116800\n",
      "tensor(2.6763, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.29074621, Culminative Send Cost: 2156000\n",
      "tensor(2.6765, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.28297472, Culminative Send Cost: 2195200\n",
      "tensor(2.6768, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.27535343, Culminative Send Cost: 2234400\n",
      "tensor(2.6771, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.26787663, Culminative Send Cost: 2273600\n",
      "tensor(2.6774, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.26053810, Culminative Send Cost: 2312800\n",
      "tensor(2.6777, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.25333357, Culminative Send Cost: 2352000\n",
      "tensor(2.6780, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.24625731, Culminative Send Cost: 2391200\n",
      "tensor(2.6784, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.23930311, Culminative Send Cost: 2430400\n",
      "tensor(2.6787, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.23246908, Culminative Send Cost: 2469600\n",
      "tensor(2.6791, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.22574949, Culminative Send Cost: 2508800\n",
      "tensor(2.6795, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.21913910, Culminative Send Cost: 2548000\n",
      "tensor(2.6800, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.21263647, Culminative Send Cost: 2587200\n",
      "tensor(2.6804, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.20623684, Culminative Send Cost: 2626400\n",
      "tensor(2.6809, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.19993591, Culminative Send Cost: 2665600\n",
      "tensor(2.6814, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.19373083, Culminative Send Cost: 2704800\n",
      "tensor(2.6819, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.18761921, Culminative Send Cost: 2744000\n",
      "tensor(2.6824, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.18159676, Culminative Send Cost: 2783200\n",
      "tensor(2.6830, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 4.17566204, Culminative Send Cost: 2822400\n",
      "tensor(2.6836, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 4.16981125, Culminative Send Cost: 2861600\n",
      "tensor(2.6842, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 4.16404200, Culminative Send Cost: 2900800\n",
      "tensor(2.6848, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 4.15835238, Culminative Send Cost: 2940000\n",
      "tensor(2.6855, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 4.15273952, Culminative Send Cost: 2979200\n",
      "tensor(2.6862, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 4.14720154, Culminative Send Cost: 3018400\n",
      "tensor(2.6869, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 4.14173555, Culminative Send Cost: 3057600\n",
      "tensor(2.6876, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 4.13634014, Culminative Send Cost: 3096800\n",
      "tensor(2.6883, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 4.13101339, Culminative Send Cost: 3136000\n",
      "tensor(2.6891, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 4.12575245, Culminative Send Cost: 3175200\n",
      "tensor(2.6899, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 4.12055731, Culminative Send Cost: 3214400\n",
      "tensor(2.6907, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 4.11542511, Culminative Send Cost: 3253600\n",
      "tensor(2.6916, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 4.11035395, Culminative Send Cost: 3292800\n",
      "tensor(2.6924, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 4.10534334, Culminative Send Cost: 3332000\n",
      "tensor(2.6933, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 4.10038996, Culminative Send Cost: 3371200\n",
      "tensor(2.6942, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 4.09549427, Culminative Send Cost: 3410400\n",
      "tensor(2.6951, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 4.09065437, Culminative Send Cost: 3449600\n",
      "tensor(2.6961, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 4.08586836, Culminative Send Cost: 3488800\n",
      "tensor(2.6971, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 4.08113527, Culminative Send Cost: 3528000\n",
      "tensor(2.6981, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 4.07645321, Culminative Send Cost: 3567200\n",
      "tensor(2.6991, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 4.07182264, Culminative Send Cost: 3606400\n",
      "tensor(2.7001, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 4.06724119, Culminative Send Cost: 3645600\n",
      "tensor(2.7012, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 4.06270790, Culminative Send Cost: 3684800\n",
      "tensor(2.7022, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 4.05822182, Culminative Send Cost: 3724000\n",
      "tensor(2.7033, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 4.05378246, Culminative Send Cost: 3763200\n",
      "tensor(2.7044, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 4.04938793, Culminative Send Cost: 3802400\n",
      "tensor(2.7056, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 4.04503727, Culminative Send Cost: 3841600\n",
      "tensor(2.7067, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 4.04073048, Culminative Send Cost: 3880800\n",
      "tensor(2.7079, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 4.03646660, Culminative Send Cost: 3920000\n",
      "tensor(2.7091, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 4.03224421, Culminative Send Cost: 3959200\n",
      "tensor(2.7103, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 4.02806282, Culminative Send Cost: 3998400\n",
      "tensor(2.7116, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 4.02392054, Culminative Send Cost: 4037600\n",
      "tensor(2.7128, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 4.01981831, Culminative Send Cost: 4076800\n",
      "tensor(2.7141, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 4.01575470, Culminative Send Cost: 4116000\n",
      "tensor(2.7154, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 4.01172924, Culminative Send Cost: 4155200\n",
      "tensor(2.7167, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 4.00774097, Culminative Send Cost: 4194400\n",
      "tensor(2.7180, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 4.00378895, Culminative Send Cost: 4233600\n",
      "tensor(2.7193, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 3.99987268, Culminative Send Cost: 4272800\n",
      "tensor(2.7207, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 3.99599147, Culminative Send Cost: 4312000\n",
      "tensor(2.7220, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 3.99214506, Culminative Send Cost: 4351200\n",
      "tensor(2.7234, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 3.98833251, Culminative Send Cost: 4390400\n",
      "tensor(2.7248, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 3.98455429, Culminative Send Cost: 4429600\n",
      "tensor(2.7262, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 3.98080826, Culminative Send Cost: 4468800\n",
      "tensor(2.7277, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 3.97709513, Culminative Send Cost: 4508000\n",
      "tensor(2.7291, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 3.97341347, Culminative Send Cost: 4547200\n",
      "tensor(2.7306, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 3.96976304, Culminative Send Cost: 4586400\n",
      "tensor(2.7321, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 3.96614385, Culminative Send Cost: 4625600\n",
      "tensor(2.7336, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 3.96255493, Culminative Send Cost: 4664800\n",
      "tensor(2.7351, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 3.95899534, Culminative Send Cost: 4704000\n",
      "tensor(2.7366, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 3.95546651, Culminative Send Cost: 4743200\n",
      "tensor(2.7381, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 3.95196581, Culminative Send Cost: 4782400\n",
      "tensor(2.7397, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 3.94849443, Culminative Send Cost: 4821600\n",
      "tensor(2.7412, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 3.94505024, Culminative Send Cost: 4860800\n",
      "tensor(2.7428, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 3.94163489, Culminative Send Cost: 4900000\n",
      "tensor(2.7444, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 3.93824601, Culminative Send Cost: 4939200\n",
      "tensor(2.7460, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 3.93488479, Culminative Send Cost: 4978400\n",
      "tensor(2.7476, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 3.93155003, Culminative Send Cost: 5017600\n",
      "tensor(2.7492, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 3.92824125, Culminative Send Cost: 5056800\n",
      "tensor(2.7509, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 3.92495871, Culminative Send Cost: 5096000\n",
      "tensor(2.7525, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 3.92170191, Culminative Send Cost: 5135200\n",
      "tensor(2.7542, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 3.91846991, Culminative Send Cost: 5174400\n",
      "tensor(2.7558, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 3.91526318, Culminative Send Cost: 5213600\n",
      "tensor(2.7575, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 3.91208076, Culminative Send Cost: 5252800\n",
      "tensor(2.7592, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 3.90892267, Culminative Send Cost: 5292000\n",
      "tensor(2.7609, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 3.90578818, Culminative Send Cost: 5331200\n",
      "tensor(2.7626, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 3.90267777, Culminative Send Cost: 5370400\n",
      "tensor(2.7643, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 3.89959073, Culminative Send Cost: 5409600\n",
      "tensor(2.7661, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 3.89652658, Culminative Send Cost: 5448800\n",
      "tensor(2.7678, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 3.89348483, Culminative Send Cost: 5488000\n",
      "tensor(2.7696, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 3.89046574, Culminative Send Cost: 5527200\n",
      "tensor(2.7713, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 3.88746905, Culminative Send Cost: 5566400\n",
      "tensor(2.7731, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 3.88449407, Culminative Send Cost: 5605600\n",
      "tensor(2.7749, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 3.88154173, Culminative Send Cost: 5644800\n",
      "tensor(2.7767, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 3.87860942, Culminative Send Cost: 5684000\n",
      "tensor(2.7784, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 3.87569928, Culminative Send Cost: 5723200\n",
      "tensor(2.7803, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 3.87280989, Culminative Send Cost: 5762400\n",
      "tensor(2.7821, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 3.86994076, Culminative Send Cost: 5801600\n",
      "tensor(2.7839, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 3.86709237, Culminative Send Cost: 5840800\n",
      "tensor(2.7857, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 3.86426401, Culminative Send Cost: 5880000\n",
      "tensor(2.7875, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 3.86145568, Culminative Send Cost: 5919200\n",
      "tensor(2.7894, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 3.85866690, Culminative Send Cost: 5958400\n",
      "tensor(2.7912, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 3.85589862, Culminative Send Cost: 5997600\n",
      "tensor(2.7931, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 3.85314918, Culminative Send Cost: 6036800\n",
      "tensor(2.7950, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 3.85041881, Culminative Send Cost: 6076000\n",
      "tensor(2.7968, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 3.84770703, Culminative Send Cost: 6115200\n",
      "tensor(2.7987, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 3.84501457, Culminative Send Cost: 6154400\n",
      "tensor(2.8006, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 3.84234071, Culminative Send Cost: 6193600\n",
      "tensor(2.8025, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 3.83968449, Culminative Send Cost: 6232800\n",
      "tensor(2.8044, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 3.83704662, Culminative Send Cost: 6272000\n",
      "tensor(2.8063, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 3.83442712, Culminative Send Cost: 6311200\n",
      "tensor(2.8082, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 3.83182526, Culminative Send Cost: 6350400\n",
      "tensor(2.8101, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 3.82924128, Culminative Send Cost: 6389600\n",
      "tensor(2.8120, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 3.82667422, Culminative Send Cost: 6428800\n",
      "tensor(2.8139, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 3.82412457, Culminative Send Cost: 6468000\n",
      "tensor(2.8159, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 3.82159209, Culminative Send Cost: 6507200\n",
      "tensor(2.8178, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 3.81907630, Culminative Send Cost: 6546400\n",
      "tensor(2.8197, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 3.81657767, Culminative Send Cost: 6585600\n",
      "tensor(2.8217, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 3.81409478, Culminative Send Cost: 6624800\n",
      "tensor(2.8236, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 3.81162882, Culminative Send Cost: 6664000\n",
      "tensor(2.8256, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 3.80917907, Culminative Send Cost: 6703200\n",
      "tensor(2.8275, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 3.80674529, Culminative Send Cost: 6742400\n",
      "tensor(2.8295, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 3.80432725, Culminative Send Cost: 6781600\n",
      "tensor(2.8315, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 3.80192566, Culminative Send Cost: 6820800\n",
      "tensor(2.8334, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 3.79953885, Culminative Send Cost: 6860000\n",
      "tensor(2.8354, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 3.79716825, Culminative Send Cost: 6899200\n",
      "tensor(2.8374, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 3.79481268, Culminative Send Cost: 6938400\n",
      "tensor(2.8394, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 3.79247212, Culminative Send Cost: 6977600\n",
      "tensor(2.8413, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 3.79014659, Culminative Send Cost: 7016800\n",
      "tensor(2.8433, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 3.78783679, Culminative Send Cost: 7056000\n",
      "tensor(2.8453, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 3.78554082, Culminative Send Cost: 7095200\n",
      "tensor(2.8473, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 3.78325987, Culminative Send Cost: 7134400\n",
      "tensor(2.8493, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 3.78099346, Culminative Send Cost: 7173600\n",
      "tensor(2.8513, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 3.77874160, Culminative Send Cost: 7212800\n",
      "tensor(2.8533, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 3.77650380, Culminative Send Cost: 7252000\n",
      "tensor(2.8553, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 3.77427983, Culminative Send Cost: 7291200\n",
      "tensor(2.8573, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 3.77207017, Culminative Send Cost: 7330400\n",
      "tensor(2.8593, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 3.76987410, Culminative Send Cost: 7369600\n",
      "tensor(2.8613, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 3.76769185, Culminative Send Cost: 7408800\n",
      "tensor(2.8633, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 3.76552367, Culminative Send Cost: 7448000\n",
      "tensor(2.8654, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 3.76336837, Culminative Send Cost: 7487200\n",
      "tensor(2.8674, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 3.76122689, Culminative Send Cost: 7526400\n",
      "tensor(2.8694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 3.75909853, Culminative Send Cost: 7565600\n",
      "tensor(2.8714, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 3.75698328, Culminative Send Cost: 7604800\n",
      "tensor(2.8734, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 3.75488162, Culminative Send Cost: 7644000\n",
      "tensor(2.8755, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 3.75279236, Culminative Send Cost: 7683200\n",
      "tensor(2.8775, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 3.75071621, Culminative Send Cost: 7722400\n",
      "tensor(2.8795, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 3.74865174, Culminative Send Cost: 7761600\n",
      "tensor(2.8815, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 3.74660087, Culminative Send Cost: 7800800\n",
      "tensor(2.8836, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 3.74456239, Culminative Send Cost: 7840000\n",
      "tensor(2.8856, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 3.74253631, Culminative Send Cost: 7879200\n",
      "tensor(2.8876, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 3.74052191, Culminative Send Cost: 7918400\n",
      "tensor(2.8897, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 3.73852038, Culminative Send Cost: 7957600\n",
      "tensor(2.8917, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 3.73653102, Culminative Send Cost: 7996800\n",
      "tensor(2.8937, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 3.73455334, Culminative Send Cost: 8036000\n",
      "tensor(2.8958, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 3.73258758, Culminative Send Cost: 8075200\n",
      "tensor(2.8978, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 3.73063350, Culminative Send Cost: 8114400\n",
      "tensor(2.8998, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 3.72869134, Culminative Send Cost: 8153600\n",
      "tensor(2.9019, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 3.72676063, Culminative Send Cost: 8192800\n",
      "tensor(2.9039, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 3.72484159, Culminative Send Cost: 8232000\n",
      "tensor(2.9059, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 3.72293377, Culminative Send Cost: 8271200\n",
      "tensor(2.9080, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 3.72103715, Culminative Send Cost: 8310400\n",
      "tensor(2.9100, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 3.71915197, Culminative Send Cost: 8349600\n",
      "tensor(2.9121, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 3.71727800, Culminative Send Cost: 8388800\n",
      "tensor(2.9141, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.71541524, Culminative Send Cost: 8428000\n",
      "tensor(2.9161, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.71356297, Culminative Send Cost: 8467200\n",
      "tensor(2.9182, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.71172190, Culminative Send Cost: 8506400\n",
      "tensor(2.9202, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.70989180, Culminative Send Cost: 8545600\n",
      "tensor(2.9222, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.70807242, Culminative Send Cost: 8584800\n",
      "tensor(2.9243, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.70626330, Culminative Send Cost: 8624000\n",
      "tensor(2.9263, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.70446515, Culminative Send Cost: 8663200\n",
      "tensor(2.9283, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.70267677, Culminative Send Cost: 8702400\n",
      "tensor(2.9304, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.70089936, Culminative Send Cost: 8741600\n",
      "tensor(2.9324, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.69913197, Culminative Send Cost: 8780800\n",
      "tensor(2.9344, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.69737482, Culminative Send Cost: 8820000\n",
      "tensor(2.9365, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.69562769, Culminative Send Cost: 8859200\n",
      "tensor(2.9385, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.69389153, Culminative Send Cost: 8898400\n",
      "tensor(2.9405, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.69216418, Culminative Send Cost: 8937600\n",
      "tensor(2.9426, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.69044733, Culminative Send Cost: 8976800\n",
      "tensor(2.9446, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.68874025, Culminative Send Cost: 9016000\n",
      "tensor(2.9466, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.68704295, Culminative Send Cost: 9055200\n",
      "tensor(2.9487, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.68535542, Culminative Send Cost: 9094400\n",
      "tensor(2.9507, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.68367743, Culminative Send Cost: 9133600\n",
      "tensor(2.9527, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.68200874, Culminative Send Cost: 9172800\n",
      "tensor(2.9547, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.68034935, Culminative Send Cost: 9212000\n",
      "tensor(2.9568, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.67869973, Culminative Send Cost: 9251200\n",
      "tensor(2.9588, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.67705965, Culminative Send Cost: 9290400\n",
      "tensor(2.9608, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.67542887, Culminative Send Cost: 9329600\n",
      "tensor(2.9628, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.67380643, Culminative Send Cost: 9368800\n",
      "tensor(2.9648, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.67219377, Culminative Send Cost: 9408000\n",
      "tensor(2.9669, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.67059040, Culminative Send Cost: 9447200\n",
      "tensor(2.9689, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.66899562, Culminative Send Cost: 9486400\n",
      "tensor(2.9709, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.66740966, Culminative Send Cost: 9525600\n",
      "tensor(2.9729, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.66583300, Culminative Send Cost: 9564800\n",
      "tensor(2.9749, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.66426516, Culminative Send Cost: 9604000\n",
      "tensor(2.9769, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.66270518, Culminative Send Cost: 9643200\n",
      "tensor(2.9789, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.66115499, Culminative Send Cost: 9682400\n",
      "tensor(2.9809, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.65961313, Culminative Send Cost: 9721600\n",
      "tensor(2.9829, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.65807939, Culminative Send Cost: 9760800\n",
      "tensor(2.9849, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.65655398, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[ 2.0256e-02,  2.1094e-02,  9.7596e-04,  4.6496e-03, -6.6251e-03,\n",
      "          2.1825e-02,  5.0315e-03, -3.5072e-02,  2.8665e-03, -1.7183e-02,\n",
      "         -2.1101e-02, -2.3860e-02, -1.9618e-02,  3.2767e-02, -2.4789e-02,\n",
      "         -1.0330e-02,  1.5129e-02, -9.7309e-03, -2.8402e-02,  3.1308e-02,\n",
      "          4.4725e-03,  1.2328e-02, -1.7217e-02, -1.9245e-02, -3.5145e-02,\n",
      "          2.5789e-02, -1.8652e-02,  1.4428e-02, -3.4297e-02, -6.3828e-03,\n",
      "          2.0537e-03,  2.9865e-02, -2.0815e-02, -3.2059e-02,  2.7460e-03,\n",
      "         -3.0796e-02,  2.3052e-02,  1.2714e-02, -2.2572e-02,  2.7187e-02,\n",
      "         -1.9229e-02,  3.5328e-02, -6.4907e-03, -1.7719e-02,  2.5944e-02,\n",
      "         -1.7636e-02,  1.4336e-03, -5.4868e-03,  1.5999e-02,  1.8704e-02,\n",
      "          2.0707e-02, -1.5184e-02, -2.6509e-02, -1.9871e-02,  5.0023e-03,\n",
      "         -6.5236e-03,  1.0710e-02,  2.3447e-03, -1.7635e-02,  5.4954e-03,\n",
      "         -3.3710e-02,  7.3958e-03,  2.3067e-02, -2.3863e-03, -3.7479e-03,\n",
      "          4.1044e-02,  7.0201e-02,  5.1144e-02,  6.5004e-02,  7.1908e-02,\n",
      "          6.2718e-02,  1.2176e-01,  7.6046e-02,  8.7023e-02,  1.2633e-01,\n",
      "          1.0076e-01,  9.0916e-02,  2.3793e-02,  9.7397e-03,  2.5934e-02,\n",
      "          2.9722e-02,  1.1690e-02,  9.3211e-03,  3.0728e-02, -8.0789e-03,\n",
      "          9.9559e-03,  4.4774e-03, -2.9970e-02, -3.0629e-02,  1.3879e-02,\n",
      "         -1.9174e-02,  1.7652e-02,  4.9726e-02,  7.3704e-02,  8.4719e-02,\n",
      "          7.0520e-02,  6.1106e-02,  9.1491e-02,  1.0100e-01,  1.1493e-01,\n",
      "          1.7085e-01,  2.1954e-01,  2.3638e-01,  1.8357e-01,  1.5756e-01,\n",
      "          1.3183e-01,  7.4375e-02,  6.1323e-02,  4.6076e-02, -1.2123e-02,\n",
      "         -8.5794e-03, -3.3792e-02, -8.7817e-03,  3.4140e-02, -3.5614e-03,\n",
      "         -2.2481e-02,  1.6230e-02,  7.3725e-03,  4.2899e-02,  5.3155e-02,\n",
      "          3.2927e-02,  4.0278e-02,  6.4529e-02,  8.0224e-02,  4.7207e-02,\n",
      "          5.6047e-02,  5.2800e-02,  5.4592e-02,  3.1862e-02,  5.5832e-02,\n",
      "          9.9516e-02,  1.3488e-01,  7.7528e-02,  1.0145e-01,  7.2848e-02,\n",
      "          6.2880e-02,  4.0048e-03,  1.6088e-02,  3.4300e-02,  3.8707e-03,\n",
      "         -3.0927e-02, -2.9759e-02, -2.3855e-02,  1.2654e-02, -9.9865e-03,\n",
      "         -2.7413e-02,  2.1156e-02, -9.3100e-05,  1.0693e-02,  3.3449e-02,\n",
      "          1.9090e-02,  2.3694e-02, -1.2884e-03, -8.1194e-02, -8.7621e-02,\n",
      "         -1.3797e-01, -5.8794e-02, -1.0202e-02, -3.1528e-02, -3.5672e-02,\n",
      "          3.8359e-03,  2.2086e-02,  1.1588e-02,  2.4678e-02,  6.0338e-02,\n",
      "          3.3626e-02,  3.0080e-02, -2.4711e-02,  1.7121e-02,  3.2193e-02,\n",
      "          2.2213e-02, -6.5934e-03, -3.2456e-02, -3.3436e-02, -5.6169e-02,\n",
      "         -6.7003e-02, -6.7335e-04, -1.9425e-02, -2.3489e-02, -2.4271e-02,\n",
      "         -2.2615e-02, -2.0879e-02, -4.9952e-02,  4.1120e-02,  4.6334e-02,\n",
      "          1.0464e-02, -3.0356e-02, -7.2879e-02, -8.8936e-02, -2.5429e-02,\n",
      "         -7.3568e-03,  6.5457e-02,  7.6520e-02,  6.5393e-02, -1.2032e-02,\n",
      "          3.0401e-02, -4.9246e-04,  9.7490e-03, -4.2808e-03,  2.4872e-02,\n",
      "         -1.7502e-02, -3.1289e-02, -4.3200e-02,  9.4827e-03,  1.8227e-02,\n",
      "          1.5873e-02,  5.0612e-02,  5.1527e-03,  1.6616e-02,  2.3919e-02,\n",
      "          6.9866e-02,  1.6239e-01,  1.2930e-01, -1.0170e-02, -5.5120e-02,\n",
      "         -7.8400e-02, -2.5161e-02, -3.1822e-02, -5.0423e-02,  9.8778e-02,\n",
      "          1.3393e-01,  6.5001e-02,  3.7972e-02,  3.1379e-02,  5.4073e-03,\n",
      "         -7.0561e-03,  2.9130e-03, -6.0481e-03,  3.6277e-02, -1.8823e-02,\n",
      "         -1.7078e-02,  2.8633e-02,  8.0499e-02,  7.9152e-02,  7.9740e-02,\n",
      "          1.0300e-01,  1.1405e-01,  9.8137e-02,  2.9282e-02,  6.7594e-02,\n",
      "         -9.1746e-03, -7.8145e-02, -9.9372e-02, -7.6209e-02, -3.7801e-02,\n",
      "         -1.3159e-02,  2.8301e-02,  1.1883e-01,  1.6468e-01,  8.3597e-02,\n",
      "          1.2477e-02,  2.7481e-02, -3.2344e-02, -2.8193e-02,  5.0312e-03,\n",
      "          2.1231e-02,  4.5127e-04,  4.0202e-02,  4.7037e-02,  1.1471e-01,\n",
      "          1.3026e-01,  1.0676e-01,  2.1195e-01,  2.0730e-01,  1.5791e-01,\n",
      "          8.5197e-03, -9.6688e-02, -1.4707e-01, -5.6560e-02, -2.6168e-02,\n",
      "         -3.1398e-02, -5.7298e-02, -2.2071e-02,  6.8947e-03,  4.5553e-02,\n",
      "          8.3842e-02,  1.0852e-01,  9.7506e-02,  2.5918e-02,  1.7874e-02,\n",
      "          9.2458e-03,  5.7960e-03,  2.6362e-02,  9.1013e-03,  6.8073e-02,\n",
      "          7.5030e-02,  1.5020e-01,  1.2804e-01,  1.5572e-01,  1.6633e-01,\n",
      "          1.7059e-01,  2.1661e-01,  1.7071e-01,  5.5090e-02, -1.6839e-01,\n",
      "         -1.6415e-01, -5.7052e-02, -3.0242e-02,  6.6783e-03, -2.0321e-02,\n",
      "         -3.4374e-02, -8.8381e-03,  1.2150e-02,  5.2815e-02,  3.8600e-02,\n",
      "          5.2934e-02, -1.4338e-02, -2.9652e-02,  1.6174e-02,  4.7749e-03,\n",
      "          7.1216e-04,  2.0075e-02,  5.6825e-02,  1.2062e-01,  1.6344e-01,\n",
      "          1.7823e-01,  1.6445e-01,  1.4057e-01,  1.3304e-01,  2.2201e-01,\n",
      "          2.7018e-01,  1.6972e-01, -1.0074e-01, -1.3738e-01,  6.2710e-02,\n",
      "          8.0349e-02,  5.0453e-02,  4.1701e-03,  1.0574e-02,  1.0752e-03,\n",
      "         -1.8161e-02, -2.9563e-03,  2.5333e-02,  3.4578e-02,  3.9699e-02,\n",
      "          1.6944e-02,  2.7013e-02,  3.7174e-02,  3.9790e-02,  9.8706e-04,\n",
      "          9.1102e-02,  1.2118e-01,  1.6350e-01,  1.3296e-01,  8.7236e-02,\n",
      "          1.0764e-01,  7.9326e-02,  2.0868e-01,  3.2321e-01,  1.6151e-01,\n",
      "         -3.9175e-02,  3.0323e-02,  2.1580e-01,  2.3144e-01,  1.6685e-01,\n",
      "          1.1337e-01,  7.4306e-02,  3.0632e-02, -1.3355e-02, -6.0075e-02,\n",
      "          1.2207e-02, -1.2718e-02,  3.4240e-02,  1.4705e-02, -1.3563e-02,\n",
      "          2.4038e-02, -6.0486e-03,  6.7696e-02,  7.7671e-02,  1.1310e-01,\n",
      "          4.1408e-02,  6.5764e-02,  2.4319e-02, -5.8476e-04, -1.3487e-02,\n",
      "          1.4626e-01,  3.0316e-01,  9.8192e-02, -4.1813e-03,  1.6048e-01,\n",
      "          2.3693e-01,  2.9813e-01,  1.7806e-01,  1.2624e-01,  4.4209e-02,\n",
      "         -9.6338e-03, -3.8849e-02, -5.9554e-02,  7.5691e-03,  2.3174e-02,\n",
      "          4.0038e-03, -1.5451e-02, -1.9116e-02, -1.4358e-02,  8.6978e-03,\n",
      "          3.1216e-03,  9.5751e-02,  2.4197e-02, -6.7891e-02, -7.3230e-02,\n",
      "         -2.3169e-02, -9.9363e-02, -8.5730e-02,  1.1176e-01,  2.1903e-01,\n",
      "         -1.3594e-02,  3.1794e-02,  8.4581e-02,  3.0282e-01,  2.2866e-01,\n",
      "          1.0919e-01,  1.5486e-02, -2.5682e-02, -8.1219e-02, -3.7398e-02,\n",
      "         -4.7519e-02, -4.6630e-02,  1.2769e-02,  1.5553e-02,  2.9808e-04,\n",
      "         -2.8174e-03, -3.2971e-02,  1.6467e-02,  4.1070e-02,  5.2904e-02,\n",
      "         -1.0145e-01, -9.0426e-02, -9.2140e-02, -1.4078e-01, -9.2072e-02,\n",
      "         -6.6665e-02,  1.3618e-01,  1.0809e-01, -6.4100e-03,  7.8979e-03,\n",
      "          9.9456e-02,  3.1319e-01,  1.7171e-01, -1.3374e-02, -9.1305e-02,\n",
      "         -8.2806e-02, -2.9488e-02, -4.6607e-02, -8.2147e-02,  1.1281e-02,\n",
      "         -1.6686e-02, -3.5379e-02, -9.3171e-03,  2.4369e-02, -1.9203e-02,\n",
      "         -4.2950e-03,  5.9998e-03, -1.2240e-02, -1.0060e-01, -8.3430e-02,\n",
      "         -1.2863e-01, -9.8064e-02, -5.6740e-02,  7.2908e-02,  1.7157e-01,\n",
      "          1.4539e-01, -3.2104e-02, -2.7246e-02,  1.1642e-01,  2.7094e-01,\n",
      "          6.0034e-02, -1.4055e-01, -1.4486e-01, -9.2237e-02, -6.5557e-02,\n",
      "         -2.1985e-02, -4.6772e-02, -5.0766e-02,  8.7772e-03, -4.1049e-02,\n",
      "         -5.5415e-03,  3.2919e-02,  1.4988e-02,  1.4034e-02, -1.6584e-02,\n",
      "         -3.6624e-02, -1.2370e-01, -1.1115e-01, -1.2184e-01, -3.9930e-02,\n",
      "          5.2071e-02,  1.2242e-01,  2.4223e-01,  1.1086e-01, -5.2967e-02,\n",
      "          2.3241e-02,  1.2018e-01,  1.1178e-01, -2.5561e-02, -8.6129e-02,\n",
      "         -3.0729e-02, -5.5924e-02, -7.3052e-02, -7.9609e-02, -4.2996e-02,\n",
      "         -1.0844e-02, -5.6474e-02,  9.6272e-03, -1.4224e-02,  1.9338e-02,\n",
      "         -9.6221e-03, -1.4796e-02, -1.7442e-02, -8.0818e-03, -9.0686e-02,\n",
      "         -5.2397e-02, -1.1630e-01, -5.0828e-02,  3.7701e-02,  1.3295e-01,\n",
      "          1.4676e-01,  2.8570e-02, -1.2158e-01, -3.1426e-02,  2.2771e-02,\n",
      "         -5.9986e-03, -3.5232e-02, -3.5213e-02, -1.2072e-02, -4.6967e-04,\n",
      "         -8.2607e-02, -6.3488e-02, -5.7131e-02, -3.6678e-02, -1.9345e-02,\n",
      "         -2.3403e-02,  1.6241e-02, -3.5616e-02, -6.9820e-03,  1.4475e-02,\n",
      "          1.6712e-02, -6.8043e-03, -7.1553e-02, -6.1807e-02, -5.8986e-02,\n",
      "         -3.9077e-02, -2.1023e-02,  1.6003e-02,  4.9362e-02, -6.5770e-02,\n",
      "         -1.2415e-01, -5.2588e-02, -2.4159e-02,  1.3693e-02,  4.6633e-04,\n",
      "         -3.4385e-02, -1.5830e-02, -5.0419e-02, -8.4913e-02, -7.2369e-02,\n",
      "         -1.0218e-01, -9.9474e-02,  6.7605e-04, -2.7322e-02,  1.3686e-02,\n",
      "         -2.8013e-02, -2.0989e-02, -2.5251e-02, -3.5615e-02, -5.1192e-02,\n",
      "         -3.0733e-02, -4.7122e-03,  1.3919e-03, -5.6621e-02, -6.6606e-02,\n",
      "         -4.5160e-02, -6.4816e-02, -1.6819e-02,  1.1728e-02, -2.7010e-02,\n",
      "         -3.4338e-02, -1.1427e-02, -3.1393e-02, -4.1342e-02, -4.6060e-02,\n",
      "         -2.0089e-02, -9.7519e-02, -7.0011e-02, -5.4784e-02, -7.5353e-02,\n",
      "         -5.4385e-02,  2.2177e-02,  3.0036e-02, -1.8019e-02, -2.8034e-02,\n",
      "          9.5249e-03, -6.4532e-03, -8.0449e-02, -8.5555e-02, -3.3114e-02,\n",
      "         -2.7389e-02, -7.7383e-02, -1.2182e-01, -6.6083e-02, -3.3902e-02,\n",
      "          1.6452e-02,  1.2043e-02,  2.4886e-02, -3.5524e-02, -6.6940e-03,\n",
      "         -1.5122e-02,  2.2921e-02, -4.1734e-02, -5.7499e-02, -9.3579e-02,\n",
      "         -6.0550e-02, -9.1217e-02, -6.2789e-02, -2.1926e-02, -1.5613e-02,\n",
      "         -3.0798e-02, -1.5070e-02,  2.9440e-02, -3.6350e-02, -2.8059e-02,\n",
      "         -3.5385e-02, -5.5736e-02, -7.7998e-02, -5.8802e-02, -7.7568e-02,\n",
      "         -1.3117e-01, -4.9687e-02, -6.8061e-03,  1.2089e-02, -2.3779e-02,\n",
      "         -6.7937e-02, -1.2955e-01, -4.9212e-02, -6.5779e-02,  2.5268e-03,\n",
      "         -4.2425e-02,  2.0317e-02, -9.3727e-03, -4.2882e-02,  5.1944e-03,\n",
      "         -5.1876e-02,  3.5250e-04, -1.7383e-02,  2.4641e-02, -3.8156e-03,\n",
      "          1.9977e-02,  9.8142e-04, -4.1409e-02, -2.2739e-02, -7.1719e-02,\n",
      "         -5.6792e-02, -2.3365e-02, -6.0233e-02, -4.3829e-02, -3.9967e-02,\n",
      "          1.4149e-02,  3.3407e-02,  2.5696e-02, -2.3026e-02, -2.7816e-02,\n",
      "         -1.4058e-02,  2.1057e-02,  5.6975e-02,  5.7695e-02,  9.0641e-02,\n",
      "          4.6753e-02,  2.0126e-02,  2.2248e-02,  3.4495e-03, -2.5470e-02,\n",
      "          2.7411e-02, -2.4932e-03, -2.0933e-02,  1.4564e-02,  7.8916e-03,\n",
      "          2.0756e-02, -2.1538e-02,  1.6661e-02,  6.9933e-03,  7.1220e-02,\n",
      "          1.4274e-01,  1.4748e-01,  1.3320e-01,  1.2236e-01,  1.1584e-01,\n",
      "          9.6406e-02,  7.8803e-02,  1.0885e-01,  1.3667e-01,  1.6833e-01,\n",
      "          1.7778e-01,  1.5127e-01,  1.4726e-01,  8.4983e-02,  5.8115e-02,\n",
      "          4.1771e-02, -1.3549e-02, -5.9846e-03, -2.2751e-02,  6.9896e-03,\n",
      "          1.9871e-02,  1.4336e-02, -2.2982e-03, -7.7632e-03, -4.7797e-03,\n",
      "          6.2932e-03,  8.5979e-02,  1.4097e-01,  1.5170e-01,  2.3794e-01,\n",
      "          2.1943e-01,  2.3928e-01,  2.5369e-01,  2.7919e-01,  3.0504e-01,\n",
      "          2.6485e-01,  3.0383e-01,  2.6873e-01,  2.1005e-01,  1.6270e-01,\n",
      "          1.2064e-01,  6.4726e-02,  2.5579e-02,  6.5169e-03,  2.2542e-02,\n",
      "          2.1315e-02, -2.6877e-02,  4.6417e-03,  2.2478e-02,  3.3599e-02,\n",
      "          3.1547e-02, -1.8274e-02,  3.4023e-02,  9.5896e-03,  2.9353e-03,\n",
      "          8.7557e-02,  1.0224e-01,  1.0346e-01,  1.4334e-01,  1.4625e-01,\n",
      "          1.1723e-01,  1.9322e-01,  1.5399e-01,  1.8251e-01,  1.6012e-01,\n",
      "          1.1264e-01,  1.0751e-01,  8.7730e-02,  6.9294e-02,  2.5781e-02,\n",
      "          3.7646e-02,  1.2029e-02,  1.0916e-02,  3.5491e-02,  3.2822e-02,\n",
      "         -3.3113e-02, -1.2207e-02,  8.9842e-03, -3.2644e-02, -3.3672e-02,\n",
      "          8.8835e-03,  4.9748e-03,  3.7745e-02,  3.7365e-02, -2.3713e-02,\n",
      "          5.3743e-04, -1.5223e-02, -1.6718e-02,  4.7925e-02, -4.2049e-03,\n",
      "          1.9966e-02,  5.3197e-02,  3.7443e-02,  4.7455e-02,  3.1012e-02,\n",
      "          2.7756e-02,  3.8085e-02, -3.2500e-02,  2.9465e-02,  2.3655e-02,\n",
      "         -2.1201e-02,  1.0422e-02, -1.8044e-03,  2.1717e-02]])\n",
      "activation_stack.0.bias: tensor([1.5357])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for local_update_epochs is 3 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[ 1.1933e-02, -1.9147e-02,  1.8220e-02,  7.4622e-03, -3.2880e-02,\n",
      "         -2.3076e-02,  3.2804e-02,  4.9011e-03, -4.7908e-03,  1.0213e-02,\n",
      "          2.6629e-02,  2.8018e-02, -5.5802e-03,  2.4019e-02, -7.1732e-03,\n",
      "          5.6952e-03, -3.3956e-02, -3.1555e-02,  1.4554e-02, -1.9959e-02,\n",
      "          2.7410e-02, -3.5489e-02, -1.8207e-02, -4.8126e-03,  2.1642e-02,\n",
      "          1.1476e-02,  9.1914e-03,  2.8835e-03, -6.5598e-03, -1.7171e-02,\n",
      "         -1.3088e-02, -1.1427e-02, -1.1535e-02,  2.3083e-02, -3.4116e-02,\n",
      "          3.5159e-02, -1.2632e-02, -1.6057e-02,  5.2583e-04, -1.4718e-02,\n",
      "          1.7994e-02, -2.7958e-02, -7.7251e-03, -1.9940e-02, -2.4249e-02,\n",
      "          1.0807e-02,  2.5282e-02,  3.5299e-02, -4.8399e-05,  4.5131e-03,\n",
      "         -2.6314e-02, -7.8208e-03, -2.9044e-02,  1.9611e-02,  1.7303e-03,\n",
      "          1.6824e-02,  6.7530e-03, -1.2362e-02,  2.3799e-02,  7.9362e-03,\n",
      "         -7.7603e-03,  6.5762e-03,  1.5311e-02,  4.5107e-03,  1.7990e-02,\n",
      "          6.3696e-03,  1.3127e-02, -9.2175e-04, -4.6492e-03,  7.4142e-03,\n",
      "         -2.1240e-02,  1.7909e-02,  1.3505e-02,  1.7369e-02,  3.1573e-02,\n",
      "         -3.5237e-02,  1.4415e-02, -1.9931e-02,  8.0178e-03,  1.6093e-02,\n",
      "          3.4407e-02,  9.2457e-03, -5.3698e-04, -8.1731e-03,  1.0477e-02,\n",
      "         -2.5945e-02, -1.1365e-02,  3.3598e-02, -1.9373e-02, -3.0360e-02,\n",
      "         -2.3221e-02,  1.2595e-03,  1.1165e-02,  5.0856e-03,  1.3399e-02,\n",
      "         -2.7041e-02,  9.9125e-03,  2.6011e-02, -1.1043e-02,  3.4755e-02,\n",
      "         -3.7376e-03,  2.0877e-02, -2.8617e-02,  3.3848e-02,  2.9478e-05,\n",
      "         -1.6173e-02, -8.8147e-03,  1.2242e-02,  3.3423e-02, -2.7637e-02,\n",
      "         -3.9865e-03, -3.4179e-02, -4.3884e-06, -1.2382e-02,  2.0256e-02,\n",
      "          2.2870e-02,  5.0790e-03, -1.9472e-02, -4.9161e-03, -1.0218e-02,\n",
      "          7.8093e-03, -1.9226e-02, -7.7484e-04, -1.2271e-03,  3.5429e-02,\n",
      "          8.4016e-03, -2.7268e-02, -9.0546e-03,  3.2731e-02,  2.0777e-02,\n",
      "          2.4826e-02,  3.1094e-02, -1.0255e-02,  2.3172e-02, -1.6660e-03,\n",
      "          2.3082e-02, -3.2690e-02,  2.3618e-02, -1.3329e-02, -1.5075e-03,\n",
      "         -1.5890e-02,  1.6124e-02,  3.5043e-02, -3.5502e-02,  3.5154e-02,\n",
      "          1.5235e-02,  8.9593e-03, -1.0523e-02, -6.3135e-03,  2.4660e-02,\n",
      "         -1.4194e-02, -2.0943e-02, -3.4839e-02, -1.6083e-02,  2.0681e-02,\n",
      "         -3.4948e-02,  3.1127e-02, -2.3393e-02, -8.7538e-04,  1.4618e-02,\n",
      "         -1.1503e-02,  1.1221e-02, -2.9597e-02, -2.8386e-02, -2.4037e-02,\n",
      "          9.1534e-03,  1.0712e-02,  7.5460e-03, -2.7187e-02,  4.2469e-03,\n",
      "          2.8331e-02,  3.3264e-02, -1.8712e-02, -1.0845e-02, -1.9898e-03,\n",
      "          1.7495e-02, -2.3998e-02,  2.4650e-02,  2.0017e-02,  3.2929e-02,\n",
      "         -1.1019e-02, -6.5119e-03, -4.1679e-03, -8.8405e-03,  2.6837e-02,\n",
      "          3.3150e-02, -2.8998e-02, -3.3226e-02,  1.3528e-02,  1.1374e-02,\n",
      "          1.4917e-02,  2.0142e-02,  3.0825e-02, -2.6460e-02,  2.2861e-02,\n",
      "         -1.6270e-02,  2.5404e-02, -5.0660e-03,  1.1936e-02,  2.4415e-02,\n",
      "          2.2295e-02, -1.8269e-02,  2.3368e-02, -6.8391e-03,  1.6100e-02,\n",
      "         -7.4478e-03, -4.0720e-03,  6.0776e-03, -3.4817e-02,  2.5014e-02,\n",
      "          2.1772e-02,  3.1318e-02,  2.3679e-02,  2.3787e-02,  2.6010e-02,\n",
      "         -1.5486e-02,  7.8174e-03,  3.5183e-02, -3.3852e-02,  2.6405e-02,\n",
      "         -1.2385e-03,  1.1160e-02,  1.5107e-03,  1.2029e-02,  1.8169e-02,\n",
      "          1.6728e-02,  1.7246e-02,  1.4650e-02,  1.1434e-02, -5.2180e-03,\n",
      "          2.0958e-02,  2.4457e-02,  2.0948e-02,  3.8954e-03, -1.8874e-02,\n",
      "          5.2143e-04,  2.9668e-02,  2.1731e-02, -2.7088e-02, -2.3898e-02,\n",
      "         -1.3707e-02,  2.1717e-02,  2.1417e-02,  2.2393e-03, -2.1368e-02,\n",
      "          2.4372e-02,  1.1793e-02, -9.3534e-03,  2.0095e-02,  2.4868e-02,\n",
      "          9.0961e-03,  6.7918e-03,  2.9369e-02,  3.3042e-02,  1.8720e-03,\n",
      "          3.8403e-03,  3.1661e-02,  3.4988e-02, -2.0703e-02,  5.3625e-03,\n",
      "         -5.2992e-03, -2.5910e-02,  1.0564e-02,  1.3031e-02,  2.7781e-02,\n",
      "          1.7616e-02, -2.7527e-02,  1.6359e-02, -2.0127e-02, -1.7540e-02,\n",
      "          1.9028e-02, -1.5107e-02, -3.1984e-02, -6.9293e-03, -4.8489e-03,\n",
      "         -3.0903e-02,  1.7224e-02,  2.1474e-02,  3.0465e-02,  8.4814e-03,\n",
      "         -8.4854e-04,  2.6308e-02, -6.0266e-04, -2.4769e-02,  1.1251e-02,\n",
      "         -1.2960e-02, -1.1989e-02, -6.3370e-03, -1.9477e-02, -3.4391e-02,\n",
      "          2.7432e-02, -9.0873e-03, -6.3982e-03,  2.0698e-03,  1.3822e-02,\n",
      "          2.2722e-02,  2.5032e-03, -1.4166e-02,  2.4618e-02, -3.4168e-02,\n",
      "          2.8009e-02,  2.5323e-02,  3.8891e-03,  2.8119e-03, -8.8304e-04,\n",
      "         -3.1842e-02,  2.6236e-02,  9.5746e-03,  1.1683e-02, -2.6387e-02,\n",
      "         -1.0752e-02,  1.5208e-02, -1.0516e-02,  2.7202e-02, -1.2046e-02,\n",
      "         -2.0850e-02, -2.6377e-02, -2.4800e-02, -3.2709e-02,  2.6780e-02,\n",
      "          5.4202e-03, -1.4799e-02, -1.2072e-02,  2.5299e-02, -2.2000e-02,\n",
      "          2.7185e-02,  3.0492e-02,  2.0320e-02, -2.9577e-02,  3.5051e-02,\n",
      "         -7.9148e-04, -1.1653e-02, -3.3248e-03,  3.3790e-03,  3.0576e-02,\n",
      "          3.2328e-02, -3.1606e-02,  1.8771e-02,  2.0977e-02, -2.5566e-02,\n",
      "          9.5276e-03, -1.1088e-02, -3.8151e-03, -2.9259e-02,  2.4074e-02,\n",
      "         -3.0012e-02,  9.3571e-03, -1.9975e-02, -1.6732e-02,  1.8217e-02,\n",
      "         -3.1099e-02, -4.4197e-03,  2.5241e-02,  9.3328e-03, -3.0715e-03,\n",
      "         -3.6309e-03,  2.2965e-02, -2.8106e-02,  2.9760e-02,  1.9501e-02,\n",
      "          1.5459e-03,  2.4051e-02, -2.1940e-02,  2.6465e-02, -1.3425e-02,\n",
      "          9.1740e-03, -4.1716e-03,  1.3857e-02, -2.2388e-02,  2.9646e-02,\n",
      "         -2.4706e-02, -3.3877e-02, -9.2784e-03, -2.7168e-02,  2.7980e-02,\n",
      "         -2.4615e-02,  2.9976e-02,  1.8205e-03,  1.7433e-02,  2.8441e-02,\n",
      "         -3.4759e-02, -1.8539e-02,  1.8150e-02,  3.0403e-02,  1.5661e-02,\n",
      "         -5.4848e-03,  7.7499e-03,  1.7491e-02, -3.5405e-02, -1.9577e-02,\n",
      "          7.3217e-03, -2.8264e-03,  1.0271e-02, -3.0563e-02, -2.4243e-02,\n",
      "          8.7894e-03, -1.9010e-02, -2.8954e-02,  3.3392e-02,  2.4620e-02,\n",
      "          9.6908e-03,  6.2197e-03, -2.3640e-02, -3.5055e-02, -3.3453e-02,\n",
      "          2.6334e-02,  3.4389e-02, -1.8964e-02,  1.4100e-02,  3.3234e-02,\n",
      "         -1.1074e-02, -1.4049e-02,  2.8729e-02,  7.6617e-03, -3.3626e-02,\n",
      "         -1.6684e-03, -8.2622e-03,  3.5045e-02, -3.2965e-02,  5.5994e-03,\n",
      "          2.9098e-02,  2.0410e-02, -1.4276e-02,  2.7952e-02, -1.2586e-02,\n",
      "          6.4593e-03,  5.5035e-03, -1.7460e-02, -5.9692e-03,  8.4434e-03,\n",
      "         -1.2073e-02,  7.1057e-03, -8.8509e-03,  2.8630e-02,  2.5243e-03,\n",
      "          3.2891e-03,  1.0546e-02, -2.5959e-03, -2.3650e-02, -9.8836e-03,\n",
      "          1.4758e-02, -2.6193e-02,  7.5275e-04,  1.2662e-02, -6.9389e-03,\n",
      "         -1.4265e-02,  2.1221e-02,  1.8736e-02,  2.5193e-02, -1.4661e-03,\n",
      "          2.4735e-02,  2.8826e-02,  2.0816e-02,  3.1876e-02, -3.3153e-02,\n",
      "          1.8078e-02,  9.0248e-03,  2.8051e-02,  2.4901e-02, -5.0460e-03,\n",
      "          1.4621e-02,  1.9579e-02, -1.7837e-02, -1.9601e-02, -1.1442e-02,\n",
      "         -2.8983e-02,  2.9687e-02, -2.2274e-02,  2.0119e-02,  3.6607e-03,\n",
      "          2.7627e-02,  8.6582e-03, -2.0192e-02,  2.6459e-02,  2.5626e-02,\n",
      "          1.4340e-02,  8.7673e-03, -1.2514e-02,  2.2088e-03,  8.0449e-03,\n",
      "         -3.2481e-03,  6.6707e-04,  3.4510e-03, -7.2398e-03,  5.8076e-03,\n",
      "         -1.3314e-02, -2.3782e-02,  3.1888e-02, -2.1006e-02, -4.3736e-03,\n",
      "         -2.8354e-02,  2.6727e-02,  2.8254e-02, -2.3424e-02, -1.9769e-02,\n",
      "          2.4828e-02, -2.0920e-02,  1.3334e-02,  2.2627e-02, -1.3233e-02,\n",
      "         -2.4784e-02,  3.0742e-02, -2.6036e-02,  1.1525e-02,  3.1343e-02,\n",
      "         -2.7891e-02,  2.0063e-02,  3.3712e-02,  5.4681e-03, -3.1085e-02,\n",
      "         -1.9515e-02, -2.2112e-02,  2.8181e-03,  5.2967e-03,  9.0739e-03,\n",
      "          8.7089e-03, -3.0548e-02, -2.4680e-02,  1.0251e-02,  1.0647e-02,\n",
      "          3.9939e-03,  7.1232e-03,  3.2939e-02, -4.4080e-04,  2.8233e-02,\n",
      "         -3.5635e-02,  2.5476e-02, -2.5961e-02,  2.1507e-02, -1.9407e-02,\n",
      "          4.9933e-03, -3.5537e-03,  1.2028e-02,  2.2355e-02,  8.3754e-03,\n",
      "         -1.5041e-02, -2.6824e-03, -3.3877e-03, -2.7502e-02,  2.5627e-02,\n",
      "          2.4095e-02,  7.3241e-03,  7.3910e-03,  1.6024e-02,  3.4278e-02,\n",
      "          2.2436e-02, -9.0247e-04,  2.2444e-02,  4.5750e-05,  8.5197e-03,\n",
      "         -2.5945e-02, -7.6702e-03,  3.4750e-02, -1.7379e-02,  2.8057e-02,\n",
      "         -2.0475e-02,  1.5716e-02,  1.1798e-02,  8.2122e-03,  2.4903e-02,\n",
      "         -3.3675e-02, -1.9464e-02,  1.4363e-02, -2.7396e-02, -2.2253e-02,\n",
      "         -3.2146e-02, -3.8376e-03,  1.1411e-02, -2.2378e-02, -8.9471e-03,\n",
      "          1.7433e-02,  2.4767e-02, -2.0239e-02,  4.1933e-03,  1.5018e-02,\n",
      "          2.2628e-02, -1.9986e-02,  2.6263e-02,  2.4156e-02,  4.1387e-03,\n",
      "          1.3132e-02, -2.4610e-02, -2.5582e-02, -2.7223e-02, -1.9506e-02,\n",
      "          3.1010e-02,  1.2866e-02,  1.8856e-02,  4.7494e-03,  3.1601e-03,\n",
      "         -3.2419e-02,  3.4595e-02,  1.2818e-02,  1.4505e-02,  1.3144e-02,\n",
      "          1.7403e-02, -2.5046e-02, -2.3458e-03,  3.2880e-02,  8.7343e-03,\n",
      "         -2.3105e-02, -3.0054e-02,  1.0373e-02, -2.7107e-02, -3.8161e-03,\n",
      "          1.6879e-02,  2.7818e-03, -3.3341e-02, -3.5406e-02,  6.8568e-03,\n",
      "          2.9334e-02, -2.8519e-02,  7.8847e-03,  9.8371e-03,  3.0621e-02,\n",
      "          2.2337e-02,  2.7914e-02, -2.3688e-02,  8.0529e-04, -1.2091e-02,\n",
      "          1.9930e-02,  3.0587e-02,  2.5723e-02,  8.6014e-03,  1.3626e-02,\n",
      "         -1.7495e-02,  1.6864e-02, -2.1866e-02,  9.0452e-03, -2.4368e-02,\n",
      "         -1.6667e-02,  2.1152e-03,  3.4759e-02,  3.2607e-02,  2.8550e-02,\n",
      "         -2.6478e-02,  2.9862e-03,  1.5771e-03, -3.1717e-02, -1.4597e-02,\n",
      "         -2.4266e-02, -2.5466e-02,  1.6956e-02,  1.7100e-02, -2.3177e-02,\n",
      "          2.6669e-03, -2.2859e-02, -1.9085e-02, -6.3174e-03, -3.4130e-02,\n",
      "         -1.0797e-02,  2.6147e-02,  3.2108e-02, -1.5156e-02, -2.2948e-02,\n",
      "          3.4394e-02, -2.4312e-02,  2.5492e-02, -2.5754e-02, -1.6070e-02,\n",
      "          2.8131e-02, -9.5452e-04,  2.9906e-02,  2.0231e-02, -3.2407e-03,\n",
      "         -9.9982e-03,  1.9484e-02, -1.3114e-02, -1.5400e-02, -3.3641e-02,\n",
      "          1.6654e-02, -2.1805e-02, -1.3836e-02, -3.1616e-02,  1.9663e-02,\n",
      "          2.2631e-02, -3.3520e-02,  2.5370e-02,  3.0934e-02,  2.9076e-02,\n",
      "         -2.3155e-02, -4.5316e-04, -1.5779e-02,  1.6393e-02, -1.7952e-02,\n",
      "         -3.0325e-02,  2.3811e-02, -2.0505e-02,  2.7381e-02,  3.0354e-02,\n",
      "          1.0947e-02, -2.1671e-02,  2.7420e-02,  2.2535e-02, -2.2655e-02,\n",
      "          2.1986e-02,  2.9581e-02, -1.6685e-02,  2.8827e-02,  2.8136e-02,\n",
      "          3.5260e-02,  1.5433e-02, -4.8316e-03, -2.7068e-03,  2.3711e-02,\n",
      "         -2.6256e-02,  2.9830e-02,  1.8265e-02,  2.4188e-02, -3.9582e-03,\n",
      "         -7.7412e-03,  5.5736e-04, -2.9083e-03, -3.4809e-02,  2.5089e-03,\n",
      "          1.6730e-02, -3.4249e-03,  8.5985e-03,  1.0945e-02,  2.6213e-02,\n",
      "         -2.0124e-02,  3.4464e-02, -2.2530e-02, -2.6274e-02,  1.3279e-02,\n",
      "          2.3839e-02,  3.8892e-04,  2.0541e-02,  2.1812e-02,  5.6756e-03,\n",
      "          9.1910e-03,  3.3824e-03, -2.4710e-02,  1.9510e-02,  1.9978e-02,\n",
      "         -1.7102e-02,  1.1251e-02,  1.8278e-02,  5.9151e-03,  1.2432e-02,\n",
      "         -2.3024e-02, -6.0679e-03, -3.2796e-02, -9.7556e-03,  5.3805e-03,\n",
      "         -8.3159e-03, -1.5389e-03, -2.5814e-02, -2.7836e-02, -2.9292e-03,\n",
      "         -4.1268e-03,  8.1570e-03, -1.6340e-02,  3.3298e-02, -2.4814e-02,\n",
      "         -1.2427e-02, -7.7258e-03, -1.5208e-02, -2.1997e-02,  3.1935e-02,\n",
      "          9.1048e-03,  4.3159e-04,  1.3249e-02,  2.5228e-04,  3.0055e-02,\n",
      "         -1.4296e-02,  2.9837e-02, -9.7338e-03,  2.7976e-02,  1.2774e-02,\n",
      "         -1.7967e-02, -5.1457e-03,  1.5641e-02, -7.5905e-03,  3.4743e-02,\n",
      "         -2.8942e-02,  7.3100e-03,  2.9064e-02,  2.3552e-02,  3.4123e-02,\n",
      "          1.5293e-02, -2.8307e-02,  1.3265e-02,  9.9117e-03]])), ('activation_stack.0.bias', tensor([0.0215]))])\n",
      "tensor(0.7293, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 19.69168282, Culminative Send Cost: 3920\n",
      "tensor(2.3080, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 8.00734901, Culminative Send Cost: 39200\n",
      "tensor(2.5544, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 6.90723658, Culminative Send Cost: 78400\n",
      "tensor(2.6340, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 6.26821709, Culminative Send Cost: 117600\n",
      "tensor(2.6897, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 5.84202290, Culminative Send Cost: 156800\n",
      "tensor(2.7352, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 5.55071306, Culminative Send Cost: 196000\n",
      "tensor(2.7727, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 5.34598637, Culminative Send Cost: 235200\n",
      "tensor(2.8032, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 5.19742393, Culminative Send Cost: 274400\n",
      "tensor(2.8280, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 5.08577538, Culminative Send Cost: 313600\n",
      "tensor(2.8477, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 4.99878407, Culminative Send Cost: 352800\n",
      "tensor(2.8631, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 4.92859173, Culminative Send Cost: 392000\n",
      "tensor(2.8750, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 4.87011528, Culminative Send Cost: 431200\n",
      "tensor(2.8839, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 4.82003021, Culminative Send Cost: 470400\n",
      "tensor(2.8903, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 4.77613544, Culminative Send Cost: 509600\n",
      "tensor(2.8946, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 4.73694849, Culminative Send Cost: 548800\n",
      "tensor(2.8972, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 4.70145321, Culminative Send Cost: 588000\n",
      "tensor(2.8984, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 4.66893578, Culminative Send Cost: 627200\n",
      "tensor(2.8984, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 4.63888502, Culminative Send Cost: 666400\n",
      "tensor(2.8975, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 4.61092138, Culminative Send Cost: 705600\n",
      "tensor(2.8958, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 4.58475876, Culminative Send Cost: 744800\n",
      "tensor(2.8935, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 4.56017303, Culminative Send Cost: 784000\n",
      "tensor(2.8907, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 4.53698397, Culminative Send Cost: 823200\n",
      "tensor(2.8875, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 4.51504469, Culminative Send Cost: 862400\n",
      "tensor(2.8841, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 4.49423265, Culminative Send Cost: 901600\n",
      "tensor(2.8804, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 4.47444344, Culminative Send Cost: 940800\n",
      "tensor(2.8766, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 4.45558691, Culminative Send Cost: 980000\n",
      "tensor(2.8727, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 4.43758345, Culminative Send Cost: 1019200\n",
      "tensor(2.8687, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 4.42036486, Culminative Send Cost: 1058400\n",
      "tensor(2.8647, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.40386868, Culminative Send Cost: 1097600\n",
      "tensor(2.8607, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.38803959, Culminative Send Cost: 1136800\n",
      "tensor(2.8567, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.37282944, Culminative Send Cost: 1176000\n",
      "tensor(2.8528, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.35819244, Culminative Send Cost: 1215200\n",
      "tensor(2.8489, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.34408903, Culminative Send Cost: 1254400\n",
      "tensor(2.8451, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.33048296, Culminative Send Cost: 1293600\n",
      "tensor(2.8414, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.31733942, Culminative Send Cost: 1332800\n",
      "tensor(2.8378, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.30463076, Culminative Send Cost: 1372000\n",
      "tensor(2.8343, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.29232645, Culminative Send Cost: 1411200\n",
      "tensor(2.8309, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.28040361, Culminative Send Cost: 1450400\n",
      "tensor(2.8276, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.26883745, Culminative Send Cost: 1489600\n",
      "tensor(2.8244, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.25760698, Culminative Send Cost: 1528800\n",
      "tensor(2.8213, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.24669313, Culminative Send Cost: 1568000\n",
      "tensor(2.8184, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.23607779, Culminative Send Cost: 1607200\n",
      "tensor(2.8156, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.22574425, Culminative Send Cost: 1646400\n",
      "tensor(2.8128, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.21567774, Culminative Send Cost: 1685600\n",
      "tensor(2.8102, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.20586300, Culminative Send Cost: 1724800\n",
      "tensor(2.8077, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.19628716, Culminative Send Cost: 1764000\n",
      "tensor(2.8054, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.18693876, Culminative Send Cost: 1803200\n",
      "tensor(2.8031, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.17780590, Culminative Send Cost: 1842400\n",
      "tensor(2.8010, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.16887856, Culminative Send Cost: 1881600\n",
      "tensor(2.7990, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.16014671, Culminative Send Cost: 1920800\n",
      "tensor(2.7970, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.15160131, Culminative Send Cost: 1960000\n",
      "tensor(2.7952, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.14323425, Culminative Send Cost: 1999200\n",
      "tensor(2.7935, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.13503742, Culminative Send Cost: 2038400\n",
      "tensor(2.7920, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.12700319, Culminative Send Cost: 2077600\n",
      "tensor(2.7905, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.11912489, Culminative Send Cost: 2116800\n",
      "tensor(2.7891, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 4.11139679, Culminative Send Cost: 2156000\n",
      "tensor(2.7878, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 4.10381222, Culminative Send Cost: 2195200\n",
      "tensor(2.7866, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 4.09636593, Culminative Send Cost: 2234400\n",
      "tensor(2.7856, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 4.08905125, Culminative Send Cost: 2273600\n",
      "tensor(2.7846, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 4.08186483, Culminative Send Cost: 2312800\n",
      "tensor(2.7837, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 4.07480097, Culminative Send Cost: 2352000\n",
      "tensor(2.7829, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 4.06785583, Culminative Send Cost: 2391200\n",
      "tensor(2.7822, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 4.06102562, Culminative Send Cost: 2430400\n",
      "tensor(2.7816, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 4.05430603, Culminative Send Cost: 2469600\n",
      "tensor(2.7810, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 4.04769182, Culminative Send Cost: 2508800\n",
      "tensor(2.7806, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 4.04118204, Culminative Send Cost: 2548000\n",
      "tensor(2.7802, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 4.03477144, Culminative Send Cost: 2587200\n",
      "tensor(2.7799, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 4.02845907, Culminative Send Cost: 2626400\n",
      "tensor(2.7797, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 4.02223921, Culminative Send Cost: 2665600\n",
      "tensor(2.7796, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 4.01611090, Culminative Send Cost: 2704800\n",
      "tensor(2.7796, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 4.01007080, Culminative Send Cost: 2744000\n",
      "tensor(2.7796, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 4.00411654, Culminative Send Cost: 2783200\n",
      "tensor(2.7797, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 3.99824595, Culminative Send Cost: 2822400\n",
      "tensor(2.7798, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 3.99245691, Culminative Send Cost: 2861600\n",
      "tensor(2.7801, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 3.98674655, Culminative Send Cost: 2900800\n",
      "tensor(2.7804, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 3.98111224, Culminative Send Cost: 2940000\n",
      "tensor(2.7808, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 3.97555351, Culminative Send Cost: 2979200\n",
      "tensor(2.7812, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 3.97006845, Culminative Send Cost: 3018400\n",
      "tensor(2.7817, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 3.96465349, Culminative Send Cost: 3057600\n",
      "tensor(2.7822, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 3.95930839, Culminative Send Cost: 3096800\n",
      "tensor(2.7829, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 3.95403028, Culminative Send Cost: 3136000\n",
      "tensor(2.7835, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 3.94881988, Culminative Send Cost: 3175200\n",
      "tensor(2.7843, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 3.94367337, Culminative Send Cost: 3214400\n",
      "tensor(2.7850, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 3.93858981, Culminative Send Cost: 3253600\n",
      "tensor(2.7859, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 3.93356824, Culminative Send Cost: 3292800\n",
      "tensor(2.7868, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 3.92860651, Culminative Send Cost: 3332000\n",
      "tensor(2.7877, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 3.92370439, Culminative Send Cost: 3371200\n",
      "tensor(2.7887, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 3.91886020, Culminative Send Cost: 3410400\n",
      "tensor(2.7898, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 3.91407299, Culminative Send Cost: 3449600\n",
      "tensor(2.7909, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 3.90934086, Culminative Send Cost: 3488800\n",
      "tensor(2.7920, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 3.90466261, Culminative Send Cost: 3528000\n",
      "tensor(2.7932, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 3.90003824, Culminative Send Cost: 3567200\n",
      "tensor(2.7944, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 3.89546657, Culminative Send Cost: 3606400\n",
      "tensor(2.7957, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 3.89094639, Culminative Send Cost: 3645600\n",
      "tensor(2.7970, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 3.88647556, Culminative Send Cost: 3684800\n",
      "tensor(2.7983, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 3.88205481, Culminative Send Cost: 3724000\n",
      "tensor(2.7997, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 3.87768245, Culminative Send Cost: 3763200\n",
      "tensor(2.8011, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 3.87335706, Culminative Send Cost: 3802400\n",
      "tensor(2.8026, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 3.86907864, Culminative Send Cost: 3841600\n",
      "tensor(2.8041, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 3.86484647, Culminative Send Cost: 3880800\n",
      "tensor(2.8056, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 3.86065912, Culminative Send Cost: 3920000\n",
      "tensor(2.8072, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 3.85651612, Culminative Send Cost: 3959200\n",
      "tensor(2.8088, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 3.85241675, Culminative Send Cost: 3998400\n",
      "tensor(2.8104, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 3.84836078, Culminative Send Cost: 4037600\n",
      "tensor(2.8121, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 3.84434628, Culminative Send Cost: 4076800\n",
      "tensor(2.8138, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 3.84037352, Culminative Send Cost: 4116000\n",
      "tensor(2.8155, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 3.83644199, Culminative Send Cost: 4155200\n",
      "tensor(2.8173, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 3.83255053, Culminative Send Cost: 4194400\n",
      "tensor(2.8191, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 3.82869816, Culminative Send Cost: 4233600\n",
      "tensor(2.8209, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 3.82488441, Culminative Send Cost: 4272800\n",
      "tensor(2.8227, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 3.82111025, Culminative Send Cost: 4312000\n",
      "tensor(2.8246, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 3.81737256, Culminative Send Cost: 4351200\n",
      "tensor(2.8265, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 3.81367254, Culminative Send Cost: 4390400\n",
      "tensor(2.8284, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 3.81000948, Culminative Send Cost: 4429600\n",
      "tensor(2.8303, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 3.80638146, Culminative Send Cost: 4468800\n",
      "tensor(2.8323, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 3.80278993, Culminative Send Cost: 4508000\n",
      "tensor(2.8343, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 3.79923272, Culminative Send Cost: 4547200\n",
      "tensor(2.8363, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 3.79571033, Culminative Send Cost: 4586400\n",
      "tensor(2.8383, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 3.79222202, Culminative Send Cost: 4625600\n",
      "tensor(2.8403, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 3.78876710, Culminative Send Cost: 4664800\n",
      "tensor(2.8424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 3.78534508, Culminative Send Cost: 4704000\n",
      "tensor(2.8445, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 3.78195572, Culminative Send Cost: 4743200\n",
      "tensor(2.8466, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 3.77859879, Culminative Send Cost: 4782400\n",
      "tensor(2.8487, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 3.77527356, Culminative Send Cost: 4821600\n",
      "tensor(2.8508, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 3.77197957, Culminative Send Cost: 4860800\n",
      "tensor(2.8530, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 3.76871657, Culminative Send Cost: 4900000\n",
      "tensor(2.8551, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 3.76548362, Culminative Send Cost: 4939200\n",
      "tensor(2.8573, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 3.76228094, Culminative Send Cost: 4978400\n",
      "tensor(2.8595, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 3.75910830, Culminative Send Cost: 5017600\n",
      "tensor(2.8617, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 3.75596428, Culminative Send Cost: 5056800\n",
      "tensor(2.8639, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 3.75285029, Culminative Send Cost: 5096000\n",
      "tensor(2.8662, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 3.74976373, Culminative Send Cost: 5135200\n",
      "tensor(2.8684, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 3.74670529, Culminative Send Cost: 5174400\n",
      "tensor(2.8707, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 3.74367571, Culminative Send Cost: 5213600\n",
      "tensor(2.8730, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 3.74067235, Culminative Send Cost: 5252800\n",
      "tensor(2.8753, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 3.73769712, Culminative Send Cost: 5292000\n",
      "tensor(2.8776, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 3.73474765, Culminative Send Cost: 5331200\n",
      "tensor(2.8799, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 3.73182511, Culminative Send Cost: 5370400\n",
      "tensor(2.8822, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 3.72892857, Culminative Send Cost: 5409600\n",
      "tensor(2.8845, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 3.72605848, Culminative Send Cost: 5448800\n",
      "tensor(2.8868, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 3.72321296, Culminative Send Cost: 5488000\n",
      "tensor(2.8892, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 3.72039318, Culminative Send Cost: 5527200\n",
      "tensor(2.8915, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 3.71759796, Culminative Send Cost: 5566400\n",
      "tensor(2.8939, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 3.71482730, Culminative Send Cost: 5605600\n",
      "tensor(2.8963, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 3.71208096, Culminative Send Cost: 5644800\n",
      "tensor(2.8987, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 3.70935845, Culminative Send Cost: 5684000\n",
      "tensor(2.9010, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 3.70666051, Culminative Send Cost: 5723200\n",
      "tensor(2.9034, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 3.70398498, Culminative Send Cost: 5762400\n",
      "tensor(2.9058, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 3.70133305, Culminative Send Cost: 5801600\n",
      "tensor(2.9082, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 3.69870400, Culminative Send Cost: 5840800\n",
      "tensor(2.9107, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 3.69609785, Culminative Send Cost: 5880000\n",
      "tensor(2.9131, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 3.69351387, Culminative Send Cost: 5919200\n",
      "tensor(2.9155, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 3.69095159, Culminative Send Cost: 5958400\n",
      "tensor(2.9179, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 3.68841147, Culminative Send Cost: 5997600\n",
      "tensor(2.9204, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 3.68589282, Culminative Send Cost: 6036800\n",
      "tensor(2.9228, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 3.68339586, Culminative Send Cost: 6076000\n",
      "tensor(2.9252, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 3.68091989, Culminative Send Cost: 6115200\n",
      "tensor(2.9277, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 3.67846465, Culminative Send Cost: 6154400\n",
      "tensor(2.9301, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 3.67602968, Culminative Send Cost: 6193600\n",
      "tensor(2.9326, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 3.67361617, Culminative Send Cost: 6232800\n",
      "tensor(2.9350, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 3.67122221, Culminative Send Cost: 6272000\n",
      "tensor(2.9375, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 3.66884851, Culminative Send Cost: 6311200\n",
      "tensor(2.9400, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 3.66649413, Culminative Send Cost: 6350400\n",
      "tensor(2.9424, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 3.66415977, Culminative Send Cost: 6389600\n",
      "tensor(2.9449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 3.66184449, Culminative Send Cost: 6428800\n",
      "tensor(2.9473, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 3.65954828, Culminative Send Cost: 6468000\n",
      "tensor(2.9498, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 3.65727091, Culminative Send Cost: 6507200\n",
      "tensor(2.9523, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 3.65501308, Culminative Send Cost: 6546400\n",
      "tensor(2.9548, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 3.65277267, Culminative Send Cost: 6585600\n",
      "tensor(2.9572, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 3.65055108, Culminative Send Cost: 6624800\n",
      "tensor(2.9597, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 3.64834785, Culminative Send Cost: 6664000\n",
      "tensor(2.9622, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 3.64616227, Culminative Send Cost: 6703200\n",
      "tensor(2.9647, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 3.64399457, Culminative Send Cost: 6742400\n",
      "tensor(2.9671, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 3.64184403, Culminative Send Cost: 6781600\n",
      "tensor(2.9696, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 3.63971114, Culminative Send Cost: 6820800\n",
      "tensor(2.9721, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 3.63759565, Culminative Send Cost: 6860000\n",
      "tensor(2.9746, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 3.63549685, Culminative Send Cost: 6899200\n",
      "tensor(2.9771, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 3.63341522, Culminative Send Cost: 6938400\n",
      "tensor(2.9795, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 3.63134980, Culminative Send Cost: 6977600\n",
      "tensor(2.9820, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 3.62930155, Culminative Send Cost: 7016800\n",
      "tensor(2.9845, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 3.62726927, Culminative Send Cost: 7056000\n",
      "tensor(2.9870, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 3.62525296, Culminative Send Cost: 7095200\n",
      "tensor(2.9894, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 3.62325287, Culminative Send Cost: 7134400\n",
      "tensor(2.9919, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 3.62126875, Culminative Send Cost: 7173600\n",
      "tensor(2.9944, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 3.61930013, Culminative Send Cost: 7212800\n",
      "tensor(2.9969, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 3.61734676, Culminative Send Cost: 7252000\n",
      "tensor(2.9993, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 3.61540890, Culminative Send Cost: 7291200\n",
      "tensor(3.0018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 3.61348677, Culminative Send Cost: 7330400\n",
      "tensor(3.0043, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 3.61157918, Culminative Send Cost: 7369600\n",
      "tensor(3.0067, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 3.60968661, Culminative Send Cost: 7408800\n",
      "tensor(3.0092, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 3.60780835, Culminative Send Cost: 7448000\n",
      "tensor(3.0116, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 3.60594559, Culminative Send Cost: 7487200\n",
      "tensor(3.0141, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 3.60409665, Culminative Send Cost: 7526400\n",
      "tensor(3.0165, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 3.60226250, Culminative Send Cost: 7565600\n",
      "tensor(3.0190, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 3.60044193, Culminative Send Cost: 7604800\n",
      "tensor(3.0215, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 3.59863544, Culminative Send Cost: 7644000\n",
      "tensor(3.0239, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 3.59684324, Culminative Send Cost: 7683200\n",
      "tensor(3.0263, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 3.59506440, Culminative Send Cost: 7722400\n",
      "tensor(3.0288, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 3.59330010, Culminative Send Cost: 7761600\n",
      "tensor(3.0312, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 3.59154868, Culminative Send Cost: 7800800\n",
      "tensor(3.0337, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 3.58981037, Culminative Send Cost: 7840000\n",
      "tensor(3.0361, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 3.58808541, Culminative Send Cost: 7879200\n",
      "tensor(3.0385, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 3.58637381, Culminative Send Cost: 7918400\n",
      "tensor(3.0409, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 3.58467484, Culminative Send Cost: 7957600\n",
      "tensor(3.0434, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 3.58298945, Culminative Send Cost: 7996800\n",
      "tensor(3.0458, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 3.58131671, Culminative Send Cost: 8036000\n",
      "tensor(3.0482, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 3.57965589, Culminative Send Cost: 8075200\n",
      "tensor(3.0506, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 3.57800770, Culminative Send Cost: 8114400\n",
      "tensor(3.0530, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 3.57637215, Culminative Send Cost: 8153600\n",
      "tensor(3.0554, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 3.57474899, Culminative Send Cost: 8192800\n",
      "tensor(3.0578, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 3.57313752, Culminative Send Cost: 8232000\n",
      "tensor(3.0602, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 3.57153869, Culminative Send Cost: 8271200\n",
      "tensor(3.0626, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 3.56995130, Culminative Send Cost: 8310400\n",
      "tensor(3.0650, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 3.56837583, Culminative Send Cost: 8349600\n",
      "tensor(3.0674, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 3.56681228, Culminative Send Cost: 8388800\n",
      "tensor(3.0698, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.56525993, Culminative Send Cost: 8428000\n",
      "tensor(3.0721, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.56371903, Culminative Send Cost: 8467200\n",
      "tensor(3.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.56218982, Culminative Send Cost: 8506400\n",
      "tensor(3.0769, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.56067181, Culminative Send Cost: 8545600\n",
      "tensor(3.0792, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.55916500, Culminative Send Cost: 8584800\n",
      "tensor(3.0816, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.55766916, Culminative Send Cost: 8624000\n",
      "tensor(3.0839, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.55618453, Culminative Send Cost: 8663200\n",
      "tensor(3.0863, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.55471087, Culminative Send Cost: 8702400\n",
      "tensor(3.0886, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.55324745, Culminative Send Cost: 8741600\n",
      "tensor(3.0910, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.55179477, Culminative Send Cost: 8780800\n",
      "tensor(3.0933, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.55035329, Culminative Send Cost: 8820000\n",
      "tensor(3.0956, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.54892159, Culminative Send Cost: 8859200\n",
      "tensor(3.0980, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.54750037, Culminative Send Cost: 8898400\n",
      "tensor(3.1003, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.54608941, Culminative Send Cost: 8937600\n",
      "tensor(3.1026, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.54468918, Culminative Send Cost: 8976800\n",
      "tensor(3.1049, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.54329848, Culminative Send Cost: 9016000\n",
      "tensor(3.1072, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.54191756, Culminative Send Cost: 9055200\n",
      "tensor(3.1095, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.54054713, Culminative Send Cost: 9094400\n",
      "tensor(3.1118, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.53918648, Culminative Send Cost: 9133600\n",
      "tensor(3.1141, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.53783512, Culminative Send Cost: 9172800\n",
      "tensor(3.1164, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.53649354, Culminative Send Cost: 9212000\n",
      "tensor(3.1186, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.53516197, Culminative Send Cost: 9251200\n",
      "tensor(3.1209, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.53383923, Culminative Send Cost: 9290400\n",
      "tensor(3.1232, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.53252673, Culminative Send Cost: 9329600\n",
      "tensor(3.1254, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.53122306, Culminative Send Cost: 9368800\n",
      "tensor(3.1277, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.52992845, Culminative Send Cost: 9408000\n",
      "tensor(3.1299, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.52864361, Culminative Send Cost: 9447200\n",
      "tensor(3.1322, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.52736712, Culminative Send Cost: 9486400\n",
      "tensor(3.1344, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.52610016, Culminative Send Cost: 9525600\n",
      "tensor(3.1367, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.52484155, Culminative Send Cost: 9564800\n",
      "tensor(3.1389, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.52359271, Culminative Send Cost: 9604000\n",
      "tensor(3.1411, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.52235198, Culminative Send Cost: 9643200\n",
      "tensor(3.1433, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.52111959, Culminative Send Cost: 9682400\n",
      "tensor(3.1455, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.51989603, Culminative Send Cost: 9721600\n",
      "tensor(3.1477, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.51868129, Culminative Send Cost: 9760800\n",
      "tensor(3.1499, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.51747489, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[ 1.1933e-02, -1.9147e-02,  1.8220e-02,  7.4622e-03, -3.2880e-02,\n",
      "         -2.3076e-02,  3.2804e-02,  4.9011e-03, -4.7908e-03,  1.0213e-02,\n",
      "          2.6629e-02,  2.8018e-02, -5.0988e-03,  2.4293e-02, -7.4457e-03,\n",
      "          5.4227e-03, -3.3956e-02, -3.1555e-02,  1.4554e-02, -1.9959e-02,\n",
      "          2.7410e-02, -3.5489e-02, -1.8207e-02, -4.8126e-03,  2.1642e-02,\n",
      "          1.1476e-02,  9.1914e-03,  2.8835e-03, -6.5598e-03, -1.7171e-02,\n",
      "         -1.3088e-02, -1.1427e-02, -1.1031e-02,  2.3675e-02, -3.2135e-02,\n",
      "          3.7868e-02, -8.3589e-03, -1.0621e-02,  8.2912e-03, -5.4508e-03,\n",
      "          2.5967e-02, -1.8860e-02,  4.0771e-03, -1.0187e-02, -1.7606e-02,\n",
      "          1.7308e-02,  2.9477e-02,  4.0478e-02,  5.5244e-03,  6.7170e-03,\n",
      "         -2.5547e-02, -7.6053e-03, -2.9044e-02,  1.9611e-02,  1.7303e-03,\n",
      "          1.6824e-02,  6.7530e-03, -1.2362e-02,  2.3740e-02,  7.1039e-03,\n",
      "         -6.3973e-03,  7.2306e-03,  1.9359e-02,  1.6022e-02,  4.0084e-02,\n",
      "          4.6222e-02,  7.2618e-02,  6.5302e-02,  7.6250e-02,  1.0178e-01,\n",
      "          9.8287e-02,  1.4935e-01,  1.5107e-01,  1.4586e-01,  1.6176e-01,\n",
      "          8.6320e-02,  1.0811e-01,  3.6714e-02,  3.7297e-02,  2.7409e-02,\n",
      "          3.7960e-02,  8.9240e-03, -5.3698e-04, -8.1731e-03,  1.0477e-02,\n",
      "         -2.5945e-02, -1.1452e-02,  3.2940e-02, -1.8007e-02, -2.8094e-02,\n",
      "         -1.1368e-02,  2.4367e-02,  5.0469e-02,  7.3416e-02,  1.0313e-01,\n",
      "          5.6794e-02,  8.4359e-02,  1.0803e-01,  1.1450e-01,  1.8326e-01,\n",
      "          1.6628e-01,  2.3824e-01,  2.3787e-01,  2.9923e-01,  2.1964e-01,\n",
      "          1.4228e-01,  8.6146e-02,  5.9137e-02,  5.5141e-02, -2.4123e-02,\n",
      "         -2.7219e-03, -3.4179e-02, -4.3884e-06, -1.2086e-02,  2.1013e-02,\n",
      "          2.3516e-02,  6.9577e-03, -1.5661e-02,  1.0722e-02,  2.4140e-02,\n",
      "          6.4968e-02,  7.7987e-02,  1.1361e-01,  9.2084e-02,  7.8696e-02,\n",
      "          4.7964e-02,  3.5697e-02,  2.2087e-02,  5.9324e-02,  7.2296e-02,\n",
      "          1.0433e-01,  1.2710e-01,  1.0836e-01,  1.5330e-01,  1.1125e-01,\n",
      "          1.1186e-01,  2.1435e-02,  4.0496e-02, -9.1381e-03,  1.0316e-03,\n",
      "         -1.5890e-02,  1.6124e-02,  3.5539e-02, -3.6944e-02,  3.1400e-02,\n",
      "          6.7173e-03,  1.4746e-04, -1.2067e-02,  1.7380e-02,  6.6736e-02,\n",
      "          2.4117e-02,  1.0979e-02, -2.5512e-03, -6.3380e-02, -7.3804e-02,\n",
      "         -1.5034e-01, -5.7943e-02, -5.3114e-02, -4.1995e-03, -1.6285e-02,\n",
      "         -2.8711e-02,  2.9042e-02,  2.0764e-02,  3.4421e-02,  4.8335e-02,\n",
      "          5.0333e-02,  1.9076e-02,  1.0062e-02, -2.7187e-02,  4.0380e-03,\n",
      "          2.9875e-02,  3.0272e-02, -3.1055e-02, -2.6590e-02, -3.6058e-02,\n",
      "         -2.2947e-02, -4.3779e-02,  1.6528e-02, -4.2264e-03,  2.6004e-02,\n",
      "         -3.2364e-02, -3.4117e-02, -1.6115e-02,  3.4277e-02,  5.1795e-02,\n",
      "          5.0672e-02, -3.5554e-02, -7.3436e-02, -6.9056e-02, -5.6328e-02,\n",
      "         -8.1727e-03,  7.0490e-02,  1.1952e-01,  4.0890e-02,  3.7068e-02,\n",
      "         -1.3789e-02,  2.5189e-02, -5.0588e-03,  1.8542e-02,  2.7350e-02,\n",
      "          1.0142e-02, -5.0365e-02, -3.4083e-02, -2.8609e-02,  3.5506e-02,\n",
      "          5.3895e-03,  4.1950e-02,  4.8233e-02, -2.1213e-02,  3.4778e-02,\n",
      "          9.0571e-02,  1.9393e-01,  1.3159e-01, -2.0761e-02, -6.1009e-02,\n",
      "         -8.0245e-02, -2.3936e-02, -2.2330e-02, -7.3955e-02,  1.1810e-01,\n",
      "          1.5769e-01,  1.0793e-01,  2.3202e-02,  1.4590e-02,  1.7954e-02,\n",
      "          1.7755e-02,  3.2377e-02,  3.3721e-02,  1.5022e-02, -3.2977e-02,\n",
      "         -6.0719e-03,  5.8914e-02,  9.4113e-02,  6.0908e-02,  7.8297e-02,\n",
      "          1.1087e-01,  1.1859e-01,  5.7886e-02,  6.5685e-03,  2.1860e-02,\n",
      "          2.1127e-05, -8.5069e-02, -9.0003e-02, -6.2539e-02, -4.4409e-02,\n",
      "          3.5285e-03,  2.4925e-02,  1.3494e-01,  2.3082e-01,  1.5192e-01,\n",
      "          3.3253e-02,  9.5586e-03,  2.9154e-02,  3.6577e-02,  2.1746e-02,\n",
      "          3.5374e-02,  4.7771e-02,  4.8240e-02,  1.9791e-02,  8.6763e-02,\n",
      "          1.3199e-01,  1.1056e-01,  1.8086e-01,  2.1965e-01,  1.6174e-01,\n",
      "          6.1402e-03, -1.1120e-01, -1.1733e-01, -7.6965e-02, -6.8187e-02,\n",
      "         -6.0359e-02, -4.0575e-02, -3.6958e-02, -8.9404e-03,  2.0615e-02,\n",
      "          9.5176e-02,  1.9175e-01,  1.2338e-01,  5.0908e-02,  1.1563e-02,\n",
      "         -8.4388e-05,  3.1852e-02,  1.5860e-02, -1.3689e-03,  5.1843e-02,\n",
      "          6.2909e-02,  1.2030e-01,  1.3995e-01,  1.3781e-01,  1.2000e-01,\n",
      "          2.1005e-01,  2.0979e-01,  1.5876e-01,  1.9580e-02, -1.7620e-01,\n",
      "         -2.0358e-01, -4.3131e-02, -3.1028e-02, -1.2596e-02, -3.2128e-02,\n",
      "         -1.1148e-02, -1.0603e-02, -1.3406e-03,  4.6826e-02,  9.1368e-02,\n",
      "          3.6919e-02,  4.3767e-02,  1.1230e-02,  1.2454e-02, -2.2969e-02,\n",
      "          1.5602e-03,  4.6506e-02,  6.5176e-02,  1.7627e-01,  1.7444e-01,\n",
      "          1.6946e-01,  1.4733e-01,  1.5563e-01,  1.2120e-01,  2.2033e-01,\n",
      "          3.0818e-01,  1.7919e-01, -1.2012e-01, -1.2315e-01,  2.3003e-02,\n",
      "          1.0579e-01,  6.5139e-02,  2.2415e-02, -1.4863e-02,  1.5305e-02,\n",
      "         -2.2308e-02, -3.2178e-02,  2.8808e-02,  4.0709e-02,  3.8622e-02,\n",
      "          3.3392e-02, -3.1538e-02,  2.1972e-02,  3.3611e-02,  1.6179e-02,\n",
      "          1.2681e-01,  1.6909e-01,  1.6785e-01,  1.2939e-01,  1.2988e-01,\n",
      "          9.1887e-02,  6.6080e-02,  1.7708e-01,  3.4871e-01,  2.0012e-01,\n",
      "         -1.0388e-01,  3.4860e-02,  2.3327e-01,  2.3535e-01,  1.2275e-01,\n",
      "          8.7482e-02,  7.1651e-02, -1.1810e-03,  3.8051e-03, -3.2702e-02,\n",
      "         -1.2926e-02,  3.2337e-02, -2.0126e-02,  2.6751e-02, -1.3597e-02,\n",
      "          1.1004e-02,  4.6472e-03,  5.7404e-02,  1.0329e-01,  1.5323e-01,\n",
      "          4.8572e-02,  2.3162e-02,  5.3069e-02, -1.1378e-02, -1.0493e-02,\n",
      "          1.2457e-01,  3.6122e-01,  5.6951e-02, -3.6070e-02,  1.8386e-01,\n",
      "          2.2106e-01,  2.4297e-01,  1.8017e-01,  1.5589e-01,  6.2431e-02,\n",
      "          3.7054e-03, -4.3556e-02, -6.3082e-02, -5.2341e-02, -1.2619e-02,\n",
      "          4.9467e-03, -3.6203e-03,  1.0384e-02, -2.9498e-02, -1.9164e-02,\n",
      "          4.5396e-02,  6.1177e-02, -2.1471e-02, -3.8887e-02, -3.9903e-02,\n",
      "         -3.8805e-02, -9.3393e-02, -1.2816e-01,  1.1842e-01,  2.1231e-01,\n",
      "         -1.8987e-02,  4.0067e-02,  5.7051e-02,  2.8612e-01,  2.7547e-01,\n",
      "          5.2769e-02,  2.0416e-02, -8.2461e-03, -5.2192e-02, -8.7989e-02,\n",
      "         -7.0995e-02, -2.5404e-02,  4.5659e-02, -3.7736e-02,  4.4615e-03,\n",
      "          2.9340e-02,  2.0519e-02, -1.0997e-02,  5.8584e-02,  1.0504e-02,\n",
      "         -9.3872e-02, -1.2289e-01, -1.3949e-01, -1.2328e-01, -1.0260e-01,\n",
      "         -6.6819e-02,  1.3932e-01,  1.2386e-01, -3.1010e-02, -5.7628e-03,\n",
      "          6.8984e-02,  3.1575e-01,  1.7874e-01, -6.7405e-02, -9.5687e-02,\n",
      "         -8.2598e-02, -8.0725e-02, -3.7057e-02, -3.9268e-02, -1.9205e-02,\n",
      "         -7.3254e-03,  1.1804e-02,  1.7631e-02,  2.5873e-02, -1.9326e-03,\n",
      "          2.6360e-02,  5.6418e-02,  1.1047e-02, -9.2205e-02, -1.4208e-01,\n",
      "         -8.0213e-02, -8.7657e-02, -4.5956e-02,  4.8067e-02,  1.7736e-01,\n",
      "          1.5692e-01, -4.0027e-02, -4.8551e-02,  1.1254e-01,  2.5817e-01,\n",
      "          1.6690e-02, -1.1132e-01, -1.5745e-01, -7.7672e-02, -8.5160e-02,\n",
      "         -2.5025e-02, -3.2776e-02, -4.3011e-02,  6.6368e-03,  1.2157e-02,\n",
      "          1.3558e-02,  8.7673e-03, -1.2968e-02,  4.7394e-03,  2.9670e-02,\n",
      "         -2.7878e-02, -1.0236e-01, -9.6967e-02, -1.1120e-01, -4.0700e-02,\n",
      "          3.3034e-02,  1.0975e-01,  2.6824e-01,  9.0829e-02, -5.2331e-02,\n",
      "         -2.5462e-02,  1.4485e-01,  1.2872e-01, -7.6802e-02, -9.9731e-02,\n",
      "         -5.0143e-02, -5.6030e-02, -6.5039e-02, -5.2999e-02, -5.3725e-02,\n",
      "         -6.8520e-02, -1.1252e-02, -4.0914e-02,  9.1484e-03,  3.1823e-02,\n",
      "         -2.7662e-02,  2.3121e-02,  5.0130e-02, -2.1230e-02, -1.0945e-01,\n",
      "         -7.4780e-02, -1.1473e-01, -7.0532e-02,  3.7048e-02,  1.5481e-01,\n",
      "          1.7455e-01, -1.5103e-02, -1.3407e-01, -3.5466e-02,  1.0009e-02,\n",
      "         -6.1367e-03, -2.4804e-02,  1.4180e-02, -2.6239e-02,  6.1155e-03,\n",
      "         -1.0917e-01, -4.8756e-02, -9.1492e-02, -6.4204e-02, -6.1670e-02,\n",
      "          4.3922e-04, -4.6077e-03,  1.2028e-02,  2.2795e-02,  1.0947e-02,\n",
      "         -1.3578e-02, -2.4719e-02, -5.0137e-02, -4.6016e-02, -2.0566e-02,\n",
      "         -3.4094e-02, -3.9941e-02,  3.1505e-02,  2.3564e-02, -3.3902e-02,\n",
      "         -1.3981e-01, -8.8388e-02, -4.3097e-02, -1.7365e-02, -1.7202e-02,\n",
      "         -8.4758e-02, -4.6922e-02, -1.9423e-02, -1.0777e-01, -4.9627e-02,\n",
      "         -1.1126e-01, -7.1323e-02, -2.8277e-02,  9.5434e-04,  2.4357e-02,\n",
      "         -3.3675e-02, -1.9825e-02,  1.3092e-02, -4.7062e-02, -7.5932e-02,\n",
      "         -6.8857e-02, -1.7229e-02,  2.8034e-03, -5.9991e-02, -1.0889e-01,\n",
      "         -7.4696e-02, -5.0379e-02, -4.8607e-02, -8.7604e-03, -2.4643e-02,\n",
      "         -9.6565e-03, -4.8075e-02, -2.8247e-02, -2.5251e-02, -3.0968e-02,\n",
      "         -4.4218e-02, -1.1138e-01, -1.0849e-01, -1.1603e-01, -9.1415e-02,\n",
      "          1.4561e-03,  6.3051e-03,  1.8839e-02,  4.3734e-03,  3.3160e-03,\n",
      "         -3.6539e-02, -1.7894e-03, -6.0449e-02, -5.7798e-02, -1.5453e-02,\n",
      "         -1.7259e-03, -8.2247e-02, -1.0004e-01, -6.6655e-02, -3.0911e-02,\n",
      "          2.6311e-02,  2.4315e-02,  4.5250e-03, -7.4163e-02, -1.7224e-02,\n",
      "         -2.2105e-02, -2.5892e-02, -6.1941e-02, -7.5197e-02, -6.9431e-02,\n",
      "         -4.5976e-02, -9.8277e-02, -4.0354e-02, -1.3117e-02,  2.5672e-02,\n",
      "          2.1524e-02,  2.7538e-02, -2.3305e-02, -3.5103e-03, -3.9434e-02,\n",
      "         -4.1754e-02, -6.3688e-02, -7.3484e-02, -9.6569e-02, -9.2825e-02,\n",
      "         -1.3362e-01, -5.5513e-02, -3.5404e-02,  1.9410e-02, -5.0144e-02,\n",
      "         -7.3948e-02, -1.0221e-01, -5.5896e-02, -4.8841e-02, -1.8287e-02,\n",
      "         -6.0873e-02, -2.2237e-02, -2.9300e-02, -6.7964e-02, -4.3617e-02,\n",
      "         -4.7139e-02, -3.7049e-02,  1.4260e-02,  1.6288e-02, -2.3177e-02,\n",
      "          2.6669e-03, -2.4352e-02, -2.7240e-02, -3.5280e-02, -8.9042e-02,\n",
      "         -8.1542e-02, -4.6127e-02, -1.9935e-02, -8.3736e-02, -6.3283e-02,\n",
      "          3.1799e-02, -2.5452e-03,  7.6476e-02, -1.4822e-02, -9.2850e-02,\n",
      "         -3.9742e-02, -2.1506e-02,  4.9383e-02,  7.6756e-02,  7.9993e-02,\n",
      "          4.8234e-02,  5.0532e-02,  2.4808e-03, -1.7022e-02, -3.7495e-02,\n",
      "          1.4387e-02, -2.1805e-02, -1.3836e-02, -3.1616e-02,  1.9999e-02,\n",
      "          1.9331e-02, -3.3972e-02,  2.3525e-02,  4.1699e-02,  1.1721e-01,\n",
      "          1.1456e-01,  1.5438e-01,  1.4300e-01,  1.2058e-01,  7.7050e-02,\n",
      "          7.9460e-02,  1.1576e-01,  7.7935e-02,  1.6796e-01,  1.9367e-01,\n",
      "          1.9354e-01,  1.7804e-01,  2.2108e-01,  1.4931e-01,  4.4779e-02,\n",
      "          5.9226e-02,  3.3086e-02, -1.9753e-02,  2.7431e-02,  2.8136e-02,\n",
      "          3.5260e-02,  1.5433e-02, -4.1694e-03, -1.6027e-03,  3.4578e-02,\n",
      "          9.4305e-03,  1.1009e-01,  1.7904e-01,  2.5313e-01,  2.7277e-01,\n",
      "          2.8045e-01,  2.7704e-01,  2.9413e-01,  2.8355e-01,  3.3220e-01,\n",
      "          3.7210e-01,  3.6439e-01,  3.4950e-01,  2.9215e-01,  2.5179e-01,\n",
      "          1.5090e-01,  1.4563e-01,  3.0102e-02, -4.0533e-03,  1.7110e-02,\n",
      "          2.3495e-02, -8.7928e-04,  2.0541e-02,  2.1812e-02,  5.6756e-03,\n",
      "          9.1910e-03,  4.6074e-03, -2.0233e-02,  3.7199e-02,  6.3214e-02,\n",
      "          6.0849e-02,  1.3390e-01,  1.6436e-01,  1.5642e-01,  1.7548e-01,\n",
      "          1.6617e-01,  2.1635e-01,  1.9753e-01,  1.9500e-01,  1.9223e-01,\n",
      "          1.4366e-01,  1.1979e-01,  5.8326e-02,  3.4292e-02,  3.5642e-02,\n",
      "          9.8251e-03,  1.3490e-02, -1.6825e-02,  3.3884e-02, -2.4814e-02,\n",
      "         -1.2427e-02, -7.7258e-03, -1.5208e-02, -2.1997e-02,  3.1935e-02,\n",
      "          9.4861e-03,  1.6482e-03,  1.7252e-02,  7.0252e-03,  3.5856e-02,\n",
      "         -6.2460e-03,  4.4413e-02,  6.9904e-03,  5.4489e-02,  4.2715e-02,\n",
      "          2.5402e-02,  3.1254e-02,  4.4311e-02,  1.3986e-02,  4.8667e-02,\n",
      "         -2.3138e-02,  1.3911e-02,  3.3481e-02,  2.8477e-02,  3.7107e-02,\n",
      "          1.5293e-02, -2.8307e-02,  1.3265e-02,  9.9117e-03]])\n",
      "activation_stack.0.bias: tensor([1.9555])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for local_update_epochs is 4 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[-1.0788e-02, -3.3491e-02, -3.4444e-02, -3.0865e-02, -3.1982e-02,\n",
      "         -2.4948e-03, -1.5580e-02,  1.6187e-02, -5.2503e-03,  6.4184e-03,\n",
      "         -1.5504e-02,  3.4533e-02, -2.3023e-02, -1.7781e-02, -3.4403e-02,\n",
      "          9.3158e-03,  2.5848e-02,  2.8187e-02, -4.4691e-03,  1.1775e-02,\n",
      "         -2.0812e-02,  2.2114e-02,  3.4636e-02,  2.1585e-02,  3.5320e-02,\n",
      "          2.3281e-02, -3.4690e-02, -1.2597e-02,  1.7974e-02, -7.8694e-03,\n",
      "          2.6294e-02, -2.0338e-02,  4.5606e-03,  3.1340e-02,  2.0091e-02,\n",
      "         -3.4016e-03, -1.3418e-02,  2.4550e-02,  2.7194e-02, -6.2127e-03,\n",
      "          1.7910e-03, -7.6189e-03, -3.3371e-02, -3.0691e-02,  3.0566e-02,\n",
      "          6.7909e-03, -3.3850e-02,  2.2945e-02,  2.0882e-02, -7.2902e-03,\n",
      "         -3.0271e-02, -1.3735e-03, -1.1603e-02, -1.8541e-02, -2.5899e-02,\n",
      "         -2.5733e-02,  2.5562e-02,  2.5576e-02, -1.5754e-02,  1.6295e-02,\n",
      "          2.5947e-02, -2.9279e-02, -3.0361e-02, -1.2027e-02,  2.9486e-02,\n",
      "          1.6805e-02,  9.7476e-03, -1.9031e-02, -4.0144e-03, -1.4310e-03,\n",
      "         -3.2081e-02, -6.6634e-03, -2.8158e-02,  1.3371e-02,  8.9471e-03,\n",
      "         -4.9877e-03,  2.6678e-02,  1.3363e-02,  3.5530e-02,  2.4273e-02,\n",
      "          8.0770e-03, -3.0614e-02, -3.1904e-02,  3.1759e-02, -2.0756e-02,\n",
      "          3.3025e-02, -2.9310e-02,  1.6316e-02, -3.5039e-02, -3.2205e-02,\n",
      "          1.5783e-02, -6.4518e-03, -8.7299e-03,  2.1966e-02,  2.8493e-02,\n",
      "          3.1386e-02, -2.8266e-02, -2.6804e-02, -2.6899e-02, -1.3633e-02,\n",
      "         -2.0116e-03, -1.2319e-02,  3.9125e-03,  1.9741e-02, -3.1285e-02,\n",
      "          2.7937e-02, -9.2528e-03, -2.8304e-02, -2.0800e-02, -1.5001e-02,\n",
      "          7.9806e-03,  7.3588e-03, -1.5232e-02,  2.1624e-02, -1.7029e-03,\n",
      "          3.0028e-03, -2.9983e-02,  2.3665e-02,  1.9138e-02, -2.1749e-02,\n",
      "          1.8387e-02, -3.1533e-02, -9.9093e-03, -2.5376e-02, -8.2460e-03,\n",
      "         -2.0964e-02, -3.3361e-02,  2.2010e-02,  2.1180e-02, -2.9432e-02,\n",
      "          1.4321e-02,  1.5381e-02, -3.2821e-02, -8.5154e-03, -3.1709e-03,\n",
      "         -6.2659e-04, -1.3059e-02, -3.4352e-02,  7.8809e-03,  1.3290e-02,\n",
      "         -2.0868e-03, -1.9435e-02, -8.9958e-03, -2.0506e-02, -7.4213e-03,\n",
      "         -3.3843e-02,  2.2867e-02, -2.9511e-03, -3.6197e-03, -8.4427e-04,\n",
      "          2.6021e-02, -3.0730e-02,  3.5486e-02, -4.6359e-03,  2.1904e-02,\n",
      "          2.2195e-02, -1.2794e-02,  6.6018e-04,  2.0079e-02, -1.6915e-02,\n",
      "         -1.8818e-02,  2.9521e-02,  3.3211e-02,  2.3379e-02, -2.8359e-02,\n",
      "          1.0215e-02, -1.3550e-02, -3.4618e-02, -1.9123e-02, -2.7699e-02,\n",
      "         -2.8594e-02,  1.2315e-02,  6.4359e-03, -3.1169e-02,  6.7327e-03,\n",
      "         -1.4080e-02, -1.3633e-02, -1.5115e-02,  8.8951e-03, -2.7883e-02,\n",
      "         -3.3832e-02,  3.5514e-02, -1.1091e-02, -5.7725e-03,  2.9381e-03,\n",
      "         -3.4815e-02,  1.3873e-02,  1.7669e-03,  2.7871e-02,  3.3399e-02,\n",
      "         -3.0252e-02,  3.1319e-02, -2.7453e-02, -4.9130e-03,  2.5042e-02,\n",
      "          1.4037e-02, -2.9291e-02,  1.5830e-02,  1.9751e-02,  2.5018e-02,\n",
      "          2.1110e-02, -3.0390e-02, -1.9000e-02,  3.2442e-02,  3.4274e-02,\n",
      "         -4.4640e-03, -6.5503e-03,  2.1248e-03, -4.9508e-04, -3.5378e-02,\n",
      "          2.3897e-02,  6.8311e-03, -3.4556e-02, -3.1271e-02, -2.6900e-02,\n",
      "         -3.4370e-02, -8.0517e-03,  1.9934e-02, -2.3211e-03,  7.0817e-03,\n",
      "          3.0301e-02,  2.7181e-02,  1.9331e-02, -2.5849e-02,  6.3714e-03,\n",
      "          8.0505e-03,  2.9901e-02,  2.4523e-02,  1.7114e-02, -1.3727e-02,\n",
      "          1.3361e-02, -3.0352e-02,  3.1042e-02, -2.4334e-02, -1.4609e-02,\n",
      "          2.7075e-02,  1.5865e-02,  1.3221e-02, -3.0550e-02,  1.8307e-02,\n",
      "          3.2355e-04,  3.3391e-02,  5.7499e-03, -4.8821e-03,  3.5675e-02,\n",
      "          1.1143e-02,  3.0267e-02, -8.1212e-03, -6.5575e-03,  2.3328e-02,\n",
      "         -3.4685e-02, -1.0195e-02, -2.2228e-02,  8.5857e-03, -2.0609e-02,\n",
      "         -3.3456e-02,  2.7163e-02, -3.5364e-02,  3.1465e-02, -3.2506e-03,\n",
      "          1.8808e-02, -2.3663e-02, -4.7967e-03, -7.9982e-03, -2.0413e-02,\n",
      "         -2.0672e-02,  1.4893e-02, -1.1327e-02, -1.2899e-02,  2.4176e-02,\n",
      "         -8.5648e-03,  1.0760e-02,  1.7252e-03,  3.1466e-02, -1.2472e-02,\n",
      "         -2.4983e-03, -2.0664e-02, -2.8527e-02, -1.3667e-02,  2.8399e-03,\n",
      "         -1.9498e-02,  6.1183e-03, -1.5068e-02,  1.4184e-02, -1.8785e-02,\n",
      "         -4.9600e-03,  3.5304e-02,  1.3355e-02,  2.9203e-02, -1.9265e-03,\n",
      "         -1.4783e-02, -2.0048e-02,  1.8813e-02, -2.5093e-02, -2.5477e-02,\n",
      "         -1.1936e-02, -1.9611e-02, -2.3932e-02, -2.3713e-02, -1.8648e-02,\n",
      "          1.2971e-02,  4.1861e-03,  8.4018e-03,  4.5029e-03,  2.3116e-02,\n",
      "         -3.4857e-02,  1.8390e-02, -3.0926e-02, -1.2546e-03, -3.2434e-02,\n",
      "         -1.4054e-02,  6.1067e-03, -2.2727e-02,  3.4714e-02,  1.7939e-02,\n",
      "         -2.4344e-02,  3.2769e-02, -9.9752e-03, -7.4323e-03, -2.3223e-02,\n",
      "          6.0140e-03, -1.1026e-02, -3.4446e-02,  1.0682e-03, -2.5083e-02,\n",
      "          2.5122e-02, -1.9334e-02, -1.2862e-02,  1.4905e-03,  3.0732e-02,\n",
      "          3.5368e-02,  1.0482e-02, -1.1527e-02, -7.6001e-03,  1.7962e-02,\n",
      "          2.9649e-02,  1.7041e-02,  1.2425e-02, -2.4198e-03, -3.3057e-02,\n",
      "         -3.5496e-02,  6.3601e-03, -7.7703e-03, -2.8500e-02, -2.6110e-02,\n",
      "          5.1225e-03,  1.7069e-03,  2.0914e-02,  9.6324e-03,  3.0516e-02,\n",
      "         -1.8287e-02,  1.1095e-02,  2.9296e-02,  3.1727e-02,  1.5606e-02,\n",
      "          2.7516e-03, -4.0535e-03, -2.3130e-02,  5.8764e-03, -2.2436e-02,\n",
      "          3.1118e-02,  2.4446e-02, -3.9484e-03, -3.3002e-02, -1.1868e-02,\n",
      "         -6.1318e-03,  2.1834e-02,  3.7786e-03,  1.4671e-02,  1.2819e-02,\n",
      "         -3.0145e-03, -3.3075e-02,  2.7217e-03,  5.2572e-03, -4.2560e-03,\n",
      "         -2.3554e-02,  1.9775e-02, -2.9570e-02, -1.9289e-02,  2.3363e-02,\n",
      "         -4.1459e-03,  6.9545e-03,  1.2347e-02, -1.2760e-02,  8.5406e-03,\n",
      "         -3.0495e-02, -7.5076e-04,  3.7534e-03,  8.7916e-03,  1.4301e-02,\n",
      "         -9.5907e-03,  4.4973e-03,  2.7324e-02,  1.1216e-03,  1.1575e-02,\n",
      "          1.4707e-02,  2.0765e-02, -2.0515e-02,  2.1784e-02, -2.1338e-02,\n",
      "          5.3301e-03, -1.6793e-02,  4.8738e-03, -1.6626e-02, -3.3596e-02,\n",
      "          4.9923e-03, -1.6056e-02,  9.9063e-03,  1.4981e-02,  1.2102e-02,\n",
      "         -1.6916e-02,  2.0481e-02,  1.8878e-02,  8.8630e-03,  2.8457e-02,\n",
      "          6.6455e-03,  1.6541e-02,  9.1708e-03,  1.0934e-02,  2.1901e-02,\n",
      "         -5.8711e-03,  2.5069e-02, -3.0121e-03,  2.9026e-02, -2.8439e-02,\n",
      "          3.5540e-02,  2.1970e-02,  3.4471e-03,  3.4089e-02, -3.2551e-02,\n",
      "          1.4154e-02,  3.2460e-02,  3.2927e-02, -1.9318e-02,  1.6647e-02,\n",
      "         -1.1672e-02, -2.5652e-02,  2.2080e-02, -2.5553e-03, -3.0433e-02,\n",
      "          8.1250e-03, -3.0964e-02,  9.7320e-03, -1.6522e-03,  2.0344e-02,\n",
      "         -1.4831e-02,  1.3444e-02, -1.7857e-03, -2.9916e-02, -2.4135e-02,\n",
      "         -6.8499e-03, -1.0835e-02, -3.5517e-02,  2.7952e-02, -6.6650e-03,\n",
      "         -3.4871e-02, -8.0257e-03,  3.3345e-03,  3.1109e-02, -1.1868e-02,\n",
      "          2.3016e-04, -3.5413e-02,  2.9143e-02, -2.3895e-02, -2.6130e-02,\n",
      "         -7.5801e-03,  4.2700e-03,  2.6891e-02,  1.4990e-02,  3.5297e-03,\n",
      "          1.9475e-02,  2.1784e-02, -4.7350e-03, -4.4661e-03, -7.6138e-03,\n",
      "          2.4126e-02,  1.2642e-03, -2.5808e-02,  3.5334e-02, -2.8231e-03,\n",
      "          1.6773e-02,  7.2278e-03, -2.0237e-02,  1.7101e-02, -1.5769e-02,\n",
      "          2.3400e-02, -2.3190e-02,  1.1073e-02, -2.6165e-02, -1.3918e-02,\n",
      "         -3.1461e-02,  2.2553e-02,  2.7537e-02,  4.9090e-03, -1.3103e-02,\n",
      "         -9.4934e-03,  1.4965e-02, -3.4710e-02, -1.3047e-02,  1.1608e-02,\n",
      "         -7.0326e-03, -1.3558e-02,  1.9882e-02,  1.5288e-02,  1.5964e-02,\n",
      "          3.0614e-02,  1.9487e-02, -2.2506e-02, -1.7780e-02,  1.8955e-02,\n",
      "          1.6882e-02,  1.6037e-02, -5.2661e-03, -2.6940e-03, -3.2994e-02,\n",
      "          3.4559e-02,  3.0974e-03, -1.6317e-02,  5.8652e-03,  1.4031e-02,\n",
      "          2.6862e-02, -8.6950e-03, -3.5168e-02, -2.8512e-02,  2.3810e-02,\n",
      "          2.7556e-02, -4.7315e-03,  1.2802e-02,  2.0327e-02, -1.4710e-02,\n",
      "         -1.6797e-02,  1.6890e-02, -4.6055e-03,  1.3461e-02, -8.4082e-03,\n",
      "          1.1087e-02, -1.4732e-02, -1.2252e-02,  3.2408e-02,  3.1624e-02,\n",
      "          6.3097e-04, -2.5270e-02, -3.3881e-02, -2.2031e-02,  1.5018e-02,\n",
      "          2.5859e-02, -2.6698e-02, -2.2552e-02,  2.0665e-02, -3.1558e-02,\n",
      "         -1.7248e-02,  9.4686e-03, -1.6394e-02,  2.1599e-02,  3.0585e-02,\n",
      "          9.6060e-03, -2.1684e-02,  3.3302e-03,  1.1652e-02, -2.5465e-02,\n",
      "         -1.3773e-02,  4.0997e-03, -3.0703e-02,  1.4944e-02,  3.5565e-02,\n",
      "          1.1429e-02,  1.7405e-02, -1.0230e-03, -4.0607e-03, -3.5634e-02,\n",
      "         -1.6815e-02, -9.7752e-03, -5.8290e-03, -3.5078e-02, -2.6831e-02,\n",
      "          1.4267e-02, -2.4758e-02, -2.1157e-02,  2.1341e-02, -2.6486e-03,\n",
      "         -1.7273e-02, -5.2161e-03,  1.6750e-02, -3.3340e-02,  3.4612e-02,\n",
      "         -2.3225e-02,  2.9520e-02, -2.5657e-02,  3.2747e-02, -1.3635e-02,\n",
      "          1.8539e-03,  1.7065e-03, -2.4473e-02, -2.6068e-02,  5.8124e-03,\n",
      "          1.2605e-02, -8.5780e-03, -1.1063e-03, -6.4724e-03, -5.8742e-03,\n",
      "          1.4789e-02, -2.9025e-02,  2.3539e-02, -3.3608e-03, -1.3249e-02,\n",
      "          3.4166e-02,  2.0249e-02, -3.0801e-02, -3.4622e-02, -3.2933e-02,\n",
      "          6.9668e-03, -9.3970e-03,  2.5272e-02, -1.4511e-02, -2.4611e-02,\n",
      "         -5.9393e-03, -2.7693e-02, -2.1146e-02,  1.5520e-02,  2.9607e-02,\n",
      "          3.2628e-04,  3.4149e-02,  8.5988e-03, -4.3116e-03,  3.2322e-02,\n",
      "          1.8557e-02, -3.3985e-02, -1.2249e-02, -2.4515e-02, -1.7718e-02,\n",
      "         -1.4767e-02,  2.0108e-02,  3.2549e-02, -3.5459e-02,  2.9017e-02,\n",
      "         -3.3345e-02, -4.3573e-03, -2.3340e-02, -1.6523e-02, -1.3156e-02,\n",
      "         -1.3843e-02, -2.7362e-02,  1.6989e-02, -1.7558e-02, -1.4281e-02,\n",
      "          9.3061e-03,  2.8326e-02, -3.1828e-02,  7.8674e-03,  2.3665e-02,\n",
      "          2.2312e-02, -2.6087e-02,  6.2239e-03, -2.7407e-02, -9.9554e-03,\n",
      "         -1.0387e-02,  2.6287e-02,  3.2561e-02,  2.2338e-02,  1.0376e-02,\n",
      "         -1.5192e-02,  3.4994e-02, -2.1215e-02, -3.3105e-02, -8.8179e-04,\n",
      "          1.1282e-02,  4.8662e-03,  1.0926e-02,  3.0875e-02, -3.1228e-02,\n",
      "         -1.2406e-02, -2.3257e-02,  7.9269e-03, -2.6500e-02,  2.5091e-03,\n",
      "          2.8394e-02,  2.8154e-02,  6.6282e-03, -1.2803e-02,  2.4878e-02,\n",
      "          3.1166e-02, -1.3387e-02,  3.1306e-02,  2.5525e-02, -2.6458e-02,\n",
      "         -1.3457e-02,  1.8165e-02, -2.2113e-02, -1.2765e-02,  1.4296e-02,\n",
      "         -2.3047e-02, -6.5579e-03, -2.3034e-02,  1.4905e-02, -2.4218e-02,\n",
      "          3.2275e-02, -2.9417e-02, -1.3600e-03,  1.5142e-02, -2.4239e-02,\n",
      "          2.0917e-02,  1.0444e-02, -1.7645e-02, -1.7677e-02,  1.6029e-02,\n",
      "          1.2842e-02, -3.1576e-03, -3.3515e-02,  3.5518e-02, -3.2386e-02,\n",
      "          7.4663e-03,  6.4931e-03,  1.0362e-02, -2.9215e-02,  7.9362e-03,\n",
      "         -1.9006e-02,  3.5404e-02,  2.7612e-02,  1.6821e-02, -3.4479e-02,\n",
      "          3.3863e-02, -2.7137e-02, -1.3631e-02,  7.7005e-03,  2.3509e-02,\n",
      "         -1.5318e-02, -2.9449e-02, -8.4588e-03, -2.8152e-02,  1.5372e-02,\n",
      "         -3.4336e-02, -4.0757e-03, -3.1530e-02,  8.4304e-03,  2.1205e-02,\n",
      "         -1.1520e-02,  2.2852e-02, -6.4976e-04, -1.6006e-02,  1.7024e-02,\n",
      "          8.5743e-03, -7.7645e-04,  1.4681e-02,  4.9829e-03, -3.1693e-02,\n",
      "         -1.2874e-02,  3.3433e-02,  1.4630e-02, -2.5544e-02,  2.0972e-03,\n",
      "         -1.3320e-02,  2.6321e-03,  7.0519e-03,  3.1825e-02,  2.3644e-02,\n",
      "         -2.2412e-02,  8.1514e-03,  3.1254e-02, -1.8273e-02, -1.7255e-02,\n",
      "          1.9946e-02, -2.3422e-02, -2.3177e-03, -3.1215e-02, -2.2264e-02,\n",
      "          2.9681e-02,  1.3244e-02, -3.3444e-02,  2.6996e-02,  3.1365e-02,\n",
      "          2.8289e-02, -2.1940e-02,  1.1059e-02,  2.7104e-02,  1.1692e-02,\n",
      "         -1.6328e-05, -2.7928e-02,  1.0713e-02, -1.4812e-02, -5.8007e-03,\n",
      "         -1.4712e-02,  2.2713e-02, -3.0990e-02, -1.8417e-02]])), ('activation_stack.0.bias', tensor([-0.0253]))])\n",
      "tensor(0.7080, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 19.79748726, Culminative Send Cost: 3920\n",
      "tensor(2.4079, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 7.49329472, Culminative Send Cost: 39200\n",
      "tensor(2.5749, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 6.43349981, Culminative Send Cost: 78400\n",
      "tensor(2.6435, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 5.83837271, Culminative Send Cost: 117600\n",
      "tensor(2.6960, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 5.47994757, Culminative Send Cost: 156800\n",
      "tensor(2.7371, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 5.25243711, Culminative Send Cost: 196000\n",
      "tensor(2.7688, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 5.09890747, Culminative Send Cost: 235200\n",
      "tensor(2.7927, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 4.98838663, Culminative Send Cost: 274400\n",
      "tensor(2.8104, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 4.90380478, Culminative Send Cost: 313600\n",
      "tensor(2.8230, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 4.83558750, Culminative Send Cost: 352800\n",
      "tensor(2.8316, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 4.77824450, Culminative Send Cost: 392000\n",
      "tensor(2.8371, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 4.72853470, Culminative Send Cost: 431200\n",
      "tensor(2.8401, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 4.68447256, Culminative Send Cost: 470400\n",
      "tensor(2.8412, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 4.64479065, Culminative Send Cost: 509600\n",
      "tensor(2.8409, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 4.60863829, Culminative Send Cost: 548800\n",
      "tensor(2.8394, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 4.57541561, Culminative Send Cost: 588000\n",
      "tensor(2.8372, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 4.54468250, Culminative Send Cost: 627200\n",
      "tensor(2.8345, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 4.51609850, Culminative Send Cost: 666400\n",
      "tensor(2.8313, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 4.48939228, Culminative Send Cost: 705600\n",
      "tensor(2.8279, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 4.46434402, Culminative Send Cost: 744800\n",
      "tensor(2.8244, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 4.44076824, Culminative Send Cost: 784000\n",
      "tensor(2.8208, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 4.41850996, Culminative Send Cost: 823200\n",
      "tensor(2.8172, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 4.39743471, Culminative Send Cost: 862400\n",
      "tensor(2.8137, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 4.37742662, Culminative Send Cost: 901600\n",
      "tensor(2.8102, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 4.35838509, Culminative Send Cost: 940800\n",
      "tensor(2.8069, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 4.34022236, Culminative Send Cost: 980000\n",
      "tensor(2.8037, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 4.32285976, Culminative Send Cost: 1019200\n",
      "tensor(2.8007, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 4.30622959, Culminative Send Cost: 1058400\n",
      "tensor(2.7979, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.29027081, Culminative Send Cost: 1097600\n",
      "tensor(2.7952, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.27493000, Culminative Send Cost: 1136800\n",
      "tensor(2.7927, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.26015854, Culminative Send Cost: 1176000\n",
      "tensor(2.7904, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.24591398, Culminative Send Cost: 1215200\n",
      "tensor(2.7883, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.23215771, Culminative Send Cost: 1254400\n",
      "tensor(2.7864, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.21885538, Culminative Send Cost: 1293600\n",
      "tensor(2.7846, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.20597553, Culminative Send Cost: 1332800\n",
      "tensor(2.7830, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.19349051, Culminative Send Cost: 1372000\n",
      "tensor(2.7816, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.18137407, Culminative Send Cost: 1411200\n",
      "tensor(2.7804, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.16960430, Culminative Send Cost: 1450400\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.15815973, Culminative Send Cost: 1489600\n",
      "tensor(2.7785, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.14702177, Culminative Send Cost: 1528800\n",
      "tensor(2.7778, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.13617277, Culminative Send Cost: 1568000\n",
      "tensor(2.7772, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.12559748, Culminative Send Cost: 1607200\n",
      "tensor(2.7768, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.11528063, Culminative Send Cost: 1646400\n",
      "tensor(2.7765, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.10520935, Culminative Send Cost: 1685600\n",
      "tensor(2.7764, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.09537125, Culminative Send Cost: 1724800\n",
      "tensor(2.7764, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 4.08575583, Culminative Send Cost: 1764000\n",
      "tensor(2.7766, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 4.07635117, Culminative Send Cost: 1803200\n",
      "tensor(2.7769, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 4.06714916, Culminative Send Cost: 1842400\n",
      "tensor(2.7773, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 4.05814075, Culminative Send Cost: 1881600\n",
      "tensor(2.7779, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 4.04931688, Culminative Send Cost: 1920800\n",
      "tensor(2.7786, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 4.04066992, Culminative Send Cost: 1960000\n",
      "tensor(2.7794, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 4.03219414, Culminative Send Cost: 1999200\n",
      "tensor(2.7803, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 4.02388144, Culminative Send Cost: 2038400\n",
      "tensor(2.7813, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 4.01572609, Culminative Send Cost: 2077600\n",
      "tensor(2.7824, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 4.00772285, Culminative Send Cost: 2116800\n",
      "tensor(2.7836, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 3.99986577, Culminative Send Cost: 2156000\n",
      "tensor(2.7849, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 3.99214983, Culminative Send Cost: 2195200\n",
      "tensor(2.7864, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 3.98457003, Culminative Send Cost: 2234400\n",
      "tensor(2.7879, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 3.97712183, Culminative Send Cost: 2273600\n",
      "tensor(2.7895, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 3.96980214, Culminative Send Cost: 2312800\n",
      "tensor(2.7911, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 3.96260524, Culminative Send Cost: 2352000\n",
      "tensor(2.7929, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 3.95552874, Culminative Send Cost: 2391200\n",
      "tensor(2.7948, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 3.94856787, Culminative Send Cost: 2430400\n",
      "tensor(2.7967, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 3.94172025, Culminative Send Cost: 2469600\n",
      "tensor(2.7987, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 3.93498230, Culminative Send Cost: 2508800\n",
      "tensor(2.8008, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 3.92835045, Culminative Send Cost: 2548000\n",
      "tensor(2.8029, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 3.92182183, Culminative Send Cost: 2587200\n",
      "tensor(2.8051, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 3.91539526, Culminative Send Cost: 2626400\n",
      "tensor(2.8074, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 3.90906620, Culminative Send Cost: 2665600\n",
      "tensor(2.8097, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 3.90283322, Culminative Send Cost: 2704800\n",
      "tensor(2.8121, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 3.89669323, Culminative Send Cost: 2744000\n",
      "tensor(2.8146, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 3.89064455, Culminative Send Cost: 2783200\n",
      "tensor(2.8171, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 3.88468480, Culminative Send Cost: 2822400\n",
      "tensor(2.8196, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 3.87881160, Culminative Send Cost: 2861600\n",
      "tensor(2.8222, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 3.87302279, Culminative Send Cost: 2900800\n",
      "tensor(2.8249, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 3.86731696, Culminative Send Cost: 2940000\n",
      "tensor(2.8276, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 3.86169171, Culminative Send Cost: 2979200\n",
      "tensor(2.8304, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 3.85614538, Culminative Send Cost: 3018400\n",
      "tensor(2.8332, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 3.85067701, Culminative Send Cost: 3057600\n",
      "tensor(2.8360, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 3.84528422, Culminative Send Cost: 3096800\n",
      "tensor(2.8389, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 3.83996487, Culminative Send Cost: 3136000\n",
      "tensor(2.8418, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 3.83471847, Culminative Send Cost: 3175200\n",
      "tensor(2.8447, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 3.82954335, Culminative Send Cost: 3214400\n",
      "tensor(2.8477, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 3.82443738, Culminative Send Cost: 3253600\n",
      "tensor(2.8507, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 3.81939912, Culminative Send Cost: 3292800\n",
      "tensor(2.8538, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 3.81442833, Culminative Send Cost: 3332000\n",
      "tensor(2.8569, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 3.80952263, Culminative Send Cost: 3371200\n",
      "tensor(2.8600, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 3.80468202, Culminative Send Cost: 3410400\n",
      "tensor(2.8631, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 3.79990339, Culminative Send Cost: 3449600\n",
      "tensor(2.8663, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 3.79518747, Culminative Send Cost: 3488800\n",
      "tensor(2.8694, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 3.79053187, Culminative Send Cost: 3528000\n",
      "tensor(2.8727, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 3.78593540, Culminative Send Cost: 3567200\n",
      "tensor(2.8759, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 3.78139782, Culminative Send Cost: 3606400\n",
      "tensor(2.8791, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 3.77691793, Culminative Send Cost: 3645600\n",
      "tensor(2.8824, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 3.77249336, Culminative Send Cost: 3684800\n",
      "tensor(2.8857, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 3.76812482, Culminative Send Cost: 3724000\n",
      "tensor(2.8890, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 3.76381087, Culminative Send Cost: 3763200\n",
      "tensor(2.8923, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 3.75955009, Culminative Send Cost: 3802400\n",
      "tensor(2.8957, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 3.75534225, Culminative Send Cost: 3841600\n",
      "tensor(2.8990, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 3.75118566, Culminative Send Cost: 3880800\n",
      "tensor(2.9024, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 3.74708009, Culminative Send Cost: 3920000\n",
      "tensor(2.9058, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 3.74302363, Culminative Send Cost: 3959200\n",
      "tensor(2.9092, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 3.73901677, Culminative Send Cost: 3998400\n",
      "tensor(2.9126, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 3.73505831, Culminative Send Cost: 4037600\n",
      "tensor(2.9160, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 3.73114681, Culminative Send Cost: 4076800\n",
      "tensor(2.9195, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 3.72728205, Culminative Send Cost: 4116000\n",
      "tensor(2.9229, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 3.72346354, Culminative Send Cost: 4155200\n",
      "tensor(2.9264, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 3.71969008, Culminative Send Cost: 4194400\n",
      "tensor(2.9298, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 3.71596146, Culminative Send Cost: 4233600\n",
      "tensor(2.9333, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 3.71227598, Culminative Send Cost: 4272800\n",
      "tensor(2.9368, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 3.70863414, Culminative Send Cost: 4312000\n",
      "tensor(2.9402, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 3.70503449, Culminative Send Cost: 4351200\n",
      "tensor(2.9437, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 3.70147610, Culminative Send Cost: 4390400\n",
      "tensor(2.9472, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 3.69795918, Culminative Send Cost: 4429600\n",
      "tensor(2.9507, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 3.69448328, Culminative Send Cost: 4468800\n",
      "tensor(2.9542, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 3.69104695, Culminative Send Cost: 4508000\n",
      "tensor(2.9577, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 3.68764973, Culminative Send Cost: 4547200\n",
      "tensor(2.9612, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 3.68429065, Culminative Send Cost: 4586400\n",
      "tensor(2.9647, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 3.68097019, Culminative Send Cost: 4625600\n",
      "tensor(2.9682, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 3.67768669, Culminative Send Cost: 4664800\n",
      "tensor(2.9717, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 3.67444110, Culminative Send Cost: 4704000\n",
      "tensor(2.9752, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 3.67123151, Culminative Send Cost: 4743200\n",
      "tensor(2.9787, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 3.66805720, Culminative Send Cost: 4782400\n",
      "tensor(2.9823, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 3.66491890, Culminative Send Cost: 4821600\n",
      "tensor(2.9858, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 3.66181517, Culminative Send Cost: 4860800\n",
      "tensor(2.9893, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 3.65874577, Culminative Send Cost: 4900000\n",
      "tensor(2.9928, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 3.65571022, Culminative Send Cost: 4939200\n",
      "tensor(2.9963, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 3.65270829, Culminative Send Cost: 4978400\n",
      "tensor(2.9998, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 3.64973879, Culminative Send Cost: 5017600\n",
      "tensor(3.0033, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 3.64680195, Culminative Send Cost: 5056800\n",
      "tensor(3.0068, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 3.64389729, Culminative Send Cost: 5096000\n",
      "tensor(3.0103, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 3.64102387, Culminative Send Cost: 5135200\n",
      "tensor(3.0137, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 3.63818169, Culminative Send Cost: 5174400\n",
      "tensor(3.0172, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 3.63537002, Culminative Send Cost: 5213600\n",
      "tensor(3.0207, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 3.63258815, Culminative Send Cost: 5252800\n",
      "tensor(3.0242, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 3.62983632, Culminative Send Cost: 5292000\n",
      "tensor(3.0276, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 3.62711406, Culminative Send Cost: 5331200\n",
      "tensor(3.0311, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 3.62442064, Culminative Send Cost: 5370400\n",
      "tensor(3.0346, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 3.62175608, Culminative Send Cost: 5409600\n",
      "tensor(3.0380, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 3.61911964, Culminative Send Cost: 5448800\n",
      "tensor(3.0415, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 3.61651063, Culminative Send Cost: 5488000\n",
      "tensor(3.0449, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 3.61392951, Culminative Send Cost: 5527200\n",
      "tensor(3.0483, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 3.61137533, Culminative Send Cost: 5566400\n",
      "tensor(3.0518, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 3.60884786, Culminative Send Cost: 5605600\n",
      "tensor(3.0552, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 3.60634661, Culminative Send Cost: 5644800\n",
      "tensor(3.0586, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 3.60387135, Culminative Send Cost: 5684000\n",
      "tensor(3.0620, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 3.60142207, Culminative Send Cost: 5723200\n",
      "tensor(3.0654, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 3.59899807, Culminative Send Cost: 5762400\n",
      "tensor(3.0688, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 3.59659863, Culminative Send Cost: 5801600\n",
      "tensor(3.0722, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 3.59422421, Culminative Send Cost: 5840800\n",
      "tensor(3.0756, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 3.59187436, Culminative Send Cost: 5880000\n",
      "tensor(3.0789, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 3.58954811, Culminative Send Cost: 5919200\n",
      "tensor(3.0823, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 3.58724594, Culminative Send Cost: 5958400\n",
      "tensor(3.0857, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 3.58496690, Culminative Send Cost: 5997600\n",
      "tensor(3.0890, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 3.58271074, Culminative Send Cost: 6036800\n",
      "tensor(3.0923, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 3.58047748, Culminative Send Cost: 6076000\n",
      "tensor(3.0957, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 3.57826757, Culminative Send Cost: 6115200\n",
      "tensor(3.0990, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 3.57607913, Culminative Send Cost: 6154400\n",
      "tensor(3.1023, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 3.57391310, Culminative Send Cost: 6193600\n",
      "tensor(3.1056, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 3.57176852, Culminative Send Cost: 6232800\n",
      "tensor(3.1089, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 3.56964493, Culminative Send Cost: 6272000\n",
      "tensor(3.1121, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 3.56754303, Culminative Send Cost: 6311200\n",
      "tensor(3.1154, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 3.56546164, Culminative Send Cost: 6350400\n",
      "tensor(3.1187, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 3.56340098, Culminative Send Cost: 6389600\n",
      "tensor(3.1219, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 3.56136131, Culminative Send Cost: 6428800\n",
      "tensor(3.1252, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 3.55934119, Culminative Send Cost: 6468000\n",
      "tensor(3.1284, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 3.55734110, Culminative Send Cost: 6507200\n",
      "tensor(3.1316, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 3.55536032, Culminative Send Cost: 6546400\n",
      "tensor(3.1348, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 3.55339885, Culminative Send Cost: 6585600\n",
      "tensor(3.1380, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 3.55145669, Culminative Send Cost: 6624800\n",
      "tensor(3.1412, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 3.54953361, Culminative Send Cost: 6664000\n",
      "tensor(3.1444, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 3.54762888, Culminative Send Cost: 6703200\n",
      "tensor(3.1476, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 3.54574323, Culminative Send Cost: 6742400\n",
      "tensor(3.1507, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 3.54387522, Culminative Send Cost: 6781600\n",
      "tensor(3.1539, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 3.54202485, Culminative Send Cost: 6820800\n",
      "tensor(3.1570, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 3.54019332, Culminative Send Cost: 6860000\n",
      "tensor(3.1601, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 3.53837895, Culminative Send Cost: 6899200\n",
      "tensor(3.1632, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 3.53658152, Culminative Send Cost: 6938400\n",
      "tensor(3.1663, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 3.53480148, Culminative Send Cost: 6977600\n",
      "tensor(3.1694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 3.53303838, Culminative Send Cost: 7016800\n",
      "tensor(3.1725, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 3.53129220, Culminative Send Cost: 7056000\n",
      "tensor(3.1756, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 3.52956247, Culminative Send Cost: 7095200\n",
      "tensor(3.1786, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 3.52784896, Culminative Send Cost: 7134400\n",
      "tensor(3.1817, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 3.52615213, Culminative Send Cost: 7173600\n",
      "tensor(3.1847, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 3.52447128, Culminative Send Cost: 7212800\n",
      "tensor(3.1878, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 3.52280521, Culminative Send Cost: 7252000\n",
      "tensor(3.1908, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 3.52115536, Culminative Send Cost: 7291200\n",
      "tensor(3.1938, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 3.51952052, Culminative Send Cost: 7330400\n",
      "tensor(3.1968, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 3.51790142, Culminative Send Cost: 7369600\n",
      "tensor(3.1997, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 3.51629710, Culminative Send Cost: 7408800\n",
      "tensor(3.2027, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 3.51470804, Culminative Send Cost: 7448000\n",
      "tensor(3.2057, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 3.51313353, Culminative Send Cost: 7487200\n",
      "tensor(3.2086, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 3.51157308, Culminative Send Cost: 7526400\n",
      "tensor(3.2115, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 3.51002789, Culminative Send Cost: 7565600\n",
      "tensor(3.2145, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 3.50849628, Culminative Send Cost: 7604800\n",
      "tensor(3.2174, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 3.50697923, Culminative Send Cost: 7644000\n",
      "tensor(3.2203, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 3.50547576, Culminative Send Cost: 7683200\n",
      "tensor(3.2232, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 3.50398564, Culminative Send Cost: 7722400\n",
      "tensor(3.2260, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 3.50250959, Culminative Send Cost: 7761600\n",
      "tensor(3.2289, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 3.50104690, Culminative Send Cost: 7800800\n",
      "tensor(3.2317, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 3.49959731, Culminative Send Cost: 7840000\n",
      "tensor(3.2346, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 3.49816108, Culminative Send Cost: 7879200\n",
      "tensor(3.2374, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 3.49673796, Culminative Send Cost: 7918400\n",
      "tensor(3.2402, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 3.49532747, Culminative Send Cost: 7957600\n",
      "tensor(3.2430, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 3.49392891, Culminative Send Cost: 7996800\n",
      "tensor(3.2458, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 3.49254417, Culminative Send Cost: 8036000\n",
      "tensor(3.2486, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 3.49117112, Culminative Send Cost: 8075200\n",
      "tensor(3.2514, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 3.48981023, Culminative Send Cost: 8114400\n",
      "tensor(3.2541, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 3.48846197, Culminative Send Cost: 8153600\n",
      "tensor(3.2569, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 3.48712540, Culminative Send Cost: 8192800\n",
      "tensor(3.2596, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 3.48580074, Culminative Send Cost: 8232000\n",
      "tensor(3.2623, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 3.48448801, Culminative Send Cost: 8271200\n",
      "tensor(3.2650, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 3.48318648, Culminative Send Cost: 8310400\n",
      "tensor(3.2677, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 3.48189640, Culminative Send Cost: 8349600\n",
      "tensor(3.2704, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 3.48061800, Culminative Send Cost: 8388800\n",
      "tensor(3.2731, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.47935057, Culminative Send Cost: 8428000\n",
      "tensor(3.2758, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.47809434, Culminative Send Cost: 8467200\n",
      "tensor(3.2784, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.47684908, Culminative Send Cost: 8506400\n",
      "tensor(3.2811, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.47561479, Culminative Send Cost: 8545600\n",
      "tensor(3.2837, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.47439146, Culminative Send Cost: 8584800\n",
      "tensor(3.2863, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.47317791, Culminative Send Cost: 8624000\n",
      "tensor(3.2889, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.47197556, Culminative Send Cost: 8663200\n",
      "tensor(3.2915, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.47078323, Culminative Send Cost: 8702400\n",
      "tensor(3.2941, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.46960187, Culminative Send Cost: 8741600\n",
      "tensor(3.2966, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.46843028, Culminative Send Cost: 8780800\n",
      "tensor(3.2992, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.46726847, Culminative Send Cost: 8820000\n",
      "tensor(3.3018, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.46611714, Culminative Send Cost: 8859200\n",
      "tensor(3.3043, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.46497512, Culminative Send Cost: 8898400\n",
      "tensor(3.3068, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.46384335, Culminative Send Cost: 8937600\n",
      "tensor(3.3093, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.46272111, Culminative Send Cost: 8976800\n",
      "tensor(3.3118, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.46160793, Culminative Send Cost: 9016000\n",
      "tensor(3.3143, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.46050429, Culminative Send Cost: 9055200\n",
      "tensor(3.3168, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.45941091, Culminative Send Cost: 9094400\n",
      "tensor(3.3193, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.45832586, Culminative Send Cost: 9133600\n",
      "tensor(3.3217, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.45725036, Culminative Send Cost: 9172800\n",
      "tensor(3.3241, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.45618343, Culminative Send Cost: 9212000\n",
      "tensor(3.3266, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.45512605, Culminative Send Cost: 9251200\n",
      "tensor(3.3290, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.45407724, Culminative Send Cost: 9290400\n",
      "tensor(3.3314, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.45303750, Culminative Send Cost: 9329600\n",
      "tensor(3.3338, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.45200562, Culminative Send Cost: 9368800\n",
      "tensor(3.3362, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.45098329, Culminative Send Cost: 9408000\n",
      "tensor(3.3386, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.44996905, Culminative Send Cost: 9447200\n",
      "tensor(3.3409, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.44896317, Culminative Send Cost: 9486400\n",
      "tensor(3.3433, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.44796562, Culminative Send Cost: 9525600\n",
      "tensor(3.3456, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.44697571, Culminative Send Cost: 9564800\n",
      "tensor(3.3479, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.44599485, Culminative Send Cost: 9604000\n",
      "tensor(3.3503, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.44502163, Culminative Send Cost: 9643200\n",
      "tensor(3.3526, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.44405675, Culminative Send Cost: 9682400\n",
      "tensor(3.3549, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.44309974, Culminative Send Cost: 9721600\n",
      "tensor(3.3571, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.44215059, Culminative Send Cost: 9760800\n",
      "tensor(3.3594, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.44120884, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[-1.0788e-02, -3.3491e-02, -3.4444e-02, -3.0865e-02, -3.1982e-02,\n",
      "         -2.4948e-03, -1.5580e-02,  1.6187e-02, -5.2503e-03,  6.4184e-03,\n",
      "         -1.5504e-02,  3.4533e-02, -2.2426e-02, -1.7435e-02, -3.4731e-02,\n",
      "          8.9881e-03,  2.5848e-02,  2.8187e-02, -4.4691e-03,  1.1775e-02,\n",
      "         -2.0812e-02,  2.2114e-02,  3.4636e-02,  2.1585e-02,  3.5320e-02,\n",
      "          2.3281e-02, -3.4690e-02, -1.2597e-02,  1.7974e-02, -7.8694e-03,\n",
      "          2.6294e-02, -2.0338e-02,  5.2850e-03,  3.2177e-02,  2.2706e-02,\n",
      "          2.0888e-04, -7.7553e-03,  3.1584e-02,  3.7236e-02,  5.6974e-03,\n",
      "          1.2109e-02,  4.8053e-03, -1.6939e-02, -1.7335e-02,  3.9235e-02,\n",
      "          1.5446e-02, -2.8047e-02,  2.9715e-02,  2.8004e-02, -4.5215e-03,\n",
      "         -2.9298e-02, -1.0810e-03, -1.1603e-02, -1.8541e-02, -2.5899e-02,\n",
      "         -2.5733e-02,  2.5562e-02,  2.5576e-02, -1.5825e-02,  1.5184e-02,\n",
      "          2.7626e-02, -2.8472e-02, -2.5148e-02,  2.8795e-03,  5.8075e-02,\n",
      "          6.7504e-02,  8.4105e-02,  6.2730e-02,  9.8844e-02,  1.2321e-01,\n",
      "          1.2467e-01,  1.6321e-01,  1.4753e-01,  1.7333e-01,  1.6657e-01,\n",
      "          1.4027e-01,  1.3978e-01,  8.2317e-02,  7.1861e-02,  3.8654e-02,\n",
      "          1.2861e-02, -3.0918e-02, -3.1904e-02,  3.1759e-02, -2.0756e-02,\n",
      "          3.3025e-02, -2.9419e-02,  1.5455e-02, -3.3258e-02, -2.9375e-02,\n",
      "          3.0706e-02,  2.2760e-02,  4.0307e-02,  1.0373e-01,  1.3234e-01,\n",
      "          1.2595e-01,  6.4515e-02,  8.2610e-02,  1.3348e-01,  1.6810e-01,\n",
      "          1.9551e-01,  2.3500e-01,  3.0910e-01,  3.3251e-01,  2.3585e-01,\n",
      "          2.2218e-01,  1.0935e-01,  3.2216e-02,  8.0902e-03, -1.0276e-02,\n",
      "          9.6621e-03,  7.3588e-03, -1.5232e-02,  2.2003e-02, -7.7531e-04,\n",
      "          4.0523e-03, -2.7069e-02,  2.9362e-02,  3.9255e-02,  2.1297e-02,\n",
      "          8.6792e-02,  8.2758e-02,  1.2115e-01,  8.2701e-02,  4.4330e-02,\n",
      "          2.6973e-02,  3.4304e-02,  3.6320e-02,  4.1014e-02,  2.7465e-02,\n",
      "          9.9511e-02,  1.2463e-01,  1.0998e-01,  1.4632e-01,  1.3028e-01,\n",
      "          1.0723e-01,  5.4798e-02, -1.2617e-02,  1.3422e-02,  1.6688e-02,\n",
      "         -2.0868e-03, -1.9435e-02, -8.2390e-03, -2.1951e-02, -9.9317e-03,\n",
      "         -3.8562e-02,  1.8758e-02,  9.7327e-04,  2.8946e-02,  5.2692e-02,\n",
      "          6.6550e-02,  4.1675e-03,  6.4794e-02, -6.7112e-02, -9.2586e-02,\n",
      "         -1.2390e-01, -1.0113e-01, -2.8617e-02,  7.0098e-03, -5.2167e-02,\n",
      "         -3.6968e-02,  3.7456e-02,  7.2632e-02,  8.4684e-02,  5.9181e-02,\n",
      "          6.3087e-02, -2.4493e-03, -3.1310e-02, -1.9123e-02, -2.7978e-02,\n",
      "         -2.6372e-02,  9.4859e-03, -3.6766e-03, -3.5913e-02, -1.6360e-02,\n",
      "         -4.7235e-02, -2.5608e-02, -7.5400e-03,  1.3543e-03, -4.5812e-03,\n",
      "         -4.5528e-02,  8.5694e-03, -1.8604e-02,  5.3036e-02,  6.9599e-02,\n",
      "          3.7528e-02,  1.5782e-02, -4.7177e-02, -6.6737e-02, -5.0125e-02,\n",
      "         -6.7770e-02,  8.0527e-02,  7.9906e-02,  7.8717e-02,  4.2386e-02,\n",
      "          1.7032e-02, -2.9497e-02,  1.5915e-02,  2.8604e-02,  3.0267e-02,\n",
      "          1.3495e-02, -5.0231e-02, -6.5545e-02,  1.8431e-02,  6.1543e-02,\n",
      "          1.9084e-02,  6.2097e-02,  5.0645e-02,  5.1523e-03, -8.5954e-03,\n",
      "          1.0066e-01,  1.9231e-01,  1.1872e-01, -3.3675e-02, -7.9153e-02,\n",
      "         -8.9943e-02, -3.1058e-02, -3.9470e-02, -6.2554e-02,  1.0378e-01,\n",
      "          2.1687e-01,  1.4287e-01,  4.5469e-02, -2.2575e-02,  6.1644e-03,\n",
      "          9.3728e-03,  4.9601e-02,  5.0339e-02,  2.9106e-02, -3.1261e-02,\n",
      "         -7.4467e-03,  1.3686e-02,  1.0583e-01,  3.2846e-02,  7.9536e-02,\n",
      "          1.2339e-01,  1.0948e-01,  5.8047e-02, -1.0984e-02,  3.9955e-02,\n",
      "         -2.1166e-03, -9.6933e-02, -9.7712e-02, -7.3303e-02, -1.2200e-02,\n",
      "         -2.1204e-02,  2.5961e-02,  1.4523e-01,  2.4376e-01,  1.8167e-01,\n",
      "         -3.1427e-03, -6.6091e-03, -2.2435e-02,  1.3237e-02,  5.2493e-03,\n",
      "          7.5234e-03,  5.0291e-02, -1.5979e-02,  5.6412e-02,  5.7284e-02,\n",
      "          1.3458e-01,  9.7426e-02,  1.6958e-01,  2.2332e-01,  1.4205e-01,\n",
      "         -2.8376e-02, -8.5374e-02, -1.4978e-01, -7.8813e-02, -4.9804e-02,\n",
      "         -7.9784e-02, -2.3817e-02, -2.9166e-02,  1.2358e-02,  2.0772e-03,\n",
      "          1.3108e-01,  1.9109e-01,  1.0368e-01,  1.4072e-02,  6.8668e-03,\n",
      "         -1.8532e-02,  1.3337e-02,  5.4183e-03,  4.1285e-02,  2.6932e-02,\n",
      "          6.6814e-02,  1.4227e-01,  1.2318e-01,  1.3988e-01,  1.2228e-01,\n",
      "          1.8144e-01,  2.2691e-01,  1.9180e-01, -1.0697e-03, -2.1104e-01,\n",
      "         -2.2163e-01, -4.7132e-02, -4.0033e-02, -3.2208e-02, -6.2213e-03,\n",
      "         -4.3851e-02, -4.6251e-02, -8.2814e-03,  4.3782e-02,  1.3094e-01,\n",
      "          5.3109e-02,  4.1455e-02, -2.8769e-02, -3.2209e-04, -2.8133e-02,\n",
      "          4.5774e-04,  4.1802e-02,  6.1809e-02,  1.8716e-01,  1.8949e-01,\n",
      "          1.4862e-01,  1.7420e-01,  1.5560e-01,  1.4609e-01,  1.7767e-01,\n",
      "          3.1534e-01,  2.0576e-01, -1.3310e-01, -1.4183e-01,  2.7077e-02,\n",
      "          1.0299e-01,  3.5848e-02, -3.5651e-03,  1.4606e-02,  5.1138e-03,\n",
      "          5.1780e-03, -2.2861e-02,  2.0604e-02,  3.9588e-02,  2.8284e-02,\n",
      "          3.1045e-02,  1.7052e-02,  1.6447e-02,  1.2565e-02,  1.6588e-02,\n",
      "          1.0153e-01,  2.0071e-01,  1.6425e-01,  1.3277e-01,  7.3226e-02,\n",
      "          1.1606e-01,  4.5679e-02,  1.9512e-01,  3.6589e-01,  2.1228e-01,\n",
      "         -1.1129e-01,  4.8571e-02,  2.2605e-01,  2.3804e-01,  1.3490e-01,\n",
      "          1.0792e-01,  5.6363e-02,  1.6989e-02, -1.4587e-02, -8.5060e-02,\n",
      "          2.9014e-03,  3.3553e-02, -1.7647e-03, -3.2727e-02, -1.2119e-02,\n",
      "         -3.8294e-03,  3.2363e-02,  5.6460e-02,  1.6351e-01,  1.4468e-01,\n",
      "          6.6281e-02,  3.4394e-02,  7.4474e-02,  1.9020e-02, -5.3048e-02,\n",
      "          1.0712e-01,  3.7063e-01,  3.5280e-02, -7.1096e-02,  1.9028e-01,\n",
      "          2.2025e-01,  2.3708e-01,  1.6703e-01,  1.3601e-01,  7.1015e-02,\n",
      "         -6.3766e-03, -5.3640e-02, -9.3954e-02, -2.2519e-02,  2.0939e-02,\n",
      "         -1.3267e-02,  3.2949e-03,  2.7480e-02,  2.4347e-03,  1.7672e-02,\n",
      "          5.9742e-02,  1.1573e-01, -2.4657e-02, -6.9445e-02, -8.0257e-02,\n",
      "         -3.8359e-02, -1.0917e-01, -1.3004e-01,  1.2633e-01,  2.3575e-01,\n",
      "         -3.1045e-02,  1.9133e-02,  7.0771e-02,  2.7912e-01,  2.4340e-01,\n",
      "          2.8623e-02,  5.5232e-02, -1.8765e-02, -5.6248e-02, -3.8679e-02,\n",
      "         -8.0420e-02, -1.2766e-02,  2.1769e-02,  3.9941e-03,  2.0341e-02,\n",
      "         -5.5333e-03,  2.5194e-02,  1.1264e-03,  6.8445e-02,  3.6671e-03,\n",
      "         -9.2528e-02, -1.2314e-01, -1.1778e-01, -8.1864e-02, -1.2905e-01,\n",
      "         -6.4727e-02,  1.4251e-01,  1.6230e-01, -7.0975e-02,  1.0387e-02,\n",
      "          3.8937e-02,  3.0286e-01,  1.9038e-01, -6.5603e-02, -1.2205e-01,\n",
      "         -9.3517e-02, -8.1503e-02, -2.7259e-02, -6.5197e-02, -8.6678e-04,\n",
      "         -5.6496e-03,  1.0801e-03, -3.2540e-03, -2.8955e-02, -2.4730e-02,\n",
      "         -4.3809e-03,  2.7376e-02, -3.5229e-02, -1.1536e-01, -1.1698e-01,\n",
      "         -1.1431e-01, -8.7362e-02, -6.5457e-02,  3.8678e-02,  1.7332e-01,\n",
      "          1.6461e-01, -8.7712e-02, -2.6142e-02,  1.0533e-01,  2.5715e-01,\n",
      "          1.1549e-02, -1.5151e-01, -1.2292e-01, -8.9772e-02, -8.2740e-02,\n",
      "         -2.4446e-02, -2.8189e-02, -3.7670e-02, -2.8712e-02, -2.5248e-02,\n",
      "          2.3093e-02,  1.2642e-03, -2.6491e-02,  3.8949e-02,  2.9623e-02,\n",
      "          9.1758e-04, -1.1031e-01, -1.2992e-01, -9.3058e-02, -5.0275e-02,\n",
      "          7.0908e-02,  1.0399e-01,  2.6137e-01,  9.3600e-02, -6.5375e-02,\n",
      "         -3.6900e-02,  1.4036e-01,  1.0840e-01, -7.5611e-02, -7.6302e-02,\n",
      "         -7.4468e-02, -2.0051e-02, -1.0785e-01, -8.5267e-02, -3.7001e-02,\n",
      "         -6.3971e-02, -6.5921e-02,  4.8089e-04,  1.2021e-02,  1.6608e-02,\n",
      "          3.0846e-02,  2.3793e-02,  3.3905e-03, -3.6668e-02, -8.0198e-02,\n",
      "         -6.3204e-02, -1.0253e-01, -7.8254e-02,  4.4958e-02,  1.3764e-01,\n",
      "          2.1001e-01,  4.8959e-04, -1.4792e-01, -3.6887e-02,  1.0589e-02,\n",
      "         -4.6871e-03, -2.7551e-02, -9.7227e-03, -3.7841e-02,  4.4032e-03,\n",
      "         -5.5505e-02, -8.2445e-02, -7.0071e-02, -8.8754e-02, -6.7126e-02,\n",
      "         -2.2798e-02,  1.5364e-02, -4.6055e-03,  1.3971e-02, -4.5067e-03,\n",
      "          1.6121e-02, -3.0173e-02, -7.3210e-02, -1.4151e-02, -3.3299e-02,\n",
      "         -4.7731e-02, -4.2873e-02,  3.3403e-02,  1.0022e-02, -5.6802e-02,\n",
      "         -1.5132e-01, -9.2659e-02, -7.5611e-02,  2.2580e-03, -3.2140e-02,\n",
      "         -7.0753e-02, -3.2685e-02, -6.6241e-02, -8.4353e-02, -5.8867e-02,\n",
      "         -1.0340e-01, -1.2941e-01, -4.6509e-02,  2.0608e-03, -2.6244e-02,\n",
      "         -1.3773e-02,  3.6184e-03, -3.1497e-02, -7.4636e-03, -2.3034e-02,\n",
      "         -2.7973e-02, -9.4212e-03, -1.6738e-02, -4.2602e-02, -1.1745e-01,\n",
      "         -7.5115e-02, -6.5660e-02, -3.3612e-02, -2.4706e-02, -4.2074e-02,\n",
      "         -1.4877e-02, -5.3996e-02, -6.6347e-02, -3.8922e-02, -3.8489e-02,\n",
      "         -6.7868e-02, -9.9617e-02, -7.8663e-02, -1.4246e-01, -5.6113e-02,\n",
      "         -5.9602e-02,  2.1147e-02, -2.5649e-02,  3.2293e-02, -1.3465e-02,\n",
      "         -2.9843e-03, -4.1464e-02, -1.0413e-01, -9.4933e-02, -1.1127e-02,\n",
      "         -1.0961e-03, -7.9268e-02, -9.9335e-02, -8.2662e-02, -3.8218e-02,\n",
      "          5.4649e-02,  2.5778e-02,  6.1538e-03, -7.7942e-02, -2.2505e-02,\n",
      "         -1.5255e-03, -9.7413e-03, -5.6008e-02, -6.7026e-02, -1.1040e-01,\n",
      "         -7.8551e-02, -9.6319e-02, -3.6858e-02, -4.3424e-02, -3.0742e-02,\n",
      "         -6.9569e-03, -2.8147e-02, -2.0615e-02,  9.9379e-03, -3.8403e-03,\n",
      "         -6.9771e-02, -6.7887e-02, -8.4959e-02, -1.0361e-01, -8.4671e-02,\n",
      "         -1.1145e-01, -8.3911e-02, -9.3211e-03, -7.5521e-03, -5.3515e-02,\n",
      "         -9.3385e-02, -1.1409e-01, -5.4289e-02, -1.0447e-01, -1.7864e-02,\n",
      "         -6.9564e-02, -2.6024e-02, -5.4129e-02, -6.0534e-02, -5.1941e-02,\n",
      "         -4.4991e-02, -4.2697e-02,  1.3585e-02, -1.8576e-02, -1.4281e-02,\n",
      "          9.3061e-03,  2.6187e-02, -4.2199e-02, -2.8193e-02, -4.4256e-02,\n",
      "         -5.6904e-02, -8.9984e-02, -3.3962e-02, -1.0225e-01, -5.6213e-02,\n",
      "         -1.3085e-02,  3.1020e-02,  6.8896e-02,  7.7375e-03, -8.9622e-02,\n",
      "         -7.6705e-02,  1.3610e-02,  3.2706e-03,  3.7723e-02,  1.0198e-01,\n",
      "          8.1718e-02,  4.1626e-02,  2.8500e-02,  2.7520e-02, -3.6417e-02,\n",
      "         -1.5324e-02, -2.3257e-02,  7.9269e-03, -2.6500e-02,  2.9238e-03,\n",
      "          2.3699e-02,  2.6734e-02,  3.3855e-03, -1.7879e-04,  1.2889e-01,\n",
      "          1.7581e-01,  1.3380e-01,  1.6504e-01,  9.9935e-02,  4.6273e-02,\n",
      "          7.8700e-02,  9.2455e-02,  7.4524e-02,  1.4262e-01,  1.9167e-01,\n",
      "          1.8619e-01,  2.3417e-01,  2.1938e-01,  1.7471e-01,  6.0687e-02,\n",
      "          7.9226e-02, -2.5264e-02, -5.4064e-03,  1.3273e-02, -2.4239e-02,\n",
      "          2.0917e-02,  1.0444e-02, -1.6782e-02, -1.6334e-02,  3.0202e-02,\n",
      "          6.0815e-02,  1.0419e-01,  1.7268e-01,  3.1226e-01,  2.8926e-01,\n",
      "          3.2726e-01,  3.0540e-01,  3.2855e-01,  3.0978e-01,  3.6666e-01,\n",
      "          3.8830e-01,  4.6233e-01,  4.2668e-01,  3.5432e-01,  2.4540e-01,\n",
      "          2.5057e-01,  1.1611e-01,  5.4006e-02,  3.6126e-02,  2.8447e-02,\n",
      "         -1.5698e-02, -3.1139e-02, -8.4588e-03, -2.8152e-02,  1.5372e-02,\n",
      "         -3.4336e-02, -2.4403e-03, -2.5430e-02,  3.2381e-02,  7.8263e-02,\n",
      "          8.8287e-02,  1.7459e-01,  1.7704e-01,  1.6288e-01,  2.0375e-01,\n",
      "          2.1800e-01,  2.4522e-01,  2.7242e-01,  2.4254e-01,  1.9063e-01,\n",
      "          1.6639e-01,  1.7749e-01,  1.1597e-01,  5.1922e-02,  5.1406e-02,\n",
      "          4.8644e-03,  9.6151e-03,  6.3401e-03,  3.2643e-02,  2.3644e-02,\n",
      "         -2.2412e-02,  8.1514e-03,  3.1254e-02, -1.8273e-02, -1.7255e-02,\n",
      "          2.0505e-02, -2.1797e-02,  2.9022e-03, -2.2596e-02, -1.5032e-02,\n",
      "          3.9506e-02,  3.1414e-02, -1.3184e-02,  5.7426e-02,  6.4054e-02,\n",
      "          7.7886e-02,  2.1295e-02,  4.5313e-02,  5.2312e-02,  2.7915e-02,\n",
      "          6.6294e-03, -1.9761e-02,  1.6351e-02, -8.3554e-03, -1.8848e-03,\n",
      "         -1.4712e-02,  2.2713e-02, -3.0990e-02, -1.8417e-02]])\n",
      "activation_stack.0.bias: tensor([2.2521])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The training for local_update_epochs is 5 ===\n",
      "Client_X_train[0]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[0]: tensor([0, 4, 1,  ..., 4, 7, 1], dtype=torch.int32)\n",
      "Client_X_train[1]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[1]: tensor([2, 8, 3,  ..., 6, 6, 4], dtype=torch.int32)\n",
      "Client_X_train[2]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[2]: tensor([7, 3, 8,  ..., 2, 3, 6], dtype=torch.int32)\n",
      "Client_X_train[3]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[3]: tensor([6, 4, 4,  ..., 6, 9, 2], dtype=torch.int32)\n",
      "Client_X_train[4]: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], dtype=torch.float64)\n",
      "Client_y_train[4]: tensor([8, 1, 3,  ..., 1, 1, 5], dtype=torch.int32)\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Initial global weights are: OrderedDict([('activation_stack.0.weight', tensor([[-2.1170e-02, -2.0452e-02, -1.8209e-02, -3.2509e-02, -2.3281e-02,\n",
      "         -2.8679e-02,  2.3779e-02, -8.0903e-03,  3.2026e-02,  1.9523e-02,\n",
      "         -1.3430e-02,  5.5745e-03,  2.6651e-02, -1.8038e-02, -2.9075e-02,\n",
      "         -2.5999e-02, -2.3444e-02, -2.9728e-02, -7.4760e-03,  1.1976e-02,\n",
      "         -1.1321e-02,  7.2756e-03,  2.8179e-02, -1.8882e-02,  1.2669e-03,\n",
      "         -3.2690e-02, -3.1121e-02,  8.6157e-03,  2.8996e-02,  1.4078e-03,\n",
      "         -3.0703e-03, -6.8697e-03, -2.8452e-02,  3.9369e-03,  2.5006e-02,\n",
      "          6.4760e-04, -1.0243e-02, -3.4505e-02, -3.4528e-02, -3.2239e-03,\n",
      "          2.1180e-02,  3.0492e-02,  5.7642e-03, -3.2081e-02, -1.6383e-02,\n",
      "         -2.1947e-02, -1.8970e-02,  2.7310e-03, -5.3887e-03,  6.3083e-03,\n",
      "         -2.4699e-02, -3.3887e-02, -5.4838e-03,  6.2393e-03,  2.1342e-02,\n",
      "          8.6847e-03, -3.4091e-02,  2.4208e-02, -5.2141e-03, -1.5139e-02,\n",
      "         -1.8462e-02,  3.2485e-03, -3.0251e-02, -3.0643e-02,  3.3054e-02,\n",
      "          2.8521e-03,  9.6872e-03,  2.2996e-02,  1.7113e-02,  2.6235e-02,\n",
      "         -3.2982e-02, -3.4763e-02,  3.4802e-02, -2.2847e-02,  2.8477e-02,\n",
      "         -3.3875e-02, -1.2601e-02,  1.7655e-02, -2.9175e-03,  1.4723e-02,\n",
      "         -1.6387e-02,  7.7269e-03, -3.1524e-02, -4.0986e-03,  2.5656e-02,\n",
      "         -1.9958e-02, -8.6605e-03,  3.1312e-02, -7.0712e-03,  1.2952e-02,\n",
      "         -2.3572e-02, -2.5451e-02,  1.1520e-02, -2.5097e-02, -3.6461e-03,\n",
      "          1.3905e-03, -3.0915e-02, -2.5344e-02,  6.6840e-03, -2.2055e-02,\n",
      "          2.3173e-02,  2.3896e-02, -3.2573e-02,  1.7200e-02, -1.8524e-03,\n",
      "          2.7291e-02,  2.2400e-02,  2.7708e-02,  2.2948e-02,  3.3438e-02,\n",
      "          3.1677e-02, -9.7568e-03, -7.8690e-03,  3.1903e-02, -1.3431e-02,\n",
      "         -2.3085e-02, -1.7402e-02,  1.0874e-02,  8.2134e-03,  1.8286e-02,\n",
      "         -1.6814e-02, -3.4789e-02, -2.2527e-02, -2.2407e-02,  2.5749e-02,\n",
      "         -1.3359e-02,  2.7668e-02, -7.7975e-03,  3.4936e-02, -1.7322e-02,\n",
      "         -3.5060e-02, -6.0185e-03, -2.6502e-05, -3.5334e-02, -1.8887e-02,\n",
      "          3.1999e-03, -2.0150e-02, -3.4548e-02, -1.2211e-02, -5.9588e-03,\n",
      "         -9.8173e-03, -2.4321e-02,  1.2224e-02, -2.9285e-02,  1.4968e-02,\n",
      "          7.3011e-03, -2.1405e-02, -3.1441e-02, -9.9602e-03,  3.5047e-03,\n",
      "         -2.2397e-02, -7.4647e-03,  4.6526e-03,  3.4888e-02,  1.1548e-02,\n",
      "         -1.8442e-02,  2.0743e-02, -2.8128e-02,  1.5923e-02, -1.3226e-02,\n",
      "         -2.1957e-02,  2.4630e-02,  3.4025e-02,  8.8254e-03,  3.4672e-02,\n",
      "         -3.2799e-03, -1.4389e-02, -1.2838e-02,  1.8844e-02,  1.1202e-03,\n",
      "          3.3818e-03, -2.3081e-02,  3.1806e-02, -2.8409e-03, -4.0181e-03,\n",
      "          2.5615e-02, -1.7450e-02, -8.6911e-03,  2.2641e-02, -2.8246e-02,\n",
      "         -2.0979e-02, -3.0872e-02, -1.6533e-02,  2.0860e-02, -9.8872e-03,\n",
      "          3.1185e-02,  1.4088e-02,  7.0027e-03, -1.0131e-02,  2.0884e-02,\n",
      "          1.7553e-02, -2.4131e-02, -3.5508e-02,  3.0568e-02, -2.9219e-03,\n",
      "          1.6690e-02, -2.7619e-02,  3.0974e-02,  1.7178e-02, -2.9330e-02,\n",
      "         -2.8675e-02,  2.5332e-02,  3.3332e-02, -4.6373e-04,  8.3794e-03,\n",
      "          2.6599e-04,  2.7783e-02, -9.7267e-03, -2.5687e-02, -1.0641e-02,\n",
      "          1.8398e-02, -2.0670e-02,  3.0097e-02,  4.2268e-03,  2.0619e-02,\n",
      "          1.7072e-02, -1.3962e-02, -3.0288e-02,  1.5964e-02,  2.8617e-02,\n",
      "          1.8432e-02,  2.1365e-02, -5.8420e-03,  3.1138e-02,  2.8158e-02,\n",
      "          2.8124e-02,  3.0479e-02,  1.4144e-03, -2.2732e-02, -1.8218e-02,\n",
      "          1.6490e-02,  1.4411e-02, -2.6649e-03, -2.1210e-02, -8.4356e-03,\n",
      "          1.0389e-02,  3.1115e-02,  3.3247e-02, -2.6450e-02,  5.8892e-04,\n",
      "          3.4621e-02, -1.3118e-02, -2.5791e-02, -1.4829e-02,  1.1997e-02,\n",
      "          6.8886e-03,  1.6699e-02, -1.0392e-02,  2.6573e-02, -1.0427e-02,\n",
      "         -6.5949e-03,  2.0030e-02,  2.9746e-02,  6.8292e-03, -6.2275e-03,\n",
      "         -4.0155e-03, -4.2338e-03,  1.4426e-02,  1.4007e-02,  2.9844e-02,\n",
      "          3.2753e-02, -2.2855e-02, -3.3338e-02, -3.1989e-02, -1.0513e-02,\n",
      "         -3.5598e-02, -3.3028e-02, -5.1343e-03, -1.9998e-02, -3.5428e-02,\n",
      "          3.4964e-02,  2.9864e-03, -2.3993e-03, -4.3955e-03, -2.4899e-02,\n",
      "         -8.0003e-04,  3.3693e-02,  2.6618e-03,  3.3022e-02,  1.7044e-02,\n",
      "         -1.9986e-02,  9.9018e-04,  3.2769e-02,  2.1098e-02,  2.7550e-03,\n",
      "          2.2447e-02,  3.1155e-03,  1.8469e-02, -2.8422e-02,  4.0934e-04,\n",
      "         -1.6539e-02,  3.1755e-02, -2.1642e-02, -2.5297e-02,  1.2349e-02,\n",
      "          1.0258e-02, -1.2898e-02,  1.0417e-02,  4.6896e-03,  1.0187e-02,\n",
      "         -1.9456e-02,  1.4249e-02, -1.1990e-03,  3.0637e-02,  1.1220e-02,\n",
      "         -2.5719e-02,  3.1278e-02, -8.1261e-03, -2.6059e-03, -7.7541e-03,\n",
      "         -1.7405e-02,  3.1582e-02,  7.2694e-03, -2.9182e-02, -4.8898e-03,\n",
      "          2.3405e-02,  2.7962e-02, -1.7416e-02, -2.0488e-02, -7.6711e-03,\n",
      "         -3.5158e-02, -6.5009e-03, -1.2740e-02, -2.8446e-02,  1.5830e-02,\n",
      "         -2.0966e-02,  2.7584e-02,  3.3991e-02, -7.4757e-03,  6.1471e-03,\n",
      "          2.2238e-02,  3.5530e-02,  3.1905e-02,  9.1167e-03,  9.6334e-03,\n",
      "          7.2126e-04,  8.9517e-03,  2.2671e-02,  3.0347e-02,  5.4593e-03,\n",
      "          2.8788e-02, -1.0092e-02,  1.2006e-03, -6.0981e-03,  6.9086e-03,\n",
      "         -3.4890e-02, -3.4913e-03,  2.0614e-02, -1.5047e-02,  9.5530e-03,\n",
      "         -1.9073e-02,  2.0238e-02,  1.1275e-02,  1.9148e-02, -1.5978e-02,\n",
      "          2.4104e-03, -3.8577e-03, -2.4819e-03,  1.4356e-02,  1.8159e-02,\n",
      "         -8.0737e-03, -2.8760e-02, -1.5619e-02, -9.3034e-03, -3.5224e-02,\n",
      "         -3.4725e-02,  3.1690e-02,  1.7080e-02, -2.8246e-02,  1.3797e-03,\n",
      "          6.0617e-03, -2.2802e-02, -6.1357e-03,  1.3221e-02,  1.1234e-02,\n",
      "          2.0008e-02,  3.5255e-02, -2.8413e-02,  2.9259e-02,  1.0686e-02,\n",
      "          1.2301e-03,  8.3278e-03, -1.8907e-03, -3.2587e-02,  3.4821e-02,\n",
      "         -6.2484e-03, -3.0252e-02,  2.3306e-02,  2.7495e-02, -2.6750e-02,\n",
      "          1.8101e-02,  6.8578e-03, -2.9384e-02, -3.4630e-02,  3.0421e-02,\n",
      "         -2.0524e-02, -3.1729e-02,  2.6521e-02, -2.5859e-02, -5.2488e-03,\n",
      "         -2.1995e-02,  1.2501e-02, -3.2260e-02,  2.4731e-02,  7.7690e-03,\n",
      "          1.4933e-02,  2.8464e-02, -1.6903e-02,  2.8688e-02, -3.1284e-02,\n",
      "         -3.1901e-02,  1.0594e-02, -3.0341e-02, -1.0227e-02,  2.2440e-02,\n",
      "         -3.5534e-02,  8.3191e-03, -2.4164e-02,  9.4076e-03, -2.6897e-02,\n",
      "          1.4036e-02, -2.4406e-02, -1.5570e-02, -9.8836e-04,  3.2618e-02,\n",
      "          1.3486e-02,  3.4330e-02, -3.3970e-02,  1.0303e-02, -2.7329e-02,\n",
      "          9.0710e-03,  1.5676e-02, -1.9319e-02, -7.9739e-03,  1.0693e-02,\n",
      "          2.9540e-02, -1.5138e-02,  1.8861e-02,  2.9453e-02,  2.6945e-02,\n",
      "         -2.1696e-05,  3.4191e-02, -6.9997e-03,  8.3939e-04,  3.2826e-02,\n",
      "         -2.6991e-03,  8.2823e-03,  1.1811e-02,  4.1020e-03, -2.6516e-02,\n",
      "         -1.0194e-02,  2.0914e-02, -9.3665e-04,  1.5593e-02,  5.8512e-03,\n",
      "          2.0013e-02,  1.0479e-02, -1.9548e-02,  2.2409e-02,  2.7531e-02,\n",
      "         -1.7252e-02, -2.5261e-02, -1.2959e-02,  2.1222e-02,  2.6344e-02,\n",
      "          1.1791e-02,  1.2337e-02, -2.6120e-02,  1.7881e-02,  1.9796e-02,\n",
      "         -2.6957e-02,  3.5475e-02,  2.5840e-02,  2.8463e-02, -3.3721e-02,\n",
      "          6.5213e-03,  1.1586e-02, -2.1082e-02,  2.9766e-03, -1.1460e-02,\n",
      "          3.9753e-03, -8.1022e-03, -2.7347e-02,  3.5461e-02,  3.0095e-04,\n",
      "         -1.5347e-02,  3.0288e-02, -8.0420e-03,  2.3427e-02,  8.3489e-03,\n",
      "          3.5136e-02, -3.8353e-03,  2.0948e-02,  3.1229e-02, -2.3122e-02,\n",
      "          9.5614e-03, -2.8709e-02, -1.5859e-02, -2.8641e-03,  3.3041e-02,\n",
      "         -1.2338e-02,  3.2802e-02, -2.1005e-02, -5.1573e-03,  3.5019e-02,\n",
      "          3.3590e-02, -1.4205e-02, -2.8693e-02, -1.1776e-02, -5.1986e-03,\n",
      "          2.2166e-02, -2.2236e-02,  1.5869e-02,  9.2091e-03,  1.1045e-02,\n",
      "          1.8736e-02, -1.9247e-02, -2.2245e-03, -1.5660e-02,  3.9587e-03,\n",
      "          2.9855e-02,  3.3894e-03, -2.6188e-02, -2.2236e-02,  2.8564e-02,\n",
      "          3.1152e-02,  3.2703e-02, -2.6171e-02,  2.1083e-02, -1.7836e-03,\n",
      "          2.9215e-02,  1.5337e-02, -1.9589e-02,  3.1292e-02, -3.3320e-02,\n",
      "         -2.2120e-02, -2.8616e-02,  1.5286e-02,  3.1679e-03,  2.2146e-02,\n",
      "          1.8718e-02,  1.2141e-02,  3.0450e-02, -3.1279e-02,  2.4791e-02,\n",
      "         -3.0768e-02, -3.0301e-02, -2.5160e-02, -2.0624e-02, -3.1646e-02,\n",
      "          2.6721e-02,  3.4290e-02, -2.4800e-02, -1.8091e-02,  5.6677e-03,\n",
      "          4.4817e-03,  3.1900e-02, -3.3098e-02,  4.4181e-03, -2.6249e-02,\n",
      "         -2.9366e-02, -1.9141e-02,  2.1183e-02, -5.7532e-03,  1.7582e-02,\n",
      "          3.0787e-02, -7.1310e-03,  4.6169e-03, -3.1455e-02,  1.4140e-02,\n",
      "         -2.9920e-02, -8.4787e-03, -1.9141e-03,  2.3751e-02, -1.9080e-02,\n",
      "          1.2105e-02, -2.9987e-02, -1.8402e-03,  2.5738e-02, -1.1511e-02,\n",
      "          2.3319e-02,  3.1369e-02,  1.9841e-02, -1.4029e-02,  2.5445e-02,\n",
      "         -2.9112e-02, -1.3779e-03,  3.4989e-02,  9.6590e-03, -2.5828e-03,\n",
      "         -2.8187e-02,  3.1046e-02,  1.3330e-02, -1.9922e-02, -3.0121e-02,\n",
      "          1.1780e-02, -3.0195e-02,  3.1578e-02, -1.8098e-02, -2.3800e-02,\n",
      "         -4.8024e-03, -3.0291e-02, -8.0636e-03,  2.1297e-02,  5.8936e-03,\n",
      "          3.3234e-02, -4.7591e-03,  2.2984e-02, -2.9263e-02,  1.6353e-02,\n",
      "          5.3494e-03,  1.0600e-02,  3.1650e-02, -1.1248e-02,  3.5706e-02,\n",
      "          2.8010e-02,  1.5540e-02, -1.2424e-02,  3.7474e-03, -3.3023e-02,\n",
      "         -2.0904e-02,  6.6569e-03,  7.3760e-03, -2.6629e-02,  3.2805e-02,\n",
      "         -2.4004e-02, -1.6707e-02, -1.7885e-02, -9.7828e-03,  2.1349e-02,\n",
      "         -1.9985e-02, -1.8081e-02, -1.4449e-02,  3.8800e-03,  1.8119e-03,\n",
      "          3.3133e-02,  1.3279e-02,  1.2184e-02,  2.2692e-02, -1.1910e-02,\n",
      "         -2.3901e-02,  5.5438e-03, -2.1553e-02,  7.3670e-03,  3.1931e-02,\n",
      "          2.7718e-02, -7.4528e-03, -3.2851e-02, -3.4924e-02,  3.2357e-02,\n",
      "         -3.2979e-02, -2.5243e-02, -2.2510e-02, -2.0356e-02, -3.1963e-02,\n",
      "          2.3485e-02,  4.8414e-03,  1.3688e-02,  3.1129e-02,  8.0068e-03,\n",
      "         -3.5194e-02,  7.7200e-03, -1.4265e-02,  6.4524e-03, -1.7628e-02,\n",
      "         -3.0812e-02, -1.7371e-02,  3.0352e-02,  6.5290e-03,  5.7310e-03,\n",
      "          2.0634e-02,  1.4482e-02, -2.8006e-02, -9.4926e-03,  1.3085e-02,\n",
      "          2.2893e-02,  9.1782e-03,  3.3534e-02, -3.7770e-03, -2.9091e-02,\n",
      "         -5.2757e-03,  2.7353e-02, -2.8862e-02,  3.2245e-02,  2.5743e-02,\n",
      "         -3.0774e-02,  1.1212e-02,  1.9640e-02, -2.1420e-02,  1.7778e-02,\n",
      "          2.8313e-02,  7.5894e-03, -1.6471e-02,  2.7124e-02,  1.4887e-02,\n",
      "          4.7614e-03,  1.7632e-02,  1.8428e-02, -3.1825e-02,  1.3356e-02,\n",
      "          3.3137e-02,  2.1959e-04,  4.7444e-03,  1.5995e-02,  9.2793e-03,\n",
      "         -1.4854e-02,  1.3514e-02, -2.7383e-02, -1.3111e-03, -3.5064e-02,\n",
      "         -2.8412e-02,  2.5612e-02, -2.6444e-02,  1.9543e-02, -9.1448e-03,\n",
      "          3.1704e-02,  3.3004e-02, -2.0869e-02, -2.4415e-02, -1.1790e-02,\n",
      "         -4.1009e-03, -2.1972e-02,  3.8408e-03,  2.8152e-02,  2.8224e-02,\n",
      "         -2.6418e-02,  2.9873e-02,  2.1000e-02,  2.8736e-02, -1.9904e-02,\n",
      "         -2.7171e-02,  4.2723e-03,  1.2519e-03, -1.8198e-02, -2.7695e-02,\n",
      "          6.2216e-03, -2.0186e-02, -2.5838e-02, -6.4819e-03,  2.9056e-02,\n",
      "         -1.4558e-03,  5.1788e-03,  1.3220e-02,  6.4824e-03,  3.3577e-02,\n",
      "         -4.0111e-03, -3.4941e-02,  4.4151e-03, -4.1832e-03, -3.5678e-02,\n",
      "          1.3872e-02,  1.3779e-02,  2.7030e-02, -2.4593e-02, -4.8473e-03,\n",
      "         -9.0243e-03,  1.0156e-02,  1.1168e-02, -3.1132e-02,  9.6812e-03,\n",
      "         -6.5739e-03, -2.5018e-02,  1.4971e-03,  2.4286e-02,  2.7712e-03,\n",
      "          3.0848e-02,  1.4548e-02, -3.4945e-03, -1.9403e-02, -1.5848e-02,\n",
      "         -6.3892e-05, -3.2104e-02, -1.2164e-02, -1.5842e-02,  1.6327e-02,\n",
      "          2.5337e-03, -5.5365e-03, -3.8226e-03,  4.8621e-03, -3.5501e-02,\n",
      "         -2.4751e-02, -6.0740e-03, -2.9215e-02, -2.5185e-03]])), ('activation_stack.0.bias', tensor([-0.0018]))])\n",
      "tensor(0.9844, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/2500], Loss: 17.23443794, Culminative Send Cost: 3920\n",
      "tensor(2.5499, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/2500], Loss: 7.12728786, Culminative Send Cost: 39200\n",
      "tensor(2.6807, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/2500], Loss: 6.07997417, Culminative Send Cost: 78400\n",
      "tensor(2.7519, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/2500], Loss: 5.54117680, Culminative Send Cost: 117600\n",
      "tensor(2.8039, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/2500], Loss: 5.24097347, Culminative Send Cost: 156800\n",
      "tensor(2.8410, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/2500], Loss: 5.05741453, Culminative Send Cost: 196000\n",
      "tensor(2.8667, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/2500], Loss: 4.93343687, Culminative Send Cost: 235200\n",
      "tensor(2.8838, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/2500], Loss: 4.84177399, Culminative Send Cost: 274400\n",
      "tensor(2.8944, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/2500], Loss: 4.76901007, Culminative Send Cost: 313600\n",
      "tensor(2.9002, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/2500], Loss: 4.70826864, Culminative Send Cost: 352800\n",
      "tensor(2.9026, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/2500], Loss: 4.65582800, Culminative Send Cost: 392000\n",
      "tensor(2.9025, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/2500], Loss: 4.60953140, Culminative Send Cost: 431200\n",
      "tensor(2.9007, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/2500], Loss: 4.56803083, Culminative Send Cost: 470400\n",
      "tensor(2.8978, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/2500], Loss: 4.53042078, Culminative Send Cost: 509600\n",
      "tensor(2.8941, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/2500], Loss: 4.49604797, Culminative Send Cost: 548800\n",
      "tensor(2.8899, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/2500], Loss: 4.46441936, Culminative Send Cost: 588000\n",
      "tensor(2.8855, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/2500], Loss: 4.43514585, Culminative Send Cost: 627200\n",
      "tensor(2.8811, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/2500], Loss: 4.40791464, Culminative Send Cost: 666400\n",
      "tensor(2.8767, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/2500], Loss: 4.38246584, Culminative Send Cost: 705600\n",
      "tensor(2.8724, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/2500], Loss: 4.35858774, Culminative Send Cost: 744800\n",
      "tensor(2.8683, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/2500], Loss: 4.33609533, Culminative Send Cost: 784000\n",
      "tensor(2.8644, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/2500], Loss: 4.31483650, Culminative Send Cost: 823200\n",
      "tensor(2.8607, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/2500], Loss: 4.29468012, Culminative Send Cost: 862400\n",
      "tensor(2.8574, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/2500], Loss: 4.27551222, Culminative Send Cost: 901600\n",
      "tensor(2.8543, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/2500], Loss: 4.25723600, Culminative Send Cost: 940800\n",
      "tensor(2.8515, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/2500], Loss: 4.23976707, Culminative Send Cost: 980000\n",
      "tensor(2.8489, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/2500], Loss: 4.22303200, Culminative Send Cost: 1019200\n",
      "tensor(2.8467, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/2500], Loss: 4.20696640, Culminative Send Cost: 1058400\n",
      "tensor(2.8447, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/2500], Loss: 4.19151449, Culminative Send Cost: 1097600\n",
      "tensor(2.8430, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/2500], Loss: 4.17662621, Culminative Send Cost: 1136800\n",
      "tensor(2.8416, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/2500], Loss: 4.16226006, Culminative Send Cost: 1176000\n",
      "tensor(2.8404, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/2500], Loss: 4.14837456, Culminative Send Cost: 1215200\n",
      "tensor(2.8395, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/2500], Loss: 4.13493729, Culminative Send Cost: 1254400\n",
      "tensor(2.8389, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/2500], Loss: 4.12191677, Culminative Send Cost: 1293600\n",
      "tensor(2.8384, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/2500], Loss: 4.10928631, Culminative Send Cost: 1332800\n",
      "tensor(2.8382, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/2500], Loss: 4.09702063, Culminative Send Cost: 1372000\n",
      "tensor(2.8383, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/2500], Loss: 4.08509827, Culminative Send Cost: 1411200\n",
      "tensor(2.8385, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/2500], Loss: 4.07349873, Culminative Send Cost: 1450400\n",
      "tensor(2.8390, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/2500], Loss: 4.06220484, Culminative Send Cost: 1489600\n",
      "tensor(2.8396, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/2500], Loss: 4.05119944, Culminative Send Cost: 1528800\n",
      "tensor(2.8405, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/2500], Loss: 4.04046822, Culminative Send Cost: 1568000\n",
      "tensor(2.8415, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/2500], Loss: 4.02999735, Culminative Send Cost: 1607200\n",
      "tensor(2.8427, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/2500], Loss: 4.01977396, Culminative Send Cost: 1646400\n",
      "tensor(2.8441, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/2500], Loss: 4.00978708, Culminative Send Cost: 1685600\n",
      "tensor(2.8456, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/2500], Loss: 4.00002527, Culminative Send Cost: 1724800\n",
      "tensor(2.8473, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/2500], Loss: 3.99048018, Culminative Send Cost: 1764000\n",
      "tensor(2.8492, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/2500], Loss: 3.98114228, Culminative Send Cost: 1803200\n",
      "tensor(2.8512, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/2500], Loss: 3.97200227, Culminative Send Cost: 1842400\n",
      "tensor(2.8533, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/2500], Loss: 3.96305370, Culminative Send Cost: 1881600\n",
      "tensor(2.8556, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/2500], Loss: 3.95428872, Culminative Send Cost: 1920800\n",
      "tensor(2.8580, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/2500], Loss: 3.94570136, Culminative Send Cost: 1960000\n",
      "tensor(2.8605, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/2500], Loss: 3.93728375, Culminative Send Cost: 1999200\n",
      "tensor(2.8631, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/2500], Loss: 3.92903137, Culminative Send Cost: 2038400\n",
      "tensor(2.8659, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/2500], Loss: 3.92093801, Culminative Send Cost: 2077600\n",
      "tensor(2.8687, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/2500], Loss: 3.91299891, Culminative Send Cost: 2116800\n",
      "tensor(2.8717, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/2500], Loss: 3.90520906, Culminative Send Cost: 2156000\n",
      "tensor(2.8747, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/2500], Loss: 3.89756370, Culminative Send Cost: 2195200\n",
      "tensor(2.8779, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/2500], Loss: 3.89005756, Culminative Send Cost: 2234400\n",
      "tensor(2.8811, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/2500], Loss: 3.88268757, Culminative Send Cost: 2273600\n",
      "tensor(2.8844, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/2500], Loss: 3.87544990, Culminative Send Cost: 2312800\n",
      "tensor(2.8878, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/2500], Loss: 3.86833978, Culminative Send Cost: 2352000\n",
      "tensor(2.8912, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/2500], Loss: 3.86135483, Culminative Send Cost: 2391200\n",
      "tensor(2.8948, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/2500], Loss: 3.85449100, Culminative Send Cost: 2430400\n",
      "tensor(2.8984, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/2500], Loss: 3.84774494, Culminative Send Cost: 2469600\n",
      "tensor(2.9020, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/2500], Loss: 3.84111381, Culminative Send Cost: 2508800\n",
      "tensor(2.9057, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/2500], Loss: 3.83459449, Culminative Send Cost: 2548000\n",
      "tensor(2.9095, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/2500], Loss: 3.82818389, Culminative Send Cost: 2587200\n",
      "tensor(2.9133, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/2500], Loss: 3.82187986, Culminative Send Cost: 2626400\n",
      "tensor(2.9172, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/2500], Loss: 3.81567931, Culminative Send Cost: 2665600\n",
      "tensor(2.9212, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/2500], Loss: 3.80958033, Culminative Send Cost: 2704800\n",
      "tensor(2.9251, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/2500], Loss: 3.80357957, Culminative Send Cost: 2744000\n",
      "tensor(2.9292, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/2500], Loss: 3.79767609, Culminative Send Cost: 2783200\n",
      "tensor(2.9332, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/2500], Loss: 3.79186606, Culminative Send Cost: 2822400\n",
      "tensor(2.9373, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/2500], Loss: 3.78614855, Culminative Send Cost: 2861600\n",
      "tensor(2.9414, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/2500], Loss: 3.78052092, Culminative Send Cost: 2900800\n",
      "tensor(2.9456, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/2500], Loss: 3.77498055, Culminative Send Cost: 2940000\n",
      "tensor(2.9498, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/2500], Loss: 3.76952672, Culminative Send Cost: 2979200\n",
      "tensor(2.9540, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/2500], Loss: 3.76415730, Culminative Send Cost: 3018400\n",
      "tensor(2.9583, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/2500], Loss: 3.75886989, Culminative Send Cost: 3057600\n",
      "tensor(2.9625, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/2500], Loss: 3.75366354, Culminative Send Cost: 3096800\n",
      "tensor(2.9668, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/2500], Loss: 3.74853539, Culminative Send Cost: 3136000\n",
      "tensor(2.9711, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/2500], Loss: 3.74348497, Culminative Send Cost: 3175200\n",
      "tensor(2.9755, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/2500], Loss: 3.73851013, Culminative Send Cost: 3214400\n",
      "tensor(2.9798, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/2500], Loss: 3.73360896, Culminative Send Cost: 3253600\n",
      "tensor(2.9842, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/2500], Loss: 3.72878075, Culminative Send Cost: 3292800\n",
      "tensor(2.9886, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/2500], Loss: 3.72402382, Culminative Send Cost: 3332000\n",
      "tensor(2.9930, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/2500], Loss: 3.71933651, Culminative Send Cost: 3371200\n",
      "tensor(2.9974, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/2500], Loss: 3.71471763, Culminative Send Cost: 3410400\n",
      "tensor(3.0018, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/2500], Loss: 3.71016574, Culminative Send Cost: 3449600\n",
      "tensor(3.0062, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/2500], Loss: 3.70567942, Culminative Send Cost: 3488800\n",
      "tensor(3.0106, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/2500], Loss: 3.70125818, Culminative Send Cost: 3528000\n",
      "tensor(3.0151, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/2500], Loss: 3.69689989, Culminative Send Cost: 3567200\n",
      "tensor(3.0195, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/2500], Loss: 3.69260359, Culminative Send Cost: 3606400\n",
      "tensor(3.0239, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/2500], Loss: 3.68836784, Culminative Send Cost: 3645600\n",
      "tensor(3.0284, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/2500], Loss: 3.68419194, Culminative Send Cost: 3684800\n",
      "tensor(3.0328, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/2500], Loss: 3.68007469, Culminative Send Cost: 3724000\n",
      "tensor(3.0373, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/2500], Loss: 3.67601562, Culminative Send Cost: 3763200\n",
      "tensor(3.0417, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/2500], Loss: 3.67201209, Culminative Send Cost: 3802400\n",
      "tensor(3.0462, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/2500], Loss: 3.66806459, Culminative Send Cost: 3841600\n",
      "tensor(3.0506, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/2500], Loss: 3.66417170, Culminative Send Cost: 3880800\n",
      "tensor(3.0550, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/2500], Loss: 3.66033220, Culminative Send Cost: 3920000\n",
      "tensor(3.0595, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/2500], Loss: 3.65654492, Culminative Send Cost: 3959200\n",
      "tensor(3.0639, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/2500], Loss: 3.65280986, Culminative Send Cost: 3998400\n",
      "tensor(3.0683, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/2500], Loss: 3.64912486, Culminative Send Cost: 4037600\n",
      "tensor(3.0728, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/2500], Loss: 3.64549041, Culminative Send Cost: 4076800\n",
      "tensor(3.0772, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/2500], Loss: 3.64190412, Culminative Send Cost: 4116000\n",
      "tensor(3.0816, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/2500], Loss: 3.63836622, Culminative Send Cost: 4155200\n",
      "tensor(3.0860, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/2500], Loss: 3.63487577, Culminative Send Cost: 4194400\n",
      "tensor(3.0904, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/2500], Loss: 3.63143134, Culminative Send Cost: 4233600\n",
      "tensor(3.0948, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/2500], Loss: 3.62803316, Culminative Send Cost: 4272800\n",
      "tensor(3.0991, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/2500], Loss: 3.62467957, Culminative Send Cost: 4312000\n",
      "tensor(3.1035, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/2500], Loss: 3.62137032, Culminative Send Cost: 4351200\n",
      "tensor(3.1078, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/2500], Loss: 3.61810470, Culminative Send Cost: 4390400\n",
      "tensor(3.1122, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/2500], Loss: 3.61488104, Culminative Send Cost: 4429600\n",
      "tensor(3.1165, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/2500], Loss: 3.61169958, Culminative Send Cost: 4468800\n",
      "tensor(3.1208, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/2500], Loss: 3.60856009, Culminative Send Cost: 4508000\n",
      "tensor(3.1251, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/2500], Loss: 3.60546017, Culminative Send Cost: 4547200\n",
      "tensor(3.1294, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/2500], Loss: 3.60240054, Culminative Send Cost: 4586400\n",
      "tensor(3.1337, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/2500], Loss: 3.59938073, Culminative Send Cost: 4625600\n",
      "tensor(3.1380, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/2500], Loss: 3.59639883, Culminative Send Cost: 4664800\n",
      "tensor(3.1422, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/2500], Loss: 3.59345508, Culminative Send Cost: 4704000\n",
      "tensor(3.1464, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/2500], Loss: 3.59054875, Culminative Send Cost: 4743200\n",
      "tensor(3.1507, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/2500], Loss: 3.58767962, Culminative Send Cost: 4782400\n",
      "tensor(3.1549, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/2500], Loss: 3.58484626, Culminative Send Cost: 4821600\n",
      "tensor(3.1591, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/2500], Loss: 3.58204865, Culminative Send Cost: 4860800\n",
      "tensor(3.1632, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/2500], Loss: 3.57928610, Culminative Send Cost: 4900000\n",
      "tensor(3.1674, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/2500], Loss: 3.57655787, Culminative Send Cost: 4939200\n",
      "tensor(3.1715, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/2500], Loss: 3.57386422, Culminative Send Cost: 4978400\n",
      "tensor(3.1757, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/2500], Loss: 3.57120323, Culminative Send Cost: 5017600\n",
      "tensor(3.1798, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/2500], Loss: 3.56857562, Culminative Send Cost: 5056800\n",
      "tensor(3.1839, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/2500], Loss: 3.56598043, Culminative Send Cost: 5096000\n",
      "tensor(3.1880, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/2500], Loss: 3.56341743, Culminative Send Cost: 5135200\n",
      "tensor(3.1920, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/2500], Loss: 3.56088495, Culminative Send Cost: 5174400\n",
      "tensor(3.1961, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/2500], Loss: 3.55838394, Culminative Send Cost: 5213600\n",
      "tensor(3.2001, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/2500], Loss: 3.55591369, Culminative Send Cost: 5252800\n",
      "tensor(3.2041, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/2500], Loss: 3.55347323, Culminative Send Cost: 5292000\n",
      "tensor(3.2081, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/2500], Loss: 3.55106258, Culminative Send Cost: 5331200\n",
      "tensor(3.2121, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/2500], Loss: 3.54868078, Culminative Send Cost: 5370400\n",
      "tensor(3.2160, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/2500], Loss: 3.54632711, Culminative Send Cost: 5409600\n",
      "tensor(3.2200, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/2500], Loss: 3.54400229, Culminative Send Cost: 5448800\n",
      "tensor(3.2239, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/2500], Loss: 3.54170537, Culminative Send Cost: 5488000\n",
      "tensor(3.2278, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/2500], Loss: 3.53943539, Culminative Send Cost: 5527200\n",
      "tensor(3.2317, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/2500], Loss: 3.53719211, Culminative Send Cost: 5566400\n",
      "tensor(3.2355, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/2500], Loss: 3.53497601, Culminative Send Cost: 5605600\n",
      "tensor(3.2394, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/2500], Loss: 3.53278589, Culminative Send Cost: 5644800\n",
      "tensor(3.2432, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/2500], Loss: 3.53062248, Culminative Send Cost: 5684000\n",
      "tensor(3.2470, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/2500], Loss: 3.52848291, Culminative Send Cost: 5723200\n",
      "tensor(3.2508, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/2500], Loss: 3.52636933, Culminative Send Cost: 5762400\n",
      "tensor(3.2546, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/2500], Loss: 3.52428007, Culminative Send Cost: 5801600\n",
      "tensor(3.2583, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/2500], Loss: 3.52221537, Culminative Send Cost: 5840800\n",
      "tensor(3.2620, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/2500], Loss: 3.52017498, Culminative Send Cost: 5880000\n",
      "tensor(3.2658, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/2500], Loss: 3.51815796, Culminative Send Cost: 5919200\n",
      "tensor(3.2694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/2500], Loss: 3.51616383, Culminative Send Cost: 5958400\n",
      "tensor(3.2731, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/2500], Loss: 3.51419306, Culminative Send Cost: 5997600\n",
      "tensor(3.2768, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/2500], Loss: 3.51224470, Culminative Send Cost: 6036800\n",
      "tensor(3.2804, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/2500], Loss: 3.51031852, Culminative Send Cost: 6076000\n",
      "tensor(3.2840, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/2500], Loss: 3.50841498, Culminative Send Cost: 6115200\n",
      "tensor(3.2876, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/2500], Loss: 3.50653267, Culminative Send Cost: 6154400\n",
      "tensor(3.2912, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/2500], Loss: 3.50467157, Culminative Send Cost: 6193600\n",
      "tensor(3.2948, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/2500], Loss: 3.50283217, Culminative Send Cost: 6232800\n",
      "tensor(3.2983, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/2500], Loss: 3.50101304, Culminative Send Cost: 6272000\n",
      "tensor(3.3018, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/2500], Loss: 3.49921417, Culminative Send Cost: 6311200\n",
      "tensor(3.3053, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/2500], Loss: 3.49743605, Culminative Send Cost: 6350400\n",
      "tensor(3.3088, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/2500], Loss: 3.49567771, Culminative Send Cost: 6389600\n",
      "tensor(3.3123, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/2500], Loss: 3.49393892, Culminative Send Cost: 6428800\n",
      "tensor(3.3157, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/2500], Loss: 3.49221945, Culminative Send Cost: 6468000\n",
      "tensor(3.3191, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/2500], Loss: 3.49051929, Culminative Send Cost: 6507200\n",
      "tensor(3.3225, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/2500], Loss: 3.48883772, Culminative Send Cost: 6546400\n",
      "tensor(3.3259, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/2500], Loss: 3.48717475, Culminative Send Cost: 6585600\n",
      "tensor(3.3293, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/2500], Loss: 3.48553038, Culminative Send Cost: 6624800\n",
      "tensor(3.3326, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/2500], Loss: 3.48390341, Culminative Send Cost: 6664000\n",
      "tensor(3.3359, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/2500], Loss: 3.48229480, Culminative Send Cost: 6703200\n",
      "tensor(3.3392, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/2500], Loss: 3.48070359, Culminative Send Cost: 6742400\n",
      "tensor(3.3425, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/2500], Loss: 3.47912979, Culminative Send Cost: 6781600\n",
      "tensor(3.3458, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/2500], Loss: 3.47757339, Culminative Send Cost: 6820800\n",
      "tensor(3.3490, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/2500], Loss: 3.47603369, Culminative Send Cost: 6860000\n",
      "tensor(3.3523, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/2500], Loss: 3.47451019, Culminative Send Cost: 6899200\n",
      "tensor(3.3555, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/2500], Loss: 3.47300339, Culminative Send Cost: 6938400\n",
      "tensor(3.3587, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/2500], Loss: 3.47151303, Culminative Send Cost: 6977600\n",
      "tensor(3.3618, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/2500], Loss: 3.47003841, Culminative Send Cost: 7016800\n",
      "tensor(3.3650, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/2500], Loss: 3.46857953, Culminative Send Cost: 7056000\n",
      "tensor(3.3681, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/2500], Loss: 3.46713662, Culminative Send Cost: 7095200\n",
      "tensor(3.3712, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/2500], Loss: 3.46570897, Culminative Send Cost: 7134400\n",
      "tensor(3.3743, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/2500], Loss: 3.46429610, Culminative Send Cost: 7173600\n",
      "tensor(3.3774, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/2500], Loss: 3.46289825, Culminative Send Cost: 7212800\n",
      "tensor(3.3804, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/2500], Loss: 3.46151543, Culminative Send Cost: 7252000\n",
      "tensor(3.3835, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/2500], Loss: 3.46014690, Culminative Send Cost: 7291200\n",
      "tensor(3.3865, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/2500], Loss: 3.45879269, Culminative Send Cost: 7330400\n",
      "tensor(3.3895, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/2500], Loss: 3.45745325, Culminative Send Cost: 7369600\n",
      "tensor(3.3925, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/2500], Loss: 3.45612741, Culminative Send Cost: 7408800\n",
      "tensor(3.3954, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/2500], Loss: 3.45481515, Culminative Send Cost: 7448000\n",
      "tensor(3.3984, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/2500], Loss: 3.45351672, Culminative Send Cost: 7487200\n",
      "tensor(3.4013, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/2500], Loss: 3.45223141, Culminative Send Cost: 7526400\n",
      "tensor(3.4042, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/2500], Loss: 3.45095992, Culminative Send Cost: 7565600\n",
      "tensor(3.4071, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/2500], Loss: 3.44970131, Culminative Send Cost: 7604800\n",
      "tensor(3.4100, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/2500], Loss: 3.44845605, Culminative Send Cost: 7644000\n",
      "tensor(3.4128, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/2500], Loss: 3.44722295, Culminative Send Cost: 7683200\n",
      "tensor(3.4156, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/2500], Loss: 3.44600248, Culminative Send Cost: 7722400\n",
      "tensor(3.4184, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/2500], Loss: 3.44479513, Culminative Send Cost: 7761600\n",
      "tensor(3.4212, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/2500], Loss: 3.44359970, Culminative Send Cost: 7800800\n",
      "tensor(3.4240, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/2500], Loss: 3.44241619, Culminative Send Cost: 7840000\n",
      "tensor(3.4268, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/2500], Loss: 3.44124484, Culminative Send Cost: 7879200\n",
      "tensor(3.4295, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/2500], Loss: 3.44008565, Culminative Send Cost: 7918400\n",
      "tensor(3.4322, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/2500], Loss: 3.43893743, Culminative Send Cost: 7957600\n",
      "tensor(3.4350, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/2500], Loss: 3.43780136, Culminative Send Cost: 7996800\n",
      "tensor(3.4376, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/2500], Loss: 3.43667674, Culminative Send Cost: 8036000\n",
      "tensor(3.4403, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/2500], Loss: 3.43556285, Culminative Send Cost: 8075200\n",
      "tensor(3.4430, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/2500], Loss: 3.43446064, Culminative Send Cost: 8114400\n",
      "tensor(3.4456, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/2500], Loss: 3.43336916, Culminative Send Cost: 8153600\n",
      "tensor(3.4482, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/2500], Loss: 3.43228841, Culminative Send Cost: 8192800\n",
      "tensor(3.4508, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/2500], Loss: 3.43121839, Culminative Send Cost: 8232000\n",
      "tensor(3.4534, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/2500], Loss: 3.43015862, Culminative Send Cost: 8271200\n",
      "tensor(3.4560, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/2500], Loss: 3.42910981, Culminative Send Cost: 8310400\n",
      "tensor(3.4585, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/2500], Loss: 3.42807150, Culminative Send Cost: 8349600\n",
      "tensor(3.4610, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/2500], Loss: 3.42704272, Culminative Send Cost: 8388800\n",
      "tensor(3.4636, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/2500], Loss: 3.42602396, Culminative Send Cost: 8428000\n",
      "tensor(3.4660, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/2500], Loss: 3.42501545, Culminative Send Cost: 8467200\n",
      "tensor(3.4685, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/2500], Loss: 3.42401671, Culminative Send Cost: 8506400\n",
      "tensor(3.4710, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/2500], Loss: 3.42302775, Culminative Send Cost: 8545600\n",
      "tensor(3.4734, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/2500], Loss: 3.42204809, Culminative Send Cost: 8584800\n",
      "tensor(3.4759, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/2500], Loss: 3.42107844, Culminative Send Cost: 8624000\n",
      "tensor(3.4783, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/2500], Loss: 3.42011738, Culminative Send Cost: 8663200\n",
      "tensor(3.4807, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/2500], Loss: 3.41916561, Culminative Send Cost: 8702400\n",
      "tensor(3.4831, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/2500], Loss: 3.41822362, Culminative Send Cost: 8741600\n",
      "tensor(3.4854, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/2500], Loss: 3.41729069, Culminative Send Cost: 8780800\n",
      "tensor(3.4878, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/2500], Loss: 3.41636586, Culminative Send Cost: 8820000\n",
      "tensor(3.4901, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/2500], Loss: 3.41545010, Culminative Send Cost: 8859200\n",
      "tensor(3.4924, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/2500], Loss: 3.41454315, Culminative Send Cost: 8898400\n",
      "tensor(3.4947, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/2500], Loss: 3.41364479, Culminative Send Cost: 8937600\n",
      "tensor(3.4970, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/2500], Loss: 3.41275477, Culminative Send Cost: 8976800\n",
      "tensor(3.4993, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/2500], Loss: 3.41187310, Culminative Send Cost: 9016000\n",
      "tensor(3.5015, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/2500], Loss: 3.41099954, Culminative Send Cost: 9055200\n",
      "tensor(3.5038, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/2500], Loss: 3.41013455, Culminative Send Cost: 9094400\n",
      "tensor(3.5060, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/2500], Loss: 3.40927744, Culminative Send Cost: 9133600\n",
      "tensor(3.5082, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/2500], Loss: 3.40842843, Culminative Send Cost: 9172800\n",
      "tensor(3.5104, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/2500], Loss: 3.40758729, Culminative Send Cost: 9212000\n",
      "tensor(3.5125, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/2500], Loss: 3.40675354, Culminative Send Cost: 9251200\n",
      "tensor(3.5147, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/2500], Loss: 3.40592766, Culminative Send Cost: 9290400\n",
      "tensor(3.5168, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/2500], Loss: 3.40511012, Culminative Send Cost: 9329600\n",
      "tensor(3.5190, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/2500], Loss: 3.40429950, Culminative Send Cost: 9368800\n",
      "tensor(3.5211, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/2500], Loss: 3.40349603, Culminative Send Cost: 9408000\n",
      "tensor(3.5232, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/2500], Loss: 3.40270042, Culminative Send Cost: 9447200\n",
      "tensor(3.5253, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/2500], Loss: 3.40191197, Culminative Send Cost: 9486400\n",
      "tensor(3.5273, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/2500], Loss: 3.40113020, Culminative Send Cost: 9525600\n",
      "tensor(3.5294, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/2500], Loss: 3.40035605, Culminative Send Cost: 9564800\n",
      "tensor(3.5314, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/2500], Loss: 3.39958906, Culminative Send Cost: 9604000\n",
      "tensor(3.5334, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/2500], Loss: 3.39882803, Culminative Send Cost: 9643200\n",
      "tensor(3.5354, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/2500], Loss: 3.39807463, Culminative Send Cost: 9682400\n",
      "tensor(3.5374, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/2500], Loss: 3.39732790, Culminative Send Cost: 9721600\n",
      "tensor(3.5394, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/2500], Loss: 3.39658761, Culminative Send Cost: 9760800\n",
      "tensor(3.5414, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/2500], Loss: 3.39585423, Culminative Send Cost: 9800000\n",
      "activation_stack.0.weight: tensor([[-2.1170e-02, -2.0452e-02, -1.8209e-02, -3.2509e-02, -2.3281e-02,\n",
      "         -2.8679e-02,  2.3779e-02, -8.0903e-03,  3.2026e-02,  1.9523e-02,\n",
      "         -1.3430e-02,  5.5745e-03,  2.7339e-02, -1.7674e-02, -2.9499e-02,\n",
      "         -2.6423e-02, -2.3444e-02, -2.9728e-02, -7.4760e-03,  1.1976e-02,\n",
      "         -1.1321e-02,  7.2756e-03,  2.8179e-02, -1.8882e-02,  1.2669e-03,\n",
      "         -3.2690e-02, -3.1121e-02,  8.6157e-03,  2.8996e-02,  1.4078e-03,\n",
      "         -3.0703e-03, -6.8697e-03, -2.7629e-02,  4.9006e-03,  2.8347e-02,\n",
      "          5.3154e-03, -2.9581e-03, -2.5329e-02, -2.1560e-02,  1.1211e-02,\n",
      "          3.2610e-02,  4.3996e-02,  2.4288e-02, -1.6584e-02, -6.5491e-03,\n",
      "         -1.1731e-02, -1.2086e-02,  1.1020e-02,  3.4412e-03,  9.6618e-03,\n",
      "         -2.3487e-02, -3.3502e-02, -5.4838e-03,  6.2393e-03,  2.1342e-02,\n",
      "          8.6847e-03, -3.4091e-02,  2.4208e-02, -5.3363e-03, -1.6491e-02,\n",
      "         -1.6374e-02,  4.1299e-03, -2.3436e-02, -1.1247e-02,  6.9944e-02,\n",
      "          6.7727e-02,  1.0252e-01,  1.1968e-01,  1.3297e-01,  1.6490e-01,\n",
      "          1.4422e-01,  1.5671e-01,  2.3010e-01,  1.5789e-01,  2.1198e-01,\n",
      "          1.3989e-01,  1.2371e-01,  1.0026e-01,  4.0562e-02,  3.1932e-02,\n",
      "         -1.0760e-02,  7.2044e-03, -3.1524e-02, -4.0986e-03,  2.5656e-02,\n",
      "         -1.9958e-02, -8.8328e-03,  3.0290e-02, -4.9528e-03,  1.6450e-02,\n",
      "         -4.0153e-03,  1.3033e-02,  7.6274e-02,  8.2549e-02,  1.2802e-01,\n",
      "          1.1050e-01,  6.0556e-02,  7.3826e-02,  1.5754e-01,  1.5235e-01,\n",
      "          2.1319e-01,  2.7804e-01,  3.0777e-01,  3.7162e-01,  3.0126e-01,\n",
      "          2.5016e-01,  1.5969e-01,  9.8133e-02,  5.6825e-02,  3.8735e-02,\n",
      "          3.3718e-02, -9.7568e-03, -7.8690e-03,  3.2404e-02, -1.2336e-02,\n",
      "         -2.1760e-02, -1.3737e-02,  1.8652e-02,  3.5470e-02,  7.5702e-02,\n",
      "          7.4642e-02,  1.0960e-01,  1.3341e-01,  8.6988e-02,  4.7843e-02,\n",
      "         -4.7140e-03,  6.7572e-02, -7.7995e-03,  3.5888e-02,  2.9720e-02,\n",
      "          6.0939e-02,  1.0636e-01,  1.4197e-01,  1.3339e-01,  1.3346e-01,\n",
      "          1.2958e-01,  5.9848e-02, -8.6276e-03, -5.4858e-03, -1.8032e-03,\n",
      "         -9.8173e-03, -2.4321e-02,  1.3082e-02, -3.1033e-02,  1.1350e-02,\n",
      "          1.0840e-03, -2.1708e-02, -1.8762e-02,  3.7268e-02,  7.3952e-02,\n",
      "          2.9533e-02,  2.1815e-02,  3.3808e-02, -3.5828e-02, -9.1439e-02,\n",
      "         -1.5341e-01, -7.3919e-02, -4.9689e-02,  1.8442e-02, -4.3791e-02,\n",
      "         -3.7599e-02,  3.1208e-02,  7.6876e-02,  7.9182e-02,  1.3405e-01,\n",
      "          5.8128e-02, -1.0875e-03, -8.8201e-03,  1.8844e-02,  6.9855e-04,\n",
      "          6.1739e-03, -2.6101e-02,  1.8530e-02, -1.2021e-02, -3.3443e-02,\n",
      "         -1.2974e-02, -2.3076e-02,  4.0457e-03,  4.5153e-03, -4.2494e-03,\n",
      "         -1.4205e-02, -1.9028e-02,  2.2559e-03,  8.6439e-02,  4.0746e-02,\n",
      "          8.0077e-02,  5.6326e-03, -4.0883e-02, -8.8155e-02, -5.8273e-02,\n",
      "         -3.7345e-02,  2.8832e-02,  8.5629e-02,  1.2573e-01,  1.6988e-02,\n",
      "          2.0241e-02, -2.7943e-02,  3.0866e-02,  2.8223e-02, -2.0777e-02,\n",
      "         -3.6374e-02, -8.5965e-03, -3.4801e-02, -1.4482e-02,  5.3031e-02,\n",
      "          2.1998e-02,  8.7603e-02,  4.9146e-02, -8.3567e-03,  8.9048e-03,\n",
      "          8.9732e-02,  1.6684e-01,  1.3971e-01, -4.6685e-02, -6.8681e-02,\n",
      "         -6.7604e-02, -1.2127e-02, -6.1288e-02, -6.1005e-02,  1.2275e-01,\n",
      "          2.2511e-01,  1.5210e-01,  2.3948e-02,  3.4899e-02,  2.7834e-02,\n",
      "          2.9183e-02,  5.3202e-02,  3.2885e-02, -8.8959e-03, -5.2311e-02,\n",
      "         -2.3134e-02,  5.3016e-02,  8.1565e-02,  2.9518e-02,  8.2833e-02,\n",
      "          1.1367e-01,  1.0921e-01,  5.4498e-02, -6.1294e-03,  2.6394e-02,\n",
      "          1.7787e-02, -1.2960e-01, -1.1976e-01, -8.4654e-02, -1.2912e-02,\n",
      "          3.9798e-03,  1.3536e-02,  1.4231e-01,  2.9488e-01,  1.6753e-01,\n",
      "          2.7567e-02,  2.4106e-02,  2.9422e-02,  1.1451e-02,  2.0278e-02,\n",
      "          3.8124e-02,  1.2332e-02,  1.7217e-02,  2.7088e-02,  7.1940e-02,\n",
      "          1.4889e-01,  1.0238e-01,  1.5672e-01,  2.1035e-01,  1.4871e-01,\n",
      "         -3.6681e-02, -1.1502e-01, -1.4529e-01, -7.5836e-02, -7.7756e-02,\n",
      "         -4.9249e-02, -3.7970e-02, -1.6445e-02,  2.0385e-03, -7.9049e-03,\n",
      "          1.2364e-01,  2.4994e-01,  1.4659e-01,  6.2120e-02,  2.1488e-02,\n",
      "         -1.8814e-02,  8.4504e-03,  5.1115e-02,  4.0470e-02,  3.6595e-02,\n",
      "          8.5680e-02,  1.1288e-01,  1.1275e-01,  8.6201e-02,  1.2788e-01,\n",
      "          1.8303e-01,  2.6266e-01,  1.6255e-01,  4.4337e-03, -2.1103e-01,\n",
      "         -2.3279e-01, -5.0956e-02, -2.4269e-02, -4.1180e-02, -2.5460e-03,\n",
      "         -7.1912e-02, -2.7471e-02, -2.0643e-02,  5.1814e-02,  1.1528e-01,\n",
      "          7.1571e-02,  5.7007e-02, -5.7964e-03, -1.4908e-03, -3.5010e-03,\n",
      "         -5.7300e-03,  6.0056e-02,  8.5310e-02,  1.3598e-01,  1.7377e-01,\n",
      "          1.7920e-01,  1.6164e-01,  1.6285e-01,  1.3264e-01,  1.8180e-01,\n",
      "          3.0872e-01,  2.3590e-01, -1.3456e-01, -1.8285e-01,  5.8923e-02,\n",
      "          5.7690e-02,  5.2554e-02,  1.0869e-02,  1.1632e-02, -1.4804e-02,\n",
      "         -1.0988e-02, -1.7860e-02,  5.8967e-02,  6.3827e-02,  2.1769e-02,\n",
      "          2.4197e-03,  8.9274e-03,  2.6829e-02,  4.4356e-02,  5.3955e-02,\n",
      "          1.7483e-01,  2.0773e-01,  1.7843e-01,  1.3725e-01,  8.7929e-02,\n",
      "          1.0028e-01,  3.5723e-02,  1.8249e-01,  3.6550e-01,  2.1253e-01,\n",
      "         -1.3499e-01,  6.0486e-02,  2.2555e-01,  2.4073e-01,  1.1031e-01,\n",
      "          1.0970e-01,  5.2919e-02,  3.8217e-02, -5.5347e-03, -5.9261e-02,\n",
      "         -4.3519e-02, -1.6855e-02, -1.2781e-02, -8.9913e-03, -3.5525e-02,\n",
      "         -3.2201e-02,  4.2284e-02,  7.4141e-02,  1.4049e-01,  1.4923e-01,\n",
      "          7.6200e-02,  3.4501e-02,  7.0230e-02,  3.4502e-02, -5.8433e-02,\n",
      "          1.0930e-01,  3.8381e-01,  1.0057e-02, -7.4934e-02,  1.8875e-01,\n",
      "          2.2768e-01,  2.4268e-01,  1.7017e-01,  1.3772e-01,  9.4785e-02,\n",
      "          1.5988e-02, -7.9990e-02, -8.4354e-02, -6.4506e-03, -1.7065e-02,\n",
      "          1.3561e-02,  5.2859e-03, -2.9207e-02, -3.3121e-02,  3.7149e-02,\n",
      "          3.1040e-02,  7.6496e-02,  1.7031e-02, -1.2370e-01, -6.6903e-02,\n",
      "         -5.3701e-02, -8.2090e-02, -1.8348e-01,  1.4389e-01,  2.7269e-01,\n",
      "         -6.2633e-02,  3.7428e-02,  3.5926e-02,  2.8903e-01,  2.1058e-01,\n",
      "          1.0832e-02,  4.8785e-02, -6.2819e-02, -8.4263e-02, -3.7010e-02,\n",
      "         -1.1867e-01, -2.0256e-02, -7.6045e-03,  4.2166e-04, -2.8897e-02,\n",
      "          1.4435e-02, -2.4237e-02, -1.0681e-02,  4.4821e-02,  6.5266e-02,\n",
      "         -1.2984e-01, -1.1682e-01, -1.5509e-01, -9.8942e-02, -1.1326e-01,\n",
      "         -6.6319e-02,  1.3881e-01,  1.4319e-01, -6.3833e-02,  2.4990e-03,\n",
      "          4.2792e-02,  2.9598e-01,  1.7691e-01, -5.4202e-02, -7.5274e-02,\n",
      "         -1.0417e-01, -3.0737e-02, -3.0534e-02, -5.6577e-02,  1.1687e-02,\n",
      "          6.5702e-03, -7.7430e-03,  9.9472e-03,  5.2727e-03, -2.7184e-02,\n",
      "         -6.9862e-03,  6.5941e-02, -1.6896e-03, -1.3382e-01, -1.0482e-01,\n",
      "         -7.1275e-02, -7.5323e-02, -7.9555e-02,  1.8996e-02,  2.0494e-01,\n",
      "          1.6003e-01, -9.0490e-02, -8.0950e-02,  1.1593e-01,  2.7743e-01,\n",
      "         -2.4439e-03, -1.5900e-01, -1.6500e-01, -8.3725e-02, -8.3420e-02,\n",
      "         -6.1221e-02, -1.4359e-02, -1.4281e-02, -5.8466e-03, -5.6582e-02,\n",
      "          5.1755e-03,  1.1586e-02, -2.2012e-02,  7.8569e-03,  2.8946e-02,\n",
      "         -7.9625e-03, -1.2311e-01, -1.3657e-01, -8.9579e-02, -3.8014e-02,\n",
      "          4.2935e-02,  1.2404e-01,  2.3063e-01,  1.2410e-01, -7.3291e-02,\n",
      "         -3.0282e-03,  1.1225e-01,  8.5656e-02, -7.6729e-02, -8.1591e-02,\n",
      "         -5.1547e-02, -4.9174e-02, -9.8862e-02, -8.0699e-02, -1.6827e-02,\n",
      "         -7.9661e-02, -3.4620e-02, -4.5793e-02, -9.2894e-03,  3.5801e-02,\n",
      "          3.3837e-02, -8.1512e-03,  6.3747e-03, -2.3872e-02, -1.0321e-01,\n",
      "         -4.8113e-02, -1.3742e-01, -6.9099e-02,  3.9475e-02,  1.4855e-01,\n",
      "          1.9682e-01, -1.9618e-02, -1.4338e-01, -5.2879e-02,  1.2429e-02,\n",
      "          4.4837e-03, -1.7676e-02, -9.3863e-03, -4.2205e-02,  1.5605e-02,\n",
      "         -5.3058e-02, -4.3247e-02, -1.0840e-01, -1.0573e-01, -6.5169e-02,\n",
      "          2.1480e-02,  1.3473e-02, -1.9589e-02,  3.1985e-02, -2.7387e-02,\n",
      "         -1.1062e-02, -3.6262e-02, -4.6396e-02, -2.8299e-02, -3.0491e-02,\n",
      "         -4.0666e-02, -4.9772e-02,  5.8268e-02,  2.9828e-03, -4.0503e-02,\n",
      "         -2.1012e-01, -9.2519e-02, -7.1886e-02, -1.7074e-02, -4.0048e-02,\n",
      "         -6.8250e-02, -2.9178e-02, -7.5935e-02, -1.2004e-01, -7.4722e-02,\n",
      "         -1.1248e-01, -9.3362e-02, -9.1505e-02, -7.3192e-03, -2.7194e-02,\n",
      "         -2.9366e-02, -1.9615e-02,  2.1511e-02, -2.7624e-02, -4.2510e-02,\n",
      "         -5.3814e-03, -1.7241e-02,  3.9912e-03, -6.9155e-02, -1.0774e-01,\n",
      "         -1.0342e-01, -6.5772e-02, -3.5602e-02,  1.2293e-02, -4.9913e-02,\n",
      "         -1.9142e-02, -6.5059e-02, -7.7196e-02, -5.5184e-02, -6.1378e-02,\n",
      "         -4.4218e-02, -7.9289e-02, -8.4038e-02, -1.3794e-01, -8.1047e-02,\n",
      "         -7.1899e-02, -1.1410e-02,  3.4935e-02,  9.0948e-03, -2.3364e-03,\n",
      "         -3.2336e-02, -1.5209e-02, -6.9737e-02, -8.3228e-02, -2.4027e-02,\n",
      "          1.6896e-02, -9.5494e-02, -8.0221e-02, -9.1919e-02, -4.3845e-02,\n",
      "          4.3358e-02,  1.6468e-02, -2.7711e-02, -7.2454e-02, -2.7546e-03,\n",
      "         -7.1899e-04, -2.6577e-02, -2.8231e-02, -9.2132e-02, -9.4626e-02,\n",
      "         -1.0256e-01, -9.4368e-02, -4.2951e-02, -4.6538e-02,  2.8122e-02,\n",
      "          2.6677e-02,  1.4976e-02, -1.1666e-02, -9.9778e-04, -6.5188e-02,\n",
      "         -8.8132e-02, -8.8850e-02, -7.3510e-02, -1.1611e-01, -7.6366e-02,\n",
      "         -1.4667e-01, -6.2418e-02, -1.2406e-02, -3.5518e-03, -3.4681e-02,\n",
      "         -9.2675e-02, -1.4001e-01, -7.4933e-02, -5.7568e-02, -5.5741e-02,\n",
      "         -5.3036e-02, -4.8222e-02, -4.5223e-02, -3.9216e-02, -6.0556e-02,\n",
      "         -6.1616e-02, -1.3567e-02, -2.5745e-02,  6.0345e-03,  3.1931e-02,\n",
      "          2.7718e-02, -9.0975e-03, -4.0962e-02, -6.6905e-02, -3.0086e-02,\n",
      "         -9.9396e-02, -7.5538e-02, -4.5279e-02, -9.5000e-02, -7.6508e-02,\n",
      "          9.5970e-03,  2.2017e-03,  6.4106e-02,  3.2553e-02, -9.2442e-02,\n",
      "         -9.1039e-02, -1.8352e-02, -1.9244e-02,  4.1679e-02,  7.7731e-02,\n",
      "          4.3172e-02,  2.2339e-02,  5.0839e-02,  2.5086e-03, -8.3781e-04,\n",
      "          1.6935e-02,  1.4482e-02, -2.8006e-02, -9.4926e-03,  1.3686e-02,\n",
      "          1.7718e-02,  9.8336e-03,  3.1749e-02,  1.4389e-02,  1.0228e-01,\n",
      "          1.6400e-01,  1.8086e-01,  1.1121e-01,  8.2990e-02,  6.9113e-02,\n",
      "          5.0079e-02,  6.6144e-02,  8.1022e-02,  1.1702e-01,  1.8644e-01,\n",
      "          2.3136e-01,  2.5557e-01,  2.5372e-01,  2.0987e-01,  1.1344e-01,\n",
      "          6.1343e-02,  2.2748e-02,  1.3355e-02, -3.4247e-02,  1.3356e-02,\n",
      "          3.3137e-02,  2.1959e-04,  5.7748e-03,  1.7518e-02,  2.6559e-02,\n",
      "          4.4249e-02,  1.4365e-01,  2.2136e-01,  3.2505e-01,  3.3125e-01,\n",
      "          3.2459e-01,  3.3856e-01,  3.0654e-01,  3.6547e-01,  3.5619e-01,\n",
      "          4.4692e-01,  4.9353e-01,  4.3504e-01,  3.6621e-01,  3.1003e-01,\n",
      "          2.4996e-01,  1.4809e-01,  8.4201e-02,  6.2437e-02,  3.4227e-02,\n",
      "         -2.6954e-02,  2.7671e-02,  2.1000e-02,  2.8736e-02, -1.9904e-02,\n",
      "         -2.7171e-02,  6.2957e-03,  8.9163e-03,  1.1928e-02,  4.3197e-02,\n",
      "          1.2944e-01,  1.6659e-01,  1.8787e-01,  1.9896e-01,  2.3825e-01,\n",
      "          2.3531e-01,  2.8247e-01,  2.9896e-01,  2.6088e-01,  2.7362e-01,\n",
      "          2.0030e-01,  1.3703e-01,  1.2395e-01,  8.7259e-02,  2.3024e-02,\n",
      "          3.5520e-02,  2.2085e-02,  2.6115e-02, -2.3532e-02, -4.8473e-03,\n",
      "         -9.0243e-03,  1.0156e-02,  1.1168e-02, -3.1132e-02,  9.6812e-03,\n",
      "         -5.8041e-03, -2.2945e-02,  7.8532e-03,  3.4470e-02,  1.1303e-02,\n",
      "          4.2847e-02,  3.6306e-02,  2.0595e-02,  1.8258e-02,  2.4502e-02,\n",
      "          5.8616e-02,  1.6578e-02,  2.5546e-02,  1.2447e-02,  3.4707e-02,\n",
      "          9.8211e-03,  3.9488e-03,  2.9605e-03,  1.2564e-02, -3.0860e-02,\n",
      "         -2.4751e-02, -6.0740e-03, -2.9215e-02, -2.5185e-03]])\n",
      "activation_stack.0.bias: tensor([2.4805])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total send cost: 9800000\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== The Experiment Result ===\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Experiment for local_update_epochs\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 2500\n",
    "batch_size = 1\n",
    "num_clients_list = [5]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        print(f'Client_X_train[{client}]: {X_train_client[client]}')\n",
    "        print(f'Client_y_train[{client}]: {y_train_client[client]}')\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "        print(client_device[client].model)\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = client_weights[client]['activation_stack.0.weight']\n",
    "            send_cost = send_cost + client_weights_size.numel()\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Record the loss\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot the cost history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot send cost history\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Record the history of loss and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "\n",
    "# Plot the error rate between cost history with local_update_epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Loss Error Rate\")\n",
    "plt.title(\"Loss Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
