{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading training data\n",
    "#dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "#dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "#train_split = int(0.8 * len(dataset))\n",
    "#X_train = dataset[:train_split, [0,1,2]]\n",
    "#y_train = dataset[:train_split, 3]\n",
    "#X_test = dataset[train_split:, [0,1,2]]\n",
    "#y_test = dataset[train_split:, 3]\n",
    "\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "#prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "#prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "#X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "#X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "#X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "#X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "#X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "#y_train_tensor = torch.from_numpy(y_train)\n",
    "#X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "#y_test_tensor = torch.from_numpy(y_test)\n",
    "#X_prediction_tensor = torch.from_numpy(X_prediction_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 -0.0108282 -0.0196004 ... -0.0107833 -0.00341473 -0.0131107]\n",
      " [-1 -0.00470349 0.0169135 ... -0.0430199 -0.032248 -0.00424447]\n",
      " [1 0.00250835 0.0168447 ... -0.038255 -0.031005 -0.0168385]\n",
      " ...\n",
      " [-1 0.0103883 -0.0310115 ... 0.00536232 -0.00887889 -0.0570236]\n",
      " [1 0.00903997 0.0155853 ... -0.0331243 -0.0315165 -0.0139837]\n",
      " [-1 -0.0113611 0.0237702 ... -0.0371067 -0.0203854 -0.00976943]]\n",
      "torch.Size([800, 1999])\n",
      "torch.Size([800])\n",
      "torch.Size([200, 1999])\n",
      "torch.Size([200])\n"
     ]
    }
   ],
   "source": [
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized\", sep=' ', header=None, nrows=1000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dd85410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, w, b):\n",
    "    num_samples = X.shape[0]\n",
    "    y_pred = np.dot(X, w) + b\n",
    "    y_pred = torch.round(torch.from_numpy(y_pred))\n",
    "    error_count = torch.count_nonzero(y_pred - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "Custom SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/1000], Loss: 1.00000000\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/1000], Loss: 0.99674284\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/1000], Loss: 0.99402458\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/1000], Loss: 0.99175298\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/1000], Loss: 0.98974782\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/1000], Loss: 0.98790249\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/1000], Loss: 0.98615391\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/1000], Loss: 0.98446477\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/1000], Loss: 0.98281297\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/1000], Loss: 0.98118543\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/1000], Loss: 0.97957440\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/1000], Loss: 0.97797528\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/1000], Loss: 0.97638534\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/1000], Loss: 0.97480296\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/1000], Loss: 0.97322715\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/1000], Loss: 0.97165733\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/1000], Loss: 0.97009313\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/1000], Loss: 0.96853433\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/1000], Loss: 0.96698079\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/1000], Loss: 0.96543239\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/1000], Loss: 0.96388908\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/1000], Loss: 0.96235080\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/1000], Loss: 0.96081751\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/1000], Loss: 0.95928916\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/1000], Loss: 0.95776573\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/1000], Loss: 0.95624718\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/1000], Loss: 0.95473349\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/1000], Loss: 0.95322462\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/1000], Loss: 0.95172055\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/1000], Loss: 0.95022124\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/1000], Loss: 0.94872667\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/1000], Loss: 0.94723682\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/1000], Loss: 0.94575165\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/1000], Loss: 0.94427114\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/1000], Loss: 0.94279527\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/1000], Loss: 0.94132400\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/1000], Loss: 0.93985731\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/1000], Loss: 0.93839517\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/1000], Loss: 0.93693757\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/1000], Loss: 0.93548446\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/1000], Loss: 0.93403584\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/1000], Loss: 0.93259167\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/1000], Loss: 0.93115192\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/1000], Loss: 0.92971658\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/1000], Loss: 0.92828562\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/1000], Loss: 0.92685902\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/1000], Loss: 0.92543674\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/1000], Loss: 0.92401878\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/1000], Loss: 0.92260510\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/1000], Loss: 0.92119568\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/1000], Loss: 0.91979050\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/1000], Loss: 0.91838953\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/1000], Loss: 0.91699275\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/1000], Loss: 0.91560015\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/1000], Loss: 0.91421169\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/1000], Loss: 0.91282735\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/1000], Loss: 0.91144712\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/1000], Loss: 0.91007097\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/1000], Loss: 0.90869888\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/1000], Loss: 0.90733082\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/1000], Loss: 0.90596679\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/1000], Loss: 0.90460674\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/1000], Loss: 0.90325067\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/1000], Loss: 0.90189856\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/1000], Loss: 0.90055038\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/1000], Loss: 0.89920611\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/1000], Loss: 0.89786573\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/1000], Loss: 0.89652922\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/1000], Loss: 0.89519657\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/1000], Loss: 0.89386774\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/1000], Loss: 0.89254273\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/1000], Loss: 0.89122151\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/1000], Loss: 0.88990407\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/1000], Loss: 0.88859038\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/1000], Loss: 0.88728042\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/1000], Loss: 0.88597418\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/1000], Loss: 0.88467164\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/1000], Loss: 0.88337278\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/1000], Loss: 0.88207758\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/1000], Loss: 0.88078601\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/1000], Loss: 0.87949808\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/1000], Loss: 0.87821375\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/1000], Loss: 0.87693300\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/1000], Loss: 0.87565583\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/1000], Loss: 0.87438221\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/1000], Loss: 0.87311212\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/1000], Loss: 0.87184555\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/1000], Loss: 0.87058248\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/1000], Loss: 0.86932290\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/1000], Loss: 0.86806678\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/1000], Loss: 0.86681411\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/1000], Loss: 0.86556487\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/1000], Loss: 0.86431905\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/1000], Loss: 0.86307663\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/1000], Loss: 0.86183759\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/1000], Loss: 0.86060192\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/1000], Loss: 0.85936961\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/1000], Loss: 0.85814062\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/1000], Loss: 0.85691496\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/1000], Loss: 0.85569260\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/1000], Loss: 0.85447353\n",
      "Learned parameters:\n",
      "w0 = -0.00234316508973874\n",
      "w1 = 0.005448050702063594\n",
      "w2 = 0.0037340829438314623\n",
      "w3 = -0.0049871583940178515\n",
      "w4 = -0.005757634275628596\n",
      "w5 = -0.004719241540897358\n",
      "w6 = 0.0020246388990234543\n",
      "w7 = -0.0012892145083276611\n",
      "w8 = -0.0007766288572116183\n",
      "w9 = -0.0017308116523988592\n",
      "w10 = -0.008448365501352965\n",
      "w11 = 0.003589527961045285\n",
      "w12 = 0.0012110905462349637\n",
      "w13 = -0.0027019268830607806\n",
      "w14 = -0.0002744414460463041\n",
      "w15 = 0.0005819005836133788\n",
      "w16 = -0.003864078596256284\n",
      "w17 = 0.002597761669058867\n",
      "w18 = -0.00113197177351218\n",
      "w19 = 0.006492491934232266\n",
      "w20 = 0.0032580615684643546\n",
      "w21 = 0.004677876968847441\n",
      "w22 = -0.0034945404650332415\n",
      "w23 = -0.001541269154055575\n",
      "w24 = 0.005958655460132272\n",
      "w25 = -0.0009924883930815672\n",
      "w26 = -0.0009547545129364845\n",
      "w27 = 0.0006022947869212283\n",
      "w28 = -0.001570644289920387\n",
      "w29 = 0.00245906218094439\n",
      "w30 = -0.003950752711942785\n",
      "w31 = 0.0004776300198717762\n",
      "w32 = -0.004481058174348202\n",
      "w33 = 0.001777658282245062\n",
      "w34 = -0.0009932506499540834\n",
      "w35 = 0.0032942119757613866\n",
      "w36 = 0.004877196634447085\n",
      "w37 = 0.0051220699031914885\n",
      "w38 = -0.00573427719838636\n",
      "w39 = -0.0012579909318603478\n",
      "w40 = 0.00449902929646447\n",
      "w41 = -0.0025662724654253346\n",
      "w42 = 0.00646841966245676\n",
      "w43 = 0.0010184679401300733\n",
      "w44 = 0.005527200161354101\n",
      "w45 = -0.006119892181885529\n",
      "w46 = -0.0048416844854154316\n",
      "w47 = 0.0013849311304153403\n",
      "w48 = 8.860319251564918e-05\n",
      "w49 = -0.01171615893905202\n",
      "w50 = 0.007220052927167588\n",
      "w51 = 0.004054361418891269\n",
      "w52 = -0.00038702104785467203\n",
      "w53 = -0.0005972531775993814\n",
      "w54 = -0.0023822062898130394\n",
      "w55 = -0.0017815910388348836\n",
      "w56 = -0.0009411127540743825\n",
      "w57 = -0.0028575336671615215\n",
      "w58 = -0.0016856959806203625\n",
      "w59 = 0.004228087755556671\n",
      "w60 = -0.002387759507968728\n",
      "w61 = -0.004891814748213737\n",
      "w62 = 0.0028568466243241095\n",
      "w63 = -0.0017637463076334576\n",
      "w64 = -0.0021867747360804573\n",
      "w65 = -0.0015554388930028979\n",
      "w66 = 0.0033820642697087082\n",
      "w67 = 0.00303687021557022\n",
      "w68 = -0.002938621129217119\n",
      "w69 = -0.0019812721509352596\n",
      "w70 = -0.002296438830741762\n",
      "w71 = -0.0001221356200781711\n",
      "w72 = -0.005726403554580146\n",
      "w73 = -0.00609414046505864\n",
      "w74 = 0.009455197159225194\n",
      "w75 = -0.005160241159157044\n",
      "w76 = -0.0008894545404070055\n",
      "w77 = 0.0030813667707323606\n",
      "w78 = -0.0009251826973670006\n",
      "w79 = 0.005129266145011707\n",
      "w80 = 0.00043477518642084153\n",
      "w81 = 0.0037148475039751245\n",
      "w82 = 0.0033863122165023124\n",
      "w83 = 0.003707648090399436\n",
      "w84 = 0.008383415013609436\n",
      "w85 = 0.0018401056715239687\n",
      "w86 = 0.0006838964303562336\n",
      "w87 = 0.0017736763676512379\n",
      "w88 = -0.002433241868526399\n",
      "w89 = -0.009107030129434293\n",
      "w90 = 0.0066792867511837285\n",
      "w91 = -0.00018835109864537278\n",
      "w92 = -0.0056306402044396095\n",
      "w93 = 0.0018161312660486225\n",
      "w94 = 0.008517857934566726\n",
      "w95 = -0.002445982863852049\n",
      "w96 = 0.0014487586181624675\n",
      "w97 = 0.001744341589844197\n",
      "w98 = 0.0031119482658091393\n",
      "w99 = 0.0035763519180040215\n",
      "w100 = 0.016228615517678603\n",
      "w101 = -0.01158234698418407\n",
      "w102 = 0.002047551002918199\n",
      "w103 = 0.00034923518007354905\n",
      "w104 = -0.0016587179318667674\n",
      "w105 = -0.014403229032835654\n",
      "w106 = -8.203137718189316e-05\n",
      "w107 = 0.004581803685699583\n",
      "w108 = -0.006660860245617423\n",
      "w109 = 0.0013286538324419274\n",
      "w110 = -0.0013261573670611494\n",
      "w111 = -0.007990564284776774\n",
      "w112 = 0.007012110569253081\n",
      "w113 = 0.00040725861067380007\n",
      "w114 = -0.006642588377066729\n",
      "w115 = 0.005377021069314938\n",
      "w116 = -0.00311044827878479\n",
      "w117 = -0.001666532136564172\n",
      "w118 = -0.0021439756928122266\n",
      "w119 = -0.001635150480119912\n",
      "w120 = 0.006779877817368496\n",
      "w121 = 0.0007943786857808281\n",
      "w122 = -0.0016456594056458414\n",
      "w123 = 0.002753288677605604\n",
      "w124 = 0.003322981122668848\n",
      "w125 = 0.0004833067048405345\n",
      "w126 = 0.0005398326853197309\n",
      "w127 = 0.005014026724075289\n",
      "w128 = -0.0006363903650539743\n",
      "w129 = -0.0003436253137480324\n",
      "w130 = -0.00266273666375515\n",
      "w131 = -0.0003941126388732923\n",
      "w132 = -0.0011524942114692876\n",
      "w133 = -0.0011481945982124296\n",
      "w134 = -0.00896931965937833\n",
      "w135 = -0.0013701673234124499\n",
      "w136 = 0.0007951684861461112\n",
      "w137 = -0.003919690342210578\n",
      "w138 = 0.0037624664846672073\n",
      "w139 = 0.010187533048725075\n",
      "w140 = -0.005356112728181781\n",
      "w141 = -0.004400640309790817\n",
      "w142 = 0.006042135993092034\n",
      "w143 = 0.0014495145427438987\n",
      "w144 = 0.003389190448396309\n",
      "w145 = -0.01779673065519572\n",
      "w146 = 0.0017032935444952083\n",
      "w147 = -0.015498468143189774\n",
      "w148 = -0.007677381709758111\n",
      "w149 = -0.0011123574554740437\n",
      "w150 = 0.006408512740191422\n",
      "w151 = 0.003639887277239505\n",
      "w152 = 0.0027597859664278486\n",
      "w153 = -0.0028033014863049236\n",
      "w154 = 0.0017147400453142734\n",
      "w155 = 0.006513117136795524\n",
      "w156 = 0.009496727295584722\n",
      "w157 = 0.011968264801814357\n",
      "w158 = 0.006299356259694633\n",
      "w159 = 0.005192051611190257\n",
      "w160 = 0.0022962552530494252\n",
      "w161 = 0.002556428961525333\n",
      "w162 = -0.002635049466637975\n",
      "w163 = -0.005095442459097076\n",
      "w164 = -0.0005792175964400496\n",
      "w165 = 0.0017995129028935681\n",
      "w166 = 0.009445786827636013\n",
      "w167 = 0.0024176174243326722\n",
      "w168 = -0.000268359730768755\n",
      "w169 = -0.006744291121113489\n",
      "w170 = -0.00013070270941410126\n",
      "w171 = 0.00026158093254164506\n",
      "w172 = -0.0018461026601060142\n",
      "w173 = 0.0052113025617860015\n",
      "w174 = 0.0009388582668270651\n",
      "w175 = 0.0012316363465338152\n",
      "w176 = -0.001549356385027301\n",
      "w177 = -0.0082123776827187\n",
      "w178 = 0.007821980543653927\n",
      "w179 = -0.006660733991366924\n",
      "w180 = -0.0034774626375031993\n",
      "w181 = -0.0061282665934867765\n",
      "w182 = -0.0006897168858062509\n",
      "w183 = 0.003323605196089689\n",
      "w184 = -0.007586418200055333\n",
      "w185 = -0.009269453442125724\n",
      "w186 = -0.012710147492861465\n",
      "w187 = 0.013594233348462395\n",
      "w188 = 8.097068540061832e-05\n",
      "w189 = -0.005124300283356078\n",
      "w190 = 0.0013091553768157043\n",
      "w191 = -0.001141463125230277\n",
      "w192 = -0.003218279000109782\n",
      "w193 = 4.2473905466472266e-05\n",
      "w194 = 0.000708342086044061\n",
      "w195 = -0.006139813307523337\n",
      "w196 = -0.004222306099317991\n",
      "w197 = -0.0038273016989862803\n",
      "w198 = -0.0011056495721604104\n",
      "w199 = 0.005734171365541868\n",
      "w200 = -0.004424855902747507\n",
      "w201 = 0.006246485250255402\n",
      "w202 = -0.0028313661416133973\n",
      "w203 = -0.004120688836831228\n",
      "w204 = -0.003941412371807264\n",
      "w205 = -0.003814173855196903\n",
      "w206 = -0.001587610043109962\n",
      "w207 = -0.0014054052631616413\n",
      "w208 = -0.004286169355980274\n",
      "w209 = 0.0011348348646963184\n",
      "w210 = 0.008002113124015487\n",
      "w211 = -0.0072453535060801495\n",
      "w212 = 0.005367241115286149\n",
      "w213 = -0.0007797062631510907\n",
      "w214 = 0.003177393952592096\n",
      "w215 = -9.45166655103369e-05\n",
      "w216 = 0.00382572013583372\n",
      "w217 = -0.020611986473146642\n",
      "w218 = 0.003726738404338798\n",
      "w219 = 0.003000758584191242\n",
      "w220 = -0.0025923924738728184\n",
      "w221 = 0.005070649531258734\n",
      "w222 = 0.0021460982317345447\n",
      "w223 = 0.004637116645496326\n",
      "w224 = 0.0032801136634262513\n",
      "w225 = 0.013525952828318246\n",
      "w226 = -0.00030229214976384706\n",
      "w227 = 0.0030735515733223136\n",
      "w228 = -0.000199827699994876\n",
      "w229 = 0.004098257066184124\n",
      "w230 = 0.008152466218789086\n",
      "w231 = 0.001957651263360367\n",
      "w232 = -0.002743602481123886\n",
      "w233 = 0.004979412588918428\n",
      "w234 = 0.0015020291417637381\n",
      "w235 = 0.003525802957115351\n",
      "w236 = 0.0013878100182658207\n",
      "w237 = 0.008061729377593129\n",
      "w238 = -0.008425074972534197\n",
      "w239 = -0.011938269382507447\n",
      "w240 = 0.0012028171697626058\n",
      "w241 = -0.008640669764270406\n",
      "w242 = 0.0008693593910428388\n",
      "w243 = 0.01106274740568879\n",
      "w244 = 0.004896282903212793\n",
      "w245 = 0.0011407039540187154\n",
      "w246 = 0.008815717449352497\n",
      "w247 = -0.008402518811745056\n",
      "w248 = -0.008634824201243894\n",
      "w249 = 0.005167037537066142\n",
      "w250 = -0.00043579543572089605\n",
      "w251 = -0.0007170896353925082\n",
      "w252 = 0.001662366810170549\n",
      "w253 = 0.0035076403717417357\n",
      "w254 = -0.006553365484504251\n",
      "w255 = -0.004062637284096472\n",
      "w256 = -0.0016383414288008641\n",
      "w257 = -0.011497938496082152\n",
      "w258 = 0.002505570334604437\n",
      "w259 = -0.004831513851943478\n",
      "w260 = -0.007868315233069684\n",
      "w261 = 0.0009719889679676966\n",
      "w262 = 0.0009504090249064383\n",
      "w263 = 0.009234959302513375\n",
      "w264 = 0.006106691559468679\n",
      "w265 = -0.011820122409332239\n",
      "w266 = 0.0017922797790965945\n",
      "w267 = -0.002807198900725582\n",
      "w268 = 0.0024280696207396135\n",
      "w269 = 0.001654441964895133\n",
      "w270 = 0.0004877172756151211\n",
      "w271 = -0.0034098601806185768\n",
      "w272 = 0.0012770758237756483\n",
      "w273 = -0.009795002749879291\n",
      "w274 = -0.004017213913948968\n",
      "w275 = 0.0005305842782505087\n",
      "w276 = -0.005886726169465837\n",
      "w277 = 0.0022084423226405177\n",
      "w278 = -0.0023066888230915818\n",
      "w279 = 0.003996001965095912\n",
      "w280 = 0.0009496082389221168\n",
      "w281 = 0.002961192671378196\n",
      "w282 = -0.003705510976539995\n",
      "w283 = 0.0007600564244955565\n",
      "w284 = -0.009843666123294303\n",
      "w285 = -0.017016382122357402\n",
      "w286 = 0.001301079802035504\n",
      "w287 = 0.005321323372844268\n",
      "w288 = -0.011797362341465759\n",
      "w289 = 0.008174089108613785\n",
      "w290 = 0.0030114053408513276\n",
      "w291 = 0.004550161760422006\n",
      "w292 = -0.009688938234055871\n",
      "w293 = 0.00392992134985137\n",
      "w294 = 8.425965769509825e-05\n",
      "w295 = -0.0034301376715160903\n",
      "w296 = 4.224256614305702e-05\n",
      "w297 = 0.003157653171876745\n",
      "w298 = -0.0003012984838859317\n",
      "w299 = 0.007163180032935557\n",
      "w300 = -0.0069183193122345716\n",
      "w301 = 0.0002968259559709645\n",
      "w302 = -0.005307396944651243\n",
      "w303 = -0.0004099414409645427\n",
      "w304 = -0.009743453784445763\n",
      "w305 = 0.0019480434079233635\n",
      "w306 = 0.007032318755970034\n",
      "w307 = -0.005798617080909978\n",
      "w308 = 0.002269558658086428\n",
      "w309 = -0.005844855699778774\n",
      "w310 = -0.004384537131974415\n",
      "w311 = -0.0031112146776506046\n",
      "w312 = -0.00615474150155248\n",
      "w313 = -0.0017043505218620775\n",
      "w314 = 0.0022893713665016516\n",
      "w315 = -0.023026293766999457\n",
      "w316 = -0.0006042420108289154\n",
      "w317 = -0.004137325937223761\n",
      "w318 = 0.00218437709868924\n",
      "w319 = 0.0012598430934218276\n",
      "w320 = -0.0010317754491918824\n",
      "w321 = 0.001749113437053187\n",
      "w322 = -0.006631777829628862\n",
      "w323 = -0.0014319739890580208\n",
      "w324 = -0.00037319470287052\n",
      "w325 = 0.0013061798667864132\n",
      "w326 = -0.002887186789538216\n",
      "w327 = -0.0038159369221689166\n",
      "w328 = 4.956106257225781e-05\n",
      "w329 = -0.00640448689989175\n",
      "w330 = 0.0008742950305979812\n",
      "w331 = 0.0038245926067139034\n",
      "w332 = 0.004445335836120175\n",
      "w333 = 0.0006751427654753775\n",
      "w334 = 0.012535278790877897\n",
      "w335 = -0.004279920881531202\n",
      "w336 = -0.004113695905449251\n",
      "w337 = -0.014820814084251984\n",
      "w338 = -0.007206244486368221\n",
      "w339 = 0.007892764881035677\n",
      "w340 = 0.010232486184728627\n",
      "w341 = 0.0022821711779847293\n",
      "w342 = 0.0004059731546511774\n",
      "w343 = 0.021892272595127795\n",
      "w344 = -0.0036074069375860575\n",
      "w345 = 0.0026952200683081793\n",
      "w346 = 0.002867495710867742\n",
      "w347 = 0.004479052044278813\n",
      "w348 = 0.0020083227684957374\n",
      "w349 = -0.0008083149932865327\n",
      "w350 = 0.000948922050716999\n",
      "w351 = -0.0008415731733729882\n",
      "w352 = -0.0002223904791830504\n",
      "w353 = -0.0042231764037708695\n",
      "w354 = 0.006418099269410548\n",
      "w355 = 0.0014376202786906996\n",
      "w356 = 0.004092667968312731\n",
      "w357 = -0.0021509100063855473\n",
      "w358 = 0.00605098845121276\n",
      "w359 = 0.0019363597682819618\n",
      "w360 = 0.0025912398041748035\n",
      "w361 = -0.009369985417453498\n",
      "w362 = -0.006674833052358097\n",
      "w363 = 0.0006536275196610481\n",
      "w364 = -0.0008304889313243295\n",
      "w365 = -0.0019329469445076774\n",
      "w366 = 0.006427214398194369\n",
      "w367 = 0.002317951242154321\n",
      "w368 = -0.013574720487653784\n",
      "w369 = 0.005764451798003099\n",
      "w370 = 0.00591630283675263\n",
      "w371 = 0.006053871244438984\n",
      "w372 = 0.0014648510774475764\n",
      "w373 = -0.0024153690631055946\n",
      "w374 = -0.002963209045893124\n",
      "w375 = -0.003941044190188634\n",
      "w376 = 0.010254626514553783\n",
      "w377 = 0.0020703554516566056\n",
      "w378 = -0.00412196429729699\n",
      "w379 = -0.002643399028257499\n",
      "w380 = -0.02509272002724355\n",
      "w381 = -0.0019497486799160723\n",
      "w382 = -0.005271792633588462\n",
      "w383 = -0.0007929807817355935\n",
      "w384 = -0.00037100268683942485\n",
      "w385 = -0.00957791463719531\n",
      "w386 = 0.0021639597882298816\n",
      "w387 = -0.0015294641983779863\n",
      "w388 = -0.0011103697542557826\n",
      "w389 = 0.0004295243763382451\n",
      "w390 = -0.002573911506676641\n",
      "w391 = 0.005153341808307448\n",
      "w392 = -0.011893454267059813\n",
      "w393 = -0.002833130598242772\n",
      "w394 = 0.00475463943889268\n",
      "w395 = 0.0009034905482712474\n",
      "w396 = 0.004147591171014954\n",
      "w397 = 0.00833391142605706\n",
      "w398 = 0.002596530870851082\n",
      "w399 = -0.0009293665005805941\n",
      "w400 = -0.003499399206760019\n",
      "w401 = -0.00143106418852164\n",
      "w402 = 0.00036565135110972673\n",
      "w403 = -0.003603767020263287\n",
      "w404 = -0.009667572507209184\n",
      "w405 = 0.007838635799957958\n",
      "w406 = -0.004195589924615253\n",
      "w407 = -0.0003696975339227318\n",
      "w408 = 0.00790404461374721\n",
      "w409 = 0.010446173759022665\n",
      "w410 = 8.345702550697449e-05\n",
      "w411 = -0.005430089764197762\n",
      "w412 = -0.007870199949711188\n",
      "w413 = -0.0024192160725802644\n",
      "w414 = -0.00998586007916555\n",
      "w415 = 0.0009161817393236289\n",
      "w416 = -0.0014413097473234121\n",
      "w417 = -0.00357117971166525\n",
      "w418 = -0.0032205271564649\n",
      "w419 = 0.00559831776760728\n",
      "w420 = -0.0015958663927675594\n",
      "w421 = -0.00494191792172061\n",
      "w422 = 0.0013002006934351446\n",
      "w423 = -0.0005614413109698362\n",
      "w424 = 0.004630278824089998\n",
      "w425 = 0.0014208318898680922\n",
      "w426 = 0.001974643615153888\n",
      "w427 = 0.0006443977967931459\n",
      "w428 = 0.0014189387904158773\n",
      "w429 = -0.011688529594759186\n",
      "w430 = 0.0053113194154036526\n",
      "w431 = 0.015843663128254835\n",
      "w432 = 0.00027340852794627496\n",
      "w433 = -0.00025171870382402245\n",
      "w434 = -0.002811389244490124\n",
      "w435 = -0.00280705982788781\n",
      "w436 = 0.000299867237023414\n",
      "w437 = -0.0002523809018648435\n",
      "w438 = 0.005446224811507045\n",
      "w439 = -0.0022337449275869506\n",
      "w440 = -0.00807699437085822\n",
      "w441 = 0.0033183399455371525\n",
      "w442 = -0.0021996320505507736\n",
      "w443 = 0.003783835417011641\n",
      "w444 = 0.0014527639372264852\n",
      "w445 = -0.0010544739479238594\n",
      "w446 = 0.002142026974821003\n",
      "w447 = 0.007984723711027465\n",
      "w448 = 0.006269814590186672\n",
      "w449 = -0.0007009091997101692\n",
      "w450 = 0.001347318589599281\n",
      "w451 = -0.001831680230548266\n",
      "w452 = -0.006476923758719219\n",
      "w453 = -0.006023936242468279\n",
      "w454 = -0.0011850764690942465\n",
      "w455 = 0.001214575500893619\n",
      "w456 = -0.0026998818026510857\n",
      "w457 = 0.0023477566831052285\n",
      "w458 = 0.002275673503304437\n",
      "w459 = -0.010172124275266361\n",
      "w460 = -0.0019491900540198799\n",
      "w461 = -0.0032573409414686093\n",
      "w462 = -0.0012759317190084891\n",
      "w463 = -0.0012856712206865343\n",
      "w464 = 0.00030932435601336596\n",
      "w465 = 0.0007716441988576097\n",
      "w466 = 0.008455189980830696\n",
      "w467 = -0.0009567043767881631\n",
      "w468 = 0.031035563348940358\n",
      "w469 = -0.00033072906005475125\n",
      "w470 = 0.002387069738227177\n",
      "w471 = -0.005033346947446225\n",
      "w472 = -0.004467132614018117\n",
      "w473 = -0.003888638385455532\n",
      "w474 = 0.009462804661579057\n",
      "w475 = -0.002741712863628365\n",
      "w476 = -0.0024893457084416105\n",
      "w477 = 0.003935625439742063\n",
      "w478 = -0.0008787872547982324\n",
      "w479 = 0.0010228739243010889\n",
      "w480 = -0.0065247053933806245\n",
      "w481 = -0.006335800787227301\n",
      "w482 = 0.011112477410940217\n",
      "w483 = -0.009373955731820016\n",
      "w484 = 0.005264477121119958\n",
      "w485 = 0.003228517499744005\n",
      "w486 = 0.003486154811718171\n",
      "w487 = 0.0021848686819912924\n",
      "w488 = 0.0007389383490644103\n",
      "w489 = -0.0009527509892794822\n",
      "w490 = 0.0019969624694924156\n",
      "w491 = 0.0018365679536250748\n",
      "w492 = -0.005786800312442879\n",
      "w493 = 0.009773418738893818\n",
      "w494 = 0.007451785114163659\n",
      "w495 = 0.011775253316771822\n",
      "w496 = 0.004238233224961818\n",
      "w497 = 0.01427231512710998\n",
      "w498 = 0.004743995374401077\n",
      "w499 = 0.0035032934365947024\n",
      "w500 = 0.003352984516349963\n",
      "w501 = 0.0017342018076109108\n",
      "w502 = 0.00862950275161492\n",
      "w503 = -0.0021457512743361627\n",
      "w504 = 0.0025127147566306597\n",
      "w505 = -0.002306917829235161\n",
      "w506 = -0.0045150813556652115\n",
      "w507 = -0.006844592486544763\n",
      "w508 = -0.0021354768826871936\n",
      "w509 = -0.0009397956455508178\n",
      "w510 = -0.0007700082448582335\n",
      "w511 = 0.001295402273715218\n",
      "w512 = -0.006149349918404491\n",
      "w513 = 0.002238827588801049\n",
      "w514 = 0.0013181713870704637\n",
      "w515 = 6.032385094872014e-05\n",
      "w516 = 0.002093201735124124\n",
      "w517 = -0.001676791728463129\n",
      "w518 = -0.006439535164849696\n",
      "w519 = 0.0006698943443271033\n",
      "w520 = -0.0012931549509900026\n",
      "w521 = -0.003249042819481868\n",
      "w522 = 0.008600649538832968\n",
      "w523 = -0.003198745651102487\n",
      "w524 = 0.00559076295630619\n",
      "w525 = 0.0038149548907107996\n",
      "w526 = -0.0027065833050075637\n",
      "w527 = 0.004775183588282284\n",
      "w528 = 0.0022938580807672095\n",
      "w529 = -0.0025424224648071237\n",
      "w530 = -0.00267263076498941\n",
      "w531 = 0.0015873751889500449\n",
      "w532 = -0.004410218016888704\n",
      "w533 = -0.0022874692096070235\n",
      "w534 = 0.0031385383023219368\n",
      "w535 = -0.0014765215073490436\n",
      "w536 = 0.00027867073886174074\n",
      "w537 = -0.001542407808523214\n",
      "w538 = -0.004210959432432077\n",
      "w539 = 0.0072766829018499546\n",
      "w540 = -0.008422025296071783\n",
      "w541 = -0.0012842495857216126\n",
      "w542 = 0.0197972253226917\n",
      "w543 = 0.00446095633671561\n",
      "w544 = -0.0003675942938859427\n",
      "w545 = 0.0037575386270953144\n",
      "w546 = 0.0007269198212478519\n",
      "w547 = -0.01284500689879744\n",
      "w548 = -0.0006093010025014331\n",
      "w549 = -2.8406543397736336e-05\n",
      "w550 = 0.013571784791248787\n",
      "w551 = 0.006160428288758514\n",
      "w552 = 4.694333404845651e-05\n",
      "w553 = -0.0004768924817670172\n",
      "w554 = -0.0018970662888683919\n",
      "w555 = -0.001997499907023729\n",
      "w556 = 0.0009757301029040626\n",
      "w557 = -0.005521752540210485\n",
      "w558 = -0.0003326231096683632\n",
      "w559 = -0.0016238925787064903\n",
      "w560 = -0.01202712980581339\n",
      "w561 = 0.004638326216602545\n",
      "w562 = 0.004127461614386695\n",
      "w563 = 0.00041447720283475557\n",
      "w564 = 0.009390771385307773\n",
      "w565 = 0.005268599255730314\n",
      "w566 = -0.0012634373751784008\n",
      "w567 = 0.0008430670844907894\n",
      "w568 = -0.007381501839239772\n",
      "w569 = -0.002918266944924675\n",
      "w570 = 9.849167410004731e-05\n",
      "w571 = 0.01329750505817877\n",
      "w572 = -0.0033242778913467295\n",
      "w573 = -0.00015752886807237872\n",
      "w574 = 0.0004599238024326223\n",
      "w575 = 0.0009429452978396807\n",
      "w576 = -0.00378380722124386\n",
      "w577 = -0.006949823848504676\n",
      "w578 = 0.006168429908603471\n",
      "w579 = -0.0070072943303254075\n",
      "w580 = 0.0007268316750648736\n",
      "w581 = -0.0044630886439813505\n",
      "w582 = 0.0026217392395993687\n",
      "w583 = 0.006016115116499977\n",
      "w584 = 4.3373520633467144e-05\n",
      "w585 = 0.0008262062300986274\n",
      "w586 = -0.001835689172025941\n",
      "w587 = 0.0025160541651951997\n",
      "w588 = 0.008034411407290219\n",
      "w589 = 0.0018138138856848977\n",
      "w590 = -0.003965371320815187\n",
      "w591 = -9.424854976916824e-05\n",
      "w592 = -0.000881798747429463\n",
      "w593 = -0.00015904354398657108\n",
      "w594 = 0.004481607752120725\n",
      "w595 = -0.007153894510508205\n",
      "w596 = 0.0004914943825562636\n",
      "w597 = -0.004609088733308132\n",
      "w598 = -0.0011738875877745415\n",
      "w599 = -0.0051880548632795755\n",
      "w600 = -0.002178269612697401\n",
      "w601 = -0.0019021543232014815\n",
      "w602 = 0.007088337708773195\n",
      "w603 = -0.002763323009615878\n",
      "w604 = 0.0006388467154092099\n",
      "w605 = 0.008138047486264448\n",
      "w606 = -0.006227640576499074\n",
      "w607 = -0.00021341442942482332\n",
      "w608 = 0.006519015546481518\n",
      "w609 = -0.004251247083270185\n",
      "w610 = -4.2635814473083074e-05\n",
      "w611 = -0.0001455436455906029\n",
      "w612 = -0.004640693683398597\n",
      "w613 = -0.003696453224740241\n",
      "w614 = 0.0003805564116647402\n",
      "w615 = -0.0028559265924576254\n",
      "w616 = 0.0025503416320402693\n",
      "w617 = 0.0072291094340437186\n",
      "w618 = -0.005007403137589999\n",
      "w619 = 0.0006513451192426314\n",
      "w620 = 0.0010145980147752622\n",
      "w621 = 0.0014733458428606088\n",
      "w622 = -0.0019169877824040393\n",
      "w623 = -0.0015558211484320518\n",
      "w624 = 0.00029532238145239554\n",
      "w625 = -0.007394950819862255\n",
      "w626 = -0.0002428534051174728\n",
      "w627 = -0.005339746315744916\n",
      "w628 = 0.0074526725463080305\n",
      "w629 = -0.001578177994744415\n",
      "w630 = 0.002224990833769367\n",
      "w631 = 0.001256707139957023\n",
      "w632 = 0.0027277680751332115\n",
      "w633 = -0.01015259945978579\n",
      "w634 = -0.0012783264094697422\n",
      "w635 = -0.008040988813203701\n",
      "w636 = -0.003128820568686553\n",
      "w637 = -0.002744270169737392\n",
      "w638 = -0.0003803505603847903\n",
      "w639 = 0.0029731570915095527\n",
      "w640 = -0.0015636384631926062\n",
      "w641 = -0.0021863196212416275\n",
      "w642 = -0.0014615270892510029\n",
      "w643 = 0.00495246769510553\n",
      "w644 = -0.0010582561927357595\n",
      "w645 = 0.005710579800277688\n",
      "w646 = -0.00045129123608042755\n",
      "w647 = -0.005586887265742508\n",
      "w648 = 0.002035964186624285\n",
      "w649 = 0.024784349939932733\n",
      "w650 = -0.010956414594696769\n",
      "w651 = 0.002572140248910846\n",
      "w652 = -0.005023187251752934\n",
      "w653 = 0.002997317790458739\n",
      "w654 = 0.007177773151689818\n",
      "w655 = 0.001969197783844565\n",
      "w656 = -0.004583448101098782\n",
      "w657 = 0.006995541793215634\n",
      "w658 = 0.0007817055797320087\n",
      "w659 = 0.002365977151662744\n",
      "w660 = 0.005720219277987589\n",
      "w661 = 0.0011890347485278782\n",
      "w662 = 0.0016442067566987163\n",
      "w663 = -0.0038010840697514105\n",
      "w664 = 0.005347859214492752\n",
      "w665 = -0.005191474573044701\n",
      "w666 = -9.593592601613947e-05\n",
      "w667 = 0.00031655973618089386\n",
      "w668 = -0.0036125733972579366\n",
      "w669 = -0.0026361848625958344\n",
      "w670 = -0.0001435825951907109\n",
      "w671 = 0.0014100801592752642\n",
      "w672 = -0.0008818959859655465\n",
      "w673 = 0.0005830272634284579\n",
      "w674 = 0.00046209797177772157\n",
      "w675 = -0.0004066571383563759\n",
      "w676 = -0.002560079737865523\n",
      "w677 = -0.006447564911665984\n",
      "w678 = 0.0025247543140764733\n",
      "w679 = -0.0026335922812575543\n",
      "w680 = -0.006425169329730026\n",
      "w681 = 0.004089555369251262\n",
      "w682 = -0.004025946143238457\n",
      "w683 = 0.012793154602184595\n",
      "w684 = -0.003728695411680655\n",
      "w685 = 0.006350675733632819\n",
      "w686 = -0.010197517506997492\n",
      "w687 = -0.0060249946177930135\n",
      "w688 = -0.0001421689888051469\n",
      "w689 = -0.008416242629347688\n",
      "w690 = -0.025554295410610998\n",
      "w691 = 0.0020686443653772325\n",
      "w692 = 0.0016540521097076896\n",
      "w693 = -0.007312918834462241\n",
      "w694 = -0.0028566286153188744\n",
      "w695 = 0.015164529739741285\n",
      "w696 = -0.0012887534797914756\n",
      "w697 = -0.0006764163851406995\n",
      "w698 = -0.0008742383084788712\n",
      "w699 = -0.00319728717480597\n",
      "w700 = -0.0009660258327953175\n",
      "w701 = -0.0016613217595679852\n",
      "w702 = -0.0006571517130142902\n",
      "w703 = -0.004148408999730271\n",
      "w704 = -0.009201134970798218\n",
      "w705 = -0.003477523483379912\n",
      "w706 = 0.006875343672177322\n",
      "w707 = -9.377147814635259e-06\n",
      "w708 = -0.002755684962970587\n",
      "w709 = -0.02718716322696582\n",
      "w710 = -0.01167613991164356\n",
      "w711 = -0.006578183374766284\n",
      "w712 = -0.00760475662608353\n",
      "w713 = -0.015613664989749405\n",
      "w714 = 0.006961483876632204\n",
      "w715 = -0.0019668655486604213\n",
      "w716 = 0.017155199850405728\n",
      "w717 = -0.0011966962071437283\n",
      "w718 = -0.009562475183403337\n",
      "w719 = 0.002217039387697927\n",
      "w720 = -0.009689024510850545\n",
      "w721 = -0.009642040957067466\n",
      "w722 = -0.002096382047847459\n",
      "w723 = 0.01322707375867941\n",
      "w724 = 0.0052475551592693565\n",
      "w725 = -0.0006115576160033699\n",
      "w726 = -0.008659645969718278\n",
      "w727 = -0.0008409277118631211\n",
      "w728 = 0.006224098636730547\n",
      "w729 = -0.0015911675802004806\n",
      "w730 = 0.0017130046755458495\n",
      "w731 = -0.008449625438303467\n",
      "w732 = -0.005401841538908747\n",
      "w733 = -0.005839431281880934\n",
      "w734 = 0.005241761925292807\n",
      "w735 = -0.011091239850595418\n",
      "w736 = 0.008434801887247214\n",
      "w737 = 0.002171132951764555\n",
      "w738 = 0.014595977266668697\n",
      "w739 = 0.0035308619559096272\n",
      "w740 = 0.00016858215768526417\n",
      "w741 = 0.0036960795139343453\n",
      "w742 = -8.755960979064093e-05\n",
      "w743 = -0.001645749714113715\n",
      "w744 = -0.006354776823082687\n",
      "w745 = -0.006470440632372683\n",
      "w746 = -0.006115267957921836\n",
      "w747 = 0.0045061717232786\n",
      "w748 = 7.845363539806561e-05\n",
      "w749 = -0.007400994909030591\n",
      "w750 = 0.0008715012857170757\n",
      "w751 = -0.006348298397893265\n",
      "w752 = 0.0023109729945370947\n",
      "w753 = -0.006246711465351402\n",
      "w754 = 0.006571665655998895\n",
      "w755 = -0.025493058944814687\n",
      "w756 = -0.001276161689123253\n",
      "w757 = -0.006415193419179564\n",
      "w758 = -0.00415869896898042\n",
      "w759 = 0.006897666336601305\n",
      "w760 = 0.0025762218434361383\n",
      "w761 = 0.00025513341526338883\n",
      "w762 = -0.00292820087756235\n",
      "w763 = 0.0018383169922972906\n",
      "w764 = 0.0013478343617705293\n",
      "w765 = -0.0014573647139972804\n",
      "w766 = 0.0055463006293382825\n",
      "w767 = -0.009182580948054752\n",
      "w768 = -0.009985198152845468\n",
      "w769 = 0.0019262177514766086\n",
      "w770 = -0.0011166019718293091\n",
      "w771 = 0.0029069377043977\n",
      "w772 = 0.01833625136516399\n",
      "w773 = -0.0012602659070352809\n",
      "w774 = -0.006288075256876473\n",
      "w775 = -0.0024602261094080146\n",
      "w776 = 0.00023105216773884341\n",
      "w777 = 0.005316510845753169\n",
      "w778 = -0.003737775409428158\n",
      "w779 = 0.0018329818470379702\n",
      "w780 = -0.002467048438409132\n",
      "w781 = 0.0025131459110627422\n",
      "w782 = 0.0013046113906569367\n",
      "w783 = 0.0018276705206569856\n",
      "w784 = 0.0072913768832439906\n",
      "w785 = 0.0030565339609979524\n",
      "w786 = -0.009714086328377463\n",
      "w787 = -0.0078887329939474\n",
      "w788 = -0.004302505749905849\n",
      "w789 = 0.023053156555338523\n",
      "w790 = 0.0016718078525786316\n",
      "w791 = -0.007727799927615792\n",
      "w792 = -0.007459076419513355\n",
      "w793 = 0.0049529501941659795\n",
      "w794 = 0.004386826035816551\n",
      "w795 = -0.002251831099841909\n",
      "w796 = -0.00568197803827429\n",
      "w797 = 0.007278183388893482\n",
      "w798 = -0.0013374481833788754\n",
      "w799 = -0.0011343070428252399\n",
      "w800 = -0.002210462281211847\n",
      "w801 = -0.007339622600231017\n",
      "w802 = 0.0010757851555782154\n",
      "w803 = -0.007446415313786728\n",
      "w804 = -0.0029470732955844017\n",
      "w805 = -0.00026756644488768936\n",
      "w806 = -0.0009313230795603552\n",
      "w807 = -0.010952868363606793\n",
      "w808 = -0.0062876260880245635\n",
      "w809 = -0.0020839918952958465\n",
      "w810 = 0.0052596424302211965\n",
      "w811 = -0.0003375190345835713\n",
      "w812 = 0.0013243253382487145\n",
      "w813 = 0.007781699126607006\n",
      "w814 = -0.004409585495954272\n",
      "w815 = 0.006434756481419356\n",
      "w816 = -0.0011961783628491305\n",
      "w817 = -0.0036506347437710676\n",
      "w818 = -0.035185660905564076\n",
      "w819 = -0.010654244518449808\n",
      "w820 = 0.00393757120473393\n",
      "w821 = 0.0006679457536512219\n",
      "w822 = 0.0042042167502880185\n",
      "w823 = -0.0016317722791397223\n",
      "w824 = 0.00016756618060539873\n",
      "w825 = -0.0015356469351132426\n",
      "w826 = 0.00691872167138502\n",
      "w827 = -0.005202157744858413\n",
      "w828 = -0.012343020818441207\n",
      "w829 = 0.015293649826669496\n",
      "w830 = 0.015727852619712202\n",
      "w831 = -0.0001792355684671825\n",
      "w832 = -0.001504567957051246\n",
      "w833 = 0.0007784014602697773\n",
      "w834 = -0.01483280776501303\n",
      "w835 = -0.006945339417824317\n",
      "w836 = -0.006510142100367963\n",
      "w837 = 0.007627501477127623\n",
      "w838 = -0.0018746342128185638\n",
      "w839 = 0.0016401972417543558\n",
      "w840 = 0.006628994541864696\n",
      "w841 = 0.0009995373351782085\n",
      "w842 = -0.0008700077174238126\n",
      "w843 = 0.0031065512405672982\n",
      "w844 = -0.0063603937452428195\n",
      "w845 = 0.010005648676613329\n",
      "w846 = -0.002538219470492415\n",
      "w847 = -0.0035931433429819072\n",
      "w848 = -0.006524638613333406\n",
      "w849 = -0.008165998396335911\n",
      "w850 = -0.0015522544572537209\n",
      "w851 = 0.0024769302395151655\n",
      "w852 = 0.002599212596289723\n",
      "w853 = -0.0006275153247445951\n",
      "w854 = -0.006439853225026249\n",
      "w855 = 0.009948316957172614\n",
      "w856 = -0.0016358608286853435\n",
      "w857 = -0.002230248374707003\n",
      "w858 = -0.000509730305733045\n",
      "w859 = -0.003397491725256437\n",
      "w860 = 0.0025963487771431463\n",
      "w861 = -0.00576731078619909\n",
      "w862 = 0.011039598016166169\n",
      "w863 = -0.002989689873397392\n",
      "w864 = -0.006758638182869715\n",
      "w865 = 0.0008201927382619733\n",
      "w866 = -0.0001111223195251414\n",
      "w867 = 0.007565567963431073\n",
      "w868 = -0.0035062405958307266\n",
      "w869 = 0.006158038343655892\n",
      "w870 = 0.0019370808472260967\n",
      "w871 = 0.0012549876334417935\n",
      "w872 = 0.015991618822030446\n",
      "w873 = -0.009126576589009484\n",
      "w874 = -0.0009157910999342385\n",
      "w875 = 0.005285225723082467\n",
      "w876 = 0.007434897797137931\n",
      "w877 = 0.008064550809884594\n",
      "w878 = 0.00887582418607754\n",
      "w879 = 0.0014830937272091872\n",
      "w880 = 0.007297559117546704\n",
      "w881 = -6.882032650435213e-05\n",
      "w882 = -0.0001423598205642879\n",
      "w883 = 0.0014551136636341503\n",
      "w884 = -0.00501280468019714\n",
      "w885 = -0.0012393733785458954\n",
      "w886 = 0.0009602090951084743\n",
      "w887 = -0.001370879589556812\n",
      "w888 = -0.0012386039802858237\n",
      "w889 = -0.0019813792292704686\n",
      "w890 = -0.00487993122528505\n",
      "w891 = -0.0025260280828088726\n",
      "w892 = 0.0007422768853635122\n",
      "w893 = 0.0007103940870998973\n",
      "w894 = 0.0027347421653645524\n",
      "w895 = 0.00189669290388453\n",
      "w896 = -0.002058467321907435\n",
      "w897 = 0.001995488278748943\n",
      "w898 = -0.003319243680452602\n",
      "w899 = 0.0023887507830993543\n",
      "w900 = 0.014074543787666014\n",
      "w901 = -0.0022425521794063163\n",
      "w902 = -0.0001081653777584788\n",
      "w903 = -0.001293110340902275\n",
      "w904 = -0.0028479858404165843\n",
      "w905 = -0.004575924007897505\n",
      "w906 = 0.0019317189663031374\n",
      "w907 = 0.0013096889053331277\n",
      "w908 = 0.010640996026857549\n",
      "w909 = 0.008919372836390323\n",
      "w910 = 0.002274174128831218\n",
      "w911 = -0.000339734071891929\n",
      "w912 = 0.0016638708919887641\n",
      "w913 = -0.019839623351701304\n",
      "w914 = 0.003072281050527311\n",
      "w915 = -0.0025094183894191486\n",
      "w916 = -0.002288766756139889\n",
      "w917 = 0.022703746374564168\n",
      "w918 = 0.0022981795157995875\n",
      "w919 = -0.006233824943742506\n",
      "w920 = 0.0003869856086488454\n",
      "w921 = -0.0009154254549377489\n",
      "w922 = 0.0015961557447779762\n",
      "w923 = -0.008311352824118441\n",
      "w924 = 0.01162863454762385\n",
      "w925 = -0.0060167639032106205\n",
      "w926 = -0.009686220281857415\n",
      "w927 = 0.0041893605892954315\n",
      "w928 = -0.0006187175351175612\n",
      "w929 = 0.003120008511473264\n",
      "w930 = -0.0006783025235001794\n",
      "w931 = -0.002948067771942252\n",
      "w932 = -0.0021032755989063546\n",
      "w933 = 0.006785204865454681\n",
      "w934 = 0.0036408777033750694\n",
      "w935 = 0.004266092527652155\n",
      "w936 = 0.02245518602898149\n",
      "w937 = 0.007529138035527364\n",
      "w938 = 0.003649187582939982\n",
      "w939 = 0.008706639471619895\n",
      "w940 = -0.0023848282910925544\n",
      "w941 = 0.007497794267417996\n",
      "w942 = -0.0025816473684900695\n",
      "w943 = -0.0055689094863187625\n",
      "w944 = 0.0029348090206173026\n",
      "w945 = 0.0040898417098825576\n",
      "w946 = -0.004761764880247733\n",
      "w947 = 0.0041793498920110585\n",
      "w948 = -0.0031489103027536334\n",
      "w949 = 0.004334672565040286\n",
      "w950 = -0.003274019981878621\n",
      "w951 = 0.0007257854079950534\n",
      "w952 = -0.009257473279417389\n",
      "w953 = -0.001487111893851845\n",
      "w954 = -0.0020023883284791483\n",
      "w955 = -0.009675185813065397\n",
      "w956 = -0.008969831938887687\n",
      "w957 = -0.006375740864576084\n",
      "w958 = 0.01448233187385899\n",
      "w959 = -0.00500851479050028\n",
      "w960 = -0.002942816477971198\n",
      "w961 = -0.0009855418604978773\n",
      "w962 = 0.0010785340521286483\n",
      "w963 = -0.003850990081658903\n",
      "w964 = -0.0006276702854737265\n",
      "w965 = 0.0025775218683178436\n",
      "w966 = -0.005084425609511646\n",
      "w967 = 0.001141371506222784\n",
      "w968 = -0.002622744685198092\n",
      "w969 = -0.004883022046052834\n",
      "w970 = -0.0020213770638876347\n",
      "w971 = -0.0026085897256553385\n",
      "w972 = 0.012170918368076652\n",
      "w973 = 0.02387458515553851\n",
      "w974 = -0.002016795102465056\n",
      "w975 = 0.0010517370084266104\n",
      "w976 = 0.0024747216158059406\n",
      "w977 = 0.001076881149698129\n",
      "w978 = 0.013753824693005007\n",
      "w979 = -0.005260368617669022\n",
      "w980 = -0.003513010240741471\n",
      "w981 = 0.0005420859889992574\n",
      "w982 = 0.0011139552750218348\n",
      "w983 = 0.002451163185484742\n",
      "w984 = -0.006389799596374573\n",
      "w985 = 0.003606168899702464\n",
      "w986 = -0.0031635900006338285\n",
      "w987 = 0.0028979033742802945\n",
      "w988 = -0.007058192459416486\n",
      "w989 = 0.004225497196692639\n",
      "w990 = 0.003176401782494582\n",
      "w991 = -0.0027458226233087347\n",
      "w992 = 0.0032880402910290393\n",
      "w993 = -0.0024424649569943077\n",
      "w994 = -0.01507243016764528\n",
      "w995 = 0.0005470472184232978\n",
      "w996 = -0.005010453411177243\n",
      "w997 = 0.0028514713154526744\n",
      "w998 = -0.005877111696567093\n",
      "w999 = -0.005034459693805654\n",
      "w1000 = -0.0035206521755691365\n",
      "w1001 = -0.004707862783106576\n",
      "w1002 = -0.0061602139372146075\n",
      "w1003 = -0.004761284465661185\n",
      "w1004 = -0.003248252731671418\n",
      "w1005 = -0.006716355719212364\n",
      "w1006 = -0.0006462605360767606\n",
      "w1007 = 0.002305136931244763\n",
      "w1008 = 0.0009989940681868574\n",
      "w1009 = 0.009814588281735162\n",
      "w1010 = 0.003546895107688139\n",
      "w1011 = -0.001465014289575936\n",
      "w1012 = -0.0068108692763276385\n",
      "w1013 = 0.005529587953456513\n",
      "w1014 = -0.0019112403586360592\n",
      "w1015 = 0.006240813337748486\n",
      "w1016 = -0.0024397982106406785\n",
      "w1017 = -0.0015760028412178948\n",
      "w1018 = -0.005758242286077798\n",
      "w1019 = 0.002769168835341027\n",
      "w1020 = -0.0006015491888432734\n",
      "w1021 = 0.002886202390713321\n",
      "w1022 = -0.0015534673360640063\n",
      "w1023 = -0.00017703284893829448\n",
      "w1024 = 0.011983582375457396\n",
      "w1025 = -0.004841258426110968\n",
      "w1026 = 0.011913172142351991\n",
      "w1027 = 0.006908375480296483\n",
      "w1028 = -0.00021076240506143857\n",
      "w1029 = -0.0010740808012781214\n",
      "w1030 = -0.0005424827465045265\n",
      "w1031 = -0.010362324687747145\n",
      "w1032 = -0.002625158019397645\n",
      "w1033 = 0.003580692486437284\n",
      "w1034 = -0.00399117263237137\n",
      "w1035 = 0.004609851740486947\n",
      "w1036 = -0.0021587705975317046\n",
      "w1037 = 0.002154360341558107\n",
      "w1038 = 0.0063237510433088884\n",
      "w1039 = 0.003200143416146006\n",
      "w1040 = 0.002897914596439919\n",
      "w1041 = 0.0073480100857872535\n",
      "w1042 = 0.003025757532398534\n",
      "w1043 = -0.002454603420020813\n",
      "w1044 = -0.0032413145701941323\n",
      "w1045 = -0.00616108505533889\n",
      "w1046 = -0.004816508956311533\n",
      "w1047 = 0.00266813775895576\n",
      "w1048 = 0.00602048638949386\n",
      "w1049 = 5.67812008592036e-05\n",
      "w1050 = 0.0010201690261682228\n",
      "w1051 = 0.020501665210993718\n",
      "w1052 = 0.0005837297821381208\n",
      "w1053 = 0.005732685805746697\n",
      "w1054 = -0.0035454682487342046\n",
      "w1055 = 0.0030838735795314406\n",
      "w1056 = -0.002763068591829302\n",
      "w1057 = -0.0002237269499225267\n",
      "w1058 = -0.0050869657023890034\n",
      "w1059 = -0.0036800021396114773\n",
      "w1060 = 0.001186125896764671\n",
      "w1061 = -0.0008068175880160785\n",
      "w1062 = 0.017320745941026932\n",
      "w1063 = 0.001826738480616716\n",
      "w1064 = 0.009498946650614402\n",
      "w1065 = 0.0060904916212876\n",
      "w1066 = 0.006523123197503477\n",
      "w1067 = -0.003401840594586185\n",
      "w1068 = -0.002188465511833872\n",
      "w1069 = 0.010705380832789458\n",
      "w1070 = 0.0015825824488130988\n",
      "w1071 = 0.004743142251256702\n",
      "w1072 = 0.0017797109562800938\n",
      "w1073 = 0.0049347570473905365\n",
      "w1074 = 0.0019535011868366194\n",
      "w1075 = -0.013614835657681651\n",
      "w1076 = 0.000821105147744779\n",
      "w1077 = 0.007313270728670219\n",
      "w1078 = 0.006000871530269555\n",
      "w1079 = 0.007886182071112729\n",
      "w1080 = 0.0013418284204469737\n",
      "w1081 = 0.004647250433760274\n",
      "w1082 = -0.005838265273994868\n",
      "w1083 = 0.004770540084178549\n",
      "w1084 = -0.004409354632690429\n",
      "w1085 = 0.0028296018294964747\n",
      "w1086 = 0.020441330138899885\n",
      "w1087 = 0.0007185257519493519\n",
      "w1088 = -0.0037076060212688756\n",
      "w1089 = 0.0034715153714051225\n",
      "w1090 = -0.0010603396501752936\n",
      "w1091 = -0.0017372779224506514\n",
      "w1092 = -0.0015756484192648965\n",
      "w1093 = 0.0025606447885409126\n",
      "w1094 = -2.0172701136180387e-05\n",
      "w1095 = 0.0010447520782907547\n",
      "w1096 = -0.00208341819250997\n",
      "w1097 = 0.0016342980084877818\n",
      "w1098 = 0.0038643206812847483\n",
      "w1099 = 0.00020148455714655509\n",
      "w1100 = -0.004828291063150005\n",
      "w1101 = 0.0021653618064946495\n",
      "w1102 = -0.0038086046064804238\n",
      "w1103 = 0.004521867633719356\n",
      "w1104 = 0.00031684632877562394\n",
      "w1105 = 0.0038665123710064377\n",
      "w1106 = -0.000396345591458564\n",
      "w1107 = -0.007594485437843686\n",
      "w1108 = 0.0004895879537000391\n",
      "w1109 = -0.01048234038621705\n",
      "w1110 = 0.004519959286968189\n",
      "w1111 = 0.005674030257803948\n",
      "w1112 = 0.0010287232440557063\n",
      "w1113 = -0.00717342373305275\n",
      "w1114 = 0.001587865421711683\n",
      "w1115 = -0.016597845052572155\n",
      "w1116 = 0.005923148276697105\n",
      "w1117 = 0.017135190944776186\n",
      "w1118 = 0.006933580333434017\n",
      "w1119 = -0.0033616765529316623\n",
      "w1120 = -0.005917372994848603\n",
      "w1121 = 0.0006015525840375615\n",
      "w1122 = 0.00037377192466030466\n",
      "w1123 = -0.0024909133648105992\n",
      "w1124 = 0.001840103458023413\n",
      "w1125 = 0.004405269271729054\n",
      "w1126 = -0.0028830096561016033\n",
      "w1127 = 0.007732279662487897\n",
      "w1128 = 0.004022821921130277\n",
      "w1129 = 0.006054187711856229\n",
      "w1130 = 0.004337962935756406\n",
      "w1131 = -0.0005086266303842505\n",
      "w1132 = 0.008354200708233941\n",
      "w1133 = -0.0013878070543440852\n",
      "w1134 = 0.011287467340091565\n",
      "w1135 = 0.0012520586509946591\n",
      "w1136 = 0.003790331881550604\n",
      "w1137 = 0.0005226928991834686\n",
      "w1138 = -0.0003126480826273787\n",
      "w1139 = -0.0001711398892972222\n",
      "w1140 = -0.0012306188781118495\n",
      "w1141 = -0.007588209329106774\n",
      "w1142 = -0.00014881228656807419\n",
      "w1143 = -0.00292091417956368\n",
      "w1144 = -0.00040210120896219095\n",
      "w1145 = 0.00032458135630847264\n",
      "w1146 = 0.0003126951885132524\n",
      "w1147 = 0.0010685557963795585\n",
      "w1148 = 0.0027092941968845175\n",
      "w1149 = -0.005808510175100659\n",
      "w1150 = 0.00499607554282624\n",
      "w1151 = -0.003267883069957593\n",
      "w1152 = -0.0020227172544381136\n",
      "w1153 = -0.008821538066302781\n",
      "w1154 = 0.0010888824714823558\n",
      "w1155 = 0.0026562067625173122\n",
      "w1156 = -8.233079128892953e-05\n",
      "w1157 = 0.009979164118650945\n",
      "w1158 = 0.0008500030043704172\n",
      "w1159 = 0.00044582576838828226\n",
      "w1160 = -0.00035579202366108314\n",
      "w1161 = 0.0007160424436080966\n",
      "w1162 = -0.010084274739559376\n",
      "w1163 = 0.0013707974451661606\n",
      "w1164 = -0.006042524478310515\n",
      "w1165 = -0.0008631306512720581\n",
      "w1166 = 0.002370353217962093\n",
      "w1167 = -0.0029560857004106494\n",
      "w1168 = 0.011269347917812155\n",
      "w1169 = -0.004826911363248221\n",
      "w1170 = 0.0019378021350426996\n",
      "w1171 = -0.003624179591297589\n",
      "w1172 = 0.002842380529233727\n",
      "w1173 = -0.007291527720007433\n",
      "w1174 = 0.00020347833893591736\n",
      "w1175 = 0.002503911680055933\n",
      "w1176 = 0.0030852244011384702\n",
      "w1177 = -0.005238261293170786\n",
      "w1178 = 0.007346196697454278\n",
      "w1179 = 0.0033568272158798337\n",
      "w1180 = 0.003300345969645487\n",
      "w1181 = -0.005202496819297631\n",
      "w1182 = 0.0028203807229663794\n",
      "w1183 = -0.004875351522105937\n",
      "w1184 = 0.004782756193754161\n",
      "w1185 = -0.0027638456215720885\n",
      "w1186 = 0.0035899927690203333\n",
      "w1187 = -0.0027937825310165474\n",
      "w1188 = 0.0016848737284830212\n",
      "w1189 = -0.0009163051550086901\n",
      "w1190 = 0.006770492582632379\n",
      "w1191 = -0.0010022646205846077\n",
      "w1192 = 0.0009318311037556324\n",
      "w1193 = 0.002028347817331018\n",
      "w1194 = -0.0007562276710457577\n",
      "w1195 = -0.0018852304492699597\n",
      "w1196 = -0.0010590308662046305\n",
      "w1197 = -0.011423359164450423\n",
      "w1198 = -0.005148351795763689\n",
      "w1199 = -0.0006874100765316388\n",
      "w1200 = 0.0031674307616519994\n",
      "w1201 = -0.018258060856373756\n",
      "w1202 = 0.002558579625869615\n",
      "w1203 = 0.0029488794158703583\n",
      "w1204 = 0.01085424887056406\n",
      "w1205 = 0.005426424227236728\n",
      "w1206 = -0.002299652875182799\n",
      "w1207 = 0.003794522696645281\n",
      "w1208 = 0.005198164860376057\n",
      "w1209 = 0.0011461924344647397\n",
      "w1210 = 0.007622686599158211\n",
      "w1211 = 0.001115519082298002\n",
      "w1212 = 0.006196744541492489\n",
      "w1213 = 0.0008451065643555536\n",
      "w1214 = 0.010481571941498161\n",
      "w1215 = 0.00011457121330210428\n",
      "w1216 = -0.0014807350097637273\n",
      "w1217 = -0.0013724354390420009\n",
      "w1218 = 0.006119944308059672\n",
      "w1219 = 0.004530145905574489\n",
      "w1220 = 0.0006293600323792204\n",
      "w1221 = -0.003914449374567597\n",
      "w1222 = 0.004947562836312584\n",
      "w1223 = 0.005764783101991167\n",
      "w1224 = 0.006521305621770301\n",
      "w1225 = -0.014006840931252202\n",
      "w1226 = -0.00041161213281768936\n",
      "w1227 = -0.004798225923634459\n",
      "w1228 = 0.005319731076661891\n",
      "w1229 = 0.007828358141597507\n",
      "w1230 = -0.008028345451956392\n",
      "w1231 = -0.0012088874849057272\n",
      "w1232 = 0.008790971611752487\n",
      "w1233 = 0.001187693624461052\n",
      "w1234 = -0.011959570020105685\n",
      "w1235 = -0.0003061308850693591\n",
      "w1236 = 0.001711459696733738\n",
      "w1237 = -0.004165623207775664\n",
      "w1238 = 0.0008530844851108293\n",
      "w1239 = -0.006304046887526454\n",
      "w1240 = 0.0018400383516998947\n",
      "w1241 = -0.0020997990916851185\n",
      "w1242 = -0.003514072614393207\n",
      "w1243 = -0.0033443955460788536\n",
      "w1244 = -0.013276004001680917\n",
      "w1245 = -0.005358542119266374\n",
      "w1246 = 0.00980078604875307\n",
      "w1247 = 0.0048992557632494835\n",
      "w1248 = -0.002530734392904063\n",
      "w1249 = -0.00972602934113843\n",
      "w1250 = -0.004531253432814301\n",
      "w1251 = 0.0038120479577515307\n",
      "w1252 = -0.00756962282041413\n",
      "w1253 = -0.008305553570992783\n",
      "w1254 = -0.0028470482965364465\n",
      "w1255 = 0.004093041768022423\n",
      "w1256 = 0.007557538185374335\n",
      "w1257 = 0.010700031893623865\n",
      "w1258 = 0.000965910714660674\n",
      "w1259 = 0.0011472071398583026\n",
      "w1260 = 0.00048122062677732647\n",
      "w1261 = -0.0032952393717107626\n",
      "w1262 = 0.0009346853506325574\n",
      "w1263 = -0.01218502782272121\n",
      "w1264 = 0.0016262637245018944\n",
      "w1265 = 0.012620150330397279\n",
      "w1266 = -0.00021527907118396193\n",
      "w1267 = -0.008031966500249593\n",
      "w1268 = -0.01172272126317203\n",
      "w1269 = -0.004874714110909771\n",
      "w1270 = 0.0034611641961408745\n",
      "w1271 = 0.010614549821690522\n",
      "w1272 = -0.007944218767706814\n",
      "w1273 = -0.006232086870125524\n",
      "w1274 = 0.010447839560389069\n",
      "w1275 = 0.010871474750497996\n",
      "w1276 = -0.004117642009437287\n",
      "w1277 = -0.002738754794786033\n",
      "w1278 = -0.015815419831728596\n",
      "w1279 = 0.002138440349037001\n",
      "w1280 = 0.006301295303505402\n",
      "w1281 = -0.0033815826732556033\n",
      "w1282 = -0.0005214125396533598\n",
      "w1283 = -0.0003157126937789143\n",
      "w1284 = 0.0016495619465316696\n",
      "w1285 = -0.003429022770610324\n",
      "w1286 = 0.0031479471792069686\n",
      "w1287 = 0.004578295520922594\n",
      "w1288 = 0.006693340000309865\n",
      "w1289 = -0.0016067705390264402\n",
      "w1290 = -0.003130189512664837\n",
      "w1291 = 0.0011768058070183557\n",
      "w1292 = 0.005514921360861428\n",
      "w1293 = -0.001108681410198058\n",
      "w1294 = -0.002798345134499541\n",
      "w1295 = -0.006753669853656672\n",
      "w1296 = -0.0017652810959508658\n",
      "w1297 = -0.002867984245164159\n",
      "w1298 = -0.0013412092476193355\n",
      "w1299 = -8.076275602464752e-05\n",
      "w1300 = 0.0031753044540950553\n",
      "w1301 = 0.004248301533843845\n",
      "w1302 = 0.009340789213475845\n",
      "w1303 = -0.009589524669666399\n",
      "w1304 = 0.008080334714217983\n",
      "w1305 = 0.007936719003377859\n",
      "w1306 = 0.0031960072477320365\n",
      "w1307 = -0.0007302414768868593\n",
      "w1308 = -0.002486942129115543\n",
      "w1309 = 0.007127383438633103\n",
      "w1310 = -0.0001522224532805791\n",
      "w1311 = -0.00028170734620790606\n",
      "w1312 = -0.0002549878109990753\n",
      "w1313 = 0.007471022592193599\n",
      "w1314 = 0.0021921030777102544\n",
      "w1315 = -0.0017852017667335086\n",
      "w1316 = -0.0001562966092467381\n",
      "w1317 = 0.00045893189619193516\n",
      "w1318 = 0.00613188216715298\n",
      "w1319 = 0.0009211517477128978\n",
      "w1320 = -0.014519982353211901\n",
      "w1321 = 0.000631332850659767\n",
      "w1322 = -0.004535847408239042\n",
      "w1323 = -0.006629851372815942\n",
      "w1324 = -0.001878025058150354\n",
      "w1325 = 0.0027959883789478927\n",
      "w1326 = -0.0036612703279551738\n",
      "w1327 = 0.0018191856130432644\n",
      "w1328 = 0.0008311823727954514\n",
      "w1329 = 0.0007711419425684961\n",
      "w1330 = -0.0017876497543215255\n",
      "w1331 = -0.0023214095670146553\n",
      "w1332 = 0.0020243714242054282\n",
      "w1333 = 0.0036875326881915965\n",
      "w1334 = 0.0038109965676084846\n",
      "w1335 = 0.009474363936922026\n",
      "w1336 = -0.002387423064614225\n",
      "w1337 = -0.015671071365448958\n",
      "w1338 = -0.002938094602541312\n",
      "w1339 = 0.0031850326793875796\n",
      "w1340 = -0.007643305410280402\n",
      "w1341 = -0.0026830127738451223\n",
      "w1342 = -0.0036640524222812843\n",
      "w1343 = 0.0017061052587730343\n",
      "w1344 = -0.008343478230892218\n",
      "w1345 = 0.0034453595255246696\n",
      "w1346 = 0.0026538371963807767\n",
      "w1347 = -0.0006123134952040161\n",
      "w1348 = -0.0011067567385642668\n",
      "w1349 = 0.004246329299157122\n",
      "w1350 = -0.008113313324251983\n",
      "w1351 = -0.00799606578726629\n",
      "w1352 = -0.0016873584323562302\n",
      "w1353 = 0.004141032443157517\n",
      "w1354 = -0.0018027425081448265\n",
      "w1355 = -0.001222073853927107\n",
      "w1356 = -0.01027192300747884\n",
      "w1357 = -0.0011952706006370336\n",
      "w1358 = 0.0006920327641800898\n",
      "w1359 = -0.000671277214264687\n",
      "w1360 = 0.01575330569135556\n",
      "w1361 = 0.0064957406532965825\n",
      "w1362 = -0.004323411851930608\n",
      "w1363 = -0.0008662711354760534\n",
      "w1364 = 0.0031033018525320278\n",
      "w1365 = 0.0004548759132539897\n",
      "w1366 = 0.004423875542245036\n",
      "w1367 = -0.0026386269758719447\n",
      "w1368 = 0.004057390923255599\n",
      "w1369 = 0.0008134243291550691\n",
      "w1370 = -0.004006387816756759\n",
      "w1371 = -0.007671239352197044\n",
      "w1372 = 0.008637227424447589\n",
      "w1373 = -0.004446123142622896\n",
      "w1374 = -0.0006979728999213327\n",
      "w1375 = -0.0023272019132637377\n",
      "w1376 = -0.0016169344019536667\n",
      "w1377 = -0.0003786992853881033\n",
      "w1378 = 0.0012363340607628689\n",
      "w1379 = -0.0009700372240782996\n",
      "w1380 = 0.0015750498566357955\n",
      "w1381 = 0.006658409371544788\n",
      "w1382 = 0.004671810828929658\n",
      "w1383 = -0.0029056199275167014\n",
      "w1384 = -0.005137032356738806\n",
      "w1385 = 0.0005885002126798701\n",
      "w1386 = -0.006352216816315773\n",
      "w1387 = -0.0009081341253301449\n",
      "w1388 = -9.019881811889924e-05\n",
      "w1389 = -0.03156925225967255\n",
      "w1390 = -0.0003047911236752528\n",
      "w1391 = -0.014293256749113222\n",
      "w1392 = -0.0008653904844243742\n",
      "w1393 = 0.0005504244489967629\n",
      "w1394 = 0.001652340109714249\n",
      "w1395 = -0.001692801043325697\n",
      "w1396 = 0.0014963539373468245\n",
      "w1397 = -0.0021319967971495557\n",
      "w1398 = -0.0029157622299198238\n",
      "w1399 = 0.00745737125285753\n",
      "w1400 = 0.015923938099681623\n",
      "w1401 = -0.002301131255584394\n",
      "w1402 = 0.0031921622696470704\n",
      "w1403 = 0.00709808704824845\n",
      "w1404 = 0.00011830986590212391\n",
      "w1405 = -0.005594975559957444\n",
      "w1406 = -0.00020132134151677907\n",
      "w1407 = -0.004468230461149823\n",
      "w1408 = -0.001959546988549495\n",
      "w1409 = 0.0007860677611854956\n",
      "w1410 = 8.920904266120002e-05\n",
      "w1411 = -0.0002908760552379929\n",
      "w1412 = -0.004374637255047783\n",
      "w1413 = -0.0030492422792063392\n",
      "w1414 = -0.00012699351785199075\n",
      "w1415 = 0.00289575770740667\n",
      "w1416 = -0.0016074321381789344\n",
      "w1417 = -0.001248509423290852\n",
      "w1418 = -0.0027231048955435903\n",
      "w1419 = -0.0035827235090113334\n",
      "w1420 = -0.003851077161264165\n",
      "w1421 = -0.0014991316235109587\n",
      "w1422 = 0.0033928038576048437\n",
      "w1423 = -0.0025930135460160734\n",
      "w1424 = 0.0028153253353267647\n",
      "w1425 = 0.0006342499852121092\n",
      "w1426 = -0.0040936863355314585\n",
      "w1427 = 0.0013484040995719176\n",
      "w1428 = -0.0008754157015554994\n",
      "w1429 = -0.005696408285054257\n",
      "w1430 = 0.0020981633270016186\n",
      "w1431 = -0.004426226991540702\n",
      "w1432 = 0.005110167459243193\n",
      "w1433 = 0.0007569400114011871\n",
      "w1434 = -0.001415907120088124\n",
      "w1435 = 0.0016190336630012244\n",
      "w1436 = 0.002467817431170471\n",
      "w1437 = 9.537670827903258e-05\n",
      "w1438 = 0.005199979874828879\n",
      "w1439 = 0.005964454431146663\n",
      "w1440 = 0.00442947937086044\n",
      "w1441 = 0.004097145213678773\n",
      "w1442 = -0.0010468799545066817\n",
      "w1443 = 0.004543546929068288\n",
      "w1444 = 0.0016560069690585457\n",
      "w1445 = -0.001662471963729201\n",
      "w1446 = 0.0012176872914484184\n",
      "w1447 = -0.005424991148369102\n",
      "w1448 = -0.004549964980711349\n",
      "w1449 = 0.003372665673423795\n",
      "w1450 = -0.00419377385802503\n",
      "w1451 = 0.004854058894015643\n",
      "w1452 = -0.006487564429533221\n",
      "w1453 = -4.047991957423631e-05\n",
      "w1454 = -0.0018973736222167934\n",
      "w1455 = -0.0017370149273350712\n",
      "w1456 = -0.0027607612561859622\n",
      "w1457 = -0.009729879736641635\n",
      "w1458 = -0.013684781834755053\n",
      "w1459 = -0.0003945452978804908\n",
      "w1460 = 0.0008799098794209949\n",
      "w1461 = 0.0028399369064195665\n",
      "w1462 = 0.0009287406490904664\n",
      "w1463 = -0.00036846622343584504\n",
      "w1464 = 0.0019348477216074847\n",
      "w1465 = -0.002773250469864504\n",
      "w1466 = -0.0010849320227353508\n",
      "w1467 = 0.0020307256740273983\n",
      "w1468 = -0.005123614034589381\n",
      "w1469 = 0.001285125971954047\n",
      "w1470 = -0.01043481704875002\n",
      "w1471 = -0.0022073202618745915\n",
      "w1472 = 0.00015575917976305627\n",
      "w1473 = 0.000564354474798008\n",
      "w1474 = 0.00013729575899822273\n",
      "w1475 = -0.0072341638181104735\n",
      "w1476 = -0.001291880022316167\n",
      "w1477 = 0.009179525018230257\n",
      "w1478 = -0.001871283528410307\n",
      "w1479 = 0.0007958538177458128\n",
      "w1480 = -0.009829832303564552\n",
      "w1481 = 0.01115536602279622\n",
      "w1482 = 0.0014994933611407189\n",
      "w1483 = 0.004755813257656538\n",
      "w1484 = -0.004463651277274119\n",
      "w1485 = -0.0018811778652487892\n",
      "w1486 = 0.0034291738458868126\n",
      "w1487 = -0.0014117091728676147\n",
      "w1488 = -0.0013485763946577516\n",
      "w1489 = -0.0006428856107058409\n",
      "w1490 = 0.00044088215238100736\n",
      "w1491 = -0.0008082770407684597\n",
      "w1492 = -0.00015700238510280495\n",
      "w1493 = 0.002128492717022798\n",
      "w1494 = -0.003081470745785625\n",
      "w1495 = -0.001670886114067374\n",
      "w1496 = -0.00038201808927080134\n",
      "w1497 = 0.003573816040542981\n",
      "w1498 = 0.0026291767943461193\n",
      "w1499 = -0.0019076816999265272\n",
      "w1500 = 0.00503706342715342\n",
      "w1501 = -0.0006210823221592042\n",
      "w1502 = 0.003224788062260145\n",
      "w1503 = -0.0056546029243545355\n",
      "w1504 = 0.006282415794974666\n",
      "w1505 = -0.009486815691578247\n",
      "w1506 = -0.00433409653938096\n",
      "w1507 = 0.004953775135013385\n",
      "w1508 = 0.0009322272103375369\n",
      "w1509 = -0.005571134417883988\n",
      "w1510 = 0.007124859557262254\n",
      "w1511 = -0.0021059204939499862\n",
      "w1512 = 0.00658899273350277\n",
      "w1513 = 0.004845118536579903\n",
      "w1514 = 0.0014451434433038215\n",
      "w1515 = -0.0019069523561828152\n",
      "w1516 = 0.0023634172752950784\n",
      "w1517 = 0.007601122862968911\n",
      "w1518 = -0.0012508075553229027\n",
      "w1519 = 0.024369614888520137\n",
      "w1520 = 0.0014753602644673029\n",
      "w1521 = -0.005478833463682481\n",
      "w1522 = -0.002363534272417529\n",
      "w1523 = -0.0021852897083036314\n",
      "w1524 = -0.008590736773006298\n",
      "w1525 = 0.0011741120433871413\n",
      "w1526 = -0.008188692792104007\n",
      "w1527 = -0.0003585968539027646\n",
      "w1528 = 0.0014168423436227697\n",
      "w1529 = 0.001250010800838068\n",
      "w1530 = 0.004096594459602421\n",
      "w1531 = 0.0012589538320076864\n",
      "w1532 = -0.001714892310237375\n",
      "w1533 = -0.00991976663550393\n",
      "w1534 = 0.0038603580736982003\n",
      "w1535 = 0.0015274495594697851\n",
      "w1536 = 0.008987438317488949\n",
      "w1537 = -0.002721385813435022\n",
      "w1538 = -0.0003062316080635642\n",
      "w1539 = -0.0024689645694841206\n",
      "w1540 = -0.002628706827702588\n",
      "w1541 = -0.008846305786020187\n",
      "w1542 = 0.0014474757558395257\n",
      "w1543 = -0.0019070379554312347\n",
      "w1544 = -0.0010340564049509254\n",
      "w1545 = 0.001642697691008035\n",
      "w1546 = -0.010157411787058698\n",
      "w1547 = 0.0009283997675977873\n",
      "w1548 = 0.002512273981559777\n",
      "w1549 = 0.0042008483873059894\n",
      "w1550 = 0.0032347503973331063\n",
      "w1551 = -0.0014922838991236317\n",
      "w1552 = -0.0017271196431828087\n",
      "w1553 = 0.0018079384166624667\n",
      "w1554 = 0.0058301368787321345\n",
      "w1555 = -0.0044606988829591285\n",
      "w1556 = -0.0028283375163445676\n",
      "w1557 = 0.002773454625495496\n",
      "w1558 = 0.0009166773891715685\n",
      "w1559 = -0.0017252420151355134\n",
      "w1560 = 0.005716660148616101\n",
      "w1561 = 0.001336719715224105\n",
      "w1562 = 0.01749352865046957\n",
      "w1563 = -0.0006991765852132081\n",
      "w1564 = 0.003133098869870077\n",
      "w1565 = 0.006148172924469167\n",
      "w1566 = 0.0028244095658897543\n",
      "w1567 = 0.001076262332082361\n",
      "w1568 = -0.00039923548501705544\n",
      "w1569 = -0.001093327282649454\n",
      "w1570 = -0.00020906068826308417\n",
      "w1571 = -0.0028592036447200398\n",
      "w1572 = 0.004024784251156816\n",
      "w1573 = -0.0011989201974787092\n",
      "w1574 = -0.0023058746935235713\n",
      "w1575 = -0.0002667436288506539\n",
      "w1576 = 0.005390889841790135\n",
      "w1577 = -0.002866618754325583\n",
      "w1578 = -0.004788160284752681\n",
      "w1579 = -0.004588708172104286\n",
      "w1580 = 0.0006232245529783299\n",
      "w1581 = 0.00031561631041536063\n",
      "w1582 = -0.0006261470916572595\n",
      "w1583 = -0.003155981564006095\n",
      "w1584 = 0.000996341074770665\n",
      "w1585 = -0.004313523041892201\n",
      "w1586 = -0.0039663494336980385\n",
      "w1587 = -0.0014590220015357719\n",
      "w1588 = 0.008681505374651809\n",
      "w1589 = 6.536177906443636e-05\n",
      "w1590 = 0.010968148646275778\n",
      "w1591 = -4.355952977237567e-05\n",
      "w1592 = -0.002548602883175501\n",
      "w1593 = -0.002466594546682169\n",
      "w1594 = 0.007146370822068698\n",
      "w1595 = 0.002161084180181389\n",
      "w1596 = -0.0007245946464539375\n",
      "w1597 = 0.004702089190376685\n",
      "w1598 = 0.006254401720746416\n",
      "w1599 = 4.871919681592504e-06\n",
      "w1600 = 0.0011843607441481498\n",
      "w1601 = -0.006495615215636718\n",
      "w1602 = -0.0057788914366758036\n",
      "w1603 = -0.0030597603512417595\n",
      "w1604 = -0.001027902481252678\n",
      "w1605 = 0.012167407253942655\n",
      "w1606 = -0.004090980831851028\n",
      "w1607 = 0.014357082669415143\n",
      "w1608 = 0.001851180377451382\n",
      "w1609 = 0.0013826744067678414\n",
      "w1610 = 0.006593747348980233\n",
      "w1611 = -0.00996038309254656\n",
      "w1612 = 0.0007274080803567095\n",
      "w1613 = -0.0014916796688588018\n",
      "w1614 = 0.00201823500502951\n",
      "w1615 = -0.0009397277352330197\n",
      "w1616 = 0.0067897459914929795\n",
      "w1617 = -0.0013542230589189558\n",
      "w1618 = -0.005114671702397915\n",
      "w1619 = -0.0012673064794296441\n",
      "w1620 = 0.006568319313734684\n",
      "w1621 = 0.011644154704552245\n",
      "w1622 = 0.0014568554376997943\n",
      "w1623 = 0.0003645984178590132\n",
      "w1624 = -0.005681068314320178\n",
      "w1625 = 0.0001638071870431881\n",
      "w1626 = -0.00980795957363387\n",
      "w1627 = -0.005775657876705209\n",
      "w1628 = 0.002325035333957231\n",
      "w1629 = 0.0004136024873068971\n",
      "w1630 = -0.002118022451379354\n",
      "w1631 = 0.005061921871091325\n",
      "w1632 = 0.004461896700538671\n",
      "w1633 = -0.0007508402518289072\n",
      "w1634 = 0.0009203691363445877\n",
      "w1635 = 0.0035175384858326035\n",
      "w1636 = 0.0009503754138042531\n",
      "w1637 = 0.004334045871660193\n",
      "w1638 = -0.001943348358075545\n",
      "w1639 = 0.002918237520300897\n",
      "w1640 = -0.0025660886739592706\n",
      "w1641 = -0.004395000420601552\n",
      "w1642 = -0.0028753157608158767\n",
      "w1643 = -0.0035648429528741103\n",
      "w1644 = 0.00823862685049874\n",
      "w1645 = -0.005551082237459287\n",
      "w1646 = 0.0010916719519493948\n",
      "w1647 = -0.00315284192366784\n",
      "w1648 = 0.0011396542091922084\n",
      "w1649 = -0.0024833279030081487\n",
      "w1650 = 0.0019865822463287965\n",
      "w1651 = 0.0013608628926648973\n",
      "w1652 = 0.005825071835723801\n",
      "w1653 = -0.0014999460822938942\n",
      "w1654 = 0.0022944216151554752\n",
      "w1655 = -0.007084033138689731\n",
      "w1656 = 0.0026284245383431364\n",
      "w1657 = -0.006623126255169051\n",
      "w1658 = 0.007006924031665071\n",
      "w1659 = 0.0020909680559781955\n",
      "w1660 = -0.009747903179056988\n",
      "w1661 = -0.006041675356431222\n",
      "w1662 = 0.006889587962387798\n",
      "w1663 = 0.002153948944246268\n",
      "w1664 = -0.0060772902680997385\n",
      "w1665 = -0.0020500912419032613\n",
      "w1666 = -0.0061180392896726605\n",
      "w1667 = -0.0031192686517648696\n",
      "w1668 = 0.0026526760257990674\n",
      "w1669 = 0.0067607121870973215\n",
      "w1670 = 0.0010380220757806112\n",
      "w1671 = -0.0015956044608471543\n",
      "w1672 = 0.0006364176264725752\n",
      "w1673 = 0.00083568561149391\n",
      "w1674 = -0.005885203968200944\n",
      "w1675 = -0.012780547404111034\n",
      "w1676 = 0.0012237266307957208\n",
      "w1677 = 0.016143491339687035\n",
      "w1678 = 0.006683740265659829\n",
      "w1679 = 0.004212532142399566\n",
      "w1680 = 0.004989705430831466\n",
      "w1681 = 0.001818262897239488\n",
      "w1682 = 0.001440970268333038\n",
      "w1683 = -0.0005366082497134237\n",
      "w1684 = 0.007186727059925837\n",
      "w1685 = 0.004948896741667998\n",
      "w1686 = -0.005951270794998237\n",
      "w1687 = 0.00577561202284795\n",
      "w1688 = -0.002383631002465187\n",
      "w1689 = 0.002691844014876775\n",
      "w1690 = 0.0008251009395844413\n",
      "w1691 = -0.008544169415332982\n",
      "w1692 = 0.003498378172409442\n",
      "w1693 = 0.006645523347116721\n",
      "w1694 = -0.00560176807267241\n",
      "w1695 = -0.002468049954888249\n",
      "w1696 = 0.005167770122162341\n",
      "w1697 = 0.031663224224603055\n",
      "w1698 = -0.0018538507163238663\n",
      "w1699 = 0.006041361917472678\n",
      "w1700 = 0.003586151475689736\n",
      "w1701 = -0.0021145035294000063\n",
      "w1702 = 0.00044094310594501265\n",
      "w1703 = 0.0016761852325441037\n",
      "w1704 = -0.0048220258383734815\n",
      "w1705 = -0.008832429288260151\n",
      "w1706 = -0.0002563259920192132\n",
      "w1707 = -0.013144898223352025\n",
      "w1708 = -0.0017984580649325764\n",
      "w1709 = 0.002733060928128233\n",
      "w1710 = 0.0007735206101790051\n",
      "w1711 = -0.006889242993255348\n",
      "w1712 = -0.0016124984855834564\n",
      "w1713 = 0.004999433992631612\n",
      "w1714 = 0.0072404897887246794\n",
      "w1715 = 0.00350726339276685\n",
      "w1716 = 0.007494744495807509\n",
      "w1717 = -0.0013917388433553838\n",
      "w1718 = 0.0017835006515044788\n",
      "w1719 = 0.00106752914192612\n",
      "w1720 = 0.003968722109951555\n",
      "w1721 = 9.966355684240914e-05\n",
      "w1722 = 0.00163250050018307\n",
      "w1723 = 0.0004379498428441204\n",
      "w1724 = 0.007648819150801831\n",
      "w1725 = -0.003250314558903108\n",
      "w1726 = 0.0020957060627667745\n",
      "w1727 = -0.0024268264558275675\n",
      "w1728 = 0.005781840146178531\n",
      "w1729 = -0.0005875589367258054\n",
      "w1730 = 0.0032313180510546258\n",
      "w1731 = 0.004294044949305428\n",
      "w1732 = 0.0024981334753680436\n",
      "w1733 = -0.001398454528616943\n",
      "w1734 = 0.0027852820289336307\n",
      "w1735 = -0.004303666810084926\n",
      "w1736 = 0.007876446630362889\n",
      "w1737 = -0.005040750940012393\n",
      "w1738 = 0.00234730488269738\n",
      "w1739 = -0.0013128092794528641\n",
      "w1740 = 0.005276958772986596\n",
      "w1741 = -0.0077546298579132715\n",
      "w1742 = 0.004750176952041247\n",
      "w1743 = -0.010692445370837808\n",
      "w1744 = 0.00350699045496469\n",
      "w1745 = -0.004000693156122256\n",
      "w1746 = -0.0031972230644608133\n",
      "w1747 = -0.008035734441974963\n",
      "w1748 = 0.014531766533273777\n",
      "w1749 = 0.009470542020852688\n",
      "w1750 = -0.005212952626682317\n",
      "w1751 = -0.006168560178043715\n",
      "w1752 = -0.004277273438231624\n",
      "w1753 = 0.0014169235821478872\n",
      "w1754 = 0.004716672756487155\n",
      "w1755 = 0.00557363613060861\n",
      "w1756 = -0.008451862598983728\n",
      "w1757 = -0.006465728483250049\n",
      "w1758 = -0.0012602521127832963\n",
      "w1759 = 0.010886257169137091\n",
      "w1760 = -0.008531273797975938\n",
      "w1761 = 0.0032510667800085625\n",
      "w1762 = 0.00368111423630803\n",
      "w1763 = -0.014439507854993624\n",
      "w1764 = 0.0013738670472351626\n",
      "w1765 = 0.00026458802095341983\n",
      "w1766 = 0.008180186172006325\n",
      "w1767 = -0.011959898703300515\n",
      "w1768 = -0.0012991485004538734\n",
      "w1769 = 0.005005081568705272\n",
      "w1770 = -0.004693288958772028\n",
      "w1771 = 0.002548657581780485\n",
      "w1772 = 0.000647938587834359\n",
      "w1773 = -0.0023049354663157904\n",
      "w1774 = 0.001997934891182961\n",
      "w1775 = 0.007606310182536033\n",
      "w1776 = -0.000905611222564939\n",
      "w1777 = -0.0005333892395735285\n",
      "w1778 = 0.0033425416099322107\n",
      "w1779 = 0.002311696969487558\n",
      "w1780 = 0.009563592274610126\n",
      "w1781 = -0.0032202747312890384\n",
      "w1782 = -0.011315145024714311\n",
      "w1783 = 0.00036771260863075367\n",
      "w1784 = -0.0037957470820349994\n",
      "w1785 = -0.0030480536436239517\n",
      "w1786 = -0.001480355684781735\n",
      "w1787 = 0.0008976679492932163\n",
      "w1788 = -0.00725303216072026\n",
      "w1789 = 0.003101489728182878\n",
      "w1790 = -0.003760387859887766\n",
      "w1791 = -0.008479272983646459\n",
      "w1792 = -0.0006137745779903497\n",
      "w1793 = -0.0023785599849454413\n",
      "w1794 = -0.025342558903707763\n",
      "w1795 = -0.012355173010729416\n",
      "w1796 = -0.00048209780977364155\n",
      "w1797 = 0.004478564722559148\n",
      "w1798 = -0.007865642125285977\n",
      "w1799 = 0.003631777269462218\n",
      "w1800 = -0.0036516387487077005\n",
      "w1801 = 0.0028833410717351055\n",
      "w1802 = 0.009875921488905873\n",
      "w1803 = -0.0038565904610517294\n",
      "w1804 = 0.0030061877728195254\n",
      "w1805 = -0.0007783781511132678\n",
      "w1806 = 0.0006461831657871784\n",
      "w1807 = -0.00126516196977689\n",
      "w1808 = -0.004438470734296594\n",
      "w1809 = -0.0030054719370278195\n",
      "w1810 = 0.0052954613654343004\n",
      "w1811 = -0.0006824343301085156\n",
      "w1812 = -0.002048387880471636\n",
      "w1813 = 0.000998874018900142\n",
      "w1814 = 0.007946211102418466\n",
      "w1815 = -0.0085000663425612\n",
      "w1816 = 0.007617709575190205\n",
      "w1817 = -0.0020575393994978057\n",
      "w1818 = -0.004294223103111968\n",
      "w1819 = -0.006559215528497731\n",
      "w1820 = 0.0038913083497110683\n",
      "w1821 = 0.002275495202443053\n",
      "w1822 = -0.01231289818493055\n",
      "w1823 = -0.0050845268239285745\n",
      "w1824 = 0.005271001056996544\n",
      "w1825 = -0.003169966583877786\n",
      "w1826 = 0.0011082167386538275\n",
      "w1827 = -0.010066852112802385\n",
      "w1828 = -0.003798044779250401\n",
      "w1829 = -0.006086677515787761\n",
      "w1830 = -0.012374584516665054\n",
      "w1831 = 0.007087213750185878\n",
      "w1832 = 0.019027750825576385\n",
      "w1833 = 0.0049423712277434995\n",
      "w1834 = 0.0002450258949951208\n",
      "w1835 = 0.005708574944337876\n",
      "w1836 = 0.0013599075154463191\n",
      "w1837 = 0.0030402955649937863\n",
      "w1838 = -0.0009855565889955412\n",
      "w1839 = -0.002136257434727337\n",
      "w1840 = -0.0008425900329529937\n",
      "w1841 = -0.013126467461740954\n",
      "w1842 = -0.00027628101605250136\n",
      "w1843 = 0.004748337268427223\n",
      "w1844 = 0.00846779663225171\n",
      "w1845 = -0.006630603436149016\n",
      "w1846 = 0.004762233056178809\n",
      "w1847 = 0.009912731942885495\n",
      "w1848 = 0.0015572788669314776\n",
      "w1849 = 0.003341258199590342\n",
      "w1850 = 0.00491997495908111\n",
      "w1851 = 0.0009397857466273908\n",
      "w1852 = 0.006070482391784042\n",
      "w1853 = 0.001596510934650968\n",
      "w1854 = -0.007974170540310315\n",
      "w1855 = 0.00020459520066841197\n",
      "w1856 = 0.004047359481267293\n",
      "w1857 = 0.0016277659621609516\n",
      "w1858 = -0.002315259482140665\n",
      "w1859 = 0.0008525436607097502\n",
      "w1860 = 0.0034226801349953842\n",
      "w1861 = -0.007391119901010893\n",
      "w1862 = 0.0018008621030188487\n",
      "w1863 = -0.0024864759219814384\n",
      "w1864 = 0.0033685542811565787\n",
      "w1865 = 0.0011977942963110055\n",
      "w1866 = 0.030841503472394678\n",
      "w1867 = -0.0005020316370756097\n",
      "w1868 = 0.0008230574179828622\n",
      "w1869 = 0.0010513466215096097\n",
      "w1870 = 0.007881542138790533\n",
      "w1871 = 0.003010009558296305\n",
      "w1872 = -0.0028141242583372473\n",
      "w1873 = 0.0009796009828828996\n",
      "w1874 = -0.0029904006655282534\n",
      "w1875 = 0.009851744277589313\n",
      "w1876 = 0.009289332481766317\n",
      "w1877 = -0.0029278339874701004\n",
      "w1878 = 0.002602734895754847\n",
      "w1879 = -0.0005961201283916822\n",
      "w1880 = 0.004901722719946355\n",
      "w1881 = -0.003180054645875117\n",
      "w1882 = -0.007724137949905892\n",
      "w1883 = 0.002813549809516678\n",
      "w1884 = 0.0006332498529076131\n",
      "w1885 = -0.0030872837006873377\n",
      "w1886 = -0.003762092292336903\n",
      "w1887 = -0.002935856111306005\n",
      "w1888 = -0.00028827634483865846\n",
      "w1889 = 0.002745715211455644\n",
      "w1890 = -0.002652334531314269\n",
      "w1891 = 0.006135883872518399\n",
      "w1892 = 0.004738202471830231\n",
      "w1893 = -0.016602594686926573\n",
      "w1894 = 0.007849513479785863\n",
      "w1895 = 0.0014809075463756778\n",
      "w1896 = 0.005804954943751326\n",
      "w1897 = -0.002876861162746342\n",
      "w1898 = 0.0026550441507256868\n",
      "w1899 = -0.00037545872060501344\n",
      "w1900 = -0.0008693273217009762\n",
      "w1901 = -0.0017984812582172416\n",
      "w1902 = 0.00908368440907008\n",
      "w1903 = -0.003231701408934601\n",
      "w1904 = 0.002171118577911479\n",
      "w1905 = -0.010368236266048993\n",
      "w1906 = -0.0038949197885914774\n",
      "w1907 = 0.00034201907508763554\n",
      "w1908 = -0.0038743453617093282\n",
      "w1909 = 0.0020447176484052546\n",
      "w1910 = -0.01147149974217445\n",
      "w1911 = -0.0022206803448375375\n",
      "w1912 = -0.0018411167944647087\n",
      "w1913 = -0.00046187477200682867\n",
      "w1914 = 0.009909662389250897\n",
      "w1915 = -0.0032405100076948207\n",
      "w1916 = 0.006574196322763085\n",
      "w1917 = 0.003384934713141338\n",
      "w1918 = -0.004565514783848679\n",
      "w1919 = 0.006714537158140895\n",
      "w1920 = 0.0044946395866383515\n",
      "w1921 = -0.0013409512302876086\n",
      "w1922 = 0.01574311271359445\n",
      "w1923 = 0.005916067820839725\n",
      "w1924 = 0.002310499657433147\n",
      "w1925 = -0.008633561783801215\n",
      "w1926 = -0.008767983642600667\n",
      "w1927 = -0.0031864792071924775\n",
      "w1928 = -0.006196655801544452\n",
      "w1929 = -0.0022467874847752266\n",
      "w1930 = -0.0039017962733156127\n",
      "w1931 = -0.010582882008985786\n",
      "w1932 = -0.0019890530151983757\n",
      "w1933 = 0.0022719992744194752\n",
      "w1934 = -0.004840592219790784\n",
      "w1935 = -0.00711817286079296\n",
      "w1936 = 0.013176893224201862\n",
      "w1937 = 0.0007867646028737827\n",
      "w1938 = -0.008387706133677428\n",
      "w1939 = -0.0007624209039325295\n",
      "w1940 = 0.0032979306977757787\n",
      "w1941 = 0.00142582574488944\n",
      "w1942 = 2.0443188675016176e-06\n",
      "w1943 = 0.000850353880090231\n",
      "w1944 = 0.0035832499478802467\n",
      "w1945 = 0.006055858109970834\n",
      "w1946 = -0.006156881590414484\n",
      "w1947 = 0.002143604401571955\n",
      "w1948 = 0.006757406678039803\n",
      "w1949 = 0.006577784123303971\n",
      "w1950 = 0.02019923322450961\n",
      "w1951 = 0.013438477757361119\n",
      "w1952 = 0.0028242096280433793\n",
      "w1953 = -0.004629777104575567\n",
      "w1954 = 0.0018495236126336956\n",
      "w1955 = 0.013517772143219219\n",
      "w1956 = 0.006249139358745119\n",
      "w1957 = -0.002805610605114104\n",
      "w1958 = -0.0018245092333579753\n",
      "w1959 = 0.000869248514297132\n",
      "w1960 = -0.00600987165988789\n",
      "w1961 = 0.0010327203628918019\n",
      "w1962 = -0.0027745735178569416\n",
      "w1963 = 0.001282210806036803\n",
      "w1964 = 0.005232145809291652\n",
      "w1965 = 0.00235646902289569\n",
      "w1966 = -0.000452258248874823\n",
      "w1967 = 0.0020229217690751933\n",
      "w1968 = 0.0027274645480136724\n",
      "w1969 = -0.0031485616620437413\n",
      "w1970 = -0.0019452674935622155\n",
      "w1971 = -0.0020798687158897506\n",
      "w1972 = -0.005810758201303958\n",
      "w1973 = 0.0026516088376612\n",
      "w1974 = -0.0008755438566645121\n",
      "w1975 = -0.0011246928763828322\n",
      "w1976 = -0.004770153624665585\n",
      "w1977 = -0.003282393221916879\n",
      "w1978 = 0.008350360491926213\n",
      "w1979 = 0.002745020720784112\n",
      "w1980 = -0.008302364143303367\n",
      "w1981 = -0.00016557720995920495\n",
      "w1982 = -0.00250110366269202\n",
      "w1983 = 0.002124301150704968\n",
      "w1984 = -0.0019123110693407047\n",
      "w1985 = 0.005373780274025218\n",
      "w1986 = 0.012525262110314793\n",
      "w1987 = 0.004979410037849236\n",
      "w1988 = 0.0075738355840239404\n",
      "w1989 = 0.005264140187537389\n",
      "w1990 = -0.0033696213385720616\n",
      "w1991 = -0.0025794737172698244\n",
      "w1992 = -0.015193003607072018\n",
      "w1993 = -0.006983896372788392\n",
      "w1994 = 0.0010409545092659902\n",
      "w1995 = 0.004434884307540191\n",
      "w1996 = 0.0014814500719649507\n",
      "w1997 = -0.0007033155618668587\n",
      "w1998 = -0.0008556858319473984\n",
      "b = -9.079824610545231e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(1.)\n",
      "Test error rate: tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_iterations):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "    \n",
    "    for epoch in range(num_iterations):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "        \n",
    "        # Calculate the cost (mean squared error)\n",
    "        cost = np.mean(np.square(error))\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "    \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = 0.001\n",
    "num_iterations = 1000\n",
    "w, b, cost_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/1000], Loss: 0.98926044\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/1000], Loss: 1.01788754\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/1000], Loss: 0.75582290\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/1000], Loss: 0.58245199\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/1000], Loss: 0.66656723\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/1000], Loss: 0.61351736\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/1000], Loss: 0.56317425\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/1000], Loss: 0.52916954\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/1000], Loss: 0.51444386\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/1000], Loss: 0.30596217\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/1000], Loss: 0.37333298\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/1000], Loss: 0.39078441\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [120/1000], Loss: 0.54051059\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/1000], Loss: 0.25151825\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/1000], Loss: 0.29448308\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/1000], Loss: 0.34794039\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/1000], Loss: 0.34855229\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/1000], Loss: 0.31417671\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/1000], Loss: 0.36343613\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/1000], Loss: 0.23762915\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/1000], Loss: 0.19513805\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/1000], Loss: 0.36373741\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/1000], Loss: 0.17724611\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/1000], Loss: 0.32351553\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/1000], Loss: 0.37151621\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/1000], Loss: 0.32485098\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/1000], Loss: 0.34232364\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/1000], Loss: 0.19029063\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/1000], Loss: 0.13124340\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/1000], Loss: 0.21733340\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/1000], Loss: 0.22282460\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/1000], Loss: 0.42622993\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/1000], Loss: 0.26589293\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/1000], Loss: 0.10315365\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/1000], Loss: 0.32618841\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/1000], Loss: 0.20083442\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/1000], Loss: 0.07415418\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/1000], Loss: 0.04110124\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/1000], Loss: 0.10617002\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/1000], Loss: 0.02045816\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/1000], Loss: 0.14336727\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/1000], Loss: 0.10880095\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/1000], Loss: 0.23683371\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/1000], Loss: 0.09326144\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/1000], Loss: 0.05963487\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/1000], Loss: 0.17717520\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/1000], Loss: 0.11262160\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/1000], Loss: 0.13917655\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/1000], Loss: 0.12510242\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/1000], Loss: 0.07120703\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/1000], Loss: 0.15524787\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/1000], Loss: 0.07035171\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/1000], Loss: 0.09245554\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/1000], Loss: 0.23617985\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/1000], Loss: 0.12246432\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/1000], Loss: 0.25222477\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/1000], Loss: 0.18262313\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/1000], Loss: 0.23246983\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/1000], Loss: 0.24896435\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/1000], Loss: 0.13321322\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/1000], Loss: 0.10117536\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/1000], Loss: 0.17755866\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/1000], Loss: 0.07178522\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/1000], Loss: 0.13628742\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/1000], Loss: 0.06816761\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/1000], Loss: 0.21561438\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/1000], Loss: 0.03600174\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/1000], Loss: 0.08099636\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/1000], Loss: 0.13544324\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/1000], Loss: 0.08332418\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/1000], Loss: 0.13798158\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/1000], Loss: 0.04008679\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/1000], Loss: 0.07303686\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/1000], Loss: 0.09391955\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/1000], Loss: 0.13539469\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/1000], Loss: 0.14002354\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/1000], Loss: 0.16525875\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/1000], Loss: 0.10534667\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/1000], Loss: 0.04429356\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/1000], Loss: 0.11893039\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/1000], Loss: 0.10880692\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/1000], Loss: 0.21253995\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/1000], Loss: 0.14063803\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/1000], Loss: 0.07971621\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/1000], Loss: 0.07201552\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/1000], Loss: 0.06384709\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/1000], Loss: 0.14352275\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/1000], Loss: 0.07775287\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/1000], Loss: 0.17173965\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/1000], Loss: 0.12476488\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/1000], Loss: 0.05137455\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/1000], Loss: 0.15625307\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/1000], Loss: 0.05547181\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/1000], Loss: 0.12032808\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/1000], Loss: 0.05777136\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/1000], Loss: 0.04752193\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/1000], Loss: 0.04116034\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/1000], Loss: 0.08030187\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/1000], Loss: 0.01178387\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/1000], Loss: 0.08175014\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/1000], Loss: 0.01542947\n",
      "Learned parameters:\n",
      "w0 = 0.006679282595882589\n",
      "w1 = 0.1011800683356283\n",
      "w2 = 0.07548015465804828\n",
      "w3 = -0.03046036134368483\n",
      "w4 = -0.036359792326782665\n",
      "w5 = -0.09336031719851684\n",
      "w6 = 0.046754768699066876\n",
      "w7 = -0.04795146594602927\n",
      "w8 = -0.011454790261185331\n",
      "w9 = 0.002349957544584953\n",
      "w10 = -0.01971333803093188\n",
      "w11 = -0.04276086261660536\n",
      "w12 = -0.007700372832846835\n",
      "w13 = 0.0035636418978094555\n",
      "w14 = -0.02634063359289732\n",
      "w15 = 0.001004136761479379\n",
      "w16 = -0.01825745731666101\n",
      "w17 = 0.014765362198813255\n",
      "w18 = 0.024391045804656678\n",
      "w19 = 0.14978049976199204\n",
      "w20 = 0.07591210770810493\n",
      "w21 = 0.10604387829253463\n",
      "w22 = -0.016073212813002762\n",
      "w23 = 0.011294890134234221\n",
      "w24 = 0.1553465538798428\n",
      "w25 = -0.008855225311522592\n",
      "w26 = -0.07025232981174838\n",
      "w27 = -0.002565623054307156\n",
      "w28 = 0.02283324642037219\n",
      "w29 = 0.007229155771374038\n",
      "w30 = -0.1828599585945324\n",
      "w31 = -0.011994210816418037\n",
      "w32 = -0.050238286974725814\n",
      "w33 = 0.06782301936833623\n",
      "w34 = 0.05904676276650498\n",
      "w35 = 0.14854468625267855\n",
      "w36 = 0.01432877835848643\n",
      "w37 = -0.07507819924674358\n",
      "w38 = -0.008160106555115523\n",
      "w39 = -0.004902600199726936\n",
      "w40 = 0.03351606114192963\n",
      "w41 = -0.047728942879412345\n",
      "w42 = 0.0911569340048319\n",
      "w43 = 0.004079578533853042\n",
      "w44 = 0.06093635699314519\n",
      "w45 = -0.21400778256868222\n",
      "w46 = -0.06179502933184476\n",
      "w47 = 0.027548302874835784\n",
      "w48 = 0.07279528401544819\n",
      "w49 = -0.1950970000869517\n",
      "w50 = 0.014908364830211289\n",
      "w51 = 0.10320678076158443\n",
      "w52 = 0.023679874789379145\n",
      "w53 = -0.014063314220427298\n",
      "w54 = -0.03509140265538876\n",
      "w55 = -0.042021630600566254\n",
      "w56 = 0.0891399953281428\n",
      "w57 = -0.027478598985138613\n",
      "w58 = -0.04581068663797383\n",
      "w59 = -0.0008881620258646249\n",
      "w60 = -0.046713741663033805\n",
      "w61 = -0.07639668619264453\n",
      "w62 = 0.14365759280363635\n",
      "w63 = -0.03142348430920636\n",
      "w64 = -0.05634145482072531\n",
      "w65 = 0.08014723866205184\n",
      "w66 = 0.05042833154864021\n",
      "w67 = 0.06006747558063819\n",
      "w68 = -0.056232024676588395\n",
      "w69 = 0.03226869053525871\n",
      "w70 = -0.11151860527350167\n",
      "w71 = 0.02692177056300379\n",
      "w72 = -0.0970441166167076\n",
      "w73 = -0.12956418502842293\n",
      "w74 = 0.11630423569450063\n",
      "w75 = -0.05747151036024794\n",
      "w76 = -0.006088567583556973\n",
      "w77 = 0.057615135850317245\n",
      "w78 = -0.0019942884648368395\n",
      "w79 = -0.006202754941858923\n",
      "w80 = 0.018666594354055252\n",
      "w81 = 0.05240054285232478\n",
      "w82 = 0.02909810928207431\n",
      "w83 = 0.026511036803841356\n",
      "w84 = 0.05940463404423262\n",
      "w85 = -0.04448851112007742\n",
      "w86 = -0.0012590088186215129\n",
      "w87 = -0.07163610526184161\n",
      "w88 = -0.057939612404933234\n",
      "w89 = -0.09514497049799796\n",
      "w90 = -0.022109974284034654\n",
      "w91 = -0.1206820218011139\n",
      "w92 = -0.019443211806529085\n",
      "w93 = 0.028061856629111537\n",
      "w94 = 0.12752955591752624\n",
      "w95 = -0.03241012856906483\n",
      "w96 = 0.06403656517565565\n",
      "w97 = 0.03006615230569352\n",
      "w98 = 0.12067742976071015\n",
      "w99 = 0.09544654954208882\n",
      "w100 = 0.09782651175019487\n",
      "w101 = -0.17015235866463538\n",
      "w102 = 0.06672042164919034\n",
      "w103 = -0.0021443004876203885\n",
      "w104 = 0.01572863575287611\n",
      "w105 = -0.2237549529273805\n",
      "w106 = 0.0013856165064551876\n",
      "w107 = 0.029396872261730617\n",
      "w108 = -0.04302235581569135\n",
      "w109 = 0.03187811272580924\n",
      "w110 = -0.02234444960758056\n",
      "w111 = -0.13892067989433343\n",
      "w112 = 0.03542341001868097\n",
      "w113 = -0.06162716521939201\n",
      "w114 = 0.012964882937807494\n",
      "w115 = 0.04187102125630237\n",
      "w116 = -0.0198940209140362\n",
      "w117 = -0.024997175536326504\n",
      "w118 = -0.03811943295664399\n",
      "w119 = -0.131133583661734\n",
      "w120 = 0.11487480411007113\n",
      "w121 = 0.09332301197987743\n",
      "w122 = -0.04819186904225253\n",
      "w123 = -0.0594834416059678\n",
      "w124 = 0.029908663716076037\n",
      "w125 = 0.015318607513914687\n",
      "w126 = 0.01033556578103385\n",
      "w127 = -0.07323998130780093\n",
      "w128 = 0.05207313489549226\n",
      "w129 = 0.026778279672668697\n",
      "w130 = -0.019889178121526192\n",
      "w131 = -0.013876358383238436\n",
      "w132 = -0.0692540932491822\n",
      "w133 = 0.05424793882153635\n",
      "w134 = -0.05403714862658343\n",
      "w135 = 0.06721958592818984\n",
      "w136 = -0.060751809559618904\n",
      "w137 = 0.046089238752819596\n",
      "w138 = 0.09308873397518963\n",
      "w139 = 0.20555616026990384\n",
      "w140 = -0.0886419753796729\n",
      "w141 = 0.014382180468061221\n",
      "w142 = 0.0697569368304976\n",
      "w143 = -0.008400229153895341\n",
      "w144 = 0.07425946482175566\n",
      "w145 = -0.11995326893452052\n",
      "w146 = 0.07043520214859328\n",
      "w147 = -0.1802522400672651\n",
      "w148 = -0.07529777316757745\n",
      "w149 = 0.09288691403123889\n",
      "w150 = 0.042504377737203704\n",
      "w151 = -0.004622170575678523\n",
      "w152 = 0.02307164227183205\n",
      "w153 = -0.028237914253470976\n",
      "w154 = 0.09802860325673246\n",
      "w155 = 0.07585077194576272\n",
      "w156 = 0.18547600726647154\n",
      "w157 = 0.15789208156873405\n",
      "w158 = 0.09726752952707543\n",
      "w159 = 0.13825481882506402\n",
      "w160 = -0.010193543963942574\n",
      "w161 = 0.0911268652797988\n",
      "w162 = 0.02910777409797107\n",
      "w163 = -0.0826595404916036\n",
      "w164 = -0.04990616516352472\n",
      "w165 = 0.012631877264733046\n",
      "w166 = 0.09078162001664179\n",
      "w167 = 0.019913263120487253\n",
      "w168 = -0.05080200765660567\n",
      "w169 = -0.1314071076348712\n",
      "w170 = 0.029469480486180914\n",
      "w171 = 0.05451958658948556\n",
      "w172 = -0.023201016764001145\n",
      "w173 = -0.02078646214386638\n",
      "w174 = 0.042102296060765806\n",
      "w175 = 0.04368434862013088\n",
      "w176 = -0.021058311017493635\n",
      "w177 = -0.14531808344596137\n",
      "w178 = 0.10584798035844324\n",
      "w179 = 0.01748752633184895\n",
      "w180 = -0.03948015807766778\n",
      "w181 = -0.20019202849001616\n",
      "w182 = 0.027896160830300002\n",
      "w183 = 0.0033814118755706984\n",
      "w184 = -0.10591843962681338\n",
      "w185 = -0.09042471758727456\n",
      "w186 = -0.13632193563324854\n",
      "w187 = 0.17678984488200247\n",
      "w188 = -0.08066351572061069\n",
      "w189 = -0.03353687671947916\n",
      "w190 = 0.04286123478104109\n",
      "w191 = -0.019398447350847798\n",
      "w192 = -0.045682322673977704\n",
      "w193 = -0.08695207161450473\n",
      "w194 = -0.007165077836276187\n",
      "w195 = -0.13279383554233135\n",
      "w196 = -0.062003331255534595\n",
      "w197 = -0.07721295060177844\n",
      "w198 = -0.09030815190162113\n",
      "w199 = 0.010846129854111507\n",
      "w200 = -0.01866674402683237\n",
      "w201 = 0.006524267340402355\n",
      "w202 = -0.10398641496100498\n",
      "w203 = 0.017513171390430054\n",
      "w204 = -0.11667551828161962\n",
      "w205 = -0.059315456335429555\n",
      "w206 = -0.022912924027753905\n",
      "w207 = -0.040553249601635755\n",
      "w208 = -0.06255865174078568\n",
      "w209 = -0.0031645526133074996\n",
      "w210 = 0.03706050043124382\n",
      "w211 = -0.02482943928162371\n",
      "w212 = 0.047759732769671596\n",
      "w213 = 0.0052680288132915705\n",
      "w214 = -0.011522607691476587\n",
      "w215 = 0.09363518867524234\n",
      "w216 = 0.03355049431954372\n",
      "w217 = -0.2524758567333301\n",
      "w218 = -0.01268915726674879\n",
      "w219 = 0.0460946511537394\n",
      "w220 = 0.009801696357955813\n",
      "w221 = 0.11496554147094415\n",
      "w222 = 0.05023942956171316\n",
      "w223 = 0.03936108534657133\n",
      "w224 = 0.09441413761566782\n",
      "w225 = 0.25016195558100734\n",
      "w226 = 0.01080154163506489\n",
      "w227 = -0.06664092898189006\n",
      "w228 = 0.05698021946123755\n",
      "w229 = 0.009974258940254936\n",
      "w230 = 0.13746504617829078\n",
      "w231 = 0.014808683818939402\n",
      "w232 = 0.0036157134073233708\n",
      "w233 = 0.0461544966226602\n",
      "w234 = -0.01155189479259355\n",
      "w235 = 0.19806046445254494\n",
      "w236 = 0.008577477285012788\n",
      "w237 = 0.0581891451745887\n",
      "w238 = -0.07541264983394573\n",
      "w239 = -0.14514373529276978\n",
      "w240 = -0.008921343297753778\n",
      "w241 = -0.11327984385503724\n",
      "w242 = 0.2118224223020572\n",
      "w243 = 0.14686040268389258\n",
      "w244 = 0.1421214464245504\n",
      "w245 = 0.0534874353587024\n",
      "w246 = 0.13548105615324402\n",
      "w247 = -0.027745965494670028\n",
      "w248 = -0.02606541181803176\n",
      "w249 = 0.11276731987982563\n",
      "w250 = -0.05973316443483707\n",
      "w251 = -0.08555714769078303\n",
      "w252 = -0.01575095248130694\n",
      "w253 = 0.09918011992043817\n",
      "w254 = 0.08456795355821237\n",
      "w255 = -0.1219328791673417\n",
      "w256 = 0.1274663112291531\n",
      "w257 = -0.1737169812491484\n",
      "w258 = 0.017088638511153515\n",
      "w259 = -0.039176738739796796\n",
      "w260 = -0.0219530620107532\n",
      "w261 = 2.745944496055641e-05\n",
      "w262 = 0.013285658745876127\n",
      "w263 = 0.17520239468214693\n",
      "w264 = 0.032626903269380066\n",
      "w265 = -0.07908460488400539\n",
      "w266 = 0.016508427673680468\n",
      "w267 = -0.03155106761687401\n",
      "w268 = 0.04245094142787203\n",
      "w269 = 0.00015202435314702704\n",
      "w270 = 0.04161713382730433\n",
      "w271 = -0.069560779465376\n",
      "w272 = 0.014331244577240039\n",
      "w273 = -0.0746149598812093\n",
      "w274 = 0.022676496481987243\n",
      "w275 = 0.016740077970255934\n",
      "w276 = -0.0907928685685787\n",
      "w277 = 0.042864181595887924\n",
      "w278 = -0.10800630697606022\n",
      "w279 = 0.007792867908745647\n",
      "w280 = -0.04326693860621065\n",
      "w281 = -0.08437774311724713\n",
      "w282 = -0.025836494104355068\n",
      "w283 = 0.03506712840556102\n",
      "w284 = -0.04494992146760833\n",
      "w285 = -0.2865850461351839\n",
      "w286 = 0.008533449819342455\n",
      "w287 = 0.07586391421265619\n",
      "w288 = -0.09826360834958558\n",
      "w289 = 0.01679717458192569\n",
      "w290 = 0.09750734405494545\n",
      "w291 = 0.15377726718359286\n",
      "w292 = -0.10321588588420644\n",
      "w293 = 0.10110892940774331\n",
      "w294 = -0.01591414765100501\n",
      "w295 = -0.04747313441922573\n",
      "w296 = -0.004423252524187378\n",
      "w297 = 0.052923296863667056\n",
      "w298 = 0.12096586015735229\n",
      "w299 = 0.033852677094737375\n",
      "w300 = -0.013413138478937296\n",
      "w301 = 0.02744187259735018\n",
      "w302 = -0.07867741353402044\n",
      "w303 = 0.04445306112883964\n",
      "w304 = -0.1635150911064682\n",
      "w305 = 0.004111235063496975\n",
      "w306 = 0.12533214737092813\n",
      "w307 = 0.0148728674121324\n",
      "w308 = 0.012346017468473034\n",
      "w309 = -0.14084878249248875\n",
      "w310 = -0.07363983991943074\n",
      "w311 = -0.08328273281413502\n",
      "w312 = -0.0039299387874455576\n",
      "w313 = 0.05809671177945036\n",
      "w314 = 0.04934455590707338\n",
      "w315 = -0.43601889563455454\n",
      "w316 = -0.00863673504916608\n",
      "w317 = 0.0025441917709914225\n",
      "w318 = 0.05998192356512487\n",
      "w319 = -0.02224359472523655\n",
      "w320 = 0.06607614979690549\n",
      "w321 = -0.026713050661899822\n",
      "w322 = -0.1349658675996233\n",
      "w323 = 0.10695468560621026\n",
      "w324 = 0.0702250139493644\n",
      "w325 = -0.0031794366483117514\n",
      "w326 = -0.03839966105756404\n",
      "w327 = -0.053244804056777825\n",
      "w328 = 0.04951468755086727\n",
      "w329 = -0.061399341774835955\n",
      "w330 = 0.059556757572588256\n",
      "w331 = 0.08350240442029255\n",
      "w332 = -0.015459799089493013\n",
      "w333 = 0.007563645774948343\n",
      "w334 = 0.12456050676941717\n",
      "w335 = 0.016059823007016742\n",
      "w336 = -0.11265203839325727\n",
      "w337 = -0.3297505602870429\n",
      "w338 = -0.014395831108296538\n",
      "w339 = 0.039324151670343636\n",
      "w340 = 0.14326216716258172\n",
      "w341 = 0.04592541795429643\n",
      "w342 = -0.010416238162345626\n",
      "w343 = 0.21670144666754226\n",
      "w344 = -0.0792105727425796\n",
      "w345 = -0.024758512286361393\n",
      "w346 = 0.00515807150006266\n",
      "w347 = 0.06431180051961403\n",
      "w348 = 0.023040705233370522\n",
      "w349 = 0.04252528218158614\n",
      "w350 = 0.002019895336040841\n",
      "w351 = 0.028886735111662298\n",
      "w352 = 0.09691842033342213\n",
      "w353 = -0.080620588335447\n",
      "w354 = -0.05472496208740126\n",
      "w355 = 0.07630831792256097\n",
      "w356 = -0.029462452276296996\n",
      "w357 = 0.00216794690895324\n",
      "w358 = 0.1473097843985152\n",
      "w359 = 0.021208347188190414\n",
      "w360 = 0.05884479131315179\n",
      "w361 = -0.06322767877247093\n",
      "w362 = -0.09890582559363152\n",
      "w363 = 0.0019591315660202682\n",
      "w364 = 0.0014628374905344026\n",
      "w365 = 0.006791770636900514\n",
      "w366 = 0.009634571517824564\n",
      "w367 = -0.06158672460001416\n",
      "w368 = -0.1467883973149948\n",
      "w369 = 0.07161734702216095\n",
      "w370 = 0.056010429374562645\n",
      "w371 = 0.07701481402412205\n",
      "w372 = 0.05825323263944056\n",
      "w373 = -0.08990815196465181\n",
      "w374 = -0.12656295696363695\n",
      "w375 = -0.07041531297847324\n",
      "w376 = 0.1854211328660638\n",
      "w377 = 0.04310869894891389\n",
      "w378 = -0.0910764560462691\n",
      "w379 = -0.02412335158056507\n",
      "w380 = -0.31128384832292366\n",
      "w381 = -0.03963480037957447\n",
      "w382 = -0.09584069717762854\n",
      "w383 = 0.031196642900323944\n",
      "w384 = 0.0259758514178312\n",
      "w385 = -0.12624718137960847\n",
      "w386 = 0.0615167317364124\n",
      "w387 = -0.05608904930709124\n",
      "w388 = -0.009882451037850312\n",
      "w389 = -0.0476933384268992\n",
      "w390 = -0.023419643987157745\n",
      "w391 = 0.028939948984846392\n",
      "w392 = -0.18889422597771088\n",
      "w393 = -0.0529386194957223\n",
      "w394 = 0.009357874686820845\n",
      "w395 = -0.017644013129432642\n",
      "w396 = 0.08804449619621273\n",
      "w397 = 0.14227797489786762\n",
      "w398 = 0.04193419014659156\n",
      "w399 = -0.10060590160395196\n",
      "w400 = -0.0011602924808768566\n",
      "w401 = -0.06784206363988154\n",
      "w402 = -0.08260005114345069\n",
      "w403 = -0.17099463016304353\n",
      "w404 = 0.001966855185987611\n",
      "w405 = 0.14008448060523004\n",
      "w406 = 0.037542615828760575\n",
      "w407 = 0.027644346238826792\n",
      "w408 = -0.021039953072327082\n",
      "w409 = 0.06730541036874631\n",
      "w410 = 0.02128410139331801\n",
      "w411 = -0.0744719897690776\n",
      "w412 = -0.18331921984399244\n",
      "w413 = -0.10845994645591554\n",
      "w414 = -0.06499233314674975\n",
      "w415 = -0.027233699654050793\n",
      "w416 = -0.14459041880813434\n",
      "w417 = -0.024531190092957345\n",
      "w418 = -0.021789692489391502\n",
      "w419 = 0.0555215111867565\n",
      "w420 = 0.008658273228962282\n",
      "w421 = -0.09688213016167847\n",
      "w422 = -0.013166258115106615\n",
      "w423 = -0.04293679420874474\n",
      "w424 = 0.08878167865710355\n",
      "w425 = 0.010007477995270372\n",
      "w426 = -0.08095103901587573\n",
      "w427 = -0.08578125752905395\n",
      "w428 = -0.07760582149067703\n",
      "w429 = -0.15404929020884547\n",
      "w430 = 0.004409930146278377\n",
      "w431 = 0.015352458060320427\n",
      "w432 = -0.006133177955320032\n",
      "w433 = -0.05420145798576609\n",
      "w434 = -0.08679755520716444\n",
      "w435 = -0.09025923243406236\n",
      "w436 = 0.009541914082163303\n",
      "w437 = -0.039238751603955555\n",
      "w438 = 0.14613684686363895\n",
      "w439 = -0.08982944565184253\n",
      "w440 = -0.014974159853413948\n",
      "w441 = 0.07776136970783633\n",
      "w442 = 0.01793848588110794\n",
      "w443 = 0.053856520309681064\n",
      "w444 = 0.02065251667394257\n",
      "w445 = -0.04545566601610447\n",
      "w446 = 0.07966803105114813\n",
      "w447 = 0.15667678630920762\n",
      "w448 = -0.04377636647555077\n",
      "w449 = 0.0576163880074957\n",
      "w450 = 0.06375868174968834\n",
      "w451 = -0.006495491940502113\n",
      "w452 = -0.08199910897588376\n",
      "w453 = -0.029326216072916668\n",
      "w454 = -0.06347493117691529\n",
      "w455 = -0.05015747069052512\n",
      "w456 = -0.023283329021197865\n",
      "w457 = 0.06173770026125937\n",
      "w458 = -0.07788836115275331\n",
      "w459 = -0.17524032923190475\n",
      "w460 = -0.12682665830700213\n",
      "w461 = -0.07703292270599982\n",
      "w462 = 0.0841940095503246\n",
      "w463 = 0.033829108665319886\n",
      "w464 = -0.002727336662135566\n",
      "w465 = 0.014897807217984284\n",
      "w466 = 0.07324542820515728\n",
      "w467 = -0.018413130391699763\n",
      "w468 = 0.30834695342829566\n",
      "w469 = -0.05271505506480469\n",
      "w470 = 0.07993469813826323\n",
      "w471 = 0.002853913227704673\n",
      "w472 = -0.09963475304305734\n",
      "w473 = -0.09366478720867605\n",
      "w474 = -0.0030012777399194748\n",
      "w475 = -0.010057321072824007\n",
      "w476 = -0.029256806224060544\n",
      "w477 = -0.0773917649963703\n",
      "w478 = -0.005426150249558233\n",
      "w479 = 0.09063041293406071\n",
      "w480 = -0.05200565763157433\n",
      "w481 = -0.12777995118917004\n",
      "w482 = 0.20027529219677545\n",
      "w483 = -0.07486349423873806\n",
      "w484 = 0.13316466606975472\n",
      "w485 = -0.042236547900212085\n",
      "w486 = 0.11951109679300341\n",
      "w487 = -0.09190427652353471\n",
      "w488 = 0.06670439472596058\n",
      "w489 = 0.04447980023996115\n",
      "w490 = -0.045387492099267436\n",
      "w491 = -0.03306538753146874\n",
      "w492 = -0.09552644820072388\n",
      "w493 = 0.011303349166690286\n",
      "w494 = 0.13069475352808735\n",
      "w495 = 0.10573774339088036\n",
      "w496 = 0.04541941797107313\n",
      "w497 = 0.15804918572927262\n",
      "w498 = 0.08454315814291975\n",
      "w499 = 0.09308664671126893\n",
      "w500 = 0.006692983926439457\n",
      "w501 = -0.1044113550513338\n",
      "w502 = 0.08473337620027842\n",
      "w503 = -0.13440581019396267\n",
      "w504 = 0.008474361881523661\n",
      "w505 = 0.06551227112985404\n",
      "w506 = -0.09198952806944807\n",
      "w507 = 0.04312440150085222\n",
      "w508 = -0.06831160521935949\n",
      "w509 = -0.014510039816333595\n",
      "w510 = 0.08705894684569597\n",
      "w511 = -0.015226062008071339\n",
      "w512 = -0.11168620417044643\n",
      "w513 = 0.0819302418318522\n",
      "w514 = 0.06597807632927023\n",
      "w515 = -0.15925594328866863\n",
      "w516 = 0.08621341588253055\n",
      "w517 = 0.057075493530534115\n",
      "w518 = -0.05929381564301441\n",
      "w519 = 0.03138604207776746\n",
      "w520 = -0.09314103805295378\n",
      "w521 = -0.058268383066788484\n",
      "w522 = 0.13777952975212937\n",
      "w523 = -0.009849957601047413\n",
      "w524 = 0.12999566648720076\n",
      "w525 = 0.055126605008594495\n",
      "w526 = -0.07835611948129233\n",
      "w527 = 0.07557788992608716\n",
      "w528 = 0.017704296390190628\n",
      "w529 = -0.010061626434795707\n",
      "w530 = -0.020765639800621846\n",
      "w531 = 0.01581807584002242\n",
      "w532 = 0.023996672299616013\n",
      "w533 = 0.054363194006401654\n",
      "w534 = 0.12999353160338198\n",
      "w535 = 0.012241013291943034\n",
      "w536 = 0.09500336592068184\n",
      "w537 = -0.031135040822143342\n",
      "w538 = -0.026427405070050453\n",
      "w539 = 0.08826681543544677\n",
      "w540 = -0.08859650414479131\n",
      "w541 = 0.03429460736864683\n",
      "w542 = 0.08526631333235984\n",
      "w543 = -0.012394575298904167\n",
      "w544 = 0.00459406852640935\n",
      "w545 = 0.04872727127577864\n",
      "w546 = 0.01885186727073227\n",
      "w547 = -0.16455013266356835\n",
      "w548 = -0.0013511617628391085\n",
      "w549 = 0.06905118196766125\n",
      "w550 = 0.03893561152645442\n",
      "w551 = 0.09306035484772437\n",
      "w552 = 0.00820658349248079\n",
      "w553 = -0.05224700674412451\n",
      "w554 = -0.0801154362359298\n",
      "w555 = -0.09383605472559675\n",
      "w556 = -0.0654268931439757\n",
      "w557 = -0.029825299457835113\n",
      "w558 = -0.02473764334351706\n",
      "w559 = -0.013916661915938018\n",
      "w560 = -0.08820964055559424\n",
      "w561 = -0.04518187284303828\n",
      "w562 = 0.0980612644009763\n",
      "w563 = 0.06809938805576951\n",
      "w564 = 0.14800896542496172\n",
      "w565 = 0.009097838031243805\n",
      "w566 = -0.01516803631085611\n",
      "w567 = -0.07028250749995829\n",
      "w568 = -0.09034898355791629\n",
      "w569 = -0.08346418895495956\n",
      "w570 = 0.019620609775184716\n",
      "w571 = 0.24826370381057622\n",
      "w572 = -0.02938653437309114\n",
      "w573 = 0.023457987957874654\n",
      "w574 = 0.01858702254952375\n",
      "w575 = -0.023148131754641088\n",
      "w576 = -0.1159140644774579\n",
      "w577 = -0.05572941330563921\n",
      "w578 = 0.07375221166369514\n",
      "w579 = 0.0008995518663917106\n",
      "w580 = 0.08084483004545215\n",
      "w581 = 0.0365512411973941\n",
      "w582 = 0.07341210894175719\n",
      "w583 = 0.007600796095921244\n",
      "w584 = 0.015799046905513394\n",
      "w585 = 0.07567270795969357\n",
      "w586 = 0.00454005467131329\n",
      "w587 = 0.01587572399931814\n",
      "w588 = 0.04154317715034367\n",
      "w589 = 0.020879184916971313\n",
      "w590 = -0.009026243437396272\n",
      "w591 = -0.03485463655888064\n",
      "w592 = -0.024360957091011156\n",
      "w593 = 0.017294177639772833\n",
      "w594 = 0.11008955723513958\n",
      "w595 = -0.10805128758674362\n",
      "w596 = 0.04161309217571459\n",
      "w597 = 0.022772851439150643\n",
      "w598 = -0.09779248186149954\n",
      "w599 = -0.0589776170626858\n",
      "w600 = -0.06266989254693466\n",
      "w601 = 0.11279424920031592\n",
      "w602 = -0.05219861650476235\n",
      "w603 = 0.028266704464872807\n",
      "w604 = 0.046393267694199755\n",
      "w605 = 0.19130500601596587\n",
      "w606 = 0.009120019817613075\n",
      "w607 = 0.00813573000995292\n",
      "w608 = 0.2517844110849225\n",
      "w609 = -0.08887008026035557\n",
      "w610 = 0.011073815477987387\n",
      "w611 = 0.07914125548382359\n",
      "w612 = -0.0183577599240595\n",
      "w613 = -0.05825015152456083\n",
      "w614 = -0.08709634988339235\n",
      "w615 = -0.10441368946681766\n",
      "w616 = 0.009367003512616231\n",
      "w617 = 0.09815170704085768\n",
      "w618 = -0.006578629194736152\n",
      "w619 = 0.03262852794477851\n",
      "w620 = -0.005550561114974667\n",
      "w621 = 0.06681314851739584\n",
      "w622 = 0.0035075962881074593\n",
      "w623 = -0.03662437704957642\n",
      "w624 = 0.07995344787033183\n",
      "w625 = -0.10293831950379681\n",
      "w626 = -0.018935415305448518\n",
      "w627 = 0.009300496228754401\n",
      "w628 = 0.021592519876514695\n",
      "w629 = -0.04271788197905823\n",
      "w630 = -0.051638124794617525\n",
      "w631 = 0.03822224487834797\n",
      "w632 = 0.014061551537887941\n",
      "w633 = -0.186388465459943\n",
      "w634 = -0.11612283074224004\n",
      "w635 = -0.13496600035637443\n",
      "w636 = -0.020839667575682137\n",
      "w637 = -0.033592584030731155\n",
      "w638 = 0.069129834171342\n",
      "w639 = 0.06656877135612137\n",
      "w640 = -0.05570235966111181\n",
      "w641 = -0.1640427010764216\n",
      "w642 = -0.027783690788835563\n",
      "w643 = 0.07853673724830668\n",
      "w644 = -0.03203959930230664\n",
      "w645 = 0.07701170384903036\n",
      "w646 = 0.04710838926612554\n",
      "w647 = -0.0009183019100515281\n",
      "w648 = 0.04843838172451712\n",
      "w649 = 0.26148584798150165\n",
      "w650 = -0.19119299621604427\n",
      "w651 = 0.11793106292674665\n",
      "w652 = -0.14504113694668636\n",
      "w653 = 0.005095752418292539\n",
      "w654 = 0.039575016314484915\n",
      "w655 = 0.0019154798513064317\n",
      "w656 = 0.035580508880547534\n",
      "w657 = 0.0681472662107486\n",
      "w658 = 0.0868018988488821\n",
      "w659 = 0.14495567770485074\n",
      "w660 = 0.22634174250112143\n",
      "w661 = -0.058386966495321706\n",
      "w662 = 0.01779534506971446\n",
      "w663 = 0.029420750329667705\n",
      "w664 = 0.07734460925111118\n",
      "w665 = -0.22145234125401808\n",
      "w666 = 0.022641315394945852\n",
      "w667 = 0.0905955668399936\n",
      "w668 = -0.03925340448562507\n",
      "w669 = 0.027515483288438114\n",
      "w670 = -0.08179817742695414\n",
      "w671 = 0.005769655920839613\n",
      "w672 = -0.0020106028484436086\n",
      "w673 = 0.01682780535010765\n",
      "w674 = -0.009180573284098843\n",
      "w675 = -0.035947668271418765\n",
      "w676 = 0.02148240745019542\n",
      "w677 = -0.06984106327657293\n",
      "w678 = 0.10030993431294263\n",
      "w679 = -0.039591584095983116\n",
      "w680 = -0.1657270670435573\n",
      "w681 = 0.03348627985167357\n",
      "w682 = -0.01818450684801504\n",
      "w683 = 0.14750152906638633\n",
      "w684 = -0.07576110819522823\n",
      "w685 = 0.128740488193743\n",
      "w686 = -0.1984844292755039\n",
      "w687 = -0.08886354493762552\n",
      "w688 = 0.015146705055798203\n",
      "w689 = -0.1572690341985354\n",
      "w690 = -0.2927877338295983\n",
      "w691 = 0.07662518391049558\n",
      "w692 = -0.08910372589671474\n",
      "w693 = -0.05759004238184981\n",
      "w694 = -0.0593696240104532\n",
      "w695 = 0.07207663635723127\n",
      "w696 = -0.04885619063226174\n",
      "w697 = -0.11457969809715833\n",
      "w698 = -0.011508874848265726\n",
      "w699 = -0.16241938540604703\n",
      "w700 = -0.039614511353309796\n",
      "w701 = 0.04103386006140998\n",
      "w702 = -0.022186195429728175\n",
      "w703 = -0.06915943655933957\n",
      "w704 = -0.15219684020935353\n",
      "w705 = -0.10015811550475923\n",
      "w706 = 0.04146402672157167\n",
      "w707 = 0.04821074106139359\n",
      "w708 = -0.12759123587771487\n",
      "w709 = -0.39253742473519704\n",
      "w710 = -0.16459210353061826\n",
      "w711 = -0.11441286944087814\n",
      "w712 = -0.07757440835269794\n",
      "w713 = -0.09406233674009344\n",
      "w714 = 0.1306866129056477\n",
      "w715 = -0.029824724880000947\n",
      "w716 = 0.2781642659645599\n",
      "w717 = 0.01345614031490024\n",
      "w718 = -0.17600994088366054\n",
      "w719 = -0.015023170274913756\n",
      "w720 = -0.06350132753159171\n",
      "w721 = -0.14843694414096423\n",
      "w722 = 0.00972827813011848\n",
      "w723 = 0.2060189498973943\n",
      "w724 = 0.12764591545779133\n",
      "w725 = -0.14431332114554363\n",
      "w726 = 0.02810724601540735\n",
      "w727 = -0.005126971393804471\n",
      "w728 = 0.08845280983175866\n",
      "w729 = -0.08201340501568435\n",
      "w730 = -0.024805641597829223\n",
      "w731 = -0.09152684738442034\n",
      "w732 = -0.14308001114844632\n",
      "w733 = -0.06775039083772551\n",
      "w734 = 0.05389260885309092\n",
      "w735 = -0.11141536323786015\n",
      "w736 = 0.0821187286499083\n",
      "w737 = -0.00042376787440249434\n",
      "w738 = 0.19836976876348086\n",
      "w739 = 0.027415445749295175\n",
      "w740 = -0.07400857290883327\n",
      "w741 = 0.11271414664475876\n",
      "w742 = -0.07609798821351806\n",
      "w743 = 0.05389151121709849\n",
      "w744 = -0.10080926464615698\n",
      "w745 = -0.11048398467553298\n",
      "w746 = -0.09936345239271135\n",
      "w747 = -0.018450179835134025\n",
      "w748 = 0.04342901865144982\n",
      "w749 = -0.17247341248567308\n",
      "w750 = 0.013670758628575258\n",
      "w751 = 0.020715385193243188\n",
      "w752 = 0.015325991686953894\n",
      "w753 = -0.09850418614917561\n",
      "w754 = 0.1411607203641035\n",
      "w755 = -0.3064031153939121\n",
      "w756 = -0.09152478208650924\n",
      "w757 = -0.03697880267271434\n",
      "w758 = -0.045385815652221044\n",
      "w759 = 0.016962613448503902\n",
      "w760 = 0.07751017040649265\n",
      "w761 = -0.038786067314946666\n",
      "w762 = 0.029786600683852207\n",
      "w763 = 0.01891174891660791\n",
      "w764 = 0.07093479061517566\n",
      "w765 = -0.007979044321116594\n",
      "w766 = 0.07079194477542783\n",
      "w767 = -0.026034190424178785\n",
      "w768 = -0.16842045923839677\n",
      "w769 = 0.03266175068537363\n",
      "w770 = -0.006285598193643431\n",
      "w771 = -0.12133203108598703\n",
      "w772 = 0.18362622574466608\n",
      "w773 = -0.04852187399186626\n",
      "w774 = -0.1553286354582673\n",
      "w775 = -0.014963702801104562\n",
      "w776 = 0.015594625325964378\n",
      "w777 = 0.05708969204339369\n",
      "w778 = 0.11557917967903462\n",
      "w779 = 0.1309718578147641\n",
      "w780 = 0.016210967295489327\n",
      "w781 = 0.06976844331585898\n",
      "w782 = 0.02224917643942156\n",
      "w783 = 0.07464827559135045\n",
      "w784 = 0.14597792880799904\n",
      "w785 = 0.040263105048764146\n",
      "w786 = -0.09925237969300726\n",
      "w787 = -0.048906730042224345\n",
      "w788 = -0.05154220875202803\n",
      "w789 = 0.27723321847902505\n",
      "w790 = 0.04881010011912185\n",
      "w791 = -0.13238304519455768\n",
      "w792 = -0.09801358235622167\n",
      "w793 = 0.05086080295825284\n",
      "w794 = -0.015619196785626873\n",
      "w795 = -0.06582955555441879\n",
      "w796 = -0.10205725883972389\n",
      "w797 = 0.09333415441854688\n",
      "w798 = -0.0524948598389321\n",
      "w799 = -0.09309770350459398\n",
      "w800 = -0.050682563098245\n",
      "w801 = -0.04275775828266101\n",
      "w802 = 0.027160279052340502\n",
      "w803 = -0.1138345025508003\n",
      "w804 = -0.023130039567081928\n",
      "w805 = -0.04915361782905253\n",
      "w806 = -0.02371785831829976\n",
      "w807 = -0.15106213755221726\n",
      "w808 = -0.13928118598114841\n",
      "w809 = -0.017792781713952464\n",
      "w810 = 0.016316777273863162\n",
      "w811 = 0.01755871519431389\n",
      "w812 = 0.15341056620431498\n",
      "w813 = 0.12875951795230564\n",
      "w814 = -0.10668821765749731\n",
      "w815 = 0.05626058059600127\n",
      "w816 = -0.00869287521253887\n",
      "w817 = -0.09419703102403842\n",
      "w818 = -0.34408263474873657\n",
      "w819 = -0.2019092487604724\n",
      "w820 = 0.00028920778155641153\n",
      "w821 = -0.056279220064392926\n",
      "w822 = 0.018460960338052435\n",
      "w823 = -0.10078769304642918\n",
      "w824 = -0.16300437208893045\n",
      "w825 = 0.03686535812353954\n",
      "w826 = 0.05559677378658214\n",
      "w827 = 0.007112922542477571\n",
      "w828 = -0.1390413443804145\n",
      "w829 = 0.14452675559655373\n",
      "w830 = 0.11095381254760402\n",
      "w831 = 0.07595465219892164\n",
      "w832 = -0.07085404645767669\n",
      "w833 = -0.008459402337443602\n",
      "w834 = -0.18285955504817325\n",
      "w835 = -0.030070856417115523\n",
      "w836 = -0.14462023187732723\n",
      "w837 = 0.05580479988422689\n",
      "w838 = -0.02812911709164551\n",
      "w839 = 0.011081321742871644\n",
      "w840 = 0.05589912519865997\n",
      "w841 = -0.005233568033125846\n",
      "w842 = -0.00770035984624265\n",
      "w843 = 0.034855120340905446\n",
      "w844 = -0.08073909999555395\n",
      "w845 = 0.18385893384695842\n",
      "w846 = -0.018281776423344664\n",
      "w847 = -0.05619530705652594\n",
      "w848 = -0.10303083757347821\n",
      "w849 = -0.13667821818530854\n",
      "w850 = -0.050782542469984725\n",
      "w851 = 0.06954323889283565\n",
      "w852 = 0.06222436233802364\n",
      "w853 = 0.0030866997476584495\n",
      "w854 = -0.055702557450302184\n",
      "w855 = 0.1263937122243606\n",
      "w856 = 0.007320369134504696\n",
      "w857 = -0.04559905267991653\n",
      "w858 = 0.04299951564819863\n",
      "w859 = -0.06557938644763885\n",
      "w860 = 0.10085097845847153\n",
      "w861 = -0.12562133150941956\n",
      "w862 = 0.14037132748928796\n",
      "w863 = -0.03936646084382009\n",
      "w864 = -0.004915617719111652\n",
      "w865 = 0.00542791998859306\n",
      "w866 = 0.027881504596855976\n",
      "w867 = 0.13261952592577672\n",
      "w868 = -0.15577818523208284\n",
      "w869 = 0.0877350498518464\n",
      "w870 = -0.022658987380573516\n",
      "w871 = 0.015702267192065678\n",
      "w872 = 0.2413477064807392\n",
      "w873 = -0.17577704043011447\n",
      "w874 = 0.07535704409377786\n",
      "w875 = 0.11783697454911564\n",
      "w876 = 0.06587847782386994\n",
      "w877 = 0.03495313192798399\n",
      "w878 = 0.04412421420795022\n",
      "w879 = 0.1630861365823011\n",
      "w880 = 0.0765196049593321\n",
      "w881 = -0.0008788614828759615\n",
      "w882 = -0.0007637183477794076\n",
      "w883 = 0.022992934351415726\n",
      "w884 = -0.027271945573206675\n",
      "w885 = 0.05560794713098866\n",
      "w886 = 0.04577159525918622\n",
      "w887 = 0.011147474747656387\n",
      "w888 = 0.006987395787329801\n",
      "w889 = 0.017650032701422604\n",
      "w890 = -0.13380223952846618\n",
      "w891 = -0.04169291124983002\n",
      "w892 = 0.02739216921790884\n",
      "w893 = 0.001685906328277362\n",
      "w894 = 0.01074325795130413\n",
      "w895 = 0.04109434946241889\n",
      "w896 = 0.07551309136286173\n",
      "w897 = 0.17231977504315493\n",
      "w898 = -0.07248973681625578\n",
      "w899 = 0.02188817001995777\n",
      "w900 = 0.2304189352615141\n",
      "w901 = -0.03337098027783467\n",
      "w902 = 0.019591390754543678\n",
      "w903 = 0.04748727839274722\n",
      "w904 = -0.09408901589199248\n",
      "w905 = -0.009374644995544438\n",
      "w906 = 0.04786411600435644\n",
      "w907 = 0.09338112604817152\n",
      "w908 = 0.15546601230147786\n",
      "w909 = 0.11959655324533339\n",
      "w910 = 0.004548782738112846\n",
      "w911 = -0.02143308688490495\n",
      "w912 = -0.0009396320407995501\n",
      "w913 = -0.11870619872148769\n",
      "w914 = 0.022184310554576262\n",
      "w915 = -0.13766532031739168\n",
      "w916 = -0.015967073220878505\n",
      "w917 = 0.20996664131438053\n",
      "w918 = 0.1571724972293074\n",
      "w919 = -0.06645360066420353\n",
      "w920 = 0.02762952575018127\n",
      "w921 = 0.009621778741534637\n",
      "w922 = -0.003481331399445072\n",
      "w923 = -0.15496660722647415\n",
      "w924 = 0.05094633031884673\n",
      "w925 = -0.13886836585466983\n",
      "w926 = -0.1199474096860471\n",
      "w927 = 0.0694317740747619\n",
      "w928 = 0.06310398428475388\n",
      "w929 = 0.003114317884872684\n",
      "w930 = 0.018340659884801155\n",
      "w931 = -0.023373002607420496\n",
      "w932 = 0.017049001697385744\n",
      "w933 = 0.056869073616961904\n",
      "w934 = 0.06249803443427835\n",
      "w935 = 0.06777924558205894\n",
      "w936 = 0.35245120129512175\n",
      "w937 = 0.03515547578063333\n",
      "w938 = -0.04524178170961064\n",
      "w939 = 0.10139928926050835\n",
      "w940 = -0.024882781598758605\n",
      "w941 = 0.12713211177431794\n",
      "w942 = -0.12609170614168957\n",
      "w943 = -0.12260531677877524\n",
      "w944 = 0.04247643405359112\n",
      "w945 = 0.017796854409294554\n",
      "w946 = -0.08515665540552002\n",
      "w947 = 0.023953782439209932\n",
      "w948 = -0.08689088643498952\n",
      "w949 = -0.015090554528346237\n",
      "w950 = -0.08068116247884174\n",
      "w951 = 0.05038680071324938\n",
      "w952 = -0.012664715978375762\n",
      "w953 = 0.05837658310623762\n",
      "w954 = 0.05644961599147681\n",
      "w955 = -0.15149293817608445\n",
      "w956 = -0.119768507808841\n",
      "w957 = -0.1347496021008555\n",
      "w958 = 0.2318506563214384\n",
      "w959 = -0.04427776388144189\n",
      "w960 = -0.07544259570097263\n",
      "w961 = -0.07715797213708388\n",
      "w962 = 0.010846084569747066\n",
      "w963 = -0.06558218056865357\n",
      "w964 = -0.03678907120467932\n",
      "w965 = 0.036285352681700765\n",
      "w966 = -0.08789893631289752\n",
      "w967 = 0.08529334488742342\n",
      "w968 = -0.05391601097243075\n",
      "w969 = 0.06257830352081671\n",
      "w970 = -0.07116750696390599\n",
      "w971 = 0.014557247016185179\n",
      "w972 = 0.21917084196989386\n",
      "w973 = 0.25123602889237195\n",
      "w974 = -0.08610373335963362\n",
      "w975 = 0.00968798638407363\n",
      "w976 = 0.009664309876073189\n",
      "w977 = -0.03627231796185073\n",
      "w978 = 0.1497223052426872\n",
      "w979 = 0.01336452526424345\n",
      "w980 = -0.049931525318763464\n",
      "w981 = 0.06626357569119608\n",
      "w982 = 0.002069789786937335\n",
      "w983 = 0.0881911058927375\n",
      "w984 = -0.03178497669804163\n",
      "w985 = -0.010105658916072494\n",
      "w986 = -0.0025831156275511902\n",
      "w987 = -0.026860712268057578\n",
      "w988 = -0.051407381859283716\n",
      "w989 = -0.03047971249772507\n",
      "w990 = -0.030830142929211258\n",
      "w991 = -0.06797016899847275\n",
      "w992 = 0.036414132608061936\n",
      "w993 = 0.02931335265723739\n",
      "w994 = -0.21485345760668564\n",
      "w995 = 0.0041148702774083164\n",
      "w996 = -0.1557947475710197\n",
      "w997 = 0.055275622078233375\n",
      "w998 = -0.012441511028409822\n",
      "w999 = -0.09389381477405556\n",
      "w1000 = -0.054901463862860614\n",
      "w1001 = -0.21145073668208544\n",
      "w1002 = -0.12312329828196318\n",
      "w1003 = -0.10192271763300957\n",
      "w1004 = -0.026450369424779276\n",
      "w1005 = -0.019658754798752478\n",
      "w1006 = -0.013966997700872404\n",
      "w1007 = 0.07673435977255534\n",
      "w1008 = -0.02663360497109716\n",
      "w1009 = 0.12602644382602984\n",
      "w1010 = 0.0606479949203935\n",
      "w1011 = 0.07364712084526973\n",
      "w1012 = -0.17962324037822983\n",
      "w1013 = 0.07996616545738447\n",
      "w1014 = -0.0366743120964639\n",
      "w1015 = 0.02359234758401384\n",
      "w1016 = -0.06922206397715466\n",
      "w1017 = -0.1334609261712909\n",
      "w1018 = -0.10398417654151251\n",
      "w1019 = -0.011524131718523923\n",
      "w1020 = 0.005572250219244274\n",
      "w1021 = 0.11463586589716593\n",
      "w1022 = 0.026336663431943186\n",
      "w1023 = -0.008297885548114742\n",
      "w1024 = 0.19556296033287807\n",
      "w1025 = -0.06962269651493214\n",
      "w1026 = 0.15233689243605555\n",
      "w1027 = 0.06491635136055483\n",
      "w1028 = -0.04203393005493939\n",
      "w1029 = -0.05114638461855574\n",
      "w1030 = 0.007780957039267875\n",
      "w1031 = -0.0820319952368757\n",
      "w1032 = -0.07536200103106971\n",
      "w1033 = 0.05271389866621466\n",
      "w1034 = -0.05253804347728933\n",
      "w1035 = -0.021349034354284084\n",
      "w1036 = 0.010301562485125176\n",
      "w1037 = 0.09702843042995639\n",
      "w1038 = 0.14582601291546374\n",
      "w1039 = 0.03757535197521837\n",
      "w1040 = 0.038724520948135395\n",
      "w1041 = 0.04147294001744727\n",
      "w1042 = 0.033945860130774115\n",
      "w1043 = -0.005510494960258508\n",
      "w1044 = -0.08430130220816123\n",
      "w1045 = -0.13479553137261172\n",
      "w1046 = -0.11630150487456122\n",
      "w1047 = 0.039060597126810974\n",
      "w1048 = 0.1443651953852463\n",
      "w1049 = 0.04063987466442992\n",
      "w1050 = -0.059065225676875666\n",
      "w1051 = 0.08002726706604897\n",
      "w1052 = 0.03382889659658498\n",
      "w1053 = 0.0034761452091458343\n",
      "w1054 = -0.021717429179767718\n",
      "w1055 = -0.004275730007912451\n",
      "w1056 = -0.09507235083776529\n",
      "w1057 = -0.04814534928785987\n",
      "w1058 = -0.12137417657340652\n",
      "w1059 = -0.06628471497762704\n",
      "w1060 = 0.06406256868259762\n",
      "w1061 = 0.02077064518187739\n",
      "w1062 = 0.29955784266899677\n",
      "w1063 = 0.04292572817561375\n",
      "w1064 = 0.0855292385538447\n",
      "w1065 = -0.004319784927702538\n",
      "w1066 = 0.01524138566646534\n",
      "w1067 = -0.018450563965635058\n",
      "w1068 = 0.09806095852148143\n",
      "w1069 = 0.1348572025619117\n",
      "w1070 = 0.0457962235457625\n",
      "w1071 = 0.07711111838341828\n",
      "w1072 = 0.016812813179963203\n",
      "w1073 = -0.044573560341440416\n",
      "w1074 = 0.003000518485239501\n",
      "w1075 = -0.10319971623438473\n",
      "w1076 = -0.006480887191553791\n",
      "w1077 = 0.05013717020858332\n",
      "w1078 = -0.06963875380792041\n",
      "w1079 = 0.09901222234439211\n",
      "w1080 = -0.033508021362326355\n",
      "w1081 = 0.07853793115835178\n",
      "w1082 = -0.07966018235180097\n",
      "w1083 = 0.00045524043123930326\n",
      "w1084 = -0.08247393566568371\n",
      "w1085 = 0.06498379205129962\n",
      "w1086 = 0.18884672504810893\n",
      "w1087 = 0.03810814759450874\n",
      "w1088 = -0.12207350294683529\n",
      "w1089 = 0.020660682483923803\n",
      "w1090 = -0.04088018849445622\n",
      "w1091 = -0.19522298864071813\n",
      "w1092 = 0.03592756546035956\n",
      "w1093 = 0.032474515412049536\n",
      "w1094 = -0.0032688321687599467\n",
      "w1095 = -0.029557882080249512\n",
      "w1096 = -0.029795659674627866\n",
      "w1097 = -0.05154904686697236\n",
      "w1098 = 0.1488028945616242\n",
      "w1099 = -0.007031334867227198\n",
      "w1100 = -0.055589059601878925\n",
      "w1101 = 0.022015326003991507\n",
      "w1102 = -0.04522919221801684\n",
      "w1103 = 0.03770683164444395\n",
      "w1104 = -0.017029680844938007\n",
      "w1105 = 0.09672005339147473\n",
      "w1106 = 0.050076769326497565\n",
      "w1107 = -0.019248204753090886\n",
      "w1108 = 0.01171288589204042\n",
      "w1109 = -0.07034818815253521\n",
      "w1110 = 0.07780606461481678\n",
      "w1111 = 0.09591398729664874\n",
      "w1112 = 0.06282067156633712\n",
      "w1113 = -0.15490059101641449\n",
      "w1114 = -0.03221616035690708\n",
      "w1115 = -0.24590885713689575\n",
      "w1116 = 0.059456793955179485\n",
      "w1117 = 0.17050512201813883\n",
      "w1118 = 0.14124069745468285\n",
      "w1119 = -0.03450201087906473\n",
      "w1120 = -0.1405704592959759\n",
      "w1121 = -0.051942005944772114\n",
      "w1122 = 0.0016722320336412392\n",
      "w1123 = 0.13226268954596954\n",
      "w1124 = -0.0008796624737215707\n",
      "w1125 = 0.11795741542684895\n",
      "w1126 = -0.12907113897845315\n",
      "w1127 = -0.003965108003535263\n",
      "w1128 = 0.04421044655719719\n",
      "w1129 = 0.03989484493615244\n",
      "w1130 = 0.06529886558260767\n",
      "w1131 = 0.048621701173518515\n",
      "w1132 = 0.06661166887215769\n",
      "w1133 = 0.03412390528173977\n",
      "w1134 = 0.10560773786228396\n",
      "w1135 = 0.01923347243864533\n",
      "w1136 = 0.12483585563682395\n",
      "w1137 = 0.04486569673856132\n",
      "w1138 = 0.04472291785099028\n",
      "w1139 = -0.03381653430819349\n",
      "w1140 = -0.03606290173911177\n",
      "w1141 = -0.09666268355526152\n",
      "w1142 = -0.11036769342385194\n",
      "w1143 = 0.002844249576990024\n",
      "w1144 = -0.018231700408529783\n",
      "w1145 = -0.019920719358058753\n",
      "w1146 = -0.043580520815276885\n",
      "w1147 = 0.01790035043346155\n",
      "w1148 = 0.03174368184365496\n",
      "w1149 = -0.05341876177906745\n",
      "w1150 = 0.14305703560333577\n",
      "w1151 = -0.014514513319138413\n",
      "w1152 = 0.013473005144637607\n",
      "w1153 = 0.007175901294272365\n",
      "w1154 = 0.03549713409020233\n",
      "w1155 = 0.09667615118315286\n",
      "w1156 = -0.02074478297196621\n",
      "w1157 = 0.15472789145088556\n",
      "w1158 = -0.1052858754747059\n",
      "w1159 = 0.0239329002249824\n",
      "w1160 = -0.028983156139749413\n",
      "w1161 = -0.004543470343331756\n",
      "w1162 = -0.11513001230881138\n",
      "w1163 = -0.02727793698402378\n",
      "w1164 = -0.09206431285553825\n",
      "w1165 = 0.06890656432429298\n",
      "w1166 = 0.07308565454977019\n",
      "w1167 = -0.08135285793161244\n",
      "w1168 = 0.0339660358172765\n",
      "w1169 = -0.14305925175101664\n",
      "w1170 = -0.024199749028364557\n",
      "w1171 = 0.032711618476815585\n",
      "w1172 = 0.08032616876101314\n",
      "w1173 = -0.031236476564113727\n",
      "w1174 = 0.015756708422263375\n",
      "w1175 = 0.023406728299441494\n",
      "w1176 = 0.04121424934184536\n",
      "w1177 = 0.01101502020265677\n",
      "w1178 = 0.024662031364609226\n",
      "w1179 = 0.015606270463730096\n",
      "w1180 = 0.10149217239793415\n",
      "w1181 = 0.06898255049987018\n",
      "w1182 = 0.10653562575086752\n",
      "w1183 = -0.08493116982492234\n",
      "w1184 = 0.17474627089017072\n",
      "w1185 = -0.04765668075639382\n",
      "w1186 = 0.03958089362902026\n",
      "w1187 = -0.07977058889981961\n",
      "w1188 = 0.09013385240330697\n",
      "w1189 = 0.010438131090252673\n",
      "w1190 = 0.14968041612622426\n",
      "w1191 = -0.11217419930075652\n",
      "w1192 = 0.0018108240848361203\n",
      "w1193 = -0.04742334975328237\n",
      "w1194 = 0.025940837620819498\n",
      "w1195 = -0.008037367319760883\n",
      "w1196 = -0.005391943119362751\n",
      "w1197 = -0.15151036622483446\n",
      "w1198 = -0.06180880957982754\n",
      "w1199 = 0.04833253008739934\n",
      "w1200 = 0.13107235633421122\n",
      "w1201 = -0.23865012768644972\n",
      "w1202 = 0.09435307922874647\n",
      "w1203 = -0.007314699633722523\n",
      "w1204 = 0.12754720689363974\n",
      "w1205 = 0.04960157975964879\n",
      "w1206 = 0.042902394483461774\n",
      "w1207 = 0.01685139888142154\n",
      "w1208 = 0.02716979378573236\n",
      "w1209 = -0.03387487098137906\n",
      "w1210 = 0.1795762352020431\n",
      "w1211 = 0.017231157416257237\n",
      "w1212 = 0.10962720244718918\n",
      "w1213 = 0.0608046197807174\n",
      "w1214 = 0.06356363986049457\n",
      "w1215 = 0.026542486086258848\n",
      "w1216 = -0.09467485446230164\n",
      "w1217 = 0.00584139095785362\n",
      "w1218 = 0.030814939450846682\n",
      "w1219 = 0.057591141874529225\n",
      "w1220 = -0.06139070454545914\n",
      "w1221 = -0.06815303018982398\n",
      "w1222 = 0.08301270731902602\n",
      "w1223 = 0.056021783734815325\n",
      "w1224 = 0.04293882770304098\n",
      "w1225 = -0.3434395518772994\n",
      "w1226 = 0.034351189522538045\n",
      "w1227 = -0.059385095605325025\n",
      "w1228 = 0.10201794754988447\n",
      "w1229 = 0.05195497823561894\n",
      "w1230 = -0.015104598376415517\n",
      "w1231 = -0.11641528289039976\n",
      "w1232 = 0.030455764357189004\n",
      "w1233 = 0.026929429871202056\n",
      "w1234 = -0.1137616799952099\n",
      "w1235 = -0.015887612354042773\n",
      "w1236 = 0.028225220408394393\n",
      "w1237 = -0.10114350838740617\n",
      "w1238 = 0.04934175621506806\n",
      "w1239 = -0.12076181627550638\n",
      "w1240 = 0.020324487027579746\n",
      "w1241 = -0.07699315596006201\n",
      "w1242 = -0.09883144199103912\n",
      "w1243 = -0.026195963656498275\n",
      "w1244 = -0.115595432197729\n",
      "w1245 = 0.01792324994060651\n",
      "w1246 = 0.0729968984044261\n",
      "w1247 = 0.07121947233540503\n",
      "w1248 = -0.014095844254721672\n",
      "w1249 = -0.17170349930782952\n",
      "w1250 = -0.11321822338386749\n",
      "w1251 = 0.0033915183118330776\n",
      "w1252 = -0.20288611128232867\n",
      "w1253 = -0.19229344776548998\n",
      "w1254 = -0.09525297927832431\n",
      "w1255 = 0.05867166515438394\n",
      "w1256 = 0.15007092592279156\n",
      "w1257 = 0.14003273319974235\n",
      "w1258 = 0.0018557286615006\n",
      "w1259 = -0.04085979174978098\n",
      "w1260 = -0.007866675140669806\n",
      "w1261 = -0.13044598051699308\n",
      "w1262 = 0.04922253600926035\n",
      "w1263 = -0.18275694702297554\n",
      "w1264 = 0.07473257719442514\n",
      "w1265 = 0.09439953273847251\n",
      "w1266 = -0.058409493069457676\n",
      "w1267 = -0.10592273053855646\n",
      "w1268 = -0.1623992569380193\n",
      "w1269 = -0.053371287174873654\n",
      "w1270 = 0.047783146426903775\n",
      "w1271 = 0.12097077313930521\n",
      "w1272 = -0.040477161153324816\n",
      "w1273 = -0.21450065707075455\n",
      "w1274 = 0.08011572266269025\n",
      "w1275 = 0.14802837940724767\n",
      "w1276 = 0.07161786596914288\n",
      "w1277 = -0.033628376295096594\n",
      "w1278 = -0.23285857370167634\n",
      "w1279 = 0.09198772814920744\n",
      "w1280 = 0.19447121103609935\n",
      "w1281 = 0.0648627635387789\n",
      "w1282 = -0.024779506524699173\n",
      "w1283 = -0.08166429662305946\n",
      "w1284 = 0.02802262886367179\n",
      "w1285 = 0.02936206118661536\n",
      "w1286 = 0.06415320243744818\n",
      "w1287 = 0.0004223554681067259\n",
      "w1288 = 0.10388665180093046\n",
      "w1289 = -0.14001035194156333\n",
      "w1290 = -0.001805125991784188\n",
      "w1291 = 0.05608449943447208\n",
      "w1292 = 0.05459435760523156\n",
      "w1293 = -0.005297711287672068\n",
      "w1294 = -0.07980357549430132\n",
      "w1295 = -0.10095392087554773\n",
      "w1296 = 0.013104375779274788\n",
      "w1297 = 0.007551936637134014\n",
      "w1298 = -0.01641978924593873\n",
      "w1299 = -0.04073322209927847\n",
      "w1300 = 0.06693986584471258\n",
      "w1301 = 0.13883656886981016\n",
      "w1302 = 0.08587460862755703\n",
      "w1303 = -0.10727249069992066\n",
      "w1304 = 0.11969073642223456\n",
      "w1305 = 0.10061298486724396\n",
      "w1306 = 0.0057241350411019395\n",
      "w1307 = 0.05882156927278008\n",
      "w1308 = -0.11719842311514274\n",
      "w1309 = 0.0892332591948321\n",
      "w1310 = 0.04026088365110292\n",
      "w1311 = -0.051581511646926814\n",
      "w1312 = -0.01121297595437252\n",
      "w1313 = 0.10308218737958615\n",
      "w1314 = 0.09183335660501198\n",
      "w1315 = -0.0951520281965781\n",
      "w1316 = 0.06767840382041874\n",
      "w1317 = 0.01192372964849038\n",
      "w1318 = 0.16415475721571102\n",
      "w1319 = 0.048154329545556665\n",
      "w1320 = -0.08577498172274199\n",
      "w1321 = 0.02350948306202966\n",
      "w1322 = -0.058022788411925263\n",
      "w1323 = -0.009071468240522563\n",
      "w1324 = -0.11639923413009128\n",
      "w1325 = 0.040085466561921176\n",
      "w1326 = 0.036534254583666805\n",
      "w1327 = -0.02795347408410967\n",
      "w1328 = 0.15672702634319477\n",
      "w1329 = -0.010299431776463936\n",
      "w1330 = -0.04031048937917772\n",
      "w1331 = -0.07210189095084313\n",
      "w1332 = 0.07908456437206433\n",
      "w1333 = -0.05107216642389309\n",
      "w1334 = 0.08434603876684228\n",
      "w1335 = 0.0764526973142931\n",
      "w1336 = 0.046955710280811754\n",
      "w1337 = -0.21020606833245545\n",
      "w1338 = -0.036869554653185165\n",
      "w1339 = 0.06704698311315284\n",
      "w1340 = -0.1868543767954404\n",
      "w1341 = 0.02946795234953661\n",
      "w1342 = 0.04459499068738599\n",
      "w1343 = 0.06798555318193826\n",
      "w1344 = -0.09646463352818128\n",
      "w1345 = 0.02623319253210235\n",
      "w1346 = 0.11852663037286768\n",
      "w1347 = -0.020932722505054777\n",
      "w1348 = -0.016680097673149208\n",
      "w1349 = 0.05069893778536399\n",
      "w1350 = -0.0965957601087022\n",
      "w1351 = 0.019404745657294477\n",
      "w1352 = -0.017886674751881725\n",
      "w1353 = 0.03302117839065369\n",
      "w1354 = 0.07852918236172667\n",
      "w1355 = 0.0036104710472790657\n",
      "w1356 = -0.03404805493489731\n",
      "w1357 = -0.035733431784674065\n",
      "w1358 = 0.005321770432028479\n",
      "w1359 = -0.11011546096416532\n",
      "w1360 = 0.12295070248067327\n",
      "w1361 = -0.03217278817343041\n",
      "w1362 = -0.18484458928946856\n",
      "w1363 = -0.015180576986961195\n",
      "w1364 = 0.09321215727376674\n",
      "w1365 = 0.02432053106718709\n",
      "w1366 = 0.04299915588096415\n",
      "w1367 = -0.15863661926585626\n",
      "w1368 = 0.03143930812449581\n",
      "w1369 = -0.07310574680684052\n",
      "w1370 = -0.06506498673546765\n",
      "w1371 = -0.09256115213759009\n",
      "w1372 = 0.14282474472347095\n",
      "w1373 = -0.10830618644706239\n",
      "w1374 = 0.07301603676628561\n",
      "w1375 = 0.03913398574171193\n",
      "w1376 = -0.010279937808934676\n",
      "w1377 = 0.08990753201623267\n",
      "w1378 = 0.013656751073293354\n",
      "w1379 = 5.25479667065513e-05\n",
      "w1380 = -0.0065086735462785246\n",
      "w1381 = 0.09949357085195688\n",
      "w1382 = 0.039154586393997756\n",
      "w1383 = 0.035077351321889196\n",
      "w1384 = -0.03798483700079285\n",
      "w1385 = -0.04444510182654396\n",
      "w1386 = -0.062431448424064256\n",
      "w1387 = -0.006194194291543992\n",
      "w1388 = -0.070803787743311\n",
      "w1389 = -0.3651202050797417\n",
      "w1390 = -0.08165797618144918\n",
      "w1391 = -0.19402435100418106\n",
      "w1392 = 0.06558383714353103\n",
      "w1393 = -0.04154883177363508\n",
      "w1394 = 0.023557454405518066\n",
      "w1395 = -0.013575496734417604\n",
      "w1396 = 0.03489593809157465\n",
      "w1397 = 0.01055986726725799\n",
      "w1398 = -0.037552707779431545\n",
      "w1399 = 0.06480101619492432\n",
      "w1400 = 0.1431298626339785\n",
      "w1401 = -0.055690322894034215\n",
      "w1402 = -0.02736040889245607\n",
      "w1403 = 0.06984374398000584\n",
      "w1404 = 0.042155712623779124\n",
      "w1405 = -0.019339503146565653\n",
      "w1406 = -0.03689393198522433\n",
      "w1407 = -0.026610094348246724\n",
      "w1408 = -0.10328167533962387\n",
      "w1409 = -0.00018764650888760127\n",
      "w1410 = -0.016301524726694924\n",
      "w1411 = 0.04909197041238457\n",
      "w1412 = -0.12030383446882739\n",
      "w1413 = -0.05216697413310584\n",
      "w1414 = -0.018016935008731822\n",
      "w1415 = 0.10598241862616395\n",
      "w1416 = -0.041530174470763355\n",
      "w1417 = -0.05729167641095621\n",
      "w1418 = -0.06783403314471968\n",
      "w1419 = -0.07909812385197262\n",
      "w1420 = -0.15337078919041092\n",
      "w1421 = 0.08572624065809185\n",
      "w1422 = 0.12196986813191377\n",
      "w1423 = -0.03405733868833359\n",
      "w1424 = 0.01651919906940857\n",
      "w1425 = -0.0025671314020307996\n",
      "w1426 = -0.07885231304518896\n",
      "w1427 = 0.019372431547913124\n",
      "w1428 = -0.005338430189060961\n",
      "w1429 = -0.10490890754090207\n",
      "w1430 = -0.031241811161715302\n",
      "w1431 = 0.0464120350614994\n",
      "w1432 = 0.060270458166226136\n",
      "w1433 = -0.010158636858645525\n",
      "w1434 = 0.021329698996869838\n",
      "w1435 = -0.015837867776507225\n",
      "w1436 = 0.03910174536903865\n",
      "w1437 = -0.006873049141266683\n",
      "w1438 = 0.09328583107511866\n",
      "w1439 = 0.009213905564073668\n",
      "w1440 = 0.06440076208097871\n",
      "w1441 = 0.008633668272692414\n",
      "w1442 = -0.032559952386873585\n",
      "w1443 = 0.05482018883688065\n",
      "w1444 = -0.06394281995063031\n",
      "w1445 = 0.03351402255526201\n",
      "w1446 = 0.058782768304496795\n",
      "w1447 = -0.07474005674035729\n",
      "w1448 = -0.10824577402316879\n",
      "w1449 = -0.013788848198485677\n",
      "w1450 = -0.05459132018965823\n",
      "w1451 = 0.02213674910183038\n",
      "w1452 = -0.06855308055196689\n",
      "w1453 = 0.07661902105284485\n",
      "w1454 = -0.1052038973852621\n",
      "w1455 = 0.03011127823374752\n",
      "w1456 = 0.025661417762963913\n",
      "w1457 = -0.19550544910236264\n",
      "w1458 = -0.10218895029902968\n",
      "w1459 = -0.005745372709964643\n",
      "w1460 = 0.025613970729982703\n",
      "w1461 = 0.09916899848528282\n",
      "w1462 = 0.06765058453266178\n",
      "w1463 = 0.010634646068721028\n",
      "w1464 = 0.02890702070024561\n",
      "w1465 = -0.06072194620512142\n",
      "w1466 = -0.028667311422288684\n",
      "w1467 = 0.05998281925763007\n",
      "w1468 = -0.17703684914933268\n",
      "w1469 = 0.10613136840252936\n",
      "w1470 = -0.13051883092220262\n",
      "w1471 = 0.04742464007621625\n",
      "w1472 = 0.046364129927857926\n",
      "w1473 = -0.027155395204334118\n",
      "w1474 = -0.03598033456736065\n",
      "w1475 = -0.07916846988721832\n",
      "w1476 = -0.09699254011933492\n",
      "w1477 = 0.11241379108657946\n",
      "w1478 = -0.03952903885855844\n",
      "w1479 = -0.004312139374867752\n",
      "w1480 = -0.15054122273623752\n",
      "w1481 = 0.18936347732253433\n",
      "w1482 = 0.06883427750911886\n",
      "w1483 = 0.006684209621203881\n",
      "w1484 = -0.12585730051480634\n",
      "w1485 = -0.019075069801938044\n",
      "w1486 = 0.012915213289068021\n",
      "w1487 = 0.07611877186970512\n",
      "w1488 = -0.1291832072220741\n",
      "w1489 = -0.061764321767258754\n",
      "w1490 = 0.012892842290833912\n",
      "w1491 = -0.07620929348158714\n",
      "w1492 = -0.008657181817834238\n",
      "w1493 = 0.04594236275282389\n",
      "w1494 = -0.05343830268201135\n",
      "w1495 = -0.06043251090960998\n",
      "w1496 = -0.07109200847669299\n",
      "w1497 = 0.05266667459654213\n",
      "w1498 = 0.04886835208230265\n",
      "w1499 = 0.0335234723846622\n",
      "w1500 = 0.03994778965448937\n",
      "w1501 = -0.04243802927624062\n",
      "w1502 = 0.10546360653146532\n",
      "w1503 = -0.05868671289714357\n",
      "w1504 = 0.11119684942096199\n",
      "w1505 = -0.06729235218773837\n",
      "w1506 = -0.1450652296690392\n",
      "w1507 = 0.10585660521360787\n",
      "w1508 = -0.01500602708112155\n",
      "w1509 = 0.017489780829288218\n",
      "w1510 = 0.11692767903286527\n",
      "w1511 = -0.03224726044813522\n",
      "w1512 = 0.07650897220378071\n",
      "w1513 = 0.020774976993214653\n",
      "w1514 = 0.04563375136536481\n",
      "w1515 = -0.058016676494290265\n",
      "w1516 = 0.01767497707229312\n",
      "w1517 = 0.08444739115495657\n",
      "w1518 = 0.00012561691312796774\n",
      "w1519 = 0.27761237394914623\n",
      "w1520 = -0.09308876607205283\n",
      "w1521 = -0.042196555962081245\n",
      "w1522 = -0.17282310566913409\n",
      "w1523 = -0.04206421973060074\n",
      "w1524 = 0.01073909047872141\n",
      "w1525 = -0.04806255768532712\n",
      "w1526 = -0.12400321593936274\n",
      "w1527 = 0.05190900995087205\n",
      "w1528 = -0.06242789834366046\n",
      "w1529 = 0.08746847793925765\n",
      "w1530 = 0.046927605005267624\n",
      "w1531 = 0.038423689906128565\n",
      "w1532 = -0.07965755638467288\n",
      "w1533 = -0.19909751113181393\n",
      "w1534 = 0.06279507923296872\n",
      "w1535 = -0.015133571818651664\n",
      "w1536 = 0.1381369658507188\n",
      "w1537 = -0.048483566283088\n",
      "w1538 = 0.003523570114236924\n",
      "w1539 = 0.0798751317441855\n",
      "w1540 = 0.021922286819544723\n",
      "w1541 = -0.20895548273455034\n",
      "w1542 = 0.09772448603478241\n",
      "w1543 = 0.008113302156217592\n",
      "w1544 = -0.004689263139450393\n",
      "w1545 = 0.07744747103348132\n",
      "w1546 = -0.06714952170469254\n",
      "w1547 = 0.03648901094873763\n",
      "w1548 = 0.07806604559393712\n",
      "w1549 = 0.07770361114010799\n",
      "w1550 = -0.0943629431804555\n",
      "w1551 = -0.022630955098473828\n",
      "w1552 = -0.03031335326454949\n",
      "w1553 = 0.11398678444133949\n",
      "w1554 = 0.10542507679865125\n",
      "w1555 = -0.13402373176123375\n",
      "w1556 = -0.10379991947581245\n",
      "w1557 = 0.11843570425811932\n",
      "w1558 = -0.010235131314095565\n",
      "w1559 = 0.0018432948155937226\n",
      "w1560 = 0.0017143948474616726\n",
      "w1561 = 0.015151504115889867\n",
      "w1562 = 0.08078092499035978\n",
      "w1563 = 0.030918215321092762\n",
      "w1564 = 0.1423511739546438\n",
      "w1565 = 0.09247870852810193\n",
      "w1566 = 0.12433740957546155\n",
      "w1567 = 0.05823311628720419\n",
      "w1568 = 0.005741715973152941\n",
      "w1569 = -0.02627179613035731\n",
      "w1570 = -0.004544188588555271\n",
      "w1571 = -0.029566680049357592\n",
      "w1572 = 0.12457564066722747\n",
      "w1573 = -0.06085199704277772\n",
      "w1574 = 0.03203928465457155\n",
      "w1575 = 0.03369022608412328\n",
      "w1576 = 0.12446355664658244\n",
      "w1577 = 0.03376332311529813\n",
      "w1578 = 0.017247218577623576\n",
      "w1579 = -0.060057767461457565\n",
      "w1580 = -0.07105213329708153\n",
      "w1581 = -0.034497003812575464\n",
      "w1582 = -0.043188866790070723\n",
      "w1583 = -0.04508294433007753\n",
      "w1584 = 0.0385484862662563\n",
      "w1585 = -0.052404423737464165\n",
      "w1586 = 0.04609221233158609\n",
      "w1587 = 0.04735821490589555\n",
      "w1588 = 0.050304069641097855\n",
      "w1589 = -0.020582830757941057\n",
      "w1590 = 0.2338944551135885\n",
      "w1591 = 0.08839158899507844\n",
      "w1592 = 0.025736814474159972\n",
      "w1593 = -0.09810363614161481\n",
      "w1594 = 0.14047514243530537\n",
      "w1595 = -0.08116612122327856\n",
      "w1596 = 0.0878375016801654\n",
      "w1597 = 0.001930346630526231\n",
      "w1598 = 0.0579020903988018\n",
      "w1599 = -0.012739738089492273\n",
      "w1600 = -0.09926896353635094\n",
      "w1601 = -0.04232021653920868\n",
      "w1602 = -0.007056125445145039\n",
      "w1603 = -0.004376285713617436\n",
      "w1604 = -0.006695079344982131\n",
      "w1605 = 0.17789719932964185\n",
      "w1606 = -0.11588964657945977\n",
      "w1607 = 0.17456469260808036\n",
      "w1608 = 0.05397108158172479\n",
      "w1609 = 0.00010983890020560273\n",
      "w1610 = 0.09093826935702137\n",
      "w1611 = -0.1915023660328412\n",
      "w1612 = 0.008896407859545194\n",
      "w1613 = 0.11660229309795882\n",
      "w1614 = -0.018303969526872778\n",
      "w1615 = -0.02851066345478196\n",
      "w1616 = 0.02357890330965719\n",
      "w1617 = -0.08945202393729733\n",
      "w1618 = -0.04406700908653924\n",
      "w1619 = -0.048595124118591446\n",
      "w1620 = 0.022697456994529283\n",
      "w1621 = 0.1649032272703745\n",
      "w1622 = -0.01735614421800985\n",
      "w1623 = -0.012120555891486536\n",
      "w1624 = 0.017873726409748374\n",
      "w1625 = 0.017951004796846026\n",
      "w1626 = -0.1594201789520006\n",
      "w1627 = -0.08874568589859491\n",
      "w1628 = 0.09482742009594783\n",
      "w1629 = 0.1078137700659965\n",
      "w1630 = 0.050199880019374664\n",
      "w1631 = 0.16281680954371802\n",
      "w1632 = 0.03310155765541623\n",
      "w1633 = -0.08709437303955142\n",
      "w1634 = -0.09328298928566194\n",
      "w1635 = 0.021147286523076726\n",
      "w1636 = -0.005427938158358562\n",
      "w1637 = 0.035728985274238984\n",
      "w1638 = -0.043432412477915704\n",
      "w1639 = 0.01174748610549479\n",
      "w1640 = -0.023054809996854893\n",
      "w1641 = -0.0073584423240713025\n",
      "w1642 = -0.06682247399743184\n",
      "w1643 = -0.055816779700353265\n",
      "w1644 = 0.09991923821117149\n",
      "w1645 = -0.08372097371156553\n",
      "w1646 = -0.05289511142850675\n",
      "w1647 = 0.030760947713220924\n",
      "w1648 = -0.038218070652468704\n",
      "w1649 = -0.0067017051693395715\n",
      "w1650 = -0.0886516328533293\n",
      "w1651 = 0.001738226661407025\n",
      "w1652 = 0.0752567847219089\n",
      "w1653 = 0.08075164566842048\n",
      "w1654 = 0.11482037827556973\n",
      "w1655 = -0.08236068789607788\n",
      "w1656 = 0.06004323106469493\n",
      "w1657 = -0.13224392576611624\n",
      "w1658 = 0.13272523594689797\n",
      "w1659 = 0.09202568581068472\n",
      "w1660 = -0.06310027405131036\n",
      "w1661 = -0.17431075467140117\n",
      "w1662 = 0.05688609958475559\n",
      "w1663 = 0.11021421321977305\n",
      "w1664 = -0.07089962760409127\n",
      "w1665 = -0.08974541089645816\n",
      "w1666 = -0.12527546266085124\n",
      "w1667 = 0.03357346185452373\n",
      "w1668 = -0.004577933054478725\n",
      "w1669 = 0.1292293064552822\n",
      "w1670 = 0.0951630689222762\n",
      "w1671 = -0.020966183857807394\n",
      "w1672 = 0.006653545994962973\n",
      "w1673 = 0.03191212846297959\n",
      "w1674 = -0.14948716510412233\n",
      "w1675 = -0.1077873443361993\n",
      "w1676 = -0.0011023081319669853\n",
      "w1677 = 0.13629839651756884\n",
      "w1678 = 0.06996323029993423\n",
      "w1679 = 0.022454393848454764\n",
      "w1680 = 0.08608094906813102\n",
      "w1681 = 0.07631941260053678\n",
      "w1682 = 0.07488193393124655\n",
      "w1683 = -0.009009413623520859\n",
      "w1684 = -0.0026861501076611985\n",
      "w1685 = -0.018673769014216312\n",
      "w1686 = -0.019017888034287344\n",
      "w1687 = 0.07092578976490663\n",
      "w1688 = -0.05933332650933098\n",
      "w1689 = -0.04291371129965323\n",
      "w1690 = -0.0008928704195688882\n",
      "w1691 = -0.20592323678928162\n",
      "w1692 = 0.019661530948503026\n",
      "w1693 = 0.10389623813451845\n",
      "w1694 = -0.05745391062394037\n",
      "w1695 = 0.050159748936164796\n",
      "w1696 = 0.007070328901005757\n",
      "w1697 = 0.3811982836089952\n",
      "w1698 = -0.015776762715732782\n",
      "w1699 = 0.07138397207536827\n",
      "w1700 = 0.10299906171622718\n",
      "w1701 = 0.027405454484194635\n",
      "w1702 = -0.032295776039429916\n",
      "w1703 = -0.03281741307587164\n",
      "w1704 = -0.14124584110983254\n",
      "w1705 = -0.07122573316497118\n",
      "w1706 = -0.047729694713853674\n",
      "w1707 = -0.11817807587498166\n",
      "w1708 = -0.08105436068797846\n",
      "w1709 = 0.07126164538882611\n",
      "w1710 = -0.0001188220823451853\n",
      "w1711 = -0.22670134893377725\n",
      "w1712 = 0.10935890348886265\n",
      "w1713 = -0.04783687315245556\n",
      "w1714 = 0.15251792149586735\n",
      "w1715 = 0.21599454975540913\n",
      "w1716 = 0.11342949040798317\n",
      "w1717 = -0.04160524244666181\n",
      "w1718 = 0.09785072075540498\n",
      "w1719 = 0.0797660345458725\n",
      "w1720 = 0.10910892704250277\n",
      "w1721 = -0.006719593417992693\n",
      "w1722 = 0.016898633708858267\n",
      "w1723 = -0.01584877949877618\n",
      "w1724 = 0.13428986064812118\n",
      "w1725 = -0.07203326607859675\n",
      "w1726 = -0.029656985639343672\n",
      "w1727 = -0.10159092652423667\n",
      "w1728 = 0.06234902377424654\n",
      "w1729 = 0.024233613268673685\n",
      "w1730 = -0.01880423812652108\n",
      "w1731 = 0.05767950438371208\n",
      "w1732 = -0.004184960582202399\n",
      "w1733 = -0.020458107946430864\n",
      "w1734 = -0.0019812526673956654\n",
      "w1735 = -0.011985210582448459\n",
      "w1736 = 0.1302731875135044\n",
      "w1737 = -0.035372616002609124\n",
      "w1738 = 0.09306585015456034\n",
      "w1739 = -0.030968908862693156\n",
      "w1740 = 0.07487259353415887\n",
      "w1741 = -0.10101527441718064\n",
      "w1742 = 0.1411223595329205\n",
      "w1743 = 0.04970732646550671\n",
      "w1744 = -0.036272600290399214\n",
      "w1745 = -0.05333633397452659\n",
      "w1746 = -0.04980481079042145\n",
      "w1747 = -0.0712608851892428\n",
      "w1748 = 0.2196610296071606\n",
      "w1749 = 0.0925394383049039\n",
      "w1750 = 0.03874146707163083\n",
      "w1751 = -0.034225511104452155\n",
      "w1752 = -0.009003993794446241\n",
      "w1753 = 0.02784659193669323\n",
      "w1754 = 0.1122344188983514\n",
      "w1755 = 0.0924090163800485\n",
      "w1756 = -0.09355636408153635\n",
      "w1757 = -0.13956841159993066\n",
      "w1758 = 0.06477724204308392\n",
      "w1759 = 0.2842664756049336\n",
      "w1760 = -0.09215735309313691\n",
      "w1761 = 0.07428779250268364\n",
      "w1762 = 0.08892853609638215\n",
      "w1763 = -0.1686575042563191\n",
      "w1764 = 0.029117098780326738\n",
      "w1765 = 0.06138542672972257\n",
      "w1766 = 0.11224337913681898\n",
      "w1767 = -0.06522213034708407\n",
      "w1768 = -0.0518724635774786\n",
      "w1769 = 0.04417579830859667\n",
      "w1770 = -0.09511445991834623\n",
      "w1771 = 0.020271419255772794\n",
      "w1772 = 0.07334728832330478\n",
      "w1773 = -0.038363121366348465\n",
      "w1774 = 0.11201888754269543\n",
      "w1775 = 0.030283969107285612\n",
      "w1776 = -0.002846769093207412\n",
      "w1777 = -0.04315449944603781\n",
      "w1778 = 0.033334290280263495\n",
      "w1779 = 0.11619308894486528\n",
      "w1780 = 0.050573562685774696\n",
      "w1781 = -0.06045407225183261\n",
      "w1782 = -0.0704521450405816\n",
      "w1783 = 0.0073323310365030455\n",
      "w1784 = -0.025186049519310645\n",
      "w1785 = -0.0486862436395477\n",
      "w1786 = 0.011441662975633095\n",
      "w1787 = 0.05704857313969873\n",
      "w1788 = -0.15139805413550125\n",
      "w1789 = 0.06412991155867069\n",
      "w1790 = -0.061621506315661545\n",
      "w1791 = -0.08609543086425506\n",
      "w1792 = 0.05279309781621388\n",
      "w1793 = 0.026904301510969782\n",
      "w1794 = -0.34269360329057535\n",
      "w1795 = -0.13996715217583724\n",
      "w1796 = 0.054871339196753056\n",
      "w1797 = 0.06709860372560428\n",
      "w1798 = -0.1155376419030681\n",
      "w1799 = 0.2364456855587689\n",
      "w1800 = -0.07824213177371012\n",
      "w1801 = 0.0720494103822247\n",
      "w1802 = 0.15802668306070222\n",
      "w1803 = 0.019293852966420007\n",
      "w1804 = 0.09267047776664075\n",
      "w1805 = -0.03593060755932702\n",
      "w1806 = -0.00539517921519319\n",
      "w1807 = 0.043411746373231454\n",
      "w1808 = -0.050459601870779136\n",
      "w1809 = -0.08678806638782699\n",
      "w1810 = -0.01694289538258647\n",
      "w1811 = -0.01202837998410255\n",
      "w1812 = 0.060873388277819095\n",
      "w1813 = -0.08879291545073394\n",
      "w1814 = 0.16899147837878564\n",
      "w1815 = -0.16424256078734872\n",
      "w1816 = 0.046632418705593336\n",
      "w1817 = -0.058514965175546337\n",
      "w1818 = -0.0463255021367755\n",
      "w1819 = -0.2101492907580212\n",
      "w1820 = 0.12042330116462469\n",
      "w1821 = 0.06291594982619618\n",
      "w1822 = 0.005388841817857071\n",
      "w1823 = 0.06870942153653802\n",
      "w1824 = 0.06324871100129627\n",
      "w1825 = -0.03138857859069105\n",
      "w1826 = 0.04957882351701818\n",
      "w1827 = -0.1812740900952217\n",
      "w1828 = -0.05947490959070207\n",
      "w1829 = -0.06854324480461363\n",
      "w1830 = -0.07458732081688563\n",
      "w1831 = 0.02552291445170525\n",
      "w1832 = 0.21764466282151546\n",
      "w1833 = 0.0367835143534594\n",
      "w1834 = 0.018157447915754885\n",
      "w1835 = 0.11780057396118734\n",
      "w1836 = 0.00779386642392682\n",
      "w1837 = 0.03449497359778148\n",
      "w1838 = 0.01455600513701034\n",
      "w1839 = -0.018515908765432416\n",
      "w1840 = -0.03903247885212487\n",
      "w1841 = -0.17482552763341444\n",
      "w1842 = 0.025903690822091253\n",
      "w1843 = 0.11403925952247235\n",
      "w1844 = 0.018732622493348925\n",
      "w1845 = -0.12972449936342634\n",
      "w1846 = 0.11585108862912882\n",
      "w1847 = 0.17979748002071022\n",
      "w1848 = -0.0011043437178138342\n",
      "w1849 = 0.02448713727661131\n",
      "w1850 = 0.05724288615891629\n",
      "w1851 = -0.051709093699725976\n",
      "w1852 = 0.02130409206990589\n",
      "w1853 = -0.010367163829949911\n",
      "w1854 = -0.0781466441458412\n",
      "w1855 = -0.02191685927627791\n",
      "w1856 = -0.05813784459659055\n",
      "w1857 = 0.046386257896384064\n",
      "w1858 = -0.05651274606029185\n",
      "w1859 = 0.04155047861318961\n",
      "w1860 = 0.00468060940497757\n",
      "w1861 = -0.10987833767766264\n",
      "w1862 = -0.03403525088025685\n",
      "w1863 = -0.06015008452870527\n",
      "w1864 = 0.06594744491491536\n",
      "w1865 = 0.031799817017828656\n",
      "w1866 = 0.36685600554562453\n",
      "w1867 = -0.006206251410053793\n",
      "w1868 = 0.0292985765048928\n",
      "w1869 = -0.022350867959797214\n",
      "w1870 = 0.11655650435650618\n",
      "w1871 = -0.09702998012731419\n",
      "w1872 = 0.023465849972847994\n",
      "w1873 = -0.022846487364750034\n",
      "w1874 = -0.013808540349961294\n",
      "w1875 = 0.1768390821800903\n",
      "w1876 = 0.08123376720330372\n",
      "w1877 = -0.04119339585989975\n",
      "w1878 = -0.01788254732629844\n",
      "w1879 = -0.069484398021858\n",
      "w1880 = 0.14696683644600758\n",
      "w1881 = 0.018575263870674253\n",
      "w1882 = -0.09855181585050421\n",
      "w1883 = -0.019765051478832268\n",
      "w1884 = -0.00783321242225924\n",
      "w1885 = 0.028794028576593297\n",
      "w1886 = -0.15600285209472411\n",
      "w1887 = -0.09885460874453786\n",
      "w1888 = -0.011091340755585112\n",
      "w1889 = 0.060410481957769775\n",
      "w1890 = -0.043956086095907634\n",
      "w1891 = 0.04951450695690393\n",
      "w1892 = 0.014363809906218918\n",
      "w1893 = -0.19560181254533449\n",
      "w1894 = 0.09533191406419013\n",
      "w1895 = 0.04075560449659057\n",
      "w1896 = 0.07773776345845268\n",
      "w1897 = -0.045238586044682075\n",
      "w1898 = -0.011126075054562694\n",
      "w1899 = 0.05192662498403367\n",
      "w1900 = 0.002316489187196576\n",
      "w1901 = -0.030147719675824344\n",
      "w1902 = 0.1259466127281552\n",
      "w1903 = -0.008090952881110273\n",
      "w1904 = -0.019831774037406526\n",
      "w1905 = -0.17735945779344744\n",
      "w1906 = -0.11111381743766292\n",
      "w1907 = 0.02918928836449529\n",
      "w1908 = -0.08337895331038787\n",
      "w1909 = -0.006535865125933175\n",
      "w1910 = -0.14466901943429816\n",
      "w1911 = -0.025205801485877033\n",
      "w1912 = -0.056333525723277676\n",
      "w1913 = -0.06489796727791519\n",
      "w1914 = 0.1701004210711295\n",
      "w1915 = -0.025834342062602916\n",
      "w1916 = 0.06118521428557494\n",
      "w1917 = -0.010790765201816347\n",
      "w1918 = -0.10393204290013118\n",
      "w1919 = 0.0011277614410006493\n",
      "w1920 = 0.09585974896579684\n",
      "w1921 = -0.034424933539639095\n",
      "w1922 = 0.13219478363538198\n",
      "w1923 = 0.06405984844577443\n",
      "w1924 = 0.03537951124900687\n",
      "w1925 = -0.19900411610065322\n",
      "w1926 = -0.12018703767329945\n",
      "w1927 = -0.007527653152375107\n",
      "w1928 = -0.0854212330018133\n",
      "w1929 = 0.05279948848814066\n",
      "w1930 = -0.0445967282444052\n",
      "w1931 = -0.18352131347827463\n",
      "w1932 = -0.046774371093205924\n",
      "w1933 = 0.06405575019319289\n",
      "w1934 = -0.03289488334177455\n",
      "w1935 = -0.13275317606782633\n",
      "w1936 = 0.12930061305984522\n",
      "w1937 = 0.0218025747396404\n",
      "w1938 = -0.09005197571612582\n",
      "w1939 = -0.0476081532497806\n",
      "w1940 = 0.09632092789518196\n",
      "w1941 = 0.033267218650785246\n",
      "w1942 = 0.05841523159106923\n",
      "w1943 = -0.003644712450592636\n",
      "w1944 = 0.029334625151594504\n",
      "w1945 = 0.0941328422029578\n",
      "w1946 = -0.19905510750590316\n",
      "w1947 = 0.1309184006916164\n",
      "w1948 = 0.02416991511351424\n",
      "w1949 = -0.025224303028909\n",
      "w1950 = 0.18968673855983212\n",
      "w1951 = 0.3025431160937718\n",
      "w1952 = 0.09285221375119486\n",
      "w1953 = -0.03551547158268023\n",
      "w1954 = -0.0026864692524121306\n",
      "w1955 = -0.005446525579383451\n",
      "w1956 = -0.009691104641825694\n",
      "w1957 = 0.03396411538252451\n",
      "w1958 = -0.0401141951883752\n",
      "w1959 = -0.012604436861487236\n",
      "w1960 = -0.07599218892009685\n",
      "w1961 = 0.05563603398883302\n",
      "w1962 = -0.03585442300180529\n",
      "w1963 = 0.05894255671138593\n",
      "w1964 = 0.1763021424040335\n",
      "w1965 = 0.051605761286798675\n",
      "w1966 = -0.09155980681809754\n",
      "w1967 = 0.06159784671616437\n",
      "w1968 = -0.017074481177768552\n",
      "w1969 = -0.06599705977030858\n",
      "w1970 = 0.02276674357403326\n",
      "w1971 = -0.008463095194107994\n",
      "w1972 = -0.09610932723786132\n",
      "w1973 = 0.036044561236095084\n",
      "w1974 = -0.004843012072468814\n",
      "w1975 = 0.013067261599599185\n",
      "w1976 = -0.08887685859594606\n",
      "w1977 = -0.028642215811459645\n",
      "w1978 = 0.19049044671742013\n",
      "w1979 = 0.036675077931323796\n",
      "w1980 = -0.16147723252419421\n",
      "w1981 = 0.006438331070555408\n",
      "w1982 = -0.11728945739465804\n",
      "w1983 = -0.03300128145227876\n",
      "w1984 = -0.025578958599223496\n",
      "w1985 = 0.12033514552127457\n",
      "w1986 = 0.11107831703816148\n",
      "w1987 = 0.08225865956903541\n",
      "w1988 = 0.00892059746478483\n",
      "w1989 = -0.06178475993259672\n",
      "w1990 = -0.019955250699010857\n",
      "w1991 = -0.04371840735180769\n",
      "w1992 = -0.044527468430499775\n",
      "w1993 = -0.053692444821494004\n",
      "w1994 = -0.062457930884124095\n",
      "w1995 = 0.055834051955094534\n",
      "w1996 = -0.012826556468675875\n",
      "w1997 = 0.028094615192852626\n",
      "w1998 = -0.12251024894102486\n",
      "b = -0.00549737494778576\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAxfUlEQVR4nO3deXwV1fnH8c/DvogssiiKBhVFtK6IolChtYrVqu2vC7j83GltaWvbX1usdakr1dpaW61aqlZUcMWVVRZBRCDsO4Q9YUkgENZAluf3x52Em2Sykpt7k3zfr9d9ZebMmZkn5HKfO3POnGPujoiISHEN4h2AiIgkJiUIEREJpQQhIiKhlCBERCSUEoSIiIRSghARkVBKECI1zMz6mtnKeMchUh4lCKmzzOwGM0s2s71mtsXMxppZnyM85nozu7yM7f3MLDWkfKqZ3Qng7tPd/fQKnOshM3v9SOIVORJKEFInmdmvgWeAx4FOwInA88B1cQyrRplZo3jHILWbEoTUOWbWGngY+Jm7v+/u+9w9x90/dvffBnWamtkzZrY5eD1jZk2Dbe3N7BMz22VmmWY23cwamNkIIonm4+Cq5HdVjK/IVYaZ/d7M0sxsj5mtNLNvmtkA4A/Aj4JzLQzqdjazj4K4UszsrqjjPGRm75rZ62a2GxhqZvvN7JioOuebWYaZNa5K7FK/6BuG1EW9gWbA6DLq3AdcDJwLOPAh8EfgfuA3QCrQIah7MeDufrOZ9QXudPfPqiNQMzsdGAJc6O6bzSwJaOjua8zsceBUd78papdRwBKgM9AdmGhma9x9crD9OuAHwP8CTYFLgB8C/wq23wyMcvec6ohf6jZdQUhddAyw3d1zy6hzI/Cwu6e7ewbwJyIfngA5wHHAScGVx3Sv3KBlnYOrj8IXUFrbRx6RD/IeZtbY3de7+5qwimbWBbgU+L27Z7v7AmA4kWRQYKa7f+Du+e5+APgvcFOwf0NgEDCiEr+L1GNKEFIX7QDal3MPvjOwIWp9Q1AG8BSQAkwws7VmNrSS59/s7m2iX8AXYRXdPQW4B3gISDezUWbWOaxuEF+mu+8pFvfxUeubiu3zIZHk0xX4FpDl7rMr+ftIPaUEIXXRTOAgcH0ZdTYDJ0WtnxiU4e573P037n4ycC3wazP7ZlCv2oc/dvc33b1PEI8Dfy7lXJuBdmbWqljcadGHK3bsbOBtIlcRN6OrB6kEJQipc9w9C3gAeM7MrjezFmbW2MyuMrMng2ojgT+aWQczax/Ufx3AzK4xs1PNzIAsIreB8oP9tgEnV1esZna6mX0jaCDPBg4UO1eSmTUIfq9NwJfAE2bWzMzOBu4oiLsMrwG3Ekl2ShBSYUoQUie5+9PAr4k0PGcQufUyBPggqPIokAwsAhYD84IygG7AZ8BeIlcjz7v7lGDbE0QSyy4z+79qCLUpMAzYDmwFOgL3BtveCX7uMLN5wfIgIInI1cRo4MHyGszdfQaRpDPP3TeUVVckmmnCIJG6z8wmA2+6+/B4xyK1hxKESB1nZhcCE4EuxRq4RcqkW0widZiZ/ZfI7bJ7lByksnQFISIioXQFISIioerUUBvt27f3pKSkeIchIlJrzJ07d7u7dwjbVqcSRFJSEsnJyfEOQ0Sk1jCzUrs+6xaTiIiEUoIQEZFQShAiIhJKCUJEREIpQYiISCglCBERCaUEISIioZQgoizctIvFqVnxDkNEJCHE7EE5M3sZuAZId/ezQrb/lsi8wAVxnAF0cPdMM1sP7CEyUUuuu/eMVZzRrntuBgDrh11dE6cTEUlosbyCeBUYUNpGd3/K3c9193OJTJDyubtnRlXpH2yvkeQgIiJFxSxBuPs0ILPcihGDiEwBKSIiCSLubRBm1oLIlcZ7UcUOTDCzuWY2OD6RiYjUb4kwWN93gBnFbi/1cfc0M+sITDSzFcEVSQlBAhkMcOKJJ8Y+WhGReiLuVxDAQIrdXnL3tOBnOpGJ2XuVtrO7v+TuPd29Z4cOoSPWiohIFcQ1QZhZa+Ay4MOospZm1qpgGbgCWBKfCEVE6q9YdnMdCfQD2ptZKvAg0BjA3V8Iqn0XmODu+6J27QSMNrOC+N5093GxilNERMLFLEG4+6AK1HmVSHfY6LK1wDmxiSrcOX+aQI/jjq7JU4qIJLxEaIOIu6wDOcxcuyPeYYiIJBQlCBERCaUEISIioZQgREQklBKEiIiEUoIQEZFQShAiIhJKCUJEREIpQYiISCglCBERCaUEISIioZQgREQklBKEiIiEUoIQEZFQShAiIhJKCUJEREIpQYiISCglCBERCaUEISIioWKWIMzsZTNLN7MlpWzvZ2ZZZrYgeD0QtW2Ama00sxQzGxqrGEVEpHSxvIJ4FRhQTp3p7n5u8HoYwMwaAs8BVwE9gEFm1iOGcYqISIiYJQh3nwZkVmHXXkCKu69190PAKOC6ag1ORETKFe82iN5mttDMxprZmUHZ8cCmqDqpQVkoMxtsZslmlpyRkRHLWEVE6pV4Joh5wEnufg7wD+CDqhzE3V9y957u3rNDhw7VGZ+ISL0WtwTh7rvdfW+wPAZobGbtgTSgS1TVE4KyGjVlZTrPTUmp6dOKiCSMRvE6sZkdC2xzdzezXkSS1Q5gF9DNzLoSSQwDgRtqOr7bXpkDwM/6n1rTpxYRSQgxSxBmNhLoB7Q3s1TgQaAxgLu/AHwfuNvMcoEDwEB3dyDXzIYA44GGwMvuvjRWcYqISLiYJQh3H1TO9n8C/yxl2xhgTCziEhGRiol3LyYREUlQShAiIhJKCUJEREIpQVTSve8vImnop/EOQ0Qk5pQgKmnk7E3lVxIRqQOUIEREJJQShIiIhFKCEBGRUEoQISIPdIuI1G9KECGue25GvEMQEYk7JYgQi1Kz4h2CiEjcKUGIiEgoJYhyjFm8Jd4hiIjEhRJEOX76xjxmr6vK1NoiIrWbEkQF7MnOiXcIIiI1TgmiAtZt3xfvEEREapwSRAU8+unyeIcgIlLjlCBERCSUEoSIiISKWYIws5fNLN3MlpSy/UYzW2Rmi83sSzM7J2rb+qB8gZklxypGEREpXSyvIF4FBpSxfR1wmbt/DXgEeKnY9v7ufq6794xRfCIiUoZGsTqwu08zs6Qytn8ZtfoVcEKsYhERkcpLlDaIO4CxUesOTDCzuWY2uKwdzWywmSWbWXJGRkZMgxQRqU9idgVRUWbWn0iC6BNV3Mfd08ysIzDRzFa4+7Sw/d39JYLbUz179tQ43SIi1SSuVxBmdjYwHLjO3XcUlLt7WvAzHRgN9IpPhIetydgb7xBERGpU3BKEmZ0IvA/c7O6rospbmlmrgmXgCiC0J1RN+ubTn8c7BBGRGhWzW0xmNhLoB7Q3s1TgQaAxgLu/ADwAHAM8b2YAuUGPpU7A6KCsEfCmu4+LVZwiIhIulr2YBpWz/U7gzpDytcA5JfeIv4sfnxTvEEREakyi9GKqFbbuzo53CCIiNUYJQkREQilBiIhIKCWIKhr63iJ27D0Y7zBERGJGCaKKRs3ZxONjVsQ7DBGRmFGCqCbuzsHcvHiHISJSbZQgqsmrX67n9D+OI109nUSkjlCCqCYfLdwMwKadB+IciYhI9VCCEBGRUEoQR+C9eam4awBZEamblCCOUOa+QwAoT4hIXaMEcYQO5BTtuRQZY1BEpPZTgjhCff48hYO5eSzYtAuA1dv2sDs7J75BiYhUAyWIajBi5obC5d+/t5gb/z0rjtGIiFQPJYhqcDA3v8j64rSsOEUiIlJ9lCBERCSUEkQ1UFdXEamLlCBERCSUEkQ1+MuEVSXKPl+VQdLQT0nduT8OEYmIHLmYJggze9nM0s1sSSnbzcyeNbMUM1tkZudHbbvFzFYHr1tiGWcsvD1nE0Bh91cRkdom1lcQrwIDyth+FdAteA0G/gVgZu2AB4GLgF7Ag2bWNqaRVjNH7RIiUrvFNEG4+zQgs4wq1wGvecRXQBszOw64Epjo7pnuvhOYSNmJJmEZerRaRGqneLdBHA9silpPDcpKKy/BzAabWbKZJWdkZMQs0MpSxyYRqe3inSCOmLu/5O493b1nhw4d4h1OCRqbSURqq3gniDSgS9T6CUFZaeUiIlJD4p0gPgL+N+jNdDGQ5e5bgPHAFWbWNmicviIoqzV0i0lEartGFalkZiPc/ebyykL2Gwn0A9qbWSqRnkmNAdz9BWAM8G0gBdgP3BZsyzSzR4A5waEedveyGrsTTkEvJt1hEpHaqkIJAjgzesXMGgIXlLeTuw8qZ7sDPytl28vAyxWML+GMX7ot3iGIiByRMm8xmdm9ZrYHONvMdgevPUA68GGNRFjLqZFaRGqrMhOEuz/h7q2Ap9z96ODVyt2Pcfd7ayjGWi6SIV78fA1/Gb+SzH2HWL5ld5xjEhEpX0VvMX1iZi3dfZ+Z3QScD/zd3TeUt2N9V3AF8cTYFQC8Py+VzVnZrB92dRyjEhEpX0V7Mf0L2G9m5wC/AdYAr8Usqjpsc1Z2vEMQEamQiiaI3KBB+Trgn+7+HNAqdmHVHWqCEJHaqqK3mPaY2b3AzUBfM2tA0F1VyveLkfPjHYKISKVV9AriR8BB4HZ330rkyeanYhZVHWJmfLRwc7zDEBGptAoliCApvAG0NrNrgGx3VxuEiEgdVqEEYWY/BGYDPwB+CMwys+/HMrC6Qm0QIlJbVbQN4j7gQndPBzCzDsBnwLuxCqyuqM4H5VJ37mfjjv1ccmr76juoiEgpKpogGhQkh8AO4j/QX61QnQmi/1+mkpPneoZCRGpERRPEODMbD4wM1n9EZKA9qUE5eRoiVkRqTpkJwsxOBTq5+2/N7HtAn2DTTCKN1lIOTTkqIrVVeVcQzwD3Arj7+8D7AGb2tWDbd2IYW53wwudr4h2CiEiVlNeO0MndFxcvDMqSYhJRHTNrXfnTWJxx/zjuGaWH6UQksZSXINqUsa15NcZRrx3IyeODBXqYTkQSS3kJItnM7ipeaGZ3AnNjE1L9cddryXysp6xFJEGV1wZxDzDazG7kcELoCTQBvhvDuOqFicu2MXGZZp4TkcRUZoJw923AJWbWHzgrKP7U3SfHPDIREYmrCj0H4e5TgCmVPbiZDQD+DjQEhrv7sGLb/wb0D1ZbAB3dvU2wLQ8oaCDf6O7XVvb8iSxp6Keh5Snpe+ja/igaNlD3WBGJr5g9DW1mDYHngKuAHsAgM+sRXcfdf+Xu57r7ucA/CLrRBg4UbKtryaE0azL2cvlfp/HXiSsLy9ydf01dw469B+MYmYjURxV9kroqegEp7r4WwMxGEZlwaFkp9QcBD8YwnoS3bXdktrm5G3ZyMDcPd1i6OYs/j1vBnPXld5cVEalOsRxP6XhgU9R6alBWgpmdBHQFots2mplZspl9ZWbXl3YSMxsc1EvOyMiohrATQ+8nJtP9/nGFw2vsPZgb54hEpL5JlAH3BgLvunteVNlJ7t4TuAF4xsxOCdvR3V9y957u3rNDhw41EWvM3DR8FgCrt+0lc98hAFzDL4lInMQyQaQBXaLWTwjKwgzk8ECAALh7WvBzLTAVOK/6Q0ws+UEy2BEkh2hqshaRmhbLBDEH6GZmXc2sCZEk8FHxSmbWHWhLZADAgrK2ZtY0WG4PXErpbRd1mqNLCBGJj5glCHfPBYYA44HlwNvuvtTMHjaz6F5JA4FR7kVuppxB5CnuhUS61w5z93qZIAoUn1diU+Z+DhzKC68sIlINYtmLCXcfQ7F5I9z9gWLrD4Xs9yXwtVjGVlvk54eX931yCn27tWfEHRfVbEAiUm8kSiO1lGLIyHlA+LwS01dvL1GWtT9HPZ5EpFooQQB9uyXuHM+79udUqF5OXuRS45yHJ3DWg+N59JN6fUdORKqBEgRwd7/QHrQJZdmW3aVuG7dkC93uG8vKrXsKy4Z/sa4mwhKROkwJArjklMS9giiQdaD0K4mJy9IBWJS6q4aiEZH6QAmiFsrOOdx7yct4ki4/X11kRaTqlCBqodyoD/5RczaVWu837yysiXBEpI5Sggg0bVQ7/ynufX8x781LBSjxSN3o+aU9uH7kktdnMvi15Gq9Slm/fR/pwYCFIhJ/tfNTMQZm/eGbvHFn7Xim4JVSGqD3ZNdc99afvD6XCcu2hQ4LUlX9/jKVXo9PqrbjiciRUYIItGnRhI6tmsY7jAp5euKq0PJH1LVVRKqREkSU+tCku377Pjbs2BfvMESkFojpUBuSePr9ZSoA64ddHd9ARCTh6QoiiuZeqDyNNitSdylBROl0dO1og6iMDxek8dmybSQN/ZQ92eEP2y1Jy2J5GU9qi0j9pFtMUdq0aBLvEKrdL0ctKFyeuGxbaJ1r/vEFUPZtp6xgTKjWLRrj7oXPYoQNIigidYMSRD3SoPikEpVwzsMTgEgSGTVnU4UHERSR2ku3mOqR9VG9l3YHt5uiezSlpO8pUv9Qbj6rthUtm7IynQ9i+ACeiCQOJYhijj26WbxDiJlnPltduHz2QxOYtHwblz01tbDs8r9O47PgNtSHC9K4ZNgkrvjbNLZkHSisc9src4ocMycvn6/W7gAiyUNdaEXqDiWIeuyO/yaXKFuVvodtu7P55agFbN8beUq6rNtJj49ZzsCXvmJJWha3vTKnSMIRkdotpgnCzAaY2UozSzGzoSHbbzWzDDNbELzujNp2i5mtDl63xDLOaIO/fjIAM4Z+o6ZOmVCyc/K5qBLDXazetheo+MRGEj8Pf7yMBz5cEu8wpBaJWSO1mTUEngO+BaQCc8zsI3cvPh7EW+4+pNi+7YAHgZ5EHnCeG+y7M1bxFri9T1du79M11qdJWM9OWl2ibGex8ZZmrcus8PHOe3gCJ7ZrwYdD+hxxbJU1fXUGnds055QOR9X4uRPRyzMiY3g9fN1ZcY5EaotYXkH0AlLcfa27HwJGAddVcN8rgYnunhkkhYnAgBjFKeW4YfisUreV9aDclBXp7Nyfw8LUrFiEVa6b/zObbz79eVzOLVIXxDJBHA9ET1aQGpQV9z9mtsjM3jWzLpXcV+Ks4OnzJZtLJoHiPaBi4Q+jFzN8+tqYn0ekPop3I/XHQJK7n03kKuG/lT2AmQ02s2QzS87IyKj2AKViho1dEZfzvjlrI49+urxaj/nclBQe1ci4IjFNEGlAl6j1E4KyQu6+w90PBqvDgQsqum/UMV5y957u3rNDhw7VErhUXGnP3qWk7yl12/5DuWTuO8Se7BzeTg6fEW9rVjYZew6Gbou1p8avZHgpc26I1CexfJJ6DtDNzLoS+XAfCNwQXcHMjnP3LcHqtUDBV8HxwONm1jZYvwK4N4axShWtCnoxRft44WZ+PnI+553YJnSfHg+MB+Cqs45l7JKtnNapFed2OVx3wtKtDB4xF4Dpv+vPpp376ZXUjkYN433BK1K/xOx/nLvnAkOIfNgvB95296Vm9rCZXRtU+4WZLTWzhcAvgFuDfTOBR4gkmTnAw0GZ1ALjl24FYP7GXUXKh09fy7LNhwcFnLcx0iltcVrR9osJUWNG9X1yCjf8exZ/GL24xHnmbohtp7Zr//kFPx5R8lmR+iY7J4+VW2PfniSJJ6Zfydx9jLuf5u6nuPtjQdkD7v5RsHyvu5/p7ue4e393XxG178vufmrweiWWcUr1+mTRltDyRz9dzrefnV64XjDQ3/0fLCl8GhvCh13/aOHmEmVLizWMJ6/PrNIc2fn5ztSV6XixEy9KzWL80vABDnPz8hn00lfMrkSX39rqD+8v5spnprFjb3xu+Un86Jpd4ia6jWJ1eslbVdGyc/IZ8My0IkOWPxbVOD1p+Ta+/8LMwr7+lfHyjHXc+sqcwiufikjbdYCZa3fwf+8srPT5qtOqbXtIGvopc9bHLlHNDo69/1BezM4hiUkJogxT/69fvEOoM+56reStmi1Z2YXL939w+Anf9+alhh5jxdY9XPjYZ4VjQx3MzS/cVjBsyNrtpY8FlbnvENNXl+zptjFzPwDbdpf/DfmeUfOZvCL8qiIWXp2xrszBEaev3g7AmMXhV20iR0IJogxJ7VvGO4Q6o7S5KCorOyef7z3/Zakfmm/O2oi7czC35Lfdm4bP4ub/zOZQbj6TV2zjTx8vJS/fC29pldbr6kDUN+cPFmzm9lcPJ7tYz6j30MfLuOetBaVuL35bLJaqeqpRszcyVgmsVlKCqKRHr9cwBfG2JSu7zA/Npyes4vZX55QoL3hw74uUDG5/NZlXZqxn5pod5HvB5Efhfv12yXMt2LSrklHHViwnbjqCaUQAGPr+Yu5+Y171BCM1Sgmikm7odSIdW9W9qUnrkn9OSWFGyo4S5QVfgDfu2F9YNmrORt6YtREo2nsqWlhvqeiZ+uIlfU82L02r/FPkn6/K4MMFiTunx469B8mrQmcDqX5KEJXUoIEx9Kru8Q5DKqn3E5MKP3QaNDj8lTi6x9X01dtJCWksr+pHVX5++K2u6vLLkQtIDx4mrMy3/Ftenl2lBBfr22kAu/Yf4oJHP+PP4+LzZL4UpQRRBTV421eqSXSDuJXxaXrHf0vemirrie6y3guPjVnO6X8cx6GoxvRo64MG9Z+MmEuvxz4rsu1gbh65eeH7FdhzsO4Nsb4zGDZ+QiV6lEnsKEFIvdOgCvfU35qzscztOXn5DHlzHq/NXF9YNmp2ZJ9DIR/045dupd9fpjJ+6VbGLd1aeCVQ4PQ/jmPgS1+Vec6aHooklu0ckpiUIKpAFxC1232jS580J7+US4KJy9JDywuqz92wk08WbeGBD5eWeKjvrAfHM3/j4XaMuRsy+XEwlMiStNKHQk+OavuYsHQrI6KSDxTtlrskLYspK8JjFKkqJYgKuuSUY+IdglTRO6UMCBhmU+aB0PLc/LJv90T7xcj5rNy6h/05h9sfxi/dxveen8HZD41nctQH+T8mpxQuJw39lNPuG1vkYcACg0fM5f4Plxaub9udXWT7rHWZ3BbSc2v+xp3M3RD+EF12Th5pu8J/3zA10QYh4V6buZ4VW3eXX7GaKUFU0PBbejL5N5cBNdv3XI7cb99ddMTHmLoyfCj53dk5/P7dRSVuB135zLQi7RNmMG/jLnZn5zJ6Xuk9iA7l5fPU+JWlbv/1WwvIz3fmVXAcqu8+/yX/86+ZoduGvDmfS4dNLnw/5+blc90/v+DzVUV/19KSZmlWbN1d5NkROXIPfLiUAc9ML79iNVOCqKAWTRpxcjB1ZatmjeMcjcTDjJTtJcr2ZOfyViWuUAA2Z2WXuf1gTulXK+/PT2N3yBVGeZKGflqi7LPlkW69w8au4HfvLiRz3yEWpmYVGT5kcyWuMAD2HsxlwDPTueet+ezJzim1F9fFj0/iV2U8y3IksnMSOzklenzRlCCq4MozO8U7BImDG8uYerU8lWkYLy/hlHcBO3tdJq9WYkyqF6et5e3kw8Ob7D+YW7i8Y+/h+cg37yo7scHhp86T1+/kaw9N4H/+9WVova27sxk9P428fOem4bO44JGJFY63LPM27qT7/eOYsjJ27TGz12UWaVOqjBkp2+l+/zi+DPmykYiUIMrR47ijOb5N8yJlZkabFrqKkIp78fPqnRY1rGcURMaK+uGLM3no42V8sqjkCLhlyQsyz75DeTw9YSXuXuQ8P31jbrnHKGin2LEvkliWpB2+b17Q3TfrwOEroHveWsAXKdsL6xfYUs5VVmnmro98cM9YXfUPYHfnlRnrCsf8Ku6HL87ku8+HJ77yfLkmEtcNw2extYq/Y02K5YRBdcKYX/YNLX/hpgvK7YYoUiC3Gp8M/iJlO3+buCp02wcLDieFIW/Or9Rxo0P8x+QUcvOdf01dU1hW8IxCWcq6uunz58m8cedFRZLBx1E9vv4d9VT4wdx8ZqRsp2ED4/ROrWjQwGjdvOJfytZt30dK+h5O7diqQvXz8p0pK9JJat+Sy//6OQDvJKeW+v8fIsPL90xqV6HjL0nLKjE8y8bM/RzbulmF9o8XJYgquvhk9WqS+Pj5yPm0bNKw2o976bDJRdZHzg5/9mP80q38eMRcfvGNU3l2cgoPfacHt17aFSi9mzBA+p6DfOtv0xh518Wh2x8bU3Ru8eK39NYPu7pweVPmfrq0a1HiGAXPQE5akc6kFelM/11/mjZuQMdWZX8QvzRtbYmnt8tr6/n+CzOLxFTcT9+Yy+crM1j68ACu+ccXobHOWZ/Js5NW88qtF1ZpxsTMfYd4O3kTP/76yWU+AFpVusVUjfqfrjmxpWbURD+6XSFXDNk5ebz+1QYAng266D708TI+X5XB3a/PpfcTk0vsU1xVGtmjfbJoM32fnFJ4Cy03L58VW3fz6ox1Ja5g+j45hV6PTWLIm/NKfaIdIHXn/lK3lWf+xp08Xiy5AYxZvJV9h/LKnGjp52/OZ/rq7WQUq5OTlx96Cyq608Do+akMeXMew8auKPLMTHXSFUQ1euW2XqG9RUSqW7wm7+n9xKTQW023vDy7wse45wgHOvw86HI85M359Du9I2c9OL5wW/ujmoTu88miLazJ2Mc/Bp3HqR0jvRFz8/KZuXYHfbt1oEHIt++K9Ga/Z9T8wtt6f/j2GUDk6ia6h9auA+EJ0TjcZnMwJx93L7wKuG/0Yt5OTmXpn66kZdPDH9OXDJvM+mFXs3DTLn711uHeZjnlDMtSVbqCqGbfOadzvEMQiZmKtEOU58ARdvOM/jCMTg4A2/ceKl690PItu7n8r58XXkk8P3UNN/9nNtNXZzAiuCqKlrbrAD0eGEfmvtKPGd3m88xnkXahv09aXeQb/Tef/jx032c+W13Y7tPvL1M5/5GJheNvFfQqC+sSO3bxFvZF9TQDYnZJGdMEYWYDzGylmaWY2dCQ7b82s2VmtsjMJpnZSVHb8sxsQfD6KJZxikjiy9qfw8qte0LnJ6+Mgqfi1wWDJb47N3wGQ4hcqZ1fwS64z3y2mk8XbSnzeNG+SNleZDytnftzGLOk6CCFz05aXWLu9bvfmMe0I+ilVRkxSxBm1hB4DrgK6AEMMrMexarNB3q6+9nAu8CTUdsOuPu5wevaWMUZa8O+97V4hyBSJ3z/hS+58plpHGmHsIw9Bxk+fS2jg1kJP1xQfsI55Q9jGDFzfeEIvKX52ZtHNjFSfrFf7r8zN3D1syUbuHcWu6qJVZtULNsgegEp7r4WwMxGAdcBywoquPuUqPpfATfFMJ5q16RRgzIbvoDQnhYiUnmrQ+bqqIrLnppa6X3y8r3IWFixUtWOSLEa/SeWt5iOB6IfCU0NykpzBzA2ar2ZmSWb2Vdmdn1pO5nZ4KBeckZG+Hg5NSl6nKYWTRrSW91hRaSCGpgVeR6kNKm7qt7rqjISoheTmd0E9AQuiyo+yd3TzOxkYLKZLXb3NcX3dfeXgJcAevbsWaOj6BUk+we/04NuxR7IObdLG979SW8aNDDu6NOV/3xR8aEPRKR+2rBjH+/PL3862OJT6sZqpN1YXkGkAV2i1k8Iyoows8uB+4Br3b2wxcbd04Kfa4GpwHkxjPWIDOp1In26tS9SdnufroUPvtx/TfGmFxGRkqKHf08EsUwQc4BuZtbVzJoAA4EivZHM7DzgRSLJIT2qvK2ZNQ2W2wOXEtV2kWg0+reIVIeDufmszSi7ITxMdhkjAB+JmCUId88FhgDjgeXA2+6+1MweNrOCXklPAUcB7xTrznoGkGxmC4EpwDB3T7gEEYMn20VEKu2u15JjctyYtkG4+xhgTLGyB6KWLy9lvy+BhO8fevkZnfhk0RYaRo3lXDDya9syRnu9s09XhqtNQkQSnJ6kPgJP//AcZgz9Bk0aHf5n/M0Vp/PCTefTt1vRcZmiE8agi06ssRhFRKoqIXox1VZNGzUsMVdEk0YNGHDWcSXqfvzzPixOzeKqr0W2vfOT3vzghfCpIEVEEoESRA05oW0LTmh7+KG5ru1bxjEaEZHy6RZTnLQ/qmmRseRXP3ZVHKMRESlJCSJBNK7CZCEiIrGkW0xx1v3YVpzZuXW8wxARKUEJIs7G3fP1cuv84IITeKeCQwiLiFQX3deoBfp37xjvEESkHlKCSCDj7ukbWt7/9I50adc8dJuISKwoQSSQ7scezUVd24Vue+/uS2o4GhGp75QgEsxbP+7NJz/vU7j+yHVn0rxJQzq2asb153YuUi4iEktKEAnorOMP92q6uXdS4fJvB3Tn0lOPYcQdvbjyzGMBaBo1zIeGFReR6qQEUYsc36Y5b9x5MX27dSh8buLkDkcBcFTTRtzRp2uRcaFERI6EPk1qqbYtm/DyrT359/9eAEB+MCnF2F/2Zdj3En4gXBGpBZQgarFvdO/EMS2bApGhOwBO6XAUA3uVHC32mJZN+FHPwxP8XXxyeGO4iEgBPSiXoB657kz2Hcort17zJg15+gfn0PuUY8qsN/f+b7Fi627eSt4EwC++2Y2v1s4qtX6HVk3J2HOwSNkFJ7Vl7oadFYheROoCXUEkqJt7J/GTy06pUN3/ueAEOhcbdvyuvl0Ll3ufHEkepwTtFdFlAOuHXc37Pz3cjfbyMzpxd8i53xp8ccWCF5E6QVcQddR9V/fgvqt7kLnvEC2bNgSKDghoZkz/Xf/C2fC6dYwkjz9efQZ39j2Z/HznjOOO5sKktqRk7OVgTj6NGjbg+RvPp1WzRuzYe4h73loQeu6u7Vuybnvl59UVkcSiBFHHtWvZpNSyLu0Oz0/RqlnjIsOPN2hghbetuh97dGH5t4MJj1Zu3QNEelbd1bcrnds0Z/CIudxzeTfuufw0koZ+CsC5XdqwYNMuAH7W/xQuP6MTbydvYuTsyK2uJo0acCi38hOuN2xg5OU7Jx3Tgg079ld6fxEpn3nQ+yUmBzcbAPwdaAgMd/dhxbY3BV4DLgB2AD9y9/XBtnuBO4A84BfuPr688/Xs2dOTk2MzeXddsSlzP62aNaJNi5KJo7KWb9nNaZ1aFV6FpO06wHFHN6NBA2Pb7mzuG72E5288n137D3HLK3N4+daeHNe6OYdy8zntj2O54aITuey0Dvx4xFzeuPMibhx+uE1kSP9T+eeUFO7s05VFaVkcOJRHr67tuDCpHQPOOrawXl6+c80/vmDY975Gj85H0+2+sYXbzMAd7u53ChOWbuX4ti2YtirjiH9vkUQU/QWvMsxsrrv3DN0WqwRhZg2BVcC3gFRgDjDI3ZdF1fkpcLa7/8TMBgLfdfcfmVkPYCTQC+gMfAac5u5lttoqQdQeu7NzaNmkEQ0bGOl7sunYqhnPTlrNJ4s2M+FXl+HufLAgjWvO7lypuTLy8p2sAzls33uQ49s058lxKxh61Rk0bxK5zZa+O5snxq6gZ1JbWjdvzKy1mSxK3cXrd15E8vqd7DpwiJw853fvLgJg1OCLcYdeXdvRsIEVXhn9fkB3/jxuBd2PbcXvB3TntlfnhMbzn1t60rxJQ274d9EOAQVXPn/4dnceH7OiKv+EIkXUtgTRG3jI3a8M1u8FcPcnouqMD+rMNLNGwFagAzA0um50vbLOqQQh1WVT5n6yc/Lo1qlVkfK5GzLZk51Lv9M7sjUrm1bNGtGiSUP+Pmk123Znc+WZx3LZaR34YEEaE5dt47kbzsfMCvfP2HOQTTv3c/6JbQvLsg7kkL47u8S51mTs5fg2zWnaqEGRYwD8dcJKnp2cwvFtmjNj6Dd4avwKTjqmJe8mpzJ7fWaRur+6/DQu7NqWMzu3plEDIzsnj8fGLOen/U7lrteSufTUY3j9q428ePMFXHLKMSRv2Mm97y3mkevP4pvdO/LyjHV87fjWnNrxKC549DOuPLMTAy88MTQp3tGnK7ddmkT7o5ry2fJtDHlzfpHt153bmWvO7sxT41ewatveItu6dTyK1emRsuNaN6N5k4a0ad6YeRt3FdZp17IJmfsOhf7NGhjkV/PH2VnHH82StN3Ve9AYqW0J4vvAAHe/M1i/GbjI3YdE1VkS1EkN1tcAFwEPAV+5++tB+X+Ase7+bsh5BgODAU488cQLNmzYEJPfRySR5OU7i9OyOKvz0TQqdoW1JesABw7lFT5lHys5efnVPhPi0s1Z9Dju6CIJcXd2Dgs37eKcLm04ulnjwvI1GXvZtf8QF5wUeaYnNy+fAzl5tArq5AXZouAW6Pa9B9l3MJc1GXvpf3rHwnOMW7KV/t07kHUgh61Z2ZzS4Sg+WbSZ6887nqaNGhaJLzsnj4w9B4u0341bsoWORzfjUG4+J7dvSfMmDWnVrDHTV2cwas4mnh14Htk5ebRo0rDwnLuzI18KOrRqRqMGxpasbDq3acai1CzmbdzJ7Zd2Df1ikLU/h2VbdnNs62a0bdGY1s0jv2vxepVRpxNENF1BiIhUTlkJIpbPQaQBXaLWTwjKQusEt5haE2msrsi+IiISQ7FMEHOAbmbW1cyaAAOBj4rV+Qi4JVj+PjDZI5c0HwEDzaypmXUFugGzYxiriIgUE7PnINw918yGAOOJdHN92d2XmtnDQLK7fwT8BxhhZilAJpEkQlDvbWAZkAv8rLweTCIiUr1i+hxETVMbhIhI5cSrDUJERGoxJQgREQmlBCEiIqGUIEREJFSdaqQ2swygqo9Stwe2V2M41UVxVY7iqhzFVTl1Ma6T3L1D2IY6lSCOhJkll9aSH0+Kq3IUV+Uorsqpb3HpFpOIiIRSghARkVBKEIe9FO8ASqG4KkdxVY7iqpx6FZfaIEREJJSuIEREJJQShIiIhKr3CcLMBpjZSjNLMbOhMTrHy2aWHkyQVFDWzswmmtnq4GfboNzM7NkgnkVmdn7UPrcE9Veb2S1R5ReY2eJgn2etgtNLmVkXM5tiZsvMbKmZ/TIRYjOzZmY228wWBnH9KSjvamazgmO9FQwjTzAs/FtB+SwzS4o61r1B+UozuzKqvMp/dzNraGbzzeyTRInLzNYH/84LzCw5KEuE91gbM3vXzFaY2XIz6x3vuMzs9ODfqeC128zuiXdcwX6/ssh7fomZjbTI/4X4vb/cvd6+iAxDvgY4GWgCLAR6xOA8XwfOB5ZElT0JDA2WhwJ/Dpa/DYwFDLgYmBWUtwPWBj/bBsttg22zg7oW7HtVBeM6Djg/WG4FrAJ6xDu2oO5RwXJjYFZwjLeBgUH5C8DdwfJPgReC5YHAW8Fyj+Bv2hToGvytGx7p3x34NfAm8EmwHve4gPVA+2JlifAe+y9wZ7DcBGiTCHEV+wzYCpwU77iA44F1QPOo99Wt8Xx/xf1DOp4voDcwPmr9XuDeGJ0riaIJYiVwXLB8HLAyWH4RGFS8HjAIeDGq/MWg7DhgRVR5kXqVjPFD4FuJFBvQAphHZCra7UCj4n87InOO9A6WGwX1rPjfs6DekfzdicxuOAn4BvBJcJ5EiGs9JRNEXP+ORGaIXEfQGSZR4ioWyxXAjESIi0iC2EQk4TQK3l9XxvP9Vd9vMRX8QQqkBmU1oZO7bwmWtwKdyomprPLUkPJKCS5PzyPybT3usVnkNs4CIB2YSOSbzy53zw05VuH5g+1ZwDFViLcingF+B+QH68ckSFwOTDCzuWY2OCiL99+xK5ABvGKRW3LDzaxlAsQVbSAwMliOa1zungb8BdgIbCHyfplLHN9f9T1BJASPpPO49Tc2s6OA94B73H139LZ4xebuee5+LpFv7L2A7jUdQ3Fmdg2Q7u5z4x1LiD7ufj5wFfAzM/t69MY4/R0bEbm1+i93Pw/YR+TWTbzjAiC4l38t8E7xbfGIK2jzuI5IYu0MtAQG1GQMxdX3BJEGdIlaPyEoqwnbzOw4gOBnejkxlVV+Qkh5hZhZYyLJ4Q13fz+RYgNw913AFCKXx23MrGCa3OhjFZ4/2N4a2FGFeMtzKXCtma0HRhG5zfT3BIir4Nsn7p4OjCaSVOP9d0wFUt19VrD+LpGEEe+4ClwFzHP3bcF6vOO6HFjn7hnungO8T+Q9F7/3V2Xu19W1F5FvOGuJZOyCRpszY3SuJIq2QTxF0QaxJ4PlqynaIDY7KG9H5H5u2+C1DmgXbCveIPbtCsZkwGvAM8XK4xob0AFoEyw3B6YD1xD5phfdWPfTYPlnFG2seztYPpOijXVriTTUHfHfHejH4UbquMZF5Jtmq6jlL4l880yE99h04PRg+aEgprjHFew7Crgtgd73FwFLibS7GZEG/p/H8/0V9w/peL+I9FBYReQe930xOsdIIvcUc4h8q7qDyL3CScBq4LOoN5YBzwXxLAZ6Rh3ndiAleEW/sXsCS4J9/kmxRsEy4upD5DJ6EbAgeH073rEBZwPzg7iWAA8E5ScH//FSgv80TYPyZsF6SrD95Khj3ReceyVRPUmO9O9O0QQR17iC8y8MXksL9ov33zHY71wgOfhbfkDkgzQR4mpJ5Nt266iyRIjrT8CKYN8RRD7k4/b+0lAbIiISqr63QYiISCmUIEREJJQShIiIhFKCEBGRUEoQIiISSglCJISZ7Q1+JpnZDdV87D8UW/+yOo8vUl2UIETKlgRUKkFEPfVamiIJwt0vqWRMIjVCCUKkbMOAvsG8Ab8KBhF8yszmBHMD/BjAzPqZ2XQz+whYFpR9EAyet7RgAD0zGwY0D473RlBWcLViwbGXBHMJ/Cjq2FPt8LwKb1R0fgGRI1HeNx2R+m4o8H/ufg1A8EGf5e4XmllTYIaZTQjqng+c5e7rgvXb3T3TzJoDc8zsPXcfamZDPDIQYXHfI/Lk8TlA+2CfacG284gMobAZmEFkjJ4vqvuXFYmmKwiRyrkC+N9gKPJZRIZn6BZsmx2VHAB+YWYLga+IDJLWjbL1AUZ6ZCTbbcDnwIVRx05193wiQ6IkVcPvIlImXUGIVI4BP3f38UUKzfoRGc46ev1yIhO67DezqUTGzqmqg1HLeej/rtQAXUGIlG0PkelYC4wH7g6GScfMTgsmwSmuNbAzSA7diYzsWSCnYP9ipgM/Cto5OhCZqnZ2tfwWIlWgbyEiZVsE5AW3il4lMv9DEjAvaCjOAK4P2W8c8BMzW05kRM2vora9BCwys3nufmNU+Wgi814sJDLK7u/cfWuQYERqnEZzFRGRULrFJCIioZQgREQklBKEiIiEUoIQEZFQShAiIhJKCUJEREIpQYiISKj/Bwen5HWy+w0bAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.0988)\n",
      "Test error rate: tensor(0.6050)\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradien Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    cost_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "\n",
    "            # Calculate the cost (mean squared error)\n",
    "            cost = np.mean(np.square(error))\n",
    "            cost_history.append(cost)\n",
    "\n",
    "        # Print the loss every 100 epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(outputs[1])\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {cost.item():.8f}')\n",
    "            \n",
    "    return w, b, cost_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = 0.001\n",
    "num_epochs = 1000\n",
    "batch_size = 10\n",
    "w, b, cost_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w, b)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w, b)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38cfbbe",
   "metadata": {},
   "source": [
    "Pytorch SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "Pytorch SGD Test (This is done by Chris for testing purpose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20000], Loss: 1.00000000\n",
      "Epoch [10/20000], Loss: 0.99425143\n",
      "Epoch [20/20000], Loss: 0.98992383\n",
      "Epoch [30/20000], Loss: 0.98631668\n",
      "Epoch [40/20000], Loss: 0.98297197\n",
      "Epoch [50/20000], Loss: 0.97973198\n",
      "Epoch [60/20000], Loss: 0.97654170\n",
      "Epoch [70/20000], Loss: 0.97338241\n",
      "Epoch [80/20000], Loss: 0.97024721\n",
      "Epoch [90/20000], Loss: 0.96713364\n",
      "Epoch [100/20000], Loss: 0.96404076\n",
      "Epoch [110/20000], Loss: 0.96096802\n",
      "Epoch [120/20000], Loss: 0.95791489\n",
      "Epoch [130/20000], Loss: 0.95488143\n",
      "Epoch [140/20000], Loss: 0.95186746\n",
      "Epoch [150/20000], Loss: 0.94887245\n",
      "Epoch [160/20000], Loss: 0.94589633\n",
      "Epoch [170/20000], Loss: 0.94293869\n",
      "Epoch [180/20000], Loss: 0.93999964\n",
      "Epoch [190/20000], Loss: 0.93707895\n",
      "Epoch [200/20000], Loss: 0.93417609\n",
      "Epoch [210/20000], Loss: 0.93129098\n",
      "Epoch [220/20000], Loss: 0.92842370\n",
      "Epoch [230/20000], Loss: 0.92557383\n",
      "Epoch [240/20000], Loss: 0.92274117\n",
      "Epoch [250/20000], Loss: 0.91992551\n",
      "Epoch [260/20000], Loss: 0.91712677\n",
      "Epoch [270/20000], Loss: 0.91434479\n",
      "Epoch [280/20000], Loss: 0.91157919\n",
      "Epoch [290/20000], Loss: 0.90883005\n",
      "Epoch [300/20000], Loss: 0.90609705\n",
      "Epoch [310/20000], Loss: 0.90337998\n",
      "Epoch [320/20000], Loss: 0.90067887\n",
      "Epoch [330/20000], Loss: 0.89799327\n",
      "Epoch [340/20000], Loss: 0.89532316\n",
      "Epoch [350/20000], Loss: 0.89266849\n",
      "Epoch [360/20000], Loss: 0.89002883\n",
      "Epoch [370/20000], Loss: 0.88740438\n",
      "Epoch [380/20000], Loss: 0.88479477\n",
      "Epoch [390/20000], Loss: 0.88219988\n",
      "Epoch [400/20000], Loss: 0.87961954\n",
      "Epoch [410/20000], Loss: 0.87705362\n",
      "Epoch [420/20000], Loss: 0.87450200\n",
      "Epoch [430/20000], Loss: 0.87196457\n",
      "Epoch [440/20000], Loss: 0.86944115\n",
      "Epoch [450/20000], Loss: 0.86693156\n",
      "Epoch [460/20000], Loss: 0.86443555\n",
      "Epoch [470/20000], Loss: 0.86195344\n",
      "Epoch [480/20000], Loss: 0.85948473\n",
      "Epoch [490/20000], Loss: 0.85702926\n",
      "Epoch [500/20000], Loss: 0.85458720\n",
      "Epoch [510/20000], Loss: 0.85215807\n",
      "Epoch [520/20000], Loss: 0.84974200\n",
      "Epoch [530/20000], Loss: 0.84733874\n",
      "Epoch [540/20000], Loss: 0.84494817\n",
      "Epoch [550/20000], Loss: 0.84257025\n",
      "Epoch [560/20000], Loss: 0.84020483\n",
      "Epoch [570/20000], Loss: 0.83785194\n",
      "Epoch [580/20000], Loss: 0.83551109\n",
      "Epoch [590/20000], Loss: 0.83318269\n",
      "Epoch [600/20000], Loss: 0.83086610\n",
      "Epoch [610/20000], Loss: 0.82856154\n",
      "Epoch [620/20000], Loss: 0.82626885\n",
      "Epoch [630/20000], Loss: 0.82398796\n",
      "Epoch [640/20000], Loss: 0.82171869\n",
      "Epoch [650/20000], Loss: 0.81946081\n",
      "Epoch [660/20000], Loss: 0.81721449\n",
      "Epoch [670/20000], Loss: 0.81497949\n",
      "Epoch [680/20000], Loss: 0.81275576\n",
      "Epoch [690/20000], Loss: 0.81054306\n",
      "Epoch [700/20000], Loss: 0.80834144\n",
      "Epoch [710/20000], Loss: 0.80615091\n",
      "Epoch [720/20000], Loss: 0.80397111\n",
      "Epoch [730/20000], Loss: 0.80180222\n",
      "Epoch [740/20000], Loss: 0.79964393\n",
      "Epoch [750/20000], Loss: 0.79749626\n",
      "Epoch [760/20000], Loss: 0.79535902\n",
      "Epoch [770/20000], Loss: 0.79323232\n",
      "Epoch [780/20000], Loss: 0.79111588\n",
      "Epoch [790/20000], Loss: 0.78900981\n",
      "Epoch [800/20000], Loss: 0.78691375\n",
      "Epoch [810/20000], Loss: 0.78482789\n",
      "Epoch [820/20000], Loss: 0.78275192\n",
      "Epoch [830/20000], Loss: 0.78068602\n",
      "Epoch [840/20000], Loss: 0.77862990\n",
      "Epoch [850/20000], Loss: 0.77658361\n",
      "Epoch [860/20000], Loss: 0.77454704\n",
      "Epoch [870/20000], Loss: 0.77252007\n",
      "Epoch [880/20000], Loss: 0.77050263\n",
      "Epoch [890/20000], Loss: 0.76849473\n",
      "Epoch [900/20000], Loss: 0.76649612\n",
      "Epoch [910/20000], Loss: 0.76450700\n",
      "Epoch [920/20000], Loss: 0.76252699\n",
      "Epoch [930/20000], Loss: 0.76055616\n",
      "Epoch [940/20000], Loss: 0.75859469\n",
      "Epoch [950/20000], Loss: 0.75664198\n",
      "Epoch [960/20000], Loss: 0.75469840\n",
      "Epoch [970/20000], Loss: 0.75276381\n",
      "Epoch [980/20000], Loss: 0.75083804\n",
      "Epoch [990/20000], Loss: 0.74892098\n",
      "Epoch [1000/20000], Loss: 0.74701262\n",
      "Epoch [1010/20000], Loss: 0.74511307\n",
      "Epoch [1020/20000], Loss: 0.74322206\n",
      "Epoch [1030/20000], Loss: 0.74133962\n",
      "Epoch [1040/20000], Loss: 0.73946565\n",
      "Epoch [1050/20000], Loss: 0.73760015\n",
      "Epoch [1060/20000], Loss: 0.73574293\n",
      "Epoch [1070/20000], Loss: 0.73389399\n",
      "Epoch [1080/20000], Loss: 0.73205340\n",
      "Epoch [1090/20000], Loss: 0.73022097\n",
      "Epoch [1100/20000], Loss: 0.72839653\n",
      "Epoch [1110/20000], Loss: 0.72658026\n",
      "Epoch [1120/20000], Loss: 0.72477204\n",
      "Epoch [1130/20000], Loss: 0.72297180\n",
      "Epoch [1140/20000], Loss: 0.72117943\n",
      "Epoch [1150/20000], Loss: 0.71939486\n",
      "Epoch [1160/20000], Loss: 0.71761823\n",
      "Epoch [1170/20000], Loss: 0.71584922\n",
      "Epoch [1180/20000], Loss: 0.71408796\n",
      "Epoch [1190/20000], Loss: 0.71233439\n",
      "Epoch [1200/20000], Loss: 0.71058828\n",
      "Epoch [1210/20000], Loss: 0.70884985\n",
      "Epoch [1220/20000], Loss: 0.70711881\n",
      "Epoch [1230/20000], Loss: 0.70539534\n",
      "Epoch [1240/20000], Loss: 0.70367920\n",
      "Epoch [1250/20000], Loss: 0.70197046\n",
      "Epoch [1260/20000], Loss: 0.70026892\n",
      "Epoch [1270/20000], Loss: 0.69857478\n",
      "Epoch [1280/20000], Loss: 0.69688767\n",
      "Epoch [1290/20000], Loss: 0.69520783\n",
      "Epoch [1300/20000], Loss: 0.69353515\n",
      "Epoch [1310/20000], Loss: 0.69186950\n",
      "Epoch [1320/20000], Loss: 0.69021088\n",
      "Epoch [1330/20000], Loss: 0.68855917\n",
      "Epoch [1340/20000], Loss: 0.68691444\n",
      "Epoch [1350/20000], Loss: 0.68527663\n",
      "Epoch [1360/20000], Loss: 0.68364573\n",
      "Epoch [1370/20000], Loss: 0.68202156\n",
      "Epoch [1380/20000], Loss: 0.68040413\n",
      "Epoch [1390/20000], Loss: 0.67879349\n",
      "Epoch [1400/20000], Loss: 0.67718947\n",
      "Epoch [1410/20000], Loss: 0.67559218\n",
      "Epoch [1420/20000], Loss: 0.67400146\n",
      "Epoch [1430/20000], Loss: 0.67241722\n",
      "Epoch [1440/20000], Loss: 0.67083961\n",
      "Epoch [1450/20000], Loss: 0.66926849\n",
      "Epoch [1460/20000], Loss: 0.66770381\n",
      "Epoch [1470/20000], Loss: 0.66614538\n",
      "Epoch [1480/20000], Loss: 0.66459352\n",
      "Epoch [1490/20000], Loss: 0.66304797\n",
      "Epoch [1500/20000], Loss: 0.66150862\n",
      "Epoch [1510/20000], Loss: 0.65997559\n",
      "Epoch [1520/20000], Loss: 0.65844882\n",
      "Epoch [1530/20000], Loss: 0.65692818\n",
      "Epoch [1540/20000], Loss: 0.65541369\n",
      "Epoch [1550/20000], Loss: 0.65390533\n",
      "Epoch [1560/20000], Loss: 0.65240312\n",
      "Epoch [1570/20000], Loss: 0.65090680\n",
      "Epoch [1580/20000], Loss: 0.64941651\n",
      "Epoch [1590/20000], Loss: 0.64793229\n",
      "Epoch [1600/20000], Loss: 0.64645404\n",
      "Epoch [1610/20000], Loss: 0.64498156\n",
      "Epoch [1620/20000], Loss: 0.64351499\n",
      "Epoch [1630/20000], Loss: 0.64205432\n",
      "Epoch [1640/20000], Loss: 0.64059937\n",
      "Epoch [1650/20000], Loss: 0.63915026\n",
      "Epoch [1660/20000], Loss: 0.63770694\n",
      "Epoch [1670/20000], Loss: 0.63626921\n",
      "Epoch [1680/20000], Loss: 0.63483721\n",
      "Epoch [1690/20000], Loss: 0.63341081\n",
      "Epoch [1700/20000], Loss: 0.63199013\n",
      "Epoch [1710/20000], Loss: 0.63057494\n",
      "Epoch [1720/20000], Loss: 0.62916529\n",
      "Epoch [1730/20000], Loss: 0.62776124\n",
      "Epoch [1740/20000], Loss: 0.62636262\n",
      "Epoch [1750/20000], Loss: 0.62496954\n",
      "Epoch [1760/20000], Loss: 0.62358183\n",
      "Epoch [1770/20000], Loss: 0.62219954\n",
      "Epoch [1780/20000], Loss: 0.62082261\n",
      "Epoch [1790/20000], Loss: 0.61945099\n",
      "Epoch [1800/20000], Loss: 0.61808473\n",
      "Epoch [1810/20000], Loss: 0.61672378\n",
      "Epoch [1820/20000], Loss: 0.61536813\n",
      "Epoch [1830/20000], Loss: 0.61401761\n",
      "Epoch [1840/20000], Loss: 0.61267233\n",
      "Epoch [1850/20000], Loss: 0.61133218\n",
      "Epoch [1860/20000], Loss: 0.60999721\n",
      "Epoch [1870/20000], Loss: 0.60866743\n",
      "Epoch [1880/20000], Loss: 0.60734272\n",
      "Epoch [1890/20000], Loss: 0.60602301\n",
      "Epoch [1900/20000], Loss: 0.60470843\n",
      "Epoch [1910/20000], Loss: 0.60339880\n",
      "Epoch [1920/20000], Loss: 0.60209417\n",
      "Epoch [1930/20000], Loss: 0.60079455\n",
      "Epoch [1940/20000], Loss: 0.59949988\n",
      "Epoch [1950/20000], Loss: 0.59821010\n",
      "Epoch [1960/20000], Loss: 0.59692514\n",
      "Epoch [1970/20000], Loss: 0.59564507\n",
      "Epoch [1980/20000], Loss: 0.59436983\n",
      "Epoch [1990/20000], Loss: 0.59309947\n",
      "Epoch [2000/20000], Loss: 0.59183389\n",
      "Epoch [2010/20000], Loss: 0.59057301\n",
      "Epoch [2020/20000], Loss: 0.58931684\n",
      "Epoch [2030/20000], Loss: 0.58806556\n",
      "Epoch [2040/20000], Loss: 0.58681881\n",
      "Epoch [2050/20000], Loss: 0.58557677\n",
      "Epoch [2060/20000], Loss: 0.58433932\n",
      "Epoch [2070/20000], Loss: 0.58310658\n",
      "Epoch [2080/20000], Loss: 0.58187842\n",
      "Epoch [2090/20000], Loss: 0.58065480\n",
      "Epoch [2100/20000], Loss: 0.57943571\n",
      "Epoch [2110/20000], Loss: 0.57822126\n",
      "Epoch [2120/20000], Loss: 0.57701129\n",
      "Epoch [2130/20000], Loss: 0.57580572\n",
      "Epoch [2140/20000], Loss: 0.57460463\n",
      "Epoch [2150/20000], Loss: 0.57340801\n",
      "Epoch [2160/20000], Loss: 0.57221580\n",
      "Epoch [2170/20000], Loss: 0.57102793\n",
      "Epoch [2180/20000], Loss: 0.56984454\n",
      "Epoch [2190/20000], Loss: 0.56866544\n",
      "Epoch [2200/20000], Loss: 0.56749076\n",
      "Epoch [2210/20000], Loss: 0.56632024\n",
      "Epoch [2220/20000], Loss: 0.56515414\n",
      "Epoch [2230/20000], Loss: 0.56399226\n",
      "Epoch [2240/20000], Loss: 0.56283462\n",
      "Epoch [2250/20000], Loss: 0.56168121\n",
      "Epoch [2260/20000], Loss: 0.56053203\n",
      "Epoch [2270/20000], Loss: 0.55938709\n",
      "Epoch [2280/20000], Loss: 0.55824625\n",
      "Epoch [2290/20000], Loss: 0.55710953\n",
      "Epoch [2300/20000], Loss: 0.55597705\n",
      "Epoch [2310/20000], Loss: 0.55484855\n",
      "Epoch [2320/20000], Loss: 0.55372417\n",
      "Epoch [2330/20000], Loss: 0.55260390\n",
      "Epoch [2340/20000], Loss: 0.55148762\n",
      "Epoch [2350/20000], Loss: 0.55037546\n",
      "Epoch [2360/20000], Loss: 0.54926717\n",
      "Epoch [2370/20000], Loss: 0.54816306\n",
      "Epoch [2380/20000], Loss: 0.54706281\n",
      "Epoch [2390/20000], Loss: 0.54596651\n",
      "Epoch [2400/20000], Loss: 0.54487413\n",
      "Epoch [2410/20000], Loss: 0.54378575\n",
      "Epoch [2420/20000], Loss: 0.54270124\n",
      "Epoch [2430/20000], Loss: 0.54162055\n",
      "Epoch [2440/20000], Loss: 0.54054379\n",
      "Epoch [2450/20000], Loss: 0.53947085\n",
      "Epoch [2460/20000], Loss: 0.53840178\n",
      "Epoch [2470/20000], Loss: 0.53733647\n",
      "Epoch [2480/20000], Loss: 0.53627503\n",
      "Epoch [2490/20000], Loss: 0.53521723\n",
      "Epoch [2500/20000], Loss: 0.53416324\n",
      "Epoch [2510/20000], Loss: 0.53311300\n",
      "Epoch [2520/20000], Loss: 0.53206652\n",
      "Epoch [2530/20000], Loss: 0.53102368\n",
      "Epoch [2540/20000], Loss: 0.52998459\n",
      "Epoch [2550/20000], Loss: 0.52894914\n",
      "Epoch [2560/20000], Loss: 0.52791733\n",
      "Epoch [2570/20000], Loss: 0.52688920\n",
      "Epoch [2580/20000], Loss: 0.52586466\n",
      "Epoch [2590/20000], Loss: 0.52484375\n",
      "Epoch [2600/20000], Loss: 0.52382642\n",
      "Epoch [2610/20000], Loss: 0.52281260\n",
      "Epoch [2620/20000], Loss: 0.52180243\n",
      "Epoch [2630/20000], Loss: 0.52079570\n",
      "Epoch [2640/20000], Loss: 0.51979250\n",
      "Epoch [2650/20000], Loss: 0.51879293\n",
      "Epoch [2660/20000], Loss: 0.51779675\n",
      "Epoch [2670/20000], Loss: 0.51680410\n",
      "Epoch [2680/20000], Loss: 0.51581490\n",
      "Epoch [2690/20000], Loss: 0.51482904\n",
      "Epoch [2700/20000], Loss: 0.51384676\n",
      "Epoch [2710/20000], Loss: 0.51286787\n",
      "Epoch [2720/20000], Loss: 0.51189232\n",
      "Epoch [2730/20000], Loss: 0.51092017\n",
      "Epoch [2740/20000], Loss: 0.50995141\n",
      "Epoch [2750/20000], Loss: 0.50898600\n",
      "Epoch [2760/20000], Loss: 0.50802392\n",
      "Epoch [2770/20000], Loss: 0.50706518\n",
      "Epoch [2780/20000], Loss: 0.50610971\n",
      "Epoch [2790/20000], Loss: 0.50515753\n",
      "Epoch [2800/20000], Loss: 0.50420862\n",
      "Epoch [2810/20000], Loss: 0.50326312\n",
      "Epoch [2820/20000], Loss: 0.50232065\n",
      "Epoch [2830/20000], Loss: 0.50138152\n",
      "Epoch [2840/20000], Loss: 0.50044560\n",
      "Epoch [2850/20000], Loss: 0.49951294\n",
      "Epoch [2860/20000], Loss: 0.49858338\n",
      "Epoch [2870/20000], Loss: 0.49765709\n",
      "Epoch [2880/20000], Loss: 0.49673393\n",
      "Epoch [2890/20000], Loss: 0.49581397\n",
      "Epoch [2900/20000], Loss: 0.49489704\n",
      "Epoch [2910/20000], Loss: 0.49398327\n",
      "Epoch [2920/20000], Loss: 0.49307266\n",
      "Epoch [2930/20000], Loss: 0.49216512\n",
      "Epoch [2940/20000], Loss: 0.49126068\n",
      "Epoch [2950/20000], Loss: 0.49035928\n",
      "Epoch [2960/20000], Loss: 0.48946097\n",
      "Epoch [2970/20000], Loss: 0.48856568\n",
      "Epoch [2980/20000], Loss: 0.48767346\n",
      "Epoch [2990/20000], Loss: 0.48678416\n",
      "Epoch [3000/20000], Loss: 0.48589790\n",
      "Epoch [3010/20000], Loss: 0.48501465\n",
      "Epoch [3020/20000], Loss: 0.48413441\n",
      "Epoch [3030/20000], Loss: 0.48325714\n",
      "Epoch [3040/20000], Loss: 0.48238277\n",
      "Epoch [3050/20000], Loss: 0.48151135\n",
      "Epoch [3060/20000], Loss: 0.48064283\n",
      "Epoch [3070/20000], Loss: 0.47977731\n",
      "Epoch [3080/20000], Loss: 0.47891465\n",
      "Epoch [3090/20000], Loss: 0.47805488\n",
      "Epoch [3100/20000], Loss: 0.47719792\n",
      "Epoch [3110/20000], Loss: 0.47634393\n",
      "Epoch [3120/20000], Loss: 0.47549272\n",
      "Epoch [3130/20000], Loss: 0.47464439\n",
      "Epoch [3140/20000], Loss: 0.47379887\n",
      "Epoch [3150/20000], Loss: 0.47295612\n",
      "Epoch [3160/20000], Loss: 0.47211623\n",
      "Epoch [3170/20000], Loss: 0.47127917\n",
      "Epoch [3180/20000], Loss: 0.47044480\n",
      "Epoch [3190/20000], Loss: 0.46961322\n",
      "Epoch [3200/20000], Loss: 0.46878445\n",
      "Epoch [3210/20000], Loss: 0.46795845\n",
      "Epoch [3220/20000], Loss: 0.46713513\n",
      "Epoch [3230/20000], Loss: 0.46631449\n",
      "Epoch [3240/20000], Loss: 0.46549657\n",
      "Epoch [3250/20000], Loss: 0.46468145\n",
      "Epoch [3260/20000], Loss: 0.46386895\n",
      "Epoch [3270/20000], Loss: 0.46305907\n",
      "Epoch [3280/20000], Loss: 0.46225193\n",
      "Epoch [3290/20000], Loss: 0.46144745\n",
      "Epoch [3300/20000], Loss: 0.46064556\n",
      "Epoch [3310/20000], Loss: 0.45984638\n",
      "Epoch [3320/20000], Loss: 0.45904967\n",
      "Epoch [3330/20000], Loss: 0.45825577\n",
      "Epoch [3340/20000], Loss: 0.45746437\n",
      "Epoch [3350/20000], Loss: 0.45667553\n",
      "Epoch [3360/20000], Loss: 0.45588920\n",
      "Epoch [3370/20000], Loss: 0.45510560\n",
      "Epoch [3380/20000], Loss: 0.45432448\n",
      "Epoch [3390/20000], Loss: 0.45354593\n",
      "Epoch [3400/20000], Loss: 0.45276988\n",
      "Epoch [3410/20000], Loss: 0.45199639\n",
      "Epoch [3420/20000], Loss: 0.45122537\n",
      "Epoch [3430/20000], Loss: 0.45045686\n",
      "Epoch [3440/20000], Loss: 0.44969079\n",
      "Epoch [3450/20000], Loss: 0.44892728\n",
      "Epoch [3460/20000], Loss: 0.44816628\n",
      "Epoch [3470/20000], Loss: 0.44740769\n",
      "Epoch [3480/20000], Loss: 0.44665155\n",
      "Epoch [3490/20000], Loss: 0.44589782\n",
      "Epoch [3500/20000], Loss: 0.44514665\n",
      "Epoch [3510/20000], Loss: 0.44439775\n",
      "Epoch [3520/20000], Loss: 0.44365132\n",
      "Epoch [3530/20000], Loss: 0.44290724\n",
      "Epoch [3540/20000], Loss: 0.44216561\n",
      "Epoch [3550/20000], Loss: 0.44142631\n",
      "Epoch [3560/20000], Loss: 0.44068947\n",
      "Epoch [3570/20000], Loss: 0.43995491\n",
      "Epoch [3580/20000], Loss: 0.43922275\n",
      "Epoch [3590/20000], Loss: 0.43849289\n",
      "Epoch [3600/20000], Loss: 0.43776542\n",
      "Epoch [3610/20000], Loss: 0.43704021\n",
      "Epoch [3620/20000], Loss: 0.43631741\n",
      "Epoch [3630/20000], Loss: 0.43559682\n",
      "Epoch [3640/20000], Loss: 0.43487853\n",
      "Epoch [3650/20000], Loss: 0.43416256\n",
      "Epoch [3660/20000], Loss: 0.43344888\n",
      "Epoch [3670/20000], Loss: 0.43273744\n",
      "Epoch [3680/20000], Loss: 0.43202826\n",
      "Epoch [3690/20000], Loss: 0.43132132\n",
      "Epoch [3700/20000], Loss: 0.43061662\n",
      "Epoch [3710/20000], Loss: 0.42991418\n",
      "Epoch [3720/20000], Loss: 0.42921391\n",
      "Epoch [3730/20000], Loss: 0.42851585\n",
      "Epoch [3740/20000], Loss: 0.42782009\n",
      "Epoch [3750/20000], Loss: 0.42712638\n",
      "Epoch [3760/20000], Loss: 0.42643493\n",
      "Epoch [3770/20000], Loss: 0.42574570\n",
      "Epoch [3780/20000], Loss: 0.42505866\n",
      "Epoch [3790/20000], Loss: 0.42437372\n",
      "Epoch [3800/20000], Loss: 0.42369092\n",
      "Epoch [3810/20000], Loss: 0.42301026\n",
      "Epoch [3820/20000], Loss: 0.42233169\n",
      "Epoch [3830/20000], Loss: 0.42165527\n",
      "Epoch [3840/20000], Loss: 0.42098102\n",
      "Epoch [3850/20000], Loss: 0.42030883\n",
      "Epoch [3860/20000], Loss: 0.41963878\n",
      "Epoch [3870/20000], Loss: 0.41897088\n",
      "Epoch [3880/20000], Loss: 0.41830489\n",
      "Epoch [3890/20000], Loss: 0.41764107\n",
      "Epoch [3900/20000], Loss: 0.41697928\n",
      "Epoch [3910/20000], Loss: 0.41631961\n",
      "Epoch [3920/20000], Loss: 0.41566196\n",
      "Epoch [3930/20000], Loss: 0.41500634\n",
      "Epoch [3940/20000], Loss: 0.41435277\n",
      "Epoch [3950/20000], Loss: 0.41370118\n",
      "Epoch [3960/20000], Loss: 0.41305164\n",
      "Epoch [3970/20000], Loss: 0.41240409\n",
      "Epoch [3980/20000], Loss: 0.41175860\n",
      "Epoch [3990/20000], Loss: 0.41111511\n",
      "Epoch [4000/20000], Loss: 0.41047356\n",
      "Epoch [4010/20000], Loss: 0.40983391\n",
      "Epoch [4020/20000], Loss: 0.40919635\n",
      "Epoch [4030/20000], Loss: 0.40856072\n",
      "Epoch [4040/20000], Loss: 0.40792698\n",
      "Epoch [4050/20000], Loss: 0.40729520\n",
      "Epoch [4060/20000], Loss: 0.40666538\n",
      "Epoch [4070/20000], Loss: 0.40603748\n",
      "Epoch [4080/20000], Loss: 0.40541160\n",
      "Epoch [4090/20000], Loss: 0.40478745\n",
      "Epoch [4100/20000], Loss: 0.40416539\n",
      "Epoch [4110/20000], Loss: 0.40354514\n",
      "Epoch [4120/20000], Loss: 0.40292680\n",
      "Epoch [4130/20000], Loss: 0.40231034\n",
      "Epoch [4140/20000], Loss: 0.40169570\n",
      "Epoch [4150/20000], Loss: 0.40108299\n",
      "Epoch [4160/20000], Loss: 0.40047210\n",
      "Epoch [4170/20000], Loss: 0.39986309\n",
      "Epoch [4180/20000], Loss: 0.39925593\n",
      "Epoch [4190/20000], Loss: 0.39865068\n",
      "Epoch [4200/20000], Loss: 0.39804718\n",
      "Epoch [4210/20000], Loss: 0.39744550\n",
      "Epoch [4220/20000], Loss: 0.39684570\n",
      "Epoch [4230/20000], Loss: 0.39624763\n",
      "Epoch [4240/20000], Loss: 0.39565143\n",
      "Epoch [4250/20000], Loss: 0.39505702\n",
      "Epoch [4260/20000], Loss: 0.39446437\n",
      "Epoch [4270/20000], Loss: 0.39387348\n",
      "Epoch [4280/20000], Loss: 0.39328444\n",
      "Epoch [4290/20000], Loss: 0.39269710\n",
      "Epoch [4300/20000], Loss: 0.39211154\n",
      "Epoch [4310/20000], Loss: 0.39152783\n",
      "Epoch [4320/20000], Loss: 0.39094573\n",
      "Epoch [4330/20000], Loss: 0.39036545\n",
      "Epoch [4340/20000], Loss: 0.38978690\n",
      "Epoch [4350/20000], Loss: 0.38921005\n",
      "Epoch [4360/20000], Loss: 0.38863492\n",
      "Epoch [4370/20000], Loss: 0.38806155\n",
      "Epoch [4380/20000], Loss: 0.38748986\n",
      "Epoch [4390/20000], Loss: 0.38691986\n",
      "Epoch [4400/20000], Loss: 0.38635156\n",
      "Epoch [4410/20000], Loss: 0.38578498\n",
      "Epoch [4420/20000], Loss: 0.38522011\n",
      "Epoch [4430/20000], Loss: 0.38465679\n",
      "Epoch [4440/20000], Loss: 0.38409522\n",
      "Epoch [4450/20000], Loss: 0.38353527\n",
      "Epoch [4460/20000], Loss: 0.38297707\n",
      "Epoch [4470/20000], Loss: 0.38242042\n",
      "Epoch [4480/20000], Loss: 0.38186550\n",
      "Epoch [4490/20000], Loss: 0.38131210\n",
      "Epoch [4500/20000], Loss: 0.38076043\n",
      "Epoch [4510/20000], Loss: 0.38021034\n",
      "Epoch [4520/20000], Loss: 0.37966195\n",
      "Epoch [4530/20000], Loss: 0.37911507\n",
      "Epoch [4540/20000], Loss: 0.37856987\n",
      "Epoch [4550/20000], Loss: 0.37802616\n",
      "Epoch [4560/20000], Loss: 0.37748414\n",
      "Epoch [4570/20000], Loss: 0.37694362\n",
      "Epoch [4580/20000], Loss: 0.37640485\n",
      "Epoch [4590/20000], Loss: 0.37586746\n",
      "Epoch [4600/20000], Loss: 0.37533176\n",
      "Epoch [4610/20000], Loss: 0.37479758\n",
      "Epoch [4620/20000], Loss: 0.37426499\n",
      "Epoch [4630/20000], Loss: 0.37373382\n",
      "Epoch [4640/20000], Loss: 0.37320438\n",
      "Epoch [4650/20000], Loss: 0.37267640\n",
      "Epoch [4660/20000], Loss: 0.37214997\n",
      "Epoch [4670/20000], Loss: 0.37162498\n",
      "Epoch [4680/20000], Loss: 0.37110162\n",
      "Epoch [4690/20000], Loss: 0.37057969\n",
      "Epoch [4700/20000], Loss: 0.37005931\n",
      "Epoch [4710/20000], Loss: 0.36954048\n",
      "Epoch [4720/20000], Loss: 0.36902317\n",
      "Epoch [4730/20000], Loss: 0.36850727\n",
      "Epoch [4740/20000], Loss: 0.36799285\n",
      "Epoch [4750/20000], Loss: 0.36747992\n",
      "Epoch [4760/20000], Loss: 0.36696851\n",
      "Epoch [4770/20000], Loss: 0.36645859\n",
      "Epoch [4780/20000], Loss: 0.36595008\n",
      "Epoch [4790/20000], Loss: 0.36544299\n",
      "Epoch [4800/20000], Loss: 0.36493742\n",
      "Epoch [4810/20000], Loss: 0.36443332\n",
      "Epoch [4820/20000], Loss: 0.36393058\n",
      "Epoch [4830/20000], Loss: 0.36342931\n",
      "Epoch [4840/20000], Loss: 0.36292949\n",
      "Epoch [4850/20000], Loss: 0.36243111\n",
      "Epoch [4860/20000], Loss: 0.36193416\n",
      "Epoch [4870/20000], Loss: 0.36143851\n",
      "Epoch [4880/20000], Loss: 0.36094436\n",
      "Epoch [4890/20000], Loss: 0.36045167\n",
      "Epoch [4900/20000], Loss: 0.35996029\n",
      "Epoch [4910/20000], Loss: 0.35947028\n",
      "Epoch [4920/20000], Loss: 0.35898173\n",
      "Epoch [4930/20000], Loss: 0.35849458\n",
      "Epoch [4940/20000], Loss: 0.35800877\n",
      "Epoch [4950/20000], Loss: 0.35752434\n",
      "Epoch [4960/20000], Loss: 0.35704127\n",
      "Epoch [4970/20000], Loss: 0.35655957\n",
      "Epoch [4980/20000], Loss: 0.35607919\n",
      "Epoch [4990/20000], Loss: 0.35560021\n",
      "Epoch [5000/20000], Loss: 0.35512254\n",
      "Epoch [5010/20000], Loss: 0.35464630\n",
      "Epoch [5020/20000], Loss: 0.35417134\n",
      "Epoch [5030/20000], Loss: 0.35369763\n",
      "Epoch [5040/20000], Loss: 0.35322541\n",
      "Epoch [5050/20000], Loss: 0.35275435\n",
      "Epoch [5060/20000], Loss: 0.35228470\n",
      "Epoch [5070/20000], Loss: 0.35181636\n",
      "Epoch [5080/20000], Loss: 0.35134932\n",
      "Epoch [5090/20000], Loss: 0.35088360\n",
      "Epoch [5100/20000], Loss: 0.35041916\n",
      "Epoch [5110/20000], Loss: 0.34995598\n",
      "Epoch [5120/20000], Loss: 0.34949413\n",
      "Epoch [5130/20000], Loss: 0.34903353\n",
      "Epoch [5140/20000], Loss: 0.34857425\n",
      "Epoch [5150/20000], Loss: 0.34811622\n",
      "Epoch [5160/20000], Loss: 0.34765950\n",
      "Epoch [5170/20000], Loss: 0.34720397\n",
      "Epoch [5180/20000], Loss: 0.34674975\n",
      "Epoch [5190/20000], Loss: 0.34629682\n",
      "Epoch [5200/20000], Loss: 0.34584504\n",
      "Epoch [5210/20000], Loss: 0.34539458\n",
      "Epoch [5220/20000], Loss: 0.34494534\n",
      "Epoch [5230/20000], Loss: 0.34449732\n",
      "Epoch [5240/20000], Loss: 0.34405059\n",
      "Epoch [5250/20000], Loss: 0.34360501\n",
      "Epoch [5260/20000], Loss: 0.34316075\n",
      "Epoch [5270/20000], Loss: 0.34271762\n",
      "Epoch [5280/20000], Loss: 0.34227577\n",
      "Epoch [5290/20000], Loss: 0.34183502\n",
      "Epoch [5300/20000], Loss: 0.34139556\n",
      "Epoch [5310/20000], Loss: 0.34095734\n",
      "Epoch [5320/20000], Loss: 0.34052026\n",
      "Epoch [5330/20000], Loss: 0.34008437\n",
      "Epoch [5340/20000], Loss: 0.33964971\n",
      "Epoch [5350/20000], Loss: 0.33921620\n",
      "Epoch [5360/20000], Loss: 0.33878383\n",
      "Epoch [5370/20000], Loss: 0.33835271\n",
      "Epoch [5380/20000], Loss: 0.33792266\n",
      "Epoch [5390/20000], Loss: 0.33749390\n",
      "Epoch [5400/20000], Loss: 0.33706620\n",
      "Epoch [5410/20000], Loss: 0.33663976\n",
      "Epoch [5420/20000], Loss: 0.33621436\n",
      "Epoch [5430/20000], Loss: 0.33579019\n",
      "Epoch [5440/20000], Loss: 0.33536711\n",
      "Epoch [5450/20000], Loss: 0.33494517\n",
      "Epoch [5460/20000], Loss: 0.33452442\n",
      "Epoch [5470/20000], Loss: 0.33410481\n",
      "Epoch [5480/20000], Loss: 0.33368626\n",
      "Epoch [5490/20000], Loss: 0.33326888\n",
      "Epoch [5500/20000], Loss: 0.33285263\n",
      "Epoch [5510/20000], Loss: 0.33243743\n",
      "Epoch [5520/20000], Loss: 0.33202338\n",
      "Epoch [5530/20000], Loss: 0.33161044\n",
      "Epoch [5540/20000], Loss: 0.33119866\n",
      "Epoch [5550/20000], Loss: 0.33078793\n",
      "Epoch [5560/20000], Loss: 0.33037829\n",
      "Epoch [5570/20000], Loss: 0.32996976\n",
      "Epoch [5580/20000], Loss: 0.32956228\n",
      "Epoch [5590/20000], Loss: 0.32915589\n",
      "Epoch [5600/20000], Loss: 0.32875061\n",
      "Epoch [5610/20000], Loss: 0.32834640\n",
      "Epoch [5620/20000], Loss: 0.32794321\n",
      "Epoch [5630/20000], Loss: 0.32754111\n",
      "Epoch [5640/20000], Loss: 0.32714012\n",
      "Epoch [5650/20000], Loss: 0.32674018\n",
      "Epoch [5660/20000], Loss: 0.32634127\n",
      "Epoch [5670/20000], Loss: 0.32594344\n",
      "Epoch [5680/20000], Loss: 0.32554668\n",
      "Epoch [5690/20000], Loss: 0.32515091\n",
      "Epoch [5700/20000], Loss: 0.32475621\n",
      "Epoch [5710/20000], Loss: 0.32436252\n",
      "Epoch [5720/20000], Loss: 0.32396987\n",
      "Epoch [5730/20000], Loss: 0.32357827\n",
      "Epoch [5740/20000], Loss: 0.32318771\n",
      "Epoch [5750/20000], Loss: 0.32279813\n",
      "Epoch [5760/20000], Loss: 0.32240960\n",
      "Epoch [5770/20000], Loss: 0.32202208\n",
      "Epoch [5780/20000], Loss: 0.32163551\n",
      "Epoch [5790/20000], Loss: 0.32125005\n",
      "Epoch [5800/20000], Loss: 0.32086554\n",
      "Epoch [5810/20000], Loss: 0.32048208\n",
      "Epoch [5820/20000], Loss: 0.32009959\n",
      "Epoch [5830/20000], Loss: 0.31971806\n",
      "Epoch [5840/20000], Loss: 0.31933755\n",
      "Epoch [5850/20000], Loss: 0.31895801\n",
      "Epoch [5860/20000], Loss: 0.31857952\n",
      "Epoch [5870/20000], Loss: 0.31820199\n",
      "Epoch [5880/20000], Loss: 0.31782532\n",
      "Epoch [5890/20000], Loss: 0.31744969\n",
      "Epoch [5900/20000], Loss: 0.31707504\n",
      "Epoch [5910/20000], Loss: 0.31670138\n",
      "Epoch [5920/20000], Loss: 0.31632870\n",
      "Epoch [5930/20000], Loss: 0.31595692\n",
      "Epoch [5940/20000], Loss: 0.31558606\n",
      "Epoch [5950/20000], Loss: 0.31521624\n",
      "Epoch [5960/20000], Loss: 0.31484738\n",
      "Epoch [5970/20000], Loss: 0.31447941\n",
      "Epoch [5980/20000], Loss: 0.31411240\n",
      "Epoch [5990/20000], Loss: 0.31374633\n",
      "Epoch [6000/20000], Loss: 0.31338122\n",
      "Epoch [6010/20000], Loss: 0.31301698\n",
      "Epoch [6020/20000], Loss: 0.31265369\n",
      "Epoch [6030/20000], Loss: 0.31229135\n",
      "Epoch [6040/20000], Loss: 0.31192991\n",
      "Epoch [6050/20000], Loss: 0.31156942\n",
      "Epoch [6060/20000], Loss: 0.31120983\n",
      "Epoch [6070/20000], Loss: 0.31085113\n",
      "Epoch [6080/20000], Loss: 0.31049335\n",
      "Epoch [6090/20000], Loss: 0.31013650\n",
      "Epoch [6100/20000], Loss: 0.30978048\n",
      "Epoch [6110/20000], Loss: 0.30942547\n",
      "Epoch [6120/20000], Loss: 0.30907124\n",
      "Epoch [6130/20000], Loss: 0.30871809\n",
      "Epoch [6140/20000], Loss: 0.30836564\n",
      "Epoch [6150/20000], Loss: 0.30801412\n",
      "Epoch [6160/20000], Loss: 0.30766353\n",
      "Epoch [6170/20000], Loss: 0.30731380\n",
      "Epoch [6180/20000], Loss: 0.30696496\n",
      "Epoch [6190/20000], Loss: 0.30661693\n",
      "Epoch [6200/20000], Loss: 0.30626988\n",
      "Epoch [6210/20000], Loss: 0.30592364\n",
      "Epoch [6220/20000], Loss: 0.30557820\n",
      "Epoch [6230/20000], Loss: 0.30523378\n",
      "Epoch [6240/20000], Loss: 0.30489004\n",
      "Epoch [6250/20000], Loss: 0.30454728\n",
      "Epoch [6260/20000], Loss: 0.30420533\n",
      "Epoch [6270/20000], Loss: 0.30386430\n",
      "Epoch [6280/20000], Loss: 0.30352399\n",
      "Epoch [6290/20000], Loss: 0.30318463\n",
      "Epoch [6300/20000], Loss: 0.30284607\n",
      "Epoch [6310/20000], Loss: 0.30250838\n",
      "Epoch [6320/20000], Loss: 0.30217144\n",
      "Epoch [6330/20000], Loss: 0.30183545\n",
      "Epoch [6340/20000], Loss: 0.30150023\n",
      "Epoch [6350/20000], Loss: 0.30116579\n",
      "Epoch [6360/20000], Loss: 0.30083230\n",
      "Epoch [6370/20000], Loss: 0.30049953\n",
      "Epoch [6380/20000], Loss: 0.30016759\n",
      "Epoch [6390/20000], Loss: 0.29983646\n",
      "Epoch [6400/20000], Loss: 0.29950619\n",
      "Epoch [6410/20000], Loss: 0.29917672\n",
      "Epoch [6420/20000], Loss: 0.29884797\n",
      "Epoch [6430/20000], Loss: 0.29852015\n",
      "Epoch [6440/20000], Loss: 0.29819307\n",
      "Epoch [6450/20000], Loss: 0.29786685\n",
      "Epoch [6460/20000], Loss: 0.29754138\n",
      "Epoch [6470/20000], Loss: 0.29721665\n",
      "Epoch [6480/20000], Loss: 0.29689273\n",
      "Epoch [6490/20000], Loss: 0.29656962\n",
      "Epoch [6500/20000], Loss: 0.29624736\n",
      "Epoch [6510/20000], Loss: 0.29592580\n",
      "Epoch [6520/20000], Loss: 0.29560506\n",
      "Epoch [6530/20000], Loss: 0.29528508\n",
      "Epoch [6540/20000], Loss: 0.29496589\n",
      "Epoch [6550/20000], Loss: 0.29464746\n",
      "Epoch [6560/20000], Loss: 0.29432979\n",
      "Epoch [6570/20000], Loss: 0.29401290\n",
      "Epoch [6580/20000], Loss: 0.29369676\n",
      "Epoch [6590/20000], Loss: 0.29338142\n",
      "Epoch [6600/20000], Loss: 0.29306680\n",
      "Epoch [6610/20000], Loss: 0.29275295\n",
      "Epoch [6620/20000], Loss: 0.29243985\n",
      "Epoch [6630/20000], Loss: 0.29212752\n",
      "Epoch [6640/20000], Loss: 0.29181591\n",
      "Epoch [6650/20000], Loss: 0.29150507\n",
      "Epoch [6660/20000], Loss: 0.29119495\n",
      "Epoch [6670/20000], Loss: 0.29088557\n",
      "Epoch [6680/20000], Loss: 0.29057696\n",
      "Epoch [6690/20000], Loss: 0.29026908\n",
      "Epoch [6700/20000], Loss: 0.28996193\n",
      "Epoch [6710/20000], Loss: 0.28965545\n",
      "Epoch [6720/20000], Loss: 0.28934982\n",
      "Epoch [6730/20000], Loss: 0.28904483\n",
      "Epoch [6740/20000], Loss: 0.28874052\n",
      "Epoch [6750/20000], Loss: 0.28843704\n",
      "Epoch [6760/20000], Loss: 0.28813422\n",
      "Epoch [6770/20000], Loss: 0.28783214\n",
      "Epoch [6780/20000], Loss: 0.28753075\n",
      "Epoch [6790/20000], Loss: 0.28723007\n",
      "Epoch [6800/20000], Loss: 0.28693017\n",
      "Epoch [6810/20000], Loss: 0.28663087\n",
      "Epoch [6820/20000], Loss: 0.28633237\n",
      "Epoch [6830/20000], Loss: 0.28603449\n",
      "Epoch [6840/20000], Loss: 0.28573734\n",
      "Epoch [6850/20000], Loss: 0.28544092\n",
      "Epoch [6860/20000], Loss: 0.28514513\n",
      "Epoch [6870/20000], Loss: 0.28485009\n",
      "Epoch [6880/20000], Loss: 0.28455573\n",
      "Epoch [6890/20000], Loss: 0.28426206\n",
      "Epoch [6900/20000], Loss: 0.28396901\n",
      "Epoch [6910/20000], Loss: 0.28367671\n",
      "Epoch [6920/20000], Loss: 0.28338510\n",
      "Epoch [6930/20000], Loss: 0.28309417\n",
      "Epoch [6940/20000], Loss: 0.28280383\n",
      "Epoch [6950/20000], Loss: 0.28251427\n",
      "Epoch [6960/20000], Loss: 0.28222537\n",
      "Epoch [6970/20000], Loss: 0.28193706\n",
      "Epoch [6980/20000], Loss: 0.28164947\n",
      "Epoch [6990/20000], Loss: 0.28136253\n",
      "Epoch [7000/20000], Loss: 0.28107631\n",
      "Epoch [7010/20000], Loss: 0.28079072\n",
      "Epoch [7020/20000], Loss: 0.28050575\n",
      "Epoch [7030/20000], Loss: 0.28022143\n",
      "Epoch [7040/20000], Loss: 0.27993780\n",
      "Epoch [7050/20000], Loss: 0.27965480\n",
      "Epoch [7060/20000], Loss: 0.27937248\n",
      "Epoch [7070/20000], Loss: 0.27909079\n",
      "Epoch [7080/20000], Loss: 0.27880976\n",
      "Epoch [7090/20000], Loss: 0.27852938\n",
      "Epoch [7100/20000], Loss: 0.27824962\n",
      "Epoch [7110/20000], Loss: 0.27797049\n",
      "Epoch [7120/20000], Loss: 0.27769202\n",
      "Epoch [7130/20000], Loss: 0.27741414\n",
      "Epoch [7140/20000], Loss: 0.27713698\n",
      "Epoch [7150/20000], Loss: 0.27686036\n",
      "Epoch [7160/20000], Loss: 0.27658445\n",
      "Epoch [7170/20000], Loss: 0.27630910\n",
      "Epoch [7180/20000], Loss: 0.27603441\n",
      "Epoch [7190/20000], Loss: 0.27576035\n",
      "Epoch [7200/20000], Loss: 0.27548686\n",
      "Epoch [7210/20000], Loss: 0.27521405\n",
      "Epoch [7220/20000], Loss: 0.27494180\n",
      "Epoch [7230/20000], Loss: 0.27467021\n",
      "Epoch [7240/20000], Loss: 0.27439925\n",
      "Epoch [7250/20000], Loss: 0.27412882\n",
      "Epoch [7260/20000], Loss: 0.27385905\n",
      "Epoch [7270/20000], Loss: 0.27358988\n",
      "Epoch [7280/20000], Loss: 0.27332139\n",
      "Epoch [7290/20000], Loss: 0.27305335\n",
      "Epoch [7300/20000], Loss: 0.27278602\n",
      "Epoch [7310/20000], Loss: 0.27251926\n",
      "Epoch [7320/20000], Loss: 0.27225307\n",
      "Epoch [7330/20000], Loss: 0.27198753\n",
      "Epoch [7340/20000], Loss: 0.27172256\n",
      "Epoch [7350/20000], Loss: 0.27145809\n",
      "Epoch [7360/20000], Loss: 0.27119434\n",
      "Epoch [7370/20000], Loss: 0.27093112\n",
      "Epoch [7380/20000], Loss: 0.27066848\n",
      "Epoch [7390/20000], Loss: 0.27040645\n",
      "Epoch [7400/20000], Loss: 0.27014497\n",
      "Epoch [7410/20000], Loss: 0.26988408\n",
      "Epoch [7420/20000], Loss: 0.26962376\n",
      "Epoch [7430/20000], Loss: 0.26936403\n",
      "Epoch [7440/20000], Loss: 0.26910487\n",
      "Epoch [7450/20000], Loss: 0.26884630\n",
      "Epoch [7460/20000], Loss: 0.26858824\n",
      "Epoch [7470/20000], Loss: 0.26833084\n",
      "Epoch [7480/20000], Loss: 0.26807395\n",
      "Epoch [7490/20000], Loss: 0.26781762\n",
      "Epoch [7500/20000], Loss: 0.26756182\n",
      "Epoch [7510/20000], Loss: 0.26730669\n",
      "Epoch [7520/20000], Loss: 0.26705199\n",
      "Epoch [7530/20000], Loss: 0.26679793\n",
      "Epoch [7540/20000], Loss: 0.26654443\n",
      "Epoch [7550/20000], Loss: 0.26629147\n",
      "Epoch [7560/20000], Loss: 0.26603901\n",
      "Epoch [7570/20000], Loss: 0.26578715\n",
      "Epoch [7580/20000], Loss: 0.26553586\n",
      "Epoch [7590/20000], Loss: 0.26528507\n",
      "Epoch [7600/20000], Loss: 0.26503485\n",
      "Epoch [7610/20000], Loss: 0.26478520\n",
      "Epoch [7620/20000], Loss: 0.26453599\n",
      "Epoch [7630/20000], Loss: 0.26428744\n",
      "Epoch [7640/20000], Loss: 0.26403937\n",
      "Epoch [7650/20000], Loss: 0.26379186\n",
      "Epoch [7660/20000], Loss: 0.26354480\n",
      "Epoch [7670/20000], Loss: 0.26329833\n",
      "Epoch [7680/20000], Loss: 0.26305243\n",
      "Epoch [7690/20000], Loss: 0.26280701\n",
      "Epoch [7700/20000], Loss: 0.26256219\n",
      "Epoch [7710/20000], Loss: 0.26231781\n",
      "Epoch [7720/20000], Loss: 0.26207402\n",
      "Epoch [7730/20000], Loss: 0.26183072\n",
      "Epoch [7740/20000], Loss: 0.26158789\n",
      "Epoch [7750/20000], Loss: 0.26134562\n",
      "Epoch [7760/20000], Loss: 0.26110387\n",
      "Epoch [7770/20000], Loss: 0.26086265\n",
      "Epoch [7780/20000], Loss: 0.26062191\n",
      "Epoch [7790/20000], Loss: 0.26038173\n",
      "Epoch [7800/20000], Loss: 0.26014203\n",
      "Epoch [7810/20000], Loss: 0.25990286\n",
      "Epoch [7820/20000], Loss: 0.25966421\n",
      "Epoch [7830/20000], Loss: 0.25942603\n",
      "Epoch [7840/20000], Loss: 0.25918838\n",
      "Epoch [7850/20000], Loss: 0.25895122\n",
      "Epoch [7860/20000], Loss: 0.25871453\n",
      "Epoch [7870/20000], Loss: 0.25847840\n",
      "Epoch [7880/20000], Loss: 0.25824273\n",
      "Epoch [7890/20000], Loss: 0.25800756\n",
      "Epoch [7900/20000], Loss: 0.25777289\n",
      "Epoch [7910/20000], Loss: 0.25753871\n",
      "Epoch [7920/20000], Loss: 0.25730503\n",
      "Epoch [7930/20000], Loss: 0.25707185\n",
      "Epoch [7940/20000], Loss: 0.25683916\n",
      "Epoch [7950/20000], Loss: 0.25660697\n",
      "Epoch [7960/20000], Loss: 0.25637519\n",
      "Epoch [7970/20000], Loss: 0.25614399\n",
      "Epoch [7980/20000], Loss: 0.25591314\n",
      "Epoch [7990/20000], Loss: 0.25568295\n",
      "Epoch [8000/20000], Loss: 0.25545314\n",
      "Epoch [8010/20000], Loss: 0.25522378\n",
      "Epoch [8020/20000], Loss: 0.25499496\n",
      "Epoch [8030/20000], Loss: 0.25476655\n",
      "Epoch [8040/20000], Loss: 0.25453866\n",
      "Epoch [8050/20000], Loss: 0.25431126\n",
      "Epoch [8060/20000], Loss: 0.25408432\n",
      "Epoch [8070/20000], Loss: 0.25385785\n",
      "Epoch [8080/20000], Loss: 0.25363180\n",
      "Epoch [8090/20000], Loss: 0.25340623\n",
      "Epoch [8100/20000], Loss: 0.25318122\n",
      "Epoch [8110/20000], Loss: 0.25295657\n",
      "Epoch [8120/20000], Loss: 0.25273243\n",
      "Epoch [8130/20000], Loss: 0.25250873\n",
      "Epoch [8140/20000], Loss: 0.25228548\n",
      "Epoch [8150/20000], Loss: 0.25206274\n",
      "Epoch [8160/20000], Loss: 0.25184035\n",
      "Epoch [8170/20000], Loss: 0.25161856\n",
      "Epoch [8180/20000], Loss: 0.25139710\n",
      "Epoch [8190/20000], Loss: 0.25117612\n",
      "Epoch [8200/20000], Loss: 0.25095567\n",
      "Epoch [8210/20000], Loss: 0.25073558\n",
      "Epoch [8220/20000], Loss: 0.25051597\n",
      "Epoch [8230/20000], Loss: 0.25029680\n",
      "Epoch [8240/20000], Loss: 0.25007805\n",
      "Epoch [8250/20000], Loss: 0.24985979\n",
      "Epoch [8260/20000], Loss: 0.24964195\n",
      "Epoch [8270/20000], Loss: 0.24942455\n",
      "Epoch [8280/20000], Loss: 0.24920769\n",
      "Epoch [8290/20000], Loss: 0.24899113\n",
      "Epoch [8300/20000], Loss: 0.24877506\n",
      "Epoch [8310/20000], Loss: 0.24855942\n",
      "Epoch [8320/20000], Loss: 0.24834423\n",
      "Epoch [8330/20000], Loss: 0.24812944\n",
      "Epoch [8340/20000], Loss: 0.24791507\n",
      "Epoch [8350/20000], Loss: 0.24770121\n",
      "Epoch [8360/20000], Loss: 0.24748769\n",
      "Epoch [8370/20000], Loss: 0.24727468\n",
      "Epoch [8380/20000], Loss: 0.24706207\n",
      "Epoch [8390/20000], Loss: 0.24684983\n",
      "Epoch [8400/20000], Loss: 0.24663809\n",
      "Epoch [8410/20000], Loss: 0.24642670\n",
      "Epoch [8420/20000], Loss: 0.24621582\n",
      "Epoch [8430/20000], Loss: 0.24600527\n",
      "Epoch [8440/20000], Loss: 0.24579518\n",
      "Epoch [8450/20000], Loss: 0.24558552\n",
      "Epoch [8460/20000], Loss: 0.24537626\n",
      "Epoch [8470/20000], Loss: 0.24516743\n",
      "Epoch [8480/20000], Loss: 0.24495903\n",
      "Epoch [8490/20000], Loss: 0.24475107\n",
      "Epoch [8500/20000], Loss: 0.24454342\n",
      "Epoch [8510/20000], Loss: 0.24433620\n",
      "Epoch [8520/20000], Loss: 0.24412945\n",
      "Epoch [8530/20000], Loss: 0.24392304\n",
      "Epoch [8540/20000], Loss: 0.24371706\n",
      "Epoch [8550/20000], Loss: 0.24351154\n",
      "Epoch [8560/20000], Loss: 0.24330635\n",
      "Epoch [8570/20000], Loss: 0.24310166\n",
      "Epoch [8580/20000], Loss: 0.24289724\n",
      "Epoch [8590/20000], Loss: 0.24269329\n",
      "Epoch [8600/20000], Loss: 0.24248974\n",
      "Epoch [8610/20000], Loss: 0.24228661\n",
      "Epoch [8620/20000], Loss: 0.24208382\n",
      "Epoch [8630/20000], Loss: 0.24188152\n",
      "Epoch [8640/20000], Loss: 0.24167953\n",
      "Epoch [8650/20000], Loss: 0.24147797\n",
      "Epoch [8660/20000], Loss: 0.24127679\n",
      "Epoch [8670/20000], Loss: 0.24107595\n",
      "Epoch [8680/20000], Loss: 0.24087554\n",
      "Epoch [8690/20000], Loss: 0.24067558\n",
      "Epoch [8700/20000], Loss: 0.24047595\n",
      "Epoch [8710/20000], Loss: 0.24027672\n",
      "Epoch [8720/20000], Loss: 0.24007785\n",
      "Epoch [8730/20000], Loss: 0.23987938\n",
      "Epoch [8740/20000], Loss: 0.23968126\n",
      "Epoch [8750/20000], Loss: 0.23948355\n",
      "Epoch [8760/20000], Loss: 0.23928630\n",
      "Epoch [8770/20000], Loss: 0.23908934\n",
      "Epoch [8780/20000], Loss: 0.23889276\n",
      "Epoch [8790/20000], Loss: 0.23869659\n",
      "Epoch [8800/20000], Loss: 0.23850080\n",
      "Epoch [8810/20000], Loss: 0.23830535\n",
      "Epoch [8820/20000], Loss: 0.23811029\n",
      "Epoch [8830/20000], Loss: 0.23791558\n",
      "Epoch [8840/20000], Loss: 0.23772129\n",
      "Epoch [8850/20000], Loss: 0.23752730\n",
      "Epoch [8860/20000], Loss: 0.23733373\n",
      "Epoch [8870/20000], Loss: 0.23714054\n",
      "Epoch [8880/20000], Loss: 0.23694767\n",
      "Epoch [8890/20000], Loss: 0.23675522\n",
      "Epoch [8900/20000], Loss: 0.23656313\n",
      "Epoch [8910/20000], Loss: 0.23637143\n",
      "Epoch [8920/20000], Loss: 0.23618004\n",
      "Epoch [8930/20000], Loss: 0.23598900\n",
      "Epoch [8940/20000], Loss: 0.23579834\n",
      "Epoch [8950/20000], Loss: 0.23560801\n",
      "Epoch [8960/20000], Loss: 0.23541807\n",
      "Epoch [8970/20000], Loss: 0.23522852\n",
      "Epoch [8980/20000], Loss: 0.23503928\n",
      "Epoch [8990/20000], Loss: 0.23485041\n",
      "Epoch [9000/20000], Loss: 0.23466191\n",
      "Epoch [9010/20000], Loss: 0.23447372\n",
      "Epoch [9020/20000], Loss: 0.23428595\n",
      "Epoch [9030/20000], Loss: 0.23409851\n",
      "Epoch [9040/20000], Loss: 0.23391142\n",
      "Epoch [9050/20000], Loss: 0.23372465\n",
      "Epoch [9060/20000], Loss: 0.23353821\n",
      "Epoch [9070/20000], Loss: 0.23335220\n",
      "Epoch [9080/20000], Loss: 0.23316650\n",
      "Epoch [9090/20000], Loss: 0.23298109\n",
      "Epoch [9100/20000], Loss: 0.23279606\n",
      "Epoch [9110/20000], Loss: 0.23261140\n",
      "Epoch [9120/20000], Loss: 0.23242711\n",
      "Epoch [9130/20000], Loss: 0.23224312\n",
      "Epoch [9140/20000], Loss: 0.23205943\n",
      "Epoch [9150/20000], Loss: 0.23187608\n",
      "Epoch [9160/20000], Loss: 0.23169313\n",
      "Epoch [9170/20000], Loss: 0.23151051\n",
      "Epoch [9180/20000], Loss: 0.23132822\n",
      "Epoch [9190/20000], Loss: 0.23114622\n",
      "Epoch [9200/20000], Loss: 0.23096466\n",
      "Epoch [9210/20000], Loss: 0.23078331\n",
      "Epoch [9220/20000], Loss: 0.23060235\n",
      "Epoch [9230/20000], Loss: 0.23042175\n",
      "Epoch [9240/20000], Loss: 0.23024143\n",
      "Epoch [9250/20000], Loss: 0.23006146\n",
      "Epoch [9260/20000], Loss: 0.22988185\n",
      "Epoch [9270/20000], Loss: 0.22970253\n",
      "Epoch [9280/20000], Loss: 0.22952354\n",
      "Epoch [9290/20000], Loss: 0.22934486\n",
      "Epoch [9300/20000], Loss: 0.22916652\n",
      "Epoch [9310/20000], Loss: 0.22898856\n",
      "Epoch [9320/20000], Loss: 0.22881091\n",
      "Epoch [9330/20000], Loss: 0.22863349\n",
      "Epoch [9340/20000], Loss: 0.22845644\n",
      "Epoch [9350/20000], Loss: 0.22827978\n",
      "Epoch [9360/20000], Loss: 0.22810337\n",
      "Epoch [9370/20000], Loss: 0.22792728\n",
      "Epoch [9380/20000], Loss: 0.22775155\n",
      "Epoch [9390/20000], Loss: 0.22757608\n",
      "Epoch [9400/20000], Loss: 0.22740097\n",
      "Epoch [9410/20000], Loss: 0.22722614\n",
      "Epoch [9420/20000], Loss: 0.22705165\n",
      "Epoch [9430/20000], Loss: 0.22687742\n",
      "Epoch [9440/20000], Loss: 0.22670358\n",
      "Epoch [9450/20000], Loss: 0.22653003\n",
      "Epoch [9460/20000], Loss: 0.22635677\n",
      "Epoch [9470/20000], Loss: 0.22618385\n",
      "Epoch [9480/20000], Loss: 0.22601119\n",
      "Epoch [9490/20000], Loss: 0.22583893\n",
      "Epoch [9500/20000], Loss: 0.22566687\n",
      "Epoch [9510/20000], Loss: 0.22549516\n",
      "Epoch [9520/20000], Loss: 0.22532377\n",
      "Epoch [9530/20000], Loss: 0.22515270\n",
      "Epoch [9540/20000], Loss: 0.22498187\n",
      "Epoch [9550/20000], Loss: 0.22481142\n",
      "Epoch [9560/20000], Loss: 0.22464119\n",
      "Epoch [9570/20000], Loss: 0.22447132\n",
      "Epoch [9580/20000], Loss: 0.22430174\n",
      "Epoch [9590/20000], Loss: 0.22413248\n",
      "Epoch [9600/20000], Loss: 0.22396347\n",
      "Epoch [9610/20000], Loss: 0.22379480\n",
      "Epoch [9620/20000], Loss: 0.22362641\n",
      "Epoch [9630/20000], Loss: 0.22345835\n",
      "Epoch [9640/20000], Loss: 0.22329056\n",
      "Epoch [9650/20000], Loss: 0.22312303\n",
      "Epoch [9660/20000], Loss: 0.22295579\n",
      "Epoch [9670/20000], Loss: 0.22278890\n",
      "Epoch [9680/20000], Loss: 0.22262228\n",
      "Epoch [9690/20000], Loss: 0.22245599\n",
      "Epoch [9700/20000], Loss: 0.22228992\n",
      "Epoch [9710/20000], Loss: 0.22212417\n",
      "Epoch [9720/20000], Loss: 0.22195873\n",
      "Epoch [9730/20000], Loss: 0.22179356\n",
      "Epoch [9740/20000], Loss: 0.22162865\n",
      "Epoch [9750/20000], Loss: 0.22146407\n",
      "Epoch [9760/20000], Loss: 0.22129974\n",
      "Epoch [9770/20000], Loss: 0.22113571\n",
      "Epoch [9780/20000], Loss: 0.22097197\n",
      "Epoch [9790/20000], Loss: 0.22080855\n",
      "Epoch [9800/20000], Loss: 0.22064537\n",
      "Epoch [9810/20000], Loss: 0.22048250\n",
      "Epoch [9820/20000], Loss: 0.22031984\n",
      "Epoch [9830/20000], Loss: 0.22015755\n",
      "Epoch [9840/20000], Loss: 0.21999545\n",
      "Epoch [9850/20000], Loss: 0.21983373\n",
      "Epoch [9860/20000], Loss: 0.21967225\n",
      "Epoch [9870/20000], Loss: 0.21951102\n",
      "Epoch [9880/20000], Loss: 0.21935008\n",
      "Epoch [9890/20000], Loss: 0.21918939\n",
      "Epoch [9900/20000], Loss: 0.21902902\n",
      "Epoch [9910/20000], Loss: 0.21886896\n",
      "Epoch [9920/20000], Loss: 0.21870908\n",
      "Epoch [9930/20000], Loss: 0.21854952\n",
      "Epoch [9940/20000], Loss: 0.21839021\n",
      "Epoch [9950/20000], Loss: 0.21823125\n",
      "Epoch [9960/20000], Loss: 0.21807243\n",
      "Epoch [9970/20000], Loss: 0.21791402\n",
      "Epoch [9980/20000], Loss: 0.21775578\n",
      "Epoch [9990/20000], Loss: 0.21759783\n",
      "Epoch [10000/20000], Loss: 0.21744014\n",
      "Epoch [10010/20000], Loss: 0.21728276\n",
      "Epoch [10020/20000], Loss: 0.21712559\n",
      "Epoch [10030/20000], Loss: 0.21696876\n",
      "Epoch [10040/20000], Loss: 0.21681213\n",
      "Epoch [10050/20000], Loss: 0.21665575\n",
      "Epoch [10060/20000], Loss: 0.21649972\n",
      "Epoch [10070/20000], Loss: 0.21634388\n",
      "Epoch [10080/20000], Loss: 0.21618828\n",
      "Epoch [10090/20000], Loss: 0.21603304\n",
      "Epoch [10100/20000], Loss: 0.21587797\n",
      "Epoch [10110/20000], Loss: 0.21572325\n",
      "Epoch [10120/20000], Loss: 0.21556869\n",
      "Epoch [10130/20000], Loss: 0.21541448\n",
      "Epoch [10140/20000], Loss: 0.21526046\n",
      "Epoch [10150/20000], Loss: 0.21510671\n",
      "Epoch [10160/20000], Loss: 0.21495323\n",
      "Epoch [10170/20000], Loss: 0.21480000\n",
      "Epoch [10180/20000], Loss: 0.21464704\n",
      "Epoch [10190/20000], Loss: 0.21449432\n",
      "Epoch [10200/20000], Loss: 0.21434183\n",
      "Epoch [10210/20000], Loss: 0.21418968\n",
      "Epoch [10220/20000], Loss: 0.21403767\n",
      "Epoch [10230/20000], Loss: 0.21388598\n",
      "Epoch [10240/20000], Loss: 0.21373454\n",
      "Epoch [10250/20000], Loss: 0.21358332\n",
      "Epoch [10260/20000], Loss: 0.21343239\n",
      "Epoch [10270/20000], Loss: 0.21328171\n",
      "Epoch [10280/20000], Loss: 0.21313122\n",
      "Epoch [10290/20000], Loss: 0.21298102\n",
      "Epoch [10300/20000], Loss: 0.21283107\n",
      "Epoch [10310/20000], Loss: 0.21268134\n",
      "Epoch [10320/20000], Loss: 0.21253189\n",
      "Epoch [10330/20000], Loss: 0.21238267\n",
      "Epoch [10340/20000], Loss: 0.21223368\n",
      "Epoch [10350/20000], Loss: 0.21208492\n",
      "Epoch [10360/20000], Loss: 0.21193649\n",
      "Epoch [10370/20000], Loss: 0.21178824\n",
      "Epoch [10380/20000], Loss: 0.21164021\n",
      "Epoch [10390/20000], Loss: 0.21149246\n",
      "Epoch [10400/20000], Loss: 0.21134493\n",
      "Epoch [10410/20000], Loss: 0.21119763\n",
      "Epoch [10420/20000], Loss: 0.21105060\n",
      "Epoch [10430/20000], Loss: 0.21090379\n",
      "Epoch [10440/20000], Loss: 0.21075718\n",
      "Epoch [10450/20000], Loss: 0.21061093\n",
      "Epoch [10460/20000], Loss: 0.21046481\n",
      "Epoch [10470/20000], Loss: 0.21031892\n",
      "Epoch [10480/20000], Loss: 0.21017332\n",
      "Epoch [10490/20000], Loss: 0.21002796\n",
      "Epoch [10500/20000], Loss: 0.20988280\n",
      "Epoch [10510/20000], Loss: 0.20973791\n",
      "Epoch [10520/20000], Loss: 0.20959322\n",
      "Epoch [10530/20000], Loss: 0.20944877\n",
      "Epoch [10540/20000], Loss: 0.20930459\n",
      "Epoch [10550/20000], Loss: 0.20916060\n",
      "Epoch [10560/20000], Loss: 0.20901680\n",
      "Epoch [10570/20000], Loss: 0.20887332\n",
      "Epoch [10580/20000], Loss: 0.20873003\n",
      "Epoch [10590/20000], Loss: 0.20858696\n",
      "Epoch [10600/20000], Loss: 0.20844412\n",
      "Epoch [10610/20000], Loss: 0.20830148\n",
      "Epoch [10620/20000], Loss: 0.20815910\n",
      "Epoch [10630/20000], Loss: 0.20801693\n",
      "Epoch [10640/20000], Loss: 0.20787510\n",
      "Epoch [10650/20000], Loss: 0.20773335\n",
      "Epoch [10660/20000], Loss: 0.20759188\n",
      "Epoch [10670/20000], Loss: 0.20745061\n",
      "Epoch [10680/20000], Loss: 0.20730960\n",
      "Epoch [10690/20000], Loss: 0.20716879\n",
      "Epoch [10700/20000], Loss: 0.20702825\n",
      "Epoch [10710/20000], Loss: 0.20688787\n",
      "Epoch [10720/20000], Loss: 0.20674771\n",
      "Epoch [10730/20000], Loss: 0.20660780\n",
      "Epoch [10740/20000], Loss: 0.20646806\n",
      "Epoch [10750/20000], Loss: 0.20632862\n",
      "Epoch [10760/20000], Loss: 0.20618938\n",
      "Epoch [10770/20000], Loss: 0.20605032\n",
      "Epoch [10780/20000], Loss: 0.20591156\n",
      "Epoch [10790/20000], Loss: 0.20577289\n",
      "Epoch [10800/20000], Loss: 0.20563453\n",
      "Epoch [10810/20000], Loss: 0.20549643\n",
      "Epoch [10820/20000], Loss: 0.20535837\n",
      "Epoch [10830/20000], Loss: 0.20522068\n",
      "Epoch [10840/20000], Loss: 0.20508312\n",
      "Epoch [10850/20000], Loss: 0.20494591\n",
      "Epoch [10860/20000], Loss: 0.20480883\n",
      "Epoch [10870/20000], Loss: 0.20467192\n",
      "Epoch [10880/20000], Loss: 0.20453522\n",
      "Epoch [10890/20000], Loss: 0.20439878\n",
      "Epoch [10900/20000], Loss: 0.20426254\n",
      "Epoch [10910/20000], Loss: 0.20412654\n",
      "Epoch [10920/20000], Loss: 0.20399068\n",
      "Epoch [10930/20000], Loss: 0.20385508\n",
      "Epoch [10940/20000], Loss: 0.20371969\n",
      "Epoch [10950/20000], Loss: 0.20358448\n",
      "Epoch [10960/20000], Loss: 0.20344952\n",
      "Epoch [10970/20000], Loss: 0.20331472\n",
      "Epoch [10980/20000], Loss: 0.20318019\n",
      "Epoch [10990/20000], Loss: 0.20304585\n",
      "Epoch [11000/20000], Loss: 0.20291173\n",
      "Epoch [11010/20000], Loss: 0.20277774\n",
      "Epoch [11020/20000], Loss: 0.20264402\n",
      "Epoch [11030/20000], Loss: 0.20251048\n",
      "Epoch [11040/20000], Loss: 0.20237717\n",
      "Epoch [11050/20000], Loss: 0.20224400\n",
      "Epoch [11060/20000], Loss: 0.20211110\n",
      "Epoch [11070/20000], Loss: 0.20197836\n",
      "Epoch [11080/20000], Loss: 0.20184582\n",
      "Epoch [11090/20000], Loss: 0.20171352\n",
      "Epoch [11100/20000], Loss: 0.20158143\n",
      "Epoch [11110/20000], Loss: 0.20144948\n",
      "Epoch [11120/20000], Loss: 0.20131783\n",
      "Epoch [11130/20000], Loss: 0.20118631\n",
      "Epoch [11140/20000], Loss: 0.20105496\n",
      "Epoch [11150/20000], Loss: 0.20092386\n",
      "Epoch [11160/20000], Loss: 0.20079298\n",
      "Epoch [11170/20000], Loss: 0.20066220\n",
      "Epoch [11180/20000], Loss: 0.20053174\n",
      "Epoch [11190/20000], Loss: 0.20040143\n",
      "Epoch [11200/20000], Loss: 0.20027122\n",
      "Epoch [11210/20000], Loss: 0.20014136\n",
      "Epoch [11220/20000], Loss: 0.20001160\n",
      "Epoch [11230/20000], Loss: 0.19988208\n",
      "Epoch [11240/20000], Loss: 0.19975273\n",
      "Epoch [11250/20000], Loss: 0.19962354\n",
      "Epoch [11260/20000], Loss: 0.19949463\n",
      "Epoch [11270/20000], Loss: 0.19936584\n",
      "Epoch [11280/20000], Loss: 0.19923727\n",
      "Epoch [11290/20000], Loss: 0.19910890\n",
      "Epoch [11300/20000], Loss: 0.19898073\n",
      "Epoch [11310/20000], Loss: 0.19885269\n",
      "Epoch [11320/20000], Loss: 0.19872488\n",
      "Epoch [11330/20000], Loss: 0.19859728\n",
      "Epoch [11340/20000], Loss: 0.19846986\n",
      "Epoch [11350/20000], Loss: 0.19834267\n",
      "Epoch [11360/20000], Loss: 0.19821557\n",
      "Epoch [11370/20000], Loss: 0.19808874\n",
      "Epoch [11380/20000], Loss: 0.19796206\n",
      "Epoch [11390/20000], Loss: 0.19783558\n",
      "Epoch [11400/20000], Loss: 0.19770929\n",
      "Epoch [11410/20000], Loss: 0.19758315\n",
      "Epoch [11420/20000], Loss: 0.19745727\n",
      "Epoch [11430/20000], Loss: 0.19733150\n",
      "Epoch [11440/20000], Loss: 0.19720602\n",
      "Epoch [11450/20000], Loss: 0.19708066\n",
      "Epoch [11460/20000], Loss: 0.19695546\n",
      "Epoch [11470/20000], Loss: 0.19683045\n",
      "Epoch [11480/20000], Loss: 0.19670562\n",
      "Epoch [11490/20000], Loss: 0.19658098\n",
      "Epoch [11500/20000], Loss: 0.19645655\n",
      "Epoch [11510/20000], Loss: 0.19633231\n",
      "Epoch [11520/20000], Loss: 0.19620819\n",
      "Epoch [11530/20000], Loss: 0.19608428\n",
      "Epoch [11540/20000], Loss: 0.19596057\n",
      "Epoch [11550/20000], Loss: 0.19583707\n",
      "Epoch [11560/20000], Loss: 0.19571370\n",
      "Epoch [11570/20000], Loss: 0.19559044\n",
      "Epoch [11580/20000], Loss: 0.19546752\n",
      "Epoch [11590/20000], Loss: 0.19534470\n",
      "Epoch [11600/20000], Loss: 0.19522205\n",
      "Epoch [11610/20000], Loss: 0.19509955\n",
      "Epoch [11620/20000], Loss: 0.19497730\n",
      "Epoch [11630/20000], Loss: 0.19485520\n",
      "Epoch [11640/20000], Loss: 0.19473326\n",
      "Epoch [11650/20000], Loss: 0.19461147\n",
      "Epoch [11660/20000], Loss: 0.19448988\n",
      "Epoch [11670/20000], Loss: 0.19436848\n",
      "Epoch [11680/20000], Loss: 0.19424723\n",
      "Epoch [11690/20000], Loss: 0.19412619\n",
      "Epoch [11700/20000], Loss: 0.19400534\n",
      "Epoch [11710/20000], Loss: 0.19388464\n",
      "Epoch [11720/20000], Loss: 0.19376408\n",
      "Epoch [11730/20000], Loss: 0.19364372\n",
      "Epoch [11740/20000], Loss: 0.19352353\n",
      "Epoch [11750/20000], Loss: 0.19340351\n",
      "Epoch [11760/20000], Loss: 0.19328365\n",
      "Epoch [11770/20000], Loss: 0.19316395\n",
      "Epoch [11780/20000], Loss: 0.19304448\n",
      "Epoch [11790/20000], Loss: 0.19292513\n",
      "Epoch [11800/20000], Loss: 0.19280596\n",
      "Epoch [11810/20000], Loss: 0.19268698\n",
      "Epoch [11820/20000], Loss: 0.19256815\n",
      "Epoch [11830/20000], Loss: 0.19244951\n",
      "Epoch [11840/20000], Loss: 0.19233103\n",
      "Epoch [11850/20000], Loss: 0.19221267\n",
      "Epoch [11860/20000], Loss: 0.19209455\n",
      "Epoch [11870/20000], Loss: 0.19197655\n",
      "Epoch [11880/20000], Loss: 0.19185877\n",
      "Epoch [11890/20000], Loss: 0.19174111\n",
      "Epoch [11900/20000], Loss: 0.19162360\n",
      "Epoch [11910/20000], Loss: 0.19150627\n",
      "Epoch [11920/20000], Loss: 0.19138913\n",
      "Epoch [11930/20000], Loss: 0.19127214\n",
      "Epoch [11940/20000], Loss: 0.19115536\n",
      "Epoch [11950/20000], Loss: 0.19103870\n",
      "Epoch [11960/20000], Loss: 0.19092213\n",
      "Epoch [11970/20000], Loss: 0.19080590\n",
      "Epoch [11980/20000], Loss: 0.19068965\n",
      "Epoch [11990/20000], Loss: 0.19057368\n",
      "Epoch [12000/20000], Loss: 0.19045784\n",
      "Epoch [12010/20000], Loss: 0.19034210\n",
      "Epoch [12020/20000], Loss: 0.19022661\n",
      "Epoch [12030/20000], Loss: 0.19011128\n",
      "Epoch [12040/20000], Loss: 0.18999606\n",
      "Epoch [12050/20000], Loss: 0.18988104\n",
      "Epoch [12060/20000], Loss: 0.18976612\n",
      "Epoch [12070/20000], Loss: 0.18965141\n",
      "Epoch [12080/20000], Loss: 0.18953687\n",
      "Epoch [12090/20000], Loss: 0.18942247\n",
      "Epoch [12100/20000], Loss: 0.18930826\n",
      "Epoch [12110/20000], Loss: 0.18919414\n",
      "Epoch [12120/20000], Loss: 0.18908025\n",
      "Epoch [12130/20000], Loss: 0.18896650\n",
      "Epoch [12140/20000], Loss: 0.18885285\n",
      "Epoch [12150/20000], Loss: 0.18873937\n",
      "Epoch [12160/20000], Loss: 0.18862611\n",
      "Epoch [12170/20000], Loss: 0.18851298\n",
      "Epoch [12180/20000], Loss: 0.18839997\n",
      "Epoch [12190/20000], Loss: 0.18828714\n",
      "Epoch [12200/20000], Loss: 0.18817447\n",
      "Epoch [12210/20000], Loss: 0.18806198\n",
      "Epoch [12220/20000], Loss: 0.18794958\n",
      "Epoch [12230/20000], Loss: 0.18783738\n",
      "Epoch [12240/20000], Loss: 0.18772532\n",
      "Epoch [12250/20000], Loss: 0.18761340\n",
      "Epoch [12260/20000], Loss: 0.18750168\n",
      "Epoch [12270/20000], Loss: 0.18739007\n",
      "Epoch [12280/20000], Loss: 0.18727863\n",
      "Epoch [12290/20000], Loss: 0.18716735\n",
      "Epoch [12300/20000], Loss: 0.18705624\n",
      "Epoch [12310/20000], Loss: 0.18694521\n",
      "Epoch [12320/20000], Loss: 0.18683438\n",
      "Epoch [12330/20000], Loss: 0.18672371\n",
      "Epoch [12340/20000], Loss: 0.18661314\n",
      "Epoch [12350/20000], Loss: 0.18650275\n",
      "Epoch [12360/20000], Loss: 0.18639249\n",
      "Epoch [12370/20000], Loss: 0.18628241\n",
      "Epoch [12380/20000], Loss: 0.18617254\n",
      "Epoch [12390/20000], Loss: 0.18606269\n",
      "Epoch [12400/20000], Loss: 0.18595307\n",
      "Epoch [12410/20000], Loss: 0.18584360\n",
      "Epoch [12420/20000], Loss: 0.18573420\n",
      "Epoch [12430/20000], Loss: 0.18562503\n",
      "Epoch [12440/20000], Loss: 0.18551594\n",
      "Epoch [12450/20000], Loss: 0.18540709\n",
      "Epoch [12460/20000], Loss: 0.18529829\n",
      "Epoch [12470/20000], Loss: 0.18518966\n",
      "Epoch [12480/20000], Loss: 0.18508118\n",
      "Epoch [12490/20000], Loss: 0.18497285\n",
      "Epoch [12500/20000], Loss: 0.18486470\n",
      "Epoch [12510/20000], Loss: 0.18475667\n",
      "Epoch [12520/20000], Loss: 0.18464878\n",
      "Epoch [12530/20000], Loss: 0.18454102\n",
      "Epoch [12540/20000], Loss: 0.18443345\n",
      "Epoch [12550/20000], Loss: 0.18432598\n",
      "Epoch [12560/20000], Loss: 0.18421866\n",
      "Epoch [12570/20000], Loss: 0.18411151\n",
      "Epoch [12580/20000], Loss: 0.18400446\n",
      "Epoch [12590/20000], Loss: 0.18389751\n",
      "Epoch [12600/20000], Loss: 0.18379086\n",
      "Epoch [12610/20000], Loss: 0.18368421\n",
      "Epoch [12620/20000], Loss: 0.18357773\n",
      "Epoch [12630/20000], Loss: 0.18347141\n",
      "Epoch [12640/20000], Loss: 0.18336526\n",
      "Epoch [12650/20000], Loss: 0.18325920\n",
      "Epoch [12660/20000], Loss: 0.18315329\n",
      "Epoch [12670/20000], Loss: 0.18304752\n",
      "Epoch [12680/20000], Loss: 0.18294193\n",
      "Epoch [12690/20000], Loss: 0.18283644\n",
      "Epoch [12700/20000], Loss: 0.18273111\n",
      "Epoch [12710/20000], Loss: 0.18262585\n",
      "Epoch [12720/20000], Loss: 0.18252079\n",
      "Epoch [12730/20000], Loss: 0.18241587\n",
      "Epoch [12740/20000], Loss: 0.18231107\n",
      "Epoch [12750/20000], Loss: 0.18220641\n",
      "Epoch [12760/20000], Loss: 0.18210192\n",
      "Epoch [12770/20000], Loss: 0.18199749\n",
      "Epoch [12780/20000], Loss: 0.18189327\n",
      "Epoch [12790/20000], Loss: 0.18178920\n",
      "Epoch [12800/20000], Loss: 0.18168518\n",
      "Epoch [12810/20000], Loss: 0.18158135\n",
      "Epoch [12820/20000], Loss: 0.18147761\n",
      "Epoch [12830/20000], Loss: 0.18137409\n",
      "Epoch [12840/20000], Loss: 0.18127060\n",
      "Epoch [12850/20000], Loss: 0.18116732\n",
      "Epoch [12860/20000], Loss: 0.18106419\n",
      "Epoch [12870/20000], Loss: 0.18096109\n",
      "Epoch [12880/20000], Loss: 0.18085827\n",
      "Epoch [12890/20000], Loss: 0.18075547\n",
      "Epoch [12900/20000], Loss: 0.18065284\n",
      "Epoch [12910/20000], Loss: 0.18055029\n",
      "Epoch [12920/20000], Loss: 0.18044797\n",
      "Epoch [12930/20000], Loss: 0.18034574\n",
      "Epoch [12940/20000], Loss: 0.18024361\n",
      "Epoch [12950/20000], Loss: 0.18014163\n",
      "Epoch [12960/20000], Loss: 0.18003981\n",
      "Epoch [12970/20000], Loss: 0.17993806\n",
      "Epoch [12980/20000], Loss: 0.17983650\n",
      "Epoch [12990/20000], Loss: 0.17973503\n",
      "Epoch [13000/20000], Loss: 0.17963375\n",
      "Epoch [13010/20000], Loss: 0.17953253\n",
      "Epoch [13020/20000], Loss: 0.17943151\n",
      "Epoch [13030/20000], Loss: 0.17933056\n",
      "Epoch [13040/20000], Loss: 0.17922978\n",
      "Epoch [13050/20000], Loss: 0.17912911\n",
      "Epoch [13060/20000], Loss: 0.17902851\n",
      "Epoch [13070/20000], Loss: 0.17892811\n",
      "Epoch [13080/20000], Loss: 0.17882781\n",
      "Epoch [13090/20000], Loss: 0.17872764\n",
      "Epoch [13100/20000], Loss: 0.17862761\n",
      "Epoch [13110/20000], Loss: 0.17852774\n",
      "Epoch [13120/20000], Loss: 0.17842792\n",
      "Epoch [13130/20000], Loss: 0.17832831\n",
      "Epoch [13140/20000], Loss: 0.17822871\n",
      "Epoch [13150/20000], Loss: 0.17812935\n",
      "Epoch [13160/20000], Loss: 0.17803007\n",
      "Epoch [13170/20000], Loss: 0.17793091\n",
      "Epoch [13180/20000], Loss: 0.17783187\n",
      "Epoch [13190/20000], Loss: 0.17773300\n",
      "Epoch [13200/20000], Loss: 0.17763419\n",
      "Epoch [13210/20000], Loss: 0.17753555\n",
      "Epoch [13220/20000], Loss: 0.17743704\n",
      "Epoch [13230/20000], Loss: 0.17733866\n",
      "Epoch [13240/20000], Loss: 0.17724037\n",
      "Epoch [13250/20000], Loss: 0.17714219\n",
      "Epoch [13260/20000], Loss: 0.17704414\n",
      "Epoch [13270/20000], Loss: 0.17694621\n",
      "Epoch [13280/20000], Loss: 0.17684846\n",
      "Epoch [13290/20000], Loss: 0.17675079\n",
      "Epoch [13300/20000], Loss: 0.17665324\n",
      "Epoch [13310/20000], Loss: 0.17655578\n",
      "Epoch [13320/20000], Loss: 0.17645848\n",
      "Epoch [13330/20000], Loss: 0.17636129\n",
      "Epoch [13340/20000], Loss: 0.17626427\n",
      "Epoch [13350/20000], Loss: 0.17616729\n",
      "Epoch [13360/20000], Loss: 0.17607048\n",
      "Epoch [13370/20000], Loss: 0.17597382\n",
      "Epoch [13380/20000], Loss: 0.17587721\n",
      "Epoch [13390/20000], Loss: 0.17578073\n",
      "Epoch [13400/20000], Loss: 0.17568444\n",
      "Epoch [13410/20000], Loss: 0.17558819\n",
      "Epoch [13420/20000], Loss: 0.17549209\n",
      "Epoch [13430/20000], Loss: 0.17539611\n",
      "Epoch [13440/20000], Loss: 0.17530024\n",
      "Epoch [13450/20000], Loss: 0.17520450\n",
      "Epoch [13460/20000], Loss: 0.17510885\n",
      "Epoch [13470/20000], Loss: 0.17501335\n",
      "Epoch [13480/20000], Loss: 0.17491797\n",
      "Epoch [13490/20000], Loss: 0.17482266\n",
      "Epoch [13500/20000], Loss: 0.17472751\n",
      "Epoch [13510/20000], Loss: 0.17463249\n",
      "Epoch [13520/20000], Loss: 0.17453754\n",
      "Epoch [13530/20000], Loss: 0.17444277\n",
      "Epoch [13540/20000], Loss: 0.17434807\n",
      "Epoch [13550/20000], Loss: 0.17425348\n",
      "Epoch [13560/20000], Loss: 0.17415905\n",
      "Epoch [13570/20000], Loss: 0.17406464\n",
      "Epoch [13580/20000], Loss: 0.17397048\n",
      "Epoch [13590/20000], Loss: 0.17387636\n",
      "Epoch [13600/20000], Loss: 0.17378233\n",
      "Epoch [13610/20000], Loss: 0.17368850\n",
      "Epoch [13620/20000], Loss: 0.17359465\n",
      "Epoch [13630/20000], Loss: 0.17350101\n",
      "Epoch [13640/20000], Loss: 0.17340748\n",
      "Epoch [13650/20000], Loss: 0.17331409\n",
      "Epoch [13660/20000], Loss: 0.17322072\n",
      "Epoch [13670/20000], Loss: 0.17312758\n",
      "Epoch [13680/20000], Loss: 0.17303447\n",
      "Epoch [13690/20000], Loss: 0.17294149\n",
      "Epoch [13700/20000], Loss: 0.17284863\n",
      "Epoch [13710/20000], Loss: 0.17275585\n",
      "Epoch [13720/20000], Loss: 0.17266317\n",
      "Epoch [13730/20000], Loss: 0.17257065\n",
      "Epoch [13740/20000], Loss: 0.17247826\n",
      "Epoch [13750/20000], Loss: 0.17238598\n",
      "Epoch [13760/20000], Loss: 0.17229378\n",
      "Epoch [13770/20000], Loss: 0.17220169\n",
      "Epoch [13780/20000], Loss: 0.17210969\n",
      "Epoch [13790/20000], Loss: 0.17201783\n",
      "Epoch [13800/20000], Loss: 0.17192610\n",
      "Epoch [13810/20000], Loss: 0.17183445\n",
      "Epoch [13820/20000], Loss: 0.17174287\n",
      "Epoch [13830/20000], Loss: 0.17165148\n",
      "Epoch [13840/20000], Loss: 0.17156017\n",
      "Epoch [13850/20000], Loss: 0.17146896\n",
      "Epoch [13860/20000], Loss: 0.17137785\n",
      "Epoch [13870/20000], Loss: 0.17128687\n",
      "Epoch [13880/20000], Loss: 0.17119600\n",
      "Epoch [13890/20000], Loss: 0.17110521\n",
      "Epoch [13900/20000], Loss: 0.17101456\n",
      "Epoch [13910/20000], Loss: 0.17092399\n",
      "Epoch [13920/20000], Loss: 0.17083354\n",
      "Epoch [13930/20000], Loss: 0.17074326\n",
      "Epoch [13940/20000], Loss: 0.17065296\n",
      "Epoch [13950/20000], Loss: 0.17056289\n",
      "Epoch [13960/20000], Loss: 0.17047280\n",
      "Epoch [13970/20000], Loss: 0.17038292\n",
      "Epoch [13980/20000], Loss: 0.17029312\n",
      "Epoch [13990/20000], Loss: 0.17020336\n",
      "Epoch [14000/20000], Loss: 0.17011377\n",
      "Epoch [14010/20000], Loss: 0.17002428\n",
      "Epoch [14020/20000], Loss: 0.16993490\n",
      "Epoch [14030/20000], Loss: 0.16984566\n",
      "Epoch [14040/20000], Loss: 0.16975646\n",
      "Epoch [14050/20000], Loss: 0.16966739\n",
      "Epoch [14060/20000], Loss: 0.16957846\n",
      "Epoch [14070/20000], Loss: 0.16948958\n",
      "Epoch [14080/20000], Loss: 0.16940084\n",
      "Epoch [14090/20000], Loss: 0.16931219\n",
      "Epoch [14100/20000], Loss: 0.16922364\n",
      "Epoch [14110/20000], Loss: 0.16913517\n",
      "Epoch [14120/20000], Loss: 0.16904682\n",
      "Epoch [14130/20000], Loss: 0.16895859\n",
      "Epoch [14140/20000], Loss: 0.16887051\n",
      "Epoch [14150/20000], Loss: 0.16878246\n",
      "Epoch [14160/20000], Loss: 0.16869448\n",
      "Epoch [14170/20000], Loss: 0.16860667\n",
      "Epoch [14180/20000], Loss: 0.16851893\n",
      "Epoch [14190/20000], Loss: 0.16843128\n",
      "Epoch [14200/20000], Loss: 0.16834380\n",
      "Epoch [14210/20000], Loss: 0.16825640\n",
      "Epoch [14220/20000], Loss: 0.16816907\n",
      "Epoch [14230/20000], Loss: 0.16808185\n",
      "Epoch [14240/20000], Loss: 0.16799472\n",
      "Epoch [14250/20000], Loss: 0.16790770\n",
      "Epoch [14260/20000], Loss: 0.16782080\n",
      "Epoch [14270/20000], Loss: 0.16773400\n",
      "Epoch [14280/20000], Loss: 0.16764724\n",
      "Epoch [14290/20000], Loss: 0.16756062\n",
      "Epoch [14300/20000], Loss: 0.16747414\n",
      "Epoch [14310/20000], Loss: 0.16738768\n",
      "Epoch [14320/20000], Loss: 0.16730145\n",
      "Epoch [14330/20000], Loss: 0.16721521\n",
      "Epoch [14340/20000], Loss: 0.16712905\n",
      "Epoch [14350/20000], Loss: 0.16704307\n",
      "Epoch [14360/20000], Loss: 0.16695713\n",
      "Epoch [14370/20000], Loss: 0.16687128\n",
      "Epoch [14380/20000], Loss: 0.16678554\n",
      "Epoch [14390/20000], Loss: 0.16669996\n",
      "Epoch [14400/20000], Loss: 0.16661446\n",
      "Epoch [14410/20000], Loss: 0.16652900\n",
      "Epoch [14420/20000], Loss: 0.16644369\n",
      "Epoch [14430/20000], Loss: 0.16635849\n",
      "Epoch [14440/20000], Loss: 0.16627334\n",
      "Epoch [14450/20000], Loss: 0.16618830\n",
      "Epoch [14460/20000], Loss: 0.16610332\n",
      "Epoch [14470/20000], Loss: 0.16601847\n",
      "Epoch [14480/20000], Loss: 0.16593373\n",
      "Epoch [14490/20000], Loss: 0.16584907\n",
      "Epoch [14500/20000], Loss: 0.16576451\n",
      "Epoch [14510/20000], Loss: 0.16568002\n",
      "Epoch [14520/20000], Loss: 0.16559568\n",
      "Epoch [14530/20000], Loss: 0.16551141\n",
      "Epoch [14540/20000], Loss: 0.16542722\n",
      "Epoch [14550/20000], Loss: 0.16534315\n",
      "Epoch [14560/20000], Loss: 0.16525917\n",
      "Epoch [14570/20000], Loss: 0.16517524\n",
      "Epoch [14580/20000], Loss: 0.16509147\n",
      "Epoch [14590/20000], Loss: 0.16500780\n",
      "Epoch [14600/20000], Loss: 0.16492416\n",
      "Epoch [14610/20000], Loss: 0.16484061\n",
      "Epoch [14620/20000], Loss: 0.16475724\n",
      "Epoch [14630/20000], Loss: 0.16467389\n",
      "Epoch [14640/20000], Loss: 0.16459070\n",
      "Epoch [14650/20000], Loss: 0.16450757\n",
      "Epoch [14660/20000], Loss: 0.16442448\n",
      "Epoch [14670/20000], Loss: 0.16434158\n",
      "Epoch [14680/20000], Loss: 0.16425869\n",
      "Epoch [14690/20000], Loss: 0.16417591\n",
      "Epoch [14700/20000], Loss: 0.16409324\n",
      "Epoch [14710/20000], Loss: 0.16401072\n",
      "Epoch [14720/20000], Loss: 0.16392818\n",
      "Epoch [14730/20000], Loss: 0.16384579\n",
      "Epoch [14740/20000], Loss: 0.16376352\n",
      "Epoch [14750/20000], Loss: 0.16368125\n",
      "Epoch [14760/20000], Loss: 0.16359921\n",
      "Epoch [14770/20000], Loss: 0.16351712\n",
      "Epoch [14780/20000], Loss: 0.16343516\n",
      "Epoch [14790/20000], Loss: 0.16335332\n",
      "Epoch [14800/20000], Loss: 0.16327158\n",
      "Epoch [14810/20000], Loss: 0.16318989\n",
      "Epoch [14820/20000], Loss: 0.16310830\n",
      "Epoch [14830/20000], Loss: 0.16302682\n",
      "Epoch [14840/20000], Loss: 0.16294542\n",
      "Epoch [14850/20000], Loss: 0.16286415\n",
      "Epoch [14860/20000], Loss: 0.16278289\n",
      "Epoch [14870/20000], Loss: 0.16270180\n",
      "Epoch [14880/20000], Loss: 0.16262074\n",
      "Epoch [14890/20000], Loss: 0.16253982\n",
      "Epoch [14900/20000], Loss: 0.16245896\n",
      "Epoch [14910/20000], Loss: 0.16237818\n",
      "Epoch [14920/20000], Loss: 0.16229746\n",
      "Epoch [14930/20000], Loss: 0.16221689\n",
      "Epoch [14940/20000], Loss: 0.16213638\n",
      "Epoch [14950/20000], Loss: 0.16205598\n",
      "Epoch [14960/20000], Loss: 0.16197568\n",
      "Epoch [14970/20000], Loss: 0.16189539\n",
      "Epoch [14980/20000], Loss: 0.16181524\n",
      "Epoch [14990/20000], Loss: 0.16173524\n",
      "Epoch [15000/20000], Loss: 0.16165528\n",
      "Epoch [15010/20000], Loss: 0.16157527\n",
      "Epoch [15020/20000], Loss: 0.16149548\n",
      "Epoch [15030/20000], Loss: 0.16141579\n",
      "Epoch [15040/20000], Loss: 0.16133621\n",
      "Epoch [15050/20000], Loss: 0.16125661\n",
      "Epoch [15060/20000], Loss: 0.16117717\n",
      "Epoch [15070/20000], Loss: 0.16109785\n",
      "Epoch [15080/20000], Loss: 0.16101855\n",
      "Epoch [15090/20000], Loss: 0.16093940\n",
      "Epoch [15100/20000], Loss: 0.16086026\n",
      "Epoch [15110/20000], Loss: 0.16078120\n",
      "Epoch [15120/20000], Loss: 0.16070224\n",
      "Epoch [15130/20000], Loss: 0.16062346\n",
      "Epoch [15140/20000], Loss: 0.16054465\n",
      "Epoch [15150/20000], Loss: 0.16046597\n",
      "Epoch [15160/20000], Loss: 0.16038734\n",
      "Epoch [15170/20000], Loss: 0.16030888\n",
      "Epoch [15180/20000], Loss: 0.16023044\n",
      "Epoch [15190/20000], Loss: 0.16015209\n",
      "Epoch [15200/20000], Loss: 0.16007380\n",
      "Epoch [15210/20000], Loss: 0.15999565\n",
      "Epoch [15220/20000], Loss: 0.15991755\n",
      "Epoch [15230/20000], Loss: 0.15983959\n",
      "Epoch [15240/20000], Loss: 0.15976165\n",
      "Epoch [15250/20000], Loss: 0.15968379\n",
      "Epoch [15260/20000], Loss: 0.15960604\n",
      "Epoch [15270/20000], Loss: 0.15952834\n",
      "Epoch [15280/20000], Loss: 0.15945080\n",
      "Epoch [15290/20000], Loss: 0.15937328\n",
      "Epoch [15300/20000], Loss: 0.15929583\n",
      "Epoch [15310/20000], Loss: 0.15921851\n",
      "Epoch [15320/20000], Loss: 0.15914129\n",
      "Epoch [15330/20000], Loss: 0.15906410\n",
      "Epoch [15340/20000], Loss: 0.15898700\n",
      "Epoch [15350/20000], Loss: 0.15890998\n",
      "Epoch [15360/20000], Loss: 0.15883303\n",
      "Epoch [15370/20000], Loss: 0.15875618\n",
      "Epoch [15380/20000], Loss: 0.15867944\n",
      "Epoch [15390/20000], Loss: 0.15860273\n",
      "Epoch [15400/20000], Loss: 0.15852615\n",
      "Epoch [15410/20000], Loss: 0.15844965\n",
      "Epoch [15420/20000], Loss: 0.15837319\n",
      "Epoch [15430/20000], Loss: 0.15829679\n",
      "Epoch [15440/20000], Loss: 0.15822054\n",
      "Epoch [15450/20000], Loss: 0.15814435\n",
      "Epoch [15460/20000], Loss: 0.15806822\n",
      "Epoch [15470/20000], Loss: 0.15799218\n",
      "Epoch [15480/20000], Loss: 0.15791625\n",
      "Epoch [15490/20000], Loss: 0.15784036\n",
      "Epoch [15500/20000], Loss: 0.15776454\n",
      "Epoch [15510/20000], Loss: 0.15768884\n",
      "Epoch [15520/20000], Loss: 0.15761322\n",
      "Epoch [15530/20000], Loss: 0.15753767\n",
      "Epoch [15540/20000], Loss: 0.15746216\n",
      "Epoch [15550/20000], Loss: 0.15738679\n",
      "Epoch [15560/20000], Loss: 0.15731142\n",
      "Epoch [15570/20000], Loss: 0.15723617\n",
      "Epoch [15580/20000], Loss: 0.15716103\n",
      "Epoch [15590/20000], Loss: 0.15708597\n",
      "Epoch [15600/20000], Loss: 0.15701099\n",
      "Epoch [15610/20000], Loss: 0.15693603\n",
      "Epoch [15620/20000], Loss: 0.15686122\n",
      "Epoch [15630/20000], Loss: 0.15678640\n",
      "Epoch [15640/20000], Loss: 0.15671174\n",
      "Epoch [15650/20000], Loss: 0.15663710\n",
      "Epoch [15660/20000], Loss: 0.15656255\n",
      "Epoch [15670/20000], Loss: 0.15648815\n",
      "Epoch [15680/20000], Loss: 0.15641376\n",
      "Epoch [15690/20000], Loss: 0.15633948\n",
      "Epoch [15700/20000], Loss: 0.15626524\n",
      "Epoch [15710/20000], Loss: 0.15619110\n",
      "Epoch [15720/20000], Loss: 0.15611701\n",
      "Epoch [15730/20000], Loss: 0.15604301\n",
      "Epoch [15740/20000], Loss: 0.15596910\n",
      "Epoch [15750/20000], Loss: 0.15589525\n",
      "Epoch [15760/20000], Loss: 0.15582147\n",
      "Epoch [15770/20000], Loss: 0.15574780\n",
      "Epoch [15780/20000], Loss: 0.15567422\n",
      "Epoch [15790/20000], Loss: 0.15560067\n",
      "Epoch [15800/20000], Loss: 0.15552720\n",
      "Epoch [15810/20000], Loss: 0.15545382\n",
      "Epoch [15820/20000], Loss: 0.15538052\n",
      "Epoch [15830/20000], Loss: 0.15530729\n",
      "Epoch [15840/20000], Loss: 0.15523408\n",
      "Epoch [15850/20000], Loss: 0.15516102\n",
      "Epoch [15860/20000], Loss: 0.15508802\n",
      "Epoch [15870/20000], Loss: 0.15501502\n",
      "Epoch [15880/20000], Loss: 0.15494229\n",
      "Epoch [15890/20000], Loss: 0.15486947\n",
      "Epoch [15900/20000], Loss: 0.15479670\n",
      "Epoch [15910/20000], Loss: 0.15472411\n",
      "Epoch [15920/20000], Loss: 0.15465155\n",
      "Epoch [15930/20000], Loss: 0.15457901\n",
      "Epoch [15940/20000], Loss: 0.15450664\n",
      "Epoch [15950/20000], Loss: 0.15443434\n",
      "Epoch [15960/20000], Loss: 0.15436207\n",
      "Epoch [15970/20000], Loss: 0.15428984\n",
      "Epoch [15980/20000], Loss: 0.15421773\n",
      "Epoch [15990/20000], Loss: 0.15414567\n",
      "Epoch [16000/20000], Loss: 0.15407373\n",
      "Epoch [16010/20000], Loss: 0.15400183\n",
      "Epoch [16020/20000], Loss: 0.15393001\n",
      "Epoch [16030/20000], Loss: 0.15385824\n",
      "Epoch [16040/20000], Loss: 0.15378658\n",
      "Epoch [16050/20000], Loss: 0.15371495\n",
      "Epoch [16060/20000], Loss: 0.15364346\n",
      "Epoch [16070/20000], Loss: 0.15357201\n",
      "Epoch [16080/20000], Loss: 0.15350057\n",
      "Epoch [16090/20000], Loss: 0.15342923\n",
      "Epoch [16100/20000], Loss: 0.15335807\n",
      "Epoch [16110/20000], Loss: 0.15328684\n",
      "Epoch [16120/20000], Loss: 0.15321577\n",
      "Epoch [16130/20000], Loss: 0.15314476\n",
      "Epoch [16140/20000], Loss: 0.15307377\n",
      "Epoch [16150/20000], Loss: 0.15300289\n",
      "Epoch [16160/20000], Loss: 0.15293208\n",
      "Epoch [16170/20000], Loss: 0.15286134\n",
      "Epoch [16180/20000], Loss: 0.15279064\n",
      "Epoch [16190/20000], Loss: 0.15272012\n",
      "Epoch [16200/20000], Loss: 0.15264958\n",
      "Epoch [16210/20000], Loss: 0.15257908\n",
      "Epoch [16220/20000], Loss: 0.15250874\n",
      "Epoch [16230/20000], Loss: 0.15243840\n",
      "Epoch [16240/20000], Loss: 0.15236816\n",
      "Epoch [16250/20000], Loss: 0.15229796\n",
      "Epoch [16260/20000], Loss: 0.15222788\n",
      "Epoch [16270/20000], Loss: 0.15215784\n",
      "Epoch [16280/20000], Loss: 0.15208791\n",
      "Epoch [16290/20000], Loss: 0.15201797\n",
      "Epoch [16300/20000], Loss: 0.15194812\n",
      "Epoch [16310/20000], Loss: 0.15187840\n",
      "Epoch [16320/20000], Loss: 0.15180874\n",
      "Epoch [16330/20000], Loss: 0.15173908\n",
      "Epoch [16340/20000], Loss: 0.15166955\n",
      "Epoch [16350/20000], Loss: 0.15160008\n",
      "Epoch [16360/20000], Loss: 0.15153067\n",
      "Epoch [16370/20000], Loss: 0.15146126\n",
      "Epoch [16380/20000], Loss: 0.15139203\n",
      "Epoch [16390/20000], Loss: 0.15132284\n",
      "Epoch [16400/20000], Loss: 0.15125372\n",
      "Epoch [16410/20000], Loss: 0.15118465\n",
      "Epoch [16420/20000], Loss: 0.15111567\n",
      "Epoch [16430/20000], Loss: 0.15104672\n",
      "Epoch [16440/20000], Loss: 0.15097786\n",
      "Epoch [16450/20000], Loss: 0.15090904\n",
      "Epoch [16460/20000], Loss: 0.15084034\n",
      "Epoch [16470/20000], Loss: 0.15077169\n",
      "Epoch [16480/20000], Loss: 0.15070307\n",
      "Epoch [16490/20000], Loss: 0.15063459\n",
      "Epoch [16500/20000], Loss: 0.15056609\n",
      "Epoch [16510/20000], Loss: 0.15049773\n",
      "Epoch [16520/20000], Loss: 0.15042940\n",
      "Epoch [16530/20000], Loss: 0.15036114\n",
      "Epoch [16540/20000], Loss: 0.15029295\n",
      "Epoch [16550/20000], Loss: 0.15022485\n",
      "Epoch [16560/20000], Loss: 0.15015677\n",
      "Epoch [16570/20000], Loss: 0.15008888\n",
      "Epoch [16580/20000], Loss: 0.15002091\n",
      "Epoch [16590/20000], Loss: 0.14995304\n",
      "Epoch [16600/20000], Loss: 0.14988525\n",
      "Epoch [16610/20000], Loss: 0.14981753\n",
      "Epoch [16620/20000], Loss: 0.14974988\n",
      "Epoch [16630/20000], Loss: 0.14968228\n",
      "Epoch [16640/20000], Loss: 0.14961477\n",
      "Epoch [16650/20000], Loss: 0.14954735\n",
      "Epoch [16660/20000], Loss: 0.14947993\n",
      "Epoch [16670/20000], Loss: 0.14941260\n",
      "Epoch [16680/20000], Loss: 0.14934532\n",
      "Epoch [16690/20000], Loss: 0.14927815\n",
      "Epoch [16700/20000], Loss: 0.14921105\n",
      "Epoch [16710/20000], Loss: 0.14914398\n",
      "Epoch [16720/20000], Loss: 0.14907700\n",
      "Epoch [16730/20000], Loss: 0.14901000\n",
      "Epoch [16740/20000], Loss: 0.14894320\n",
      "Epoch [16750/20000], Loss: 0.14887637\n",
      "Epoch [16760/20000], Loss: 0.14880961\n",
      "Epoch [16770/20000], Loss: 0.14874296\n",
      "Epoch [16780/20000], Loss: 0.14867635\n",
      "Epoch [16790/20000], Loss: 0.14860983\n",
      "Epoch [16800/20000], Loss: 0.14854331\n",
      "Epoch [16810/20000], Loss: 0.14847690\n",
      "Epoch [16820/20000], Loss: 0.14841057\n",
      "Epoch [16830/20000], Loss: 0.14834425\n",
      "Epoch [16840/20000], Loss: 0.14827806\n",
      "Epoch [16850/20000], Loss: 0.14821187\n",
      "Epoch [16860/20000], Loss: 0.14814577\n",
      "Epoch [16870/20000], Loss: 0.14807975\n",
      "Epoch [16880/20000], Loss: 0.14801374\n",
      "Epoch [16890/20000], Loss: 0.14794783\n",
      "Epoch [16900/20000], Loss: 0.14788203\n",
      "Epoch [16910/20000], Loss: 0.14781623\n",
      "Epoch [16920/20000], Loss: 0.14775051\n",
      "Epoch [16930/20000], Loss: 0.14768486\n",
      "Epoch [16940/20000], Loss: 0.14761931\n",
      "Epoch [16950/20000], Loss: 0.14755374\n",
      "Epoch [16960/20000], Loss: 0.14748827\n",
      "Epoch [16970/20000], Loss: 0.14742288\n",
      "Epoch [16980/20000], Loss: 0.14735752\n",
      "Epoch [16990/20000], Loss: 0.14729226\n",
      "Epoch [17000/20000], Loss: 0.14722703\n",
      "Epoch [17010/20000], Loss: 0.14716187\n",
      "Epoch [17020/20000], Loss: 0.14709677\n",
      "Epoch [17030/20000], Loss: 0.14703174\n",
      "Epoch [17040/20000], Loss: 0.14696683\n",
      "Epoch [17050/20000], Loss: 0.14690188\n",
      "Epoch [17060/20000], Loss: 0.14683704\n",
      "Epoch [17070/20000], Loss: 0.14677224\n",
      "Epoch [17080/20000], Loss: 0.14670749\n",
      "Epoch [17090/20000], Loss: 0.14664288\n",
      "Epoch [17100/20000], Loss: 0.14657825\n",
      "Epoch [17110/20000], Loss: 0.14651373\n",
      "Epoch [17120/20000], Loss: 0.14644924\n",
      "Epoch [17130/20000], Loss: 0.14638482\n",
      "Epoch [17140/20000], Loss: 0.14632049\n",
      "Epoch [17150/20000], Loss: 0.14625619\n",
      "Epoch [17160/20000], Loss: 0.14619192\n",
      "Epoch [17170/20000], Loss: 0.14612779\n",
      "Epoch [17180/20000], Loss: 0.14606364\n",
      "Epoch [17190/20000], Loss: 0.14599963\n",
      "Epoch [17200/20000], Loss: 0.14593560\n",
      "Epoch [17210/20000], Loss: 0.14587170\n",
      "Epoch [17220/20000], Loss: 0.14580779\n",
      "Epoch [17230/20000], Loss: 0.14574403\n",
      "Epoch [17240/20000], Loss: 0.14568023\n",
      "Epoch [17250/20000], Loss: 0.14561655\n",
      "Epoch [17260/20000], Loss: 0.14555289\n",
      "Epoch [17270/20000], Loss: 0.14548934\n",
      "Epoch [17280/20000], Loss: 0.14542578\n",
      "Epoch [17290/20000], Loss: 0.14536238\n",
      "Epoch [17300/20000], Loss: 0.14529897\n",
      "Epoch [17310/20000], Loss: 0.14523563\n",
      "Epoch [17320/20000], Loss: 0.14517234\n",
      "Epoch [17330/20000], Loss: 0.14510913\n",
      "Epoch [17340/20000], Loss: 0.14504597\n",
      "Epoch [17350/20000], Loss: 0.14498289\n",
      "Epoch [17360/20000], Loss: 0.14491983\n",
      "Epoch [17370/20000], Loss: 0.14485686\n",
      "Epoch [17380/20000], Loss: 0.14479393\n",
      "Epoch [17390/20000], Loss: 0.14473107\n",
      "Epoch [17400/20000], Loss: 0.14466825\n",
      "Epoch [17410/20000], Loss: 0.14460550\n",
      "Epoch [17420/20000], Loss: 0.14454281\n",
      "Epoch [17430/20000], Loss: 0.14448021\n",
      "Epoch [17440/20000], Loss: 0.14441757\n",
      "Epoch [17450/20000], Loss: 0.14435512\n",
      "Epoch [17460/20000], Loss: 0.14429265\n",
      "Epoch [17470/20000], Loss: 0.14423019\n",
      "Epoch [17480/20000], Loss: 0.14416790\n",
      "Epoch [17490/20000], Loss: 0.14410561\n",
      "Epoch [17500/20000], Loss: 0.14404339\n",
      "Epoch [17510/20000], Loss: 0.14398120\n",
      "Epoch [17520/20000], Loss: 0.14391908\n",
      "Epoch [17530/20000], Loss: 0.14385705\n",
      "Epoch [17540/20000], Loss: 0.14379510\n",
      "Epoch [17550/20000], Loss: 0.14373311\n",
      "Epoch [17560/20000], Loss: 0.14367124\n",
      "Epoch [17570/20000], Loss: 0.14360937\n",
      "Epoch [17580/20000], Loss: 0.14354765\n",
      "Epoch [17590/20000], Loss: 0.14348587\n",
      "Epoch [17600/20000], Loss: 0.14342424\n",
      "Epoch [17610/20000], Loss: 0.14336270\n",
      "Epoch [17620/20000], Loss: 0.14330113\n",
      "Epoch [17630/20000], Loss: 0.14323962\n",
      "Epoch [17640/20000], Loss: 0.14317821\n",
      "Epoch [17650/20000], Loss: 0.14311680\n",
      "Epoch [17660/20000], Loss: 0.14305553\n",
      "Epoch [17670/20000], Loss: 0.14299424\n",
      "Epoch [17680/20000], Loss: 0.14293304\n",
      "Epoch [17690/20000], Loss: 0.14287187\n",
      "Epoch [17700/20000], Loss: 0.14281079\n",
      "Epoch [17710/20000], Loss: 0.14274976\n",
      "Epoch [17720/20000], Loss: 0.14268878\n",
      "Epoch [17730/20000], Loss: 0.14262782\n",
      "Epoch [17740/20000], Loss: 0.14256699\n",
      "Epoch [17750/20000], Loss: 0.14250620\n",
      "Epoch [17760/20000], Loss: 0.14244542\n",
      "Epoch [17770/20000], Loss: 0.14238465\n",
      "Epoch [17780/20000], Loss: 0.14232406\n",
      "Epoch [17790/20000], Loss: 0.14226347\n",
      "Epoch [17800/20000], Loss: 0.14220291\n",
      "Epoch [17810/20000], Loss: 0.14214243\n",
      "Epoch [17820/20000], Loss: 0.14208198\n",
      "Epoch [17830/20000], Loss: 0.14202164\n",
      "Epoch [17840/20000], Loss: 0.14196128\n",
      "Epoch [17850/20000], Loss: 0.14190105\n",
      "Epoch [17860/20000], Loss: 0.14184080\n",
      "Epoch [17870/20000], Loss: 0.14178064\n",
      "Epoch [17880/20000], Loss: 0.14172055\n",
      "Epoch [17890/20000], Loss: 0.14166050\n",
      "Epoch [17900/20000], Loss: 0.14160052\n",
      "Epoch [17910/20000], Loss: 0.14154054\n",
      "Epoch [17920/20000], Loss: 0.14148067\n",
      "Epoch [17930/20000], Loss: 0.14142086\n",
      "Epoch [17940/20000], Loss: 0.14136106\n",
      "Epoch [17950/20000], Loss: 0.14130133\n",
      "Epoch [17960/20000], Loss: 0.14124168\n",
      "Epoch [17970/20000], Loss: 0.14118204\n",
      "Epoch [17980/20000], Loss: 0.14112248\n",
      "Epoch [17990/20000], Loss: 0.14106296\n",
      "Epoch [18000/20000], Loss: 0.14100349\n",
      "Epoch [18010/20000], Loss: 0.14094408\n",
      "Epoch [18020/20000], Loss: 0.14088479\n",
      "Epoch [18030/20000], Loss: 0.14082545\n",
      "Epoch [18040/20000], Loss: 0.14076619\n",
      "Epoch [18050/20000], Loss: 0.14070703\n",
      "Epoch [18060/20000], Loss: 0.14064787\n",
      "Epoch [18070/20000], Loss: 0.14058881\n",
      "Epoch [18080/20000], Loss: 0.14052974\n",
      "Epoch [18090/20000], Loss: 0.14047079\n",
      "Epoch [18100/20000], Loss: 0.14041184\n",
      "Epoch [18110/20000], Loss: 0.14035293\n",
      "Epoch [18120/20000], Loss: 0.14029413\n",
      "Epoch [18130/20000], Loss: 0.14023535\n",
      "Epoch [18140/20000], Loss: 0.14017664\n",
      "Epoch [18150/20000], Loss: 0.14011800\n",
      "Epoch [18160/20000], Loss: 0.14005934\n",
      "Epoch [18170/20000], Loss: 0.14000081\n",
      "Epoch [18180/20000], Loss: 0.13994232\n",
      "Epoch [18190/20000], Loss: 0.13988382\n",
      "Epoch [18200/20000], Loss: 0.13982540\n",
      "Epoch [18210/20000], Loss: 0.13976708\n",
      "Epoch [18220/20000], Loss: 0.13970874\n",
      "Epoch [18230/20000], Loss: 0.13965052\n",
      "Epoch [18240/20000], Loss: 0.13959232\n",
      "Epoch [18250/20000], Loss: 0.13953421\n",
      "Epoch [18260/20000], Loss: 0.13947608\n",
      "Epoch [18270/20000], Loss: 0.13941804\n",
      "Epoch [18280/20000], Loss: 0.13936004\n",
      "Epoch [18290/20000], Loss: 0.13930206\n",
      "Epoch [18300/20000], Loss: 0.13924417\n",
      "Epoch [18310/20000], Loss: 0.13918637\n",
      "Epoch [18320/20000], Loss: 0.13912855\n",
      "Epoch [18330/20000], Loss: 0.13907081\n",
      "Epoch [18340/20000], Loss: 0.13901314\n",
      "Epoch [18350/20000], Loss: 0.13895546\n",
      "Epoch [18360/20000], Loss: 0.13889787\n",
      "Epoch [18370/20000], Loss: 0.13884038\n",
      "Epoch [18380/20000], Loss: 0.13878286\n",
      "Epoch [18390/20000], Loss: 0.13872543\n",
      "Epoch [18400/20000], Loss: 0.13866808\n",
      "Epoch [18410/20000], Loss: 0.13861071\n",
      "Epoch [18420/20000], Loss: 0.13855344\n",
      "Epoch [18430/20000], Loss: 0.13849615\n",
      "Epoch [18440/20000], Loss: 0.13843897\n",
      "Epoch [18450/20000], Loss: 0.13838187\n",
      "Epoch [18460/20000], Loss: 0.13832477\n",
      "Epoch [18470/20000], Loss: 0.13826770\n",
      "Epoch [18480/20000], Loss: 0.13821073\n",
      "Epoch [18490/20000], Loss: 0.13815381\n",
      "Epoch [18500/20000], Loss: 0.13809694\n",
      "Epoch [18510/20000], Loss: 0.13804007\n",
      "Epoch [18520/20000], Loss: 0.13798331\n",
      "Epoch [18530/20000], Loss: 0.13792656\n",
      "Epoch [18540/20000], Loss: 0.13786988\n",
      "Epoch [18550/20000], Loss: 0.13781321\n",
      "Epoch [18560/20000], Loss: 0.13775662\n",
      "Epoch [18570/20000], Loss: 0.13770008\n",
      "Epoch [18580/20000], Loss: 0.13764358\n",
      "Epoch [18590/20000], Loss: 0.13758713\n",
      "Epoch [18600/20000], Loss: 0.13753073\n",
      "Epoch [18610/20000], Loss: 0.13747436\n",
      "Epoch [18620/20000], Loss: 0.13741809\n",
      "Epoch [18630/20000], Loss: 0.13736185\n",
      "Epoch [18640/20000], Loss: 0.13730565\n",
      "Epoch [18650/20000], Loss: 0.13724948\n",
      "Epoch [18660/20000], Loss: 0.13719340\n",
      "Epoch [18670/20000], Loss: 0.13713734\n",
      "Epoch [18680/20000], Loss: 0.13708133\n",
      "Epoch [18690/20000], Loss: 0.13702540\n",
      "Epoch [18700/20000], Loss: 0.13696949\n",
      "Epoch [18710/20000], Loss: 0.13691357\n",
      "Epoch [18720/20000], Loss: 0.13685781\n",
      "Epoch [18730/20000], Loss: 0.13680199\n",
      "Epoch [18740/20000], Loss: 0.13674627\n",
      "Epoch [18750/20000], Loss: 0.13669063\n",
      "Epoch [18760/20000], Loss: 0.13663501\n",
      "Epoch [18770/20000], Loss: 0.13657942\n",
      "Epoch [18780/20000], Loss: 0.13652392\n",
      "Epoch [18790/20000], Loss: 0.13646843\n",
      "Epoch [18800/20000], Loss: 0.13641301\n",
      "Epoch [18810/20000], Loss: 0.13635758\n",
      "Epoch [18820/20000], Loss: 0.13630229\n",
      "Epoch [18830/20000], Loss: 0.13624698\n",
      "Epoch [18840/20000], Loss: 0.13619174\n",
      "Epoch [18850/20000], Loss: 0.13613655\n",
      "Epoch [18860/20000], Loss: 0.13608138\n",
      "Epoch [18870/20000], Loss: 0.13602625\n",
      "Epoch [18880/20000], Loss: 0.13597125\n",
      "Epoch [18890/20000], Loss: 0.13591623\n",
      "Epoch [18900/20000], Loss: 0.13586126\n",
      "Epoch [18910/20000], Loss: 0.13580634\n",
      "Epoch [18920/20000], Loss: 0.13575146\n",
      "Epoch [18930/20000], Loss: 0.13569662\n",
      "Epoch [18940/20000], Loss: 0.13564190\n",
      "Epoch [18950/20000], Loss: 0.13558716\n",
      "Epoch [18960/20000], Loss: 0.13553247\n",
      "Epoch [18970/20000], Loss: 0.13547786\n",
      "Epoch [18980/20000], Loss: 0.13542324\n",
      "Epoch [18990/20000], Loss: 0.13536870\n",
      "Epoch [19000/20000], Loss: 0.13531423\n",
      "Epoch [19010/20000], Loss: 0.13525981\n",
      "Epoch [19020/20000], Loss: 0.13520533\n",
      "Epoch [19030/20000], Loss: 0.13515098\n",
      "Epoch [19040/20000], Loss: 0.13509665\n",
      "Epoch [19050/20000], Loss: 0.13504238\n",
      "Epoch [19060/20000], Loss: 0.13498820\n",
      "Epoch [19070/20000], Loss: 0.13493401\n",
      "Epoch [19080/20000], Loss: 0.13487987\n",
      "Epoch [19090/20000], Loss: 0.13482581\n",
      "Epoch [19100/20000], Loss: 0.13477175\n",
      "Epoch [19110/20000], Loss: 0.13471775\n",
      "Epoch [19120/20000], Loss: 0.13466381\n",
      "Epoch [19130/20000], Loss: 0.13460994\n",
      "Epoch [19140/20000], Loss: 0.13455606\n",
      "Epoch [19150/20000], Loss: 0.13450228\n",
      "Epoch [19160/20000], Loss: 0.13444851\n",
      "Epoch [19170/20000], Loss: 0.13439478\n",
      "Epoch [19180/20000], Loss: 0.13434112\n",
      "Epoch [19190/20000], Loss: 0.13428745\n",
      "Epoch [19200/20000], Loss: 0.13423385\n",
      "Epoch [19210/20000], Loss: 0.13418034\n",
      "Epoch [19220/20000], Loss: 0.13412683\n",
      "Epoch [19230/20000], Loss: 0.13407341\n",
      "Epoch [19240/20000], Loss: 0.13401997\n",
      "Epoch [19250/20000], Loss: 0.13396662\n",
      "Epoch [19260/20000], Loss: 0.13391332\n",
      "Epoch [19270/20000], Loss: 0.13386004\n",
      "Epoch [19280/20000], Loss: 0.13380679\n",
      "Epoch [19290/20000], Loss: 0.13375363\n",
      "Epoch [19300/20000], Loss: 0.13370046\n",
      "Epoch [19310/20000], Loss: 0.13364737\n",
      "Epoch [19320/20000], Loss: 0.13359432\n",
      "Epoch [19330/20000], Loss: 0.13354129\n",
      "Epoch [19340/20000], Loss: 0.13348834\n",
      "Epoch [19350/20000], Loss: 0.13343544\n",
      "Epoch [19360/20000], Loss: 0.13338257\n",
      "Epoch [19370/20000], Loss: 0.13332975\n",
      "Epoch [19380/20000], Loss: 0.13327694\n",
      "Epoch [19390/20000], Loss: 0.13322419\n",
      "Epoch [19400/20000], Loss: 0.13317150\n",
      "Epoch [19410/20000], Loss: 0.13311882\n",
      "Epoch [19420/20000], Loss: 0.13306621\n",
      "Epoch [19430/20000], Loss: 0.13301367\n",
      "Epoch [19440/20000], Loss: 0.13296108\n",
      "Epoch [19450/20000], Loss: 0.13290864\n",
      "Epoch [19460/20000], Loss: 0.13285619\n",
      "Epoch [19470/20000], Loss: 0.13280383\n",
      "Epoch [19480/20000], Loss: 0.13275142\n",
      "Epoch [19490/20000], Loss: 0.13269912\n",
      "Epoch [19500/20000], Loss: 0.13264684\n",
      "Epoch [19510/20000], Loss: 0.13259465\n",
      "Epoch [19520/20000], Loss: 0.13254248\n",
      "Epoch [19530/20000], Loss: 0.13249032\n",
      "Epoch [19540/20000], Loss: 0.13243827\n",
      "Epoch [19550/20000], Loss: 0.13238615\n",
      "Epoch [19560/20000], Loss: 0.13233413\n",
      "Epoch [19570/20000], Loss: 0.13228215\n",
      "Epoch [19580/20000], Loss: 0.13223027\n",
      "Epoch [19590/20000], Loss: 0.13217843\n",
      "Epoch [19600/20000], Loss: 0.13212658\n",
      "Epoch [19610/20000], Loss: 0.13207471\n",
      "Epoch [19620/20000], Loss: 0.13202301\n",
      "Epoch [19630/20000], Loss: 0.13197130\n",
      "Epoch [19640/20000], Loss: 0.13191964\n",
      "Epoch [19650/20000], Loss: 0.13186800\n",
      "Epoch [19660/20000], Loss: 0.13181640\n",
      "Epoch [19670/20000], Loss: 0.13176487\n",
      "Epoch [19680/20000], Loss: 0.13171335\n",
      "Epoch [19690/20000], Loss: 0.13166192\n",
      "Epoch [19700/20000], Loss: 0.13161048\n",
      "Epoch [19710/20000], Loss: 0.13155912\n",
      "Epoch [19720/20000], Loss: 0.13150780\n",
      "Epoch [19730/20000], Loss: 0.13145646\n",
      "Epoch [19740/20000], Loss: 0.13140522\n",
      "Epoch [19750/20000], Loss: 0.13135403\n",
      "Epoch [19760/20000], Loss: 0.13130288\n",
      "Epoch [19770/20000], Loss: 0.13125175\n",
      "Epoch [19780/20000], Loss: 0.13120064\n",
      "Epoch [19790/20000], Loss: 0.13114965\n",
      "Epoch [19800/20000], Loss: 0.13109860\n",
      "Epoch [19810/20000], Loss: 0.13104767\n",
      "Epoch [19820/20000], Loss: 0.13099675\n",
      "Epoch [19830/20000], Loss: 0.13094592\n",
      "Epoch [19840/20000], Loss: 0.13089503\n",
      "Epoch [19850/20000], Loss: 0.13084422\n",
      "Epoch [19860/20000], Loss: 0.13079348\n",
      "Epoch [19870/20000], Loss: 0.13074276\n",
      "Epoch [19880/20000], Loss: 0.13069208\n",
      "Epoch [19890/20000], Loss: 0.13064148\n",
      "Epoch [19900/20000], Loss: 0.13059089\n",
      "Epoch [19910/20000], Loss: 0.13054034\n",
      "Epoch [19920/20000], Loss: 0.13048986\n",
      "Epoch [19930/20000], Loss: 0.13043940\n",
      "Epoch [19940/20000], Loss: 0.13038893\n",
      "Epoch [19950/20000], Loss: 0.13033858\n",
      "Epoch [19960/20000], Loss: 0.13028824\n",
      "Epoch [19970/20000], Loss: 0.13023791\n",
      "Epoch [19980/20000], Loss: 0.13018766\n",
      "Epoch [19990/20000], Loss: 0.13013743\n",
      "Epoch [20000/20000], Loss: 0.13008726\n",
      "Trained weights: tensor([ 0.0041,  0.0850,  0.0508,  ..., -0.0090,  0.0161, -0.0640],\n",
      "       requires_grad=True)\n",
      "Trained bias: tensor([-0.0038], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error rate: tensor(0.1737)\n",
      "Test error rate: tensor(0.6150)\n"
     ]
    }
   ],
   "source": [
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 30000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "cost_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    cost_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Print the loss every 10 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.detach().numpy(), b.detach().numpy())\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.detach().numpy(), b.detach().numpy())\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9d60e",
   "metadata": {},
   "source": [
    "Custom SGD Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76c6390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Linear(in_features=1999, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "tensor(-0.0287, grad_fn=<SelectBackward0>)\n",
      "Epoch [1/5000], Loss: 1.01643372\n",
      "tensor(0.0286, grad_fn=<SelectBackward0>)\n",
      "Epoch [10/5000], Loss: 1.00187731\n",
      "tensor(0.0054, grad_fn=<SelectBackward0>)\n",
      "Epoch [20/5000], Loss: 0.99702573\n",
      "tensor(-0.0083, grad_fn=<SelectBackward0>)\n",
      "Epoch [30/5000], Loss: 0.99317557\n",
      "tensor(-0.0166, grad_fn=<SelectBackward0>)\n",
      "Epoch [40/5000], Loss: 0.98968607\n",
      "tensor(-0.0217, grad_fn=<SelectBackward0>)\n",
      "Epoch [50/5000], Loss: 0.98633593\n",
      "tensor(-0.0248, grad_fn=<SelectBackward0>)\n",
      "Epoch [60/5000], Loss: 0.98304862\n",
      "tensor(-0.0268, grad_fn=<SelectBackward0>)\n",
      "Epoch [70/5000], Loss: 0.97979760\n",
      "tensor(-0.0282, grad_fn=<SelectBackward0>)\n",
      "Epoch [80/5000], Loss: 0.97657359\n",
      "tensor(-0.0292, grad_fn=<SelectBackward0>)\n",
      "Epoch [90/5000], Loss: 0.97337312\n",
      "tensor(-0.0300, grad_fn=<SelectBackward0>)\n",
      "Epoch [100/5000], Loss: 0.97019488\n",
      "tensor(-0.0306, grad_fn=<SelectBackward0>)\n",
      "Epoch [110/5000], Loss: 0.96703827\n",
      "tensor(-0.0311, grad_fn=<SelectBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chris\\AppData\\Local\\Temp\\ipykernel_19204\\2543760084.py:31: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  param.data.add_(-lr, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/5000], Loss: 0.96390271\n",
      "tensor(-0.0316, grad_fn=<SelectBackward0>)\n",
      "Epoch [130/5000], Loss: 0.96078813\n",
      "tensor(-0.0321, grad_fn=<SelectBackward0>)\n",
      "Epoch [140/5000], Loss: 0.95769423\n",
      "tensor(-0.0325, grad_fn=<SelectBackward0>)\n",
      "Epoch [150/5000], Loss: 0.95462054\n",
      "tensor(-0.0330, grad_fn=<SelectBackward0>)\n",
      "Epoch [160/5000], Loss: 0.95156705\n",
      "tensor(-0.0334, grad_fn=<SelectBackward0>)\n",
      "Epoch [170/5000], Loss: 0.94853348\n",
      "tensor(-0.0338, grad_fn=<SelectBackward0>)\n",
      "Epoch [180/5000], Loss: 0.94551951\n",
      "tensor(-0.0342, grad_fn=<SelectBackward0>)\n",
      "Epoch [190/5000], Loss: 0.94252497\n",
      "tensor(-0.0346, grad_fn=<SelectBackward0>)\n",
      "Epoch [200/5000], Loss: 0.93954962\n",
      "tensor(-0.0350, grad_fn=<SelectBackward0>)\n",
      "Epoch [210/5000], Loss: 0.93659329\n",
      "tensor(-0.0354, grad_fn=<SelectBackward0>)\n",
      "Epoch [220/5000], Loss: 0.93365586\n",
      "tensor(-0.0358, grad_fn=<SelectBackward0>)\n",
      "Epoch [230/5000], Loss: 0.93073678\n",
      "tensor(-0.0362, grad_fn=<SelectBackward0>)\n",
      "Epoch [240/5000], Loss: 0.92783600\n",
      "tensor(-0.0365, grad_fn=<SelectBackward0>)\n",
      "Epoch [250/5000], Loss: 0.92495346\n",
      "tensor(-0.0369, grad_fn=<SelectBackward0>)\n",
      "Epoch [260/5000], Loss: 0.92208886\n",
      "tensor(-0.0373, grad_fn=<SelectBackward0>)\n",
      "Epoch [270/5000], Loss: 0.91924185\n",
      "tensor(-0.0377, grad_fn=<SelectBackward0>)\n",
      "Epoch [280/5000], Loss: 0.91641253\n",
      "tensor(-0.0380, grad_fn=<SelectBackward0>)\n",
      "Epoch [290/5000], Loss: 0.91360039\n",
      "tensor(-0.0384, grad_fn=<SelectBackward0>)\n",
      "Epoch [300/5000], Loss: 0.91080546\n",
      "tensor(-0.0388, grad_fn=<SelectBackward0>)\n",
      "Epoch [310/5000], Loss: 0.90802759\n",
      "tensor(-0.0391, grad_fn=<SelectBackward0>)\n",
      "Epoch [320/5000], Loss: 0.90526628\n",
      "tensor(-0.0395, grad_fn=<SelectBackward0>)\n",
      "Epoch [330/5000], Loss: 0.90252167\n",
      "tensor(-0.0399, grad_fn=<SelectBackward0>)\n",
      "Epoch [340/5000], Loss: 0.89979357\n",
      "tensor(-0.0402, grad_fn=<SelectBackward0>)\n",
      "Epoch [350/5000], Loss: 0.89708155\n",
      "tensor(-0.0406, grad_fn=<SelectBackward0>)\n",
      "Epoch [360/5000], Loss: 0.89438558\n",
      "tensor(-0.0409, grad_fn=<SelectBackward0>)\n",
      "Epoch [370/5000], Loss: 0.89170563\n",
      "tensor(-0.0412, grad_fn=<SelectBackward0>)\n",
      "Epoch [380/5000], Loss: 0.88904130\n",
      "tensor(-0.0416, grad_fn=<SelectBackward0>)\n",
      "Epoch [390/5000], Loss: 0.88639259\n",
      "tensor(-0.0419, grad_fn=<SelectBackward0>)\n",
      "Epoch [400/5000], Loss: 0.88375914\n",
      "tensor(-0.0422, grad_fn=<SelectBackward0>)\n",
      "Epoch [410/5000], Loss: 0.88114107\n",
      "tensor(-0.0426, grad_fn=<SelectBackward0>)\n",
      "Epoch [420/5000], Loss: 0.87853807\n",
      "tensor(-0.0429, grad_fn=<SelectBackward0>)\n",
      "Epoch [430/5000], Loss: 0.87594992\n",
      "tensor(-0.0432, grad_fn=<SelectBackward0>)\n",
      "Epoch [440/5000], Loss: 0.87337661\n",
      "tensor(-0.0435, grad_fn=<SelectBackward0>)\n",
      "Epoch [450/5000], Loss: 0.87081796\n",
      "tensor(-0.0439, grad_fn=<SelectBackward0>)\n",
      "Epoch [460/5000], Loss: 0.86827368\n",
      "tensor(-0.0442, grad_fn=<SelectBackward0>)\n",
      "Epoch [470/5000], Loss: 0.86574388\n",
      "tensor(-0.0445, grad_fn=<SelectBackward0>)\n",
      "Epoch [480/5000], Loss: 0.86322808\n",
      "tensor(-0.0448, grad_fn=<SelectBackward0>)\n",
      "Epoch [490/5000], Loss: 0.86072648\n",
      "tensor(-0.0451, grad_fn=<SelectBackward0>)\n",
      "Epoch [500/5000], Loss: 0.85823876\n",
      "tensor(-0.0454, grad_fn=<SelectBackward0>)\n",
      "Epoch [510/5000], Loss: 0.85576475\n",
      "tensor(-0.0457, grad_fn=<SelectBackward0>)\n",
      "Epoch [520/5000], Loss: 0.85330451\n",
      "tensor(-0.0460, grad_fn=<SelectBackward0>)\n",
      "Epoch [530/5000], Loss: 0.85085762\n",
      "tensor(-0.0463, grad_fn=<SelectBackward0>)\n",
      "Epoch [540/5000], Loss: 0.84842420\n",
      "tensor(-0.0466, grad_fn=<SelectBackward0>)\n",
      "Epoch [550/5000], Loss: 0.84600401\n",
      "tensor(-0.0469, grad_fn=<SelectBackward0>)\n",
      "Epoch [560/5000], Loss: 0.84359694\n",
      "tensor(-0.0472, grad_fn=<SelectBackward0>)\n",
      "Epoch [570/5000], Loss: 0.84120291\n",
      "tensor(-0.0475, grad_fn=<SelectBackward0>)\n",
      "Epoch [580/5000], Loss: 0.83882171\n",
      "tensor(-0.0478, grad_fn=<SelectBackward0>)\n",
      "Epoch [590/5000], Loss: 0.83645332\n",
      "tensor(-0.0480, grad_fn=<SelectBackward0>)\n",
      "Epoch [600/5000], Loss: 0.83409750\n",
      "tensor(-0.0483, grad_fn=<SelectBackward0>)\n",
      "Epoch [610/5000], Loss: 0.83175421\n",
      "tensor(-0.0486, grad_fn=<SelectBackward0>)\n",
      "Epoch [620/5000], Loss: 0.82942337\n",
      "tensor(-0.0489, grad_fn=<SelectBackward0>)\n",
      "Epoch [630/5000], Loss: 0.82710475\n",
      "tensor(-0.0491, grad_fn=<SelectBackward0>)\n",
      "Epoch [640/5000], Loss: 0.82479829\n",
      "tensor(-0.0494, grad_fn=<SelectBackward0>)\n",
      "Epoch [650/5000], Loss: 0.82250398\n",
      "tensor(-0.0497, grad_fn=<SelectBackward0>)\n",
      "Epoch [660/5000], Loss: 0.82022166\n",
      "tensor(-0.0499, grad_fn=<SelectBackward0>)\n",
      "Epoch [670/5000], Loss: 0.81795114\n",
      "tensor(-0.0502, grad_fn=<SelectBackward0>)\n",
      "Epoch [680/5000], Loss: 0.81569231\n",
      "tensor(-0.0505, grad_fn=<SelectBackward0>)\n",
      "Epoch [690/5000], Loss: 0.81344521\n",
      "tensor(-0.0507, grad_fn=<SelectBackward0>)\n",
      "Epoch [700/5000], Loss: 0.81120956\n",
      "tensor(-0.0510, grad_fn=<SelectBackward0>)\n",
      "Epoch [710/5000], Loss: 0.80898547\n",
      "tensor(-0.0512, grad_fn=<SelectBackward0>)\n",
      "Epoch [720/5000], Loss: 0.80677253\n",
      "tensor(-0.0515, grad_fn=<SelectBackward0>)\n",
      "Epoch [730/5000], Loss: 0.80457091\n",
      "tensor(-0.0517, grad_fn=<SelectBackward0>)\n",
      "Epoch [740/5000], Loss: 0.80238044\n",
      "tensor(-0.0520, grad_fn=<SelectBackward0>)\n",
      "Epoch [750/5000], Loss: 0.80020112\n",
      "tensor(-0.0522, grad_fn=<SelectBackward0>)\n",
      "Epoch [760/5000], Loss: 0.79803258\n",
      "tensor(-0.0524, grad_fn=<SelectBackward0>)\n",
      "Epoch [770/5000], Loss: 0.79587501\n",
      "tensor(-0.0527, grad_fn=<SelectBackward0>)\n",
      "Epoch [780/5000], Loss: 0.79372823\n",
      "tensor(-0.0529, grad_fn=<SelectBackward0>)\n",
      "Epoch [790/5000], Loss: 0.79159200\n",
      "tensor(-0.0532, grad_fn=<SelectBackward0>)\n",
      "Epoch [800/5000], Loss: 0.78946638\n",
      "tensor(-0.0534, grad_fn=<SelectBackward0>)\n",
      "Epoch [810/5000], Loss: 0.78735143\n",
      "tensor(-0.0536, grad_fn=<SelectBackward0>)\n",
      "Epoch [820/5000], Loss: 0.78524673\n",
      "tensor(-0.0539, grad_fn=<SelectBackward0>)\n",
      "Epoch [830/5000], Loss: 0.78315240\n",
      "tensor(-0.0541, grad_fn=<SelectBackward0>)\n",
      "Epoch [840/5000], Loss: 0.78106827\n",
      "tensor(-0.0543, grad_fn=<SelectBackward0>)\n",
      "Epoch [850/5000], Loss: 0.77899438\n",
      "tensor(-0.0545, grad_fn=<SelectBackward0>)\n",
      "Epoch [860/5000], Loss: 0.77693057\n",
      "tensor(-0.0547, grad_fn=<SelectBackward0>)\n",
      "Epoch [870/5000], Loss: 0.77487653\n",
      "tensor(-0.0550, grad_fn=<SelectBackward0>)\n",
      "Epoch [880/5000], Loss: 0.77283257\n",
      "tensor(-0.0552, grad_fn=<SelectBackward0>)\n",
      "Epoch [890/5000], Loss: 0.77079844\n",
      "tensor(-0.0554, grad_fn=<SelectBackward0>)\n",
      "Epoch [900/5000], Loss: 0.76877409\n",
      "tensor(-0.0556, grad_fn=<SelectBackward0>)\n",
      "Epoch [910/5000], Loss: 0.76675934\n",
      "tensor(-0.0558, grad_fn=<SelectBackward0>)\n",
      "Epoch [920/5000], Loss: 0.76475418\n",
      "tensor(-0.0560, grad_fn=<SelectBackward0>)\n",
      "Epoch [930/5000], Loss: 0.76275861\n",
      "tensor(-0.0562, grad_fn=<SelectBackward0>)\n",
      "Epoch [940/5000], Loss: 0.76077253\n",
      "tensor(-0.0564, grad_fn=<SelectBackward0>)\n",
      "Epoch [950/5000], Loss: 0.75879580\n",
      "tensor(-0.0566, grad_fn=<SelectBackward0>)\n",
      "Epoch [960/5000], Loss: 0.75682831\n",
      "tensor(-0.0568, grad_fn=<SelectBackward0>)\n",
      "Epoch [970/5000], Loss: 0.75487000\n",
      "tensor(-0.0570, grad_fn=<SelectBackward0>)\n",
      "Epoch [980/5000], Loss: 0.75292099\n",
      "tensor(-0.0572, grad_fn=<SelectBackward0>)\n",
      "Epoch [990/5000], Loss: 0.75098097\n",
      "tensor(-0.0574, grad_fn=<SelectBackward0>)\n",
      "Epoch [1000/5000], Loss: 0.74905014\n",
      "tensor(-0.0576, grad_fn=<SelectBackward0>)\n",
      "Epoch [1010/5000], Loss: 0.74712825\n",
      "tensor(-0.0578, grad_fn=<SelectBackward0>)\n",
      "Epoch [1020/5000], Loss: 0.74521500\n",
      "tensor(-0.0580, grad_fn=<SelectBackward0>)\n",
      "Epoch [1030/5000], Loss: 0.74331075\n",
      "tensor(-0.0582, grad_fn=<SelectBackward0>)\n",
      "Epoch [1040/5000], Loss: 0.74141532\n",
      "tensor(-0.0584, grad_fn=<SelectBackward0>)\n",
      "Epoch [1050/5000], Loss: 0.73952836\n",
      "tensor(-0.0586, grad_fn=<SelectBackward0>)\n",
      "Epoch [1060/5000], Loss: 0.73765022\n",
      "tensor(-0.0588, grad_fn=<SelectBackward0>)\n",
      "Epoch [1070/5000], Loss: 0.73578066\n",
      "tensor(-0.0589, grad_fn=<SelectBackward0>)\n",
      "Epoch [1080/5000], Loss: 0.73391944\n",
      "tensor(-0.0591, grad_fn=<SelectBackward0>)\n",
      "Epoch [1090/5000], Loss: 0.73206681\n",
      "tensor(-0.0593, grad_fn=<SelectBackward0>)\n",
      "Epoch [1100/5000], Loss: 0.73022252\n",
      "tensor(-0.0595, grad_fn=<SelectBackward0>)\n",
      "Epoch [1110/5000], Loss: 0.72838652\n",
      "tensor(-0.0597, grad_fn=<SelectBackward0>)\n",
      "Epoch [1120/5000], Loss: 0.72655886\n",
      "tensor(-0.0598, grad_fn=<SelectBackward0>)\n",
      "Epoch [1130/5000], Loss: 0.72473931\n",
      "tensor(-0.0600, grad_fn=<SelectBackward0>)\n",
      "Epoch [1140/5000], Loss: 0.72292793\n",
      "tensor(-0.0602, grad_fn=<SelectBackward0>)\n",
      "Epoch [1150/5000], Loss: 0.72112459\n",
      "tensor(-0.0603, grad_fn=<SelectBackward0>)\n",
      "Epoch [1160/5000], Loss: 0.71932930\n",
      "tensor(-0.0605, grad_fn=<SelectBackward0>)\n",
      "Epoch [1170/5000], Loss: 0.71754211\n",
      "tensor(-0.0607, grad_fn=<SelectBackward0>)\n",
      "Epoch [1180/5000], Loss: 0.71576273\n",
      "tensor(-0.0608, grad_fn=<SelectBackward0>)\n",
      "Epoch [1190/5000], Loss: 0.71399122\n",
      "tensor(-0.0610, grad_fn=<SelectBackward0>)\n",
      "Epoch [1200/5000], Loss: 0.71222758\n",
      "tensor(-0.0612, grad_fn=<SelectBackward0>)\n",
      "Epoch [1210/5000], Loss: 0.71047163\n",
      "tensor(-0.0613, grad_fn=<SelectBackward0>)\n",
      "Epoch [1220/5000], Loss: 0.70872337\n",
      "tensor(-0.0615, grad_fn=<SelectBackward0>)\n",
      "Epoch [1230/5000], Loss: 0.70698279\n",
      "tensor(-0.0616, grad_fn=<SelectBackward0>)\n",
      "Epoch [1240/5000], Loss: 0.70524979\n",
      "tensor(-0.0618, grad_fn=<SelectBackward0>)\n",
      "Epoch [1250/5000], Loss: 0.70352441\n",
      "tensor(-0.0619, grad_fn=<SelectBackward0>)\n",
      "Epoch [1260/5000], Loss: 0.70180643\n",
      "tensor(-0.0621, grad_fn=<SelectBackward0>)\n",
      "Epoch [1270/5000], Loss: 0.70009595\n",
      "tensor(-0.0623, grad_fn=<SelectBackward0>)\n",
      "Epoch [1280/5000], Loss: 0.69839287\n",
      "tensor(-0.0624, grad_fn=<SelectBackward0>)\n",
      "Epoch [1290/5000], Loss: 0.69669706\n",
      "tensor(-0.0626, grad_fn=<SelectBackward0>)\n",
      "Epoch [1300/5000], Loss: 0.69500870\n",
      "tensor(-0.0627, grad_fn=<SelectBackward0>)\n",
      "Epoch [1310/5000], Loss: 0.69332749\n",
      "tensor(-0.0628, grad_fn=<SelectBackward0>)\n",
      "Epoch [1320/5000], Loss: 0.69165343\n",
      "tensor(-0.0630, grad_fn=<SelectBackward0>)\n",
      "Epoch [1330/5000], Loss: 0.68998665\n",
      "tensor(-0.0631, grad_fn=<SelectBackward0>)\n",
      "Epoch [1340/5000], Loss: 0.68832690\n",
      "tensor(-0.0633, grad_fn=<SelectBackward0>)\n",
      "Epoch [1350/5000], Loss: 0.68667430\n",
      "tensor(-0.0634, grad_fn=<SelectBackward0>)\n",
      "Epoch [1360/5000], Loss: 0.68502861\n",
      "tensor(-0.0636, grad_fn=<SelectBackward0>)\n",
      "Epoch [1370/5000], Loss: 0.68338996\n",
      "tensor(-0.0637, grad_fn=<SelectBackward0>)\n",
      "Epoch [1380/5000], Loss: 0.68175828\n",
      "tensor(-0.0638, grad_fn=<SelectBackward0>)\n",
      "Epoch [1390/5000], Loss: 0.68013346\n",
      "tensor(-0.0640, grad_fn=<SelectBackward0>)\n",
      "Epoch [1400/5000], Loss: 0.67851537\n",
      "tensor(-0.0641, grad_fn=<SelectBackward0>)\n",
      "Epoch [1410/5000], Loss: 0.67690414\n",
      "tensor(-0.0642, grad_fn=<SelectBackward0>)\n",
      "Epoch [1420/5000], Loss: 0.67529970\n",
      "tensor(-0.0644, grad_fn=<SelectBackward0>)\n",
      "Epoch [1430/5000], Loss: 0.67370194\n",
      "tensor(-0.0645, grad_fn=<SelectBackward0>)\n",
      "Epoch [1440/5000], Loss: 0.67211080\n",
      "tensor(-0.0646, grad_fn=<SelectBackward0>)\n",
      "Epoch [1450/5000], Loss: 0.67052644\n",
      "tensor(-0.0647, grad_fn=<SelectBackward0>)\n",
      "Epoch [1460/5000], Loss: 0.66894859\n",
      "tensor(-0.0649, grad_fn=<SelectBackward0>)\n",
      "Epoch [1470/5000], Loss: 0.66737729\n",
      "tensor(-0.0650, grad_fn=<SelectBackward0>)\n",
      "Epoch [1480/5000], Loss: 0.66581255\n",
      "tensor(-0.0651, grad_fn=<SelectBackward0>)\n",
      "Epoch [1490/5000], Loss: 0.66425413\n",
      "tensor(-0.0652, grad_fn=<SelectBackward0>)\n",
      "Epoch [1500/5000], Loss: 0.66270232\n",
      "tensor(-0.0654, grad_fn=<SelectBackward0>)\n",
      "Epoch [1510/5000], Loss: 0.66115677\n",
      "tensor(-0.0655, grad_fn=<SelectBackward0>)\n",
      "Epoch [1520/5000], Loss: 0.65961760\n",
      "tensor(-0.0656, grad_fn=<SelectBackward0>)\n",
      "Epoch [1530/5000], Loss: 0.65808481\n",
      "tensor(-0.0657, grad_fn=<SelectBackward0>)\n",
      "Epoch [1540/5000], Loss: 0.65655822\n",
      "tensor(-0.0658, grad_fn=<SelectBackward0>)\n",
      "Epoch [1550/5000], Loss: 0.65503800\n",
      "tensor(-0.0660, grad_fn=<SelectBackward0>)\n",
      "Epoch [1560/5000], Loss: 0.65352368\n",
      "tensor(-0.0661, grad_fn=<SelectBackward0>)\n",
      "Epoch [1570/5000], Loss: 0.65201586\n",
      "tensor(-0.0662, grad_fn=<SelectBackward0>)\n",
      "Epoch [1580/5000], Loss: 0.65051389\n",
      "tensor(-0.0663, grad_fn=<SelectBackward0>)\n",
      "Epoch [1590/5000], Loss: 0.64901817\n",
      "tensor(-0.0664, grad_fn=<SelectBackward0>)\n",
      "Epoch [1600/5000], Loss: 0.64752847\n",
      "tensor(-0.0665, grad_fn=<SelectBackward0>)\n",
      "Epoch [1610/5000], Loss: 0.64604479\n",
      "tensor(-0.0666, grad_fn=<SelectBackward0>)\n",
      "Epoch [1620/5000], Loss: 0.64456719\n",
      "tensor(-0.0667, grad_fn=<SelectBackward0>)\n",
      "Epoch [1630/5000], Loss: 0.64309549\n",
      "tensor(-0.0668, grad_fn=<SelectBackward0>)\n",
      "Epoch [1640/5000], Loss: 0.64162964\n",
      "tensor(-0.0669, grad_fn=<SelectBackward0>)\n",
      "Epoch [1650/5000], Loss: 0.64016968\n",
      "tensor(-0.0671, grad_fn=<SelectBackward0>)\n",
      "Epoch [1660/5000], Loss: 0.63871557\n",
      "tensor(-0.0672, grad_fn=<SelectBackward0>)\n",
      "Epoch [1670/5000], Loss: 0.63726729\n",
      "tensor(-0.0673, grad_fn=<SelectBackward0>)\n",
      "Epoch [1680/5000], Loss: 0.63582480\n",
      "tensor(-0.0674, grad_fn=<SelectBackward0>)\n",
      "Epoch [1690/5000], Loss: 0.63438803\n",
      "tensor(-0.0675, grad_fn=<SelectBackward0>)\n",
      "Epoch [1700/5000], Loss: 0.63295704\n",
      "tensor(-0.0676, grad_fn=<SelectBackward0>)\n",
      "Epoch [1710/5000], Loss: 0.63153166\n",
      "tensor(-0.0677, grad_fn=<SelectBackward0>)\n",
      "Epoch [1720/5000], Loss: 0.63011193\n",
      "tensor(-0.0678, grad_fn=<SelectBackward0>)\n",
      "Epoch [1730/5000], Loss: 0.62869787\n",
      "tensor(-0.0679, grad_fn=<SelectBackward0>)\n",
      "Epoch [1740/5000], Loss: 0.62728935\n",
      "tensor(-0.0680, grad_fn=<SelectBackward0>)\n",
      "Epoch [1750/5000], Loss: 0.62588644\n",
      "tensor(-0.0680, grad_fn=<SelectBackward0>)\n",
      "Epoch [1760/5000], Loss: 0.62448901\n",
      "tensor(-0.0681, grad_fn=<SelectBackward0>)\n",
      "Epoch [1770/5000], Loss: 0.62309712\n",
      "tensor(-0.0682, grad_fn=<SelectBackward0>)\n",
      "Epoch [1780/5000], Loss: 0.62171060\n",
      "tensor(-0.0683, grad_fn=<SelectBackward0>)\n",
      "Epoch [1790/5000], Loss: 0.62032956\n",
      "tensor(-0.0684, grad_fn=<SelectBackward0>)\n",
      "Epoch [1800/5000], Loss: 0.61895394\n",
      "tensor(-0.0685, grad_fn=<SelectBackward0>)\n",
      "Epoch [1810/5000], Loss: 0.61758375\n",
      "tensor(-0.0686, grad_fn=<SelectBackward0>)\n",
      "Epoch [1820/5000], Loss: 0.61621881\n",
      "tensor(-0.0687, grad_fn=<SelectBackward0>)\n",
      "Epoch [1830/5000], Loss: 0.61485916\n",
      "tensor(-0.0688, grad_fn=<SelectBackward0>)\n",
      "Epoch [1840/5000], Loss: 0.61350489\n",
      "tensor(-0.0689, grad_fn=<SelectBackward0>)\n",
      "Epoch [1850/5000], Loss: 0.61215585\n",
      "tensor(-0.0689, grad_fn=<SelectBackward0>)\n",
      "Epoch [1860/5000], Loss: 0.61081207\n",
      "tensor(-0.0690, grad_fn=<SelectBackward0>)\n",
      "Epoch [1870/5000], Loss: 0.60947347\n",
      "tensor(-0.0691, grad_fn=<SelectBackward0>)\n",
      "Epoch [1880/5000], Loss: 0.60813999\n",
      "tensor(-0.0692, grad_fn=<SelectBackward0>)\n",
      "Epoch [1890/5000], Loss: 0.60681170\n",
      "tensor(-0.0693, grad_fn=<SelectBackward0>)\n",
      "Epoch [1900/5000], Loss: 0.60548854\n",
      "tensor(-0.0694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1910/5000], Loss: 0.60417032\n",
      "tensor(-0.0694, grad_fn=<SelectBackward0>)\n",
      "Epoch [1920/5000], Loss: 0.60285741\n",
      "tensor(-0.0695, grad_fn=<SelectBackward0>)\n",
      "Epoch [1930/5000], Loss: 0.60154939\n",
      "tensor(-0.0696, grad_fn=<SelectBackward0>)\n",
      "Epoch [1940/5000], Loss: 0.60024637\n",
      "tensor(-0.0697, grad_fn=<SelectBackward0>)\n",
      "Epoch [1950/5000], Loss: 0.59894842\n",
      "tensor(-0.0698, grad_fn=<SelectBackward0>)\n",
      "Epoch [1960/5000], Loss: 0.59765536\n",
      "tensor(-0.0698, grad_fn=<SelectBackward0>)\n",
      "Epoch [1970/5000], Loss: 0.59636724\n",
      "tensor(-0.0699, grad_fn=<SelectBackward0>)\n",
      "Epoch [1980/5000], Loss: 0.59508407\n",
      "tensor(-0.0700, grad_fn=<SelectBackward0>)\n",
      "Epoch [1990/5000], Loss: 0.59380567\n",
      "tensor(-0.0701, grad_fn=<SelectBackward0>)\n",
      "Epoch [2000/5000], Loss: 0.59253222\n",
      "tensor(-0.0701, grad_fn=<SelectBackward0>)\n",
      "Epoch [2010/5000], Loss: 0.59126347\n",
      "tensor(-0.0702, grad_fn=<SelectBackward0>)\n",
      "Epoch [2020/5000], Loss: 0.58999962\n",
      "tensor(-0.0703, grad_fn=<SelectBackward0>)\n",
      "Epoch [2030/5000], Loss: 0.58874059\n",
      "tensor(-0.0703, grad_fn=<SelectBackward0>)\n",
      "Epoch [2040/5000], Loss: 0.58748627\n",
      "tensor(-0.0704, grad_fn=<SelectBackward0>)\n",
      "Epoch [2050/5000], Loss: 0.58623672\n",
      "tensor(-0.0705, grad_fn=<SelectBackward0>)\n",
      "Epoch [2060/5000], Loss: 0.58499181\n",
      "tensor(-0.0706, grad_fn=<SelectBackward0>)\n",
      "Epoch [2070/5000], Loss: 0.58375162\n",
      "tensor(-0.0706, grad_fn=<SelectBackward0>)\n",
      "Epoch [2080/5000], Loss: 0.58251607\n",
      "tensor(-0.0707, grad_fn=<SelectBackward0>)\n",
      "Epoch [2090/5000], Loss: 0.58128524\n",
      "tensor(-0.0708, grad_fn=<SelectBackward0>)\n",
      "Epoch [2100/5000], Loss: 0.58005887\n",
      "tensor(-0.0708, grad_fn=<SelectBackward0>)\n",
      "Epoch [2110/5000], Loss: 0.57883722\n",
      "tensor(-0.0709, grad_fn=<SelectBackward0>)\n",
      "Epoch [2120/5000], Loss: 0.57761997\n",
      "tensor(-0.0710, grad_fn=<SelectBackward0>)\n",
      "Epoch [2130/5000], Loss: 0.57640743\n",
      "tensor(-0.0710, grad_fn=<SelectBackward0>)\n",
      "Epoch [2140/5000], Loss: 0.57519931\n",
      "tensor(-0.0711, grad_fn=<SelectBackward0>)\n",
      "Epoch [2150/5000], Loss: 0.57399577\n",
      "tensor(-0.0711, grad_fn=<SelectBackward0>)\n",
      "Epoch [2160/5000], Loss: 0.57279664\n",
      "tensor(-0.0712, grad_fn=<SelectBackward0>)\n",
      "Epoch [2170/5000], Loss: 0.57160199\n",
      "tensor(-0.0713, grad_fn=<SelectBackward0>)\n",
      "Epoch [2180/5000], Loss: 0.57041174\n",
      "tensor(-0.0713, grad_fn=<SelectBackward0>)\n",
      "Epoch [2190/5000], Loss: 0.56922591\n",
      "tensor(-0.0714, grad_fn=<SelectBackward0>)\n",
      "Epoch [2200/5000], Loss: 0.56804448\n",
      "tensor(-0.0714, grad_fn=<SelectBackward0>)\n",
      "Epoch [2210/5000], Loss: 0.56686735\n",
      "tensor(-0.0715, grad_fn=<SelectBackward0>)\n",
      "Epoch [2220/5000], Loss: 0.56569463\n",
      "tensor(-0.0716, grad_fn=<SelectBackward0>)\n",
      "Epoch [2230/5000], Loss: 0.56452620\n",
      "tensor(-0.0716, grad_fn=<SelectBackward0>)\n",
      "Epoch [2240/5000], Loss: 0.56336206\n",
      "tensor(-0.0717, grad_fn=<SelectBackward0>)\n",
      "Epoch [2250/5000], Loss: 0.56220222\n",
      "tensor(-0.0717, grad_fn=<SelectBackward0>)\n",
      "Epoch [2260/5000], Loss: 0.56104654\n",
      "tensor(-0.0718, grad_fn=<SelectBackward0>)\n",
      "Epoch [2270/5000], Loss: 0.55989522\n",
      "tensor(-0.0718, grad_fn=<SelectBackward0>)\n",
      "Epoch [2280/5000], Loss: 0.55874813\n",
      "tensor(-0.0719, grad_fn=<SelectBackward0>)\n",
      "Epoch [2290/5000], Loss: 0.55760515\n",
      "tensor(-0.0720, grad_fn=<SelectBackward0>)\n",
      "Epoch [2300/5000], Loss: 0.55646634\n",
      "tensor(-0.0720, grad_fn=<SelectBackward0>)\n",
      "Epoch [2310/5000], Loss: 0.55533177\n",
      "tensor(-0.0721, grad_fn=<SelectBackward0>)\n",
      "Epoch [2320/5000], Loss: 0.55420119\n",
      "tensor(-0.0721, grad_fn=<SelectBackward0>)\n",
      "Epoch [2330/5000], Loss: 0.55307478\n",
      "tensor(-0.0722, grad_fn=<SelectBackward0>)\n",
      "Epoch [2340/5000], Loss: 0.55195254\n",
      "tensor(-0.0722, grad_fn=<SelectBackward0>)\n",
      "Epoch [2350/5000], Loss: 0.55083436\n",
      "tensor(-0.0723, grad_fn=<SelectBackward0>)\n",
      "Epoch [2360/5000], Loss: 0.54972017\n",
      "tensor(-0.0723, grad_fn=<SelectBackward0>)\n",
      "Epoch [2370/5000], Loss: 0.54861009\n",
      "tensor(-0.0724, grad_fn=<SelectBackward0>)\n",
      "Epoch [2380/5000], Loss: 0.54750389\n",
      "tensor(-0.0724, grad_fn=<SelectBackward0>)\n",
      "Epoch [2390/5000], Loss: 0.54640180\n",
      "tensor(-0.0725, grad_fn=<SelectBackward0>)\n",
      "Epoch [2400/5000], Loss: 0.54530364\n",
      "tensor(-0.0725, grad_fn=<SelectBackward0>)\n",
      "Epoch [2410/5000], Loss: 0.54420954\n",
      "tensor(-0.0725, grad_fn=<SelectBackward0>)\n",
      "Epoch [2420/5000], Loss: 0.54311919\n",
      "tensor(-0.0726, grad_fn=<SelectBackward0>)\n",
      "Epoch [2430/5000], Loss: 0.54203290\n",
      "tensor(-0.0726, grad_fn=<SelectBackward0>)\n",
      "Epoch [2440/5000], Loss: 0.54095048\n",
      "tensor(-0.0727, grad_fn=<SelectBackward0>)\n",
      "Epoch [2450/5000], Loss: 0.53987193\n",
      "tensor(-0.0727, grad_fn=<SelectBackward0>)\n",
      "Epoch [2460/5000], Loss: 0.53879732\n",
      "tensor(-0.0728, grad_fn=<SelectBackward0>)\n",
      "Epoch [2470/5000], Loss: 0.53772640\n",
      "tensor(-0.0728, grad_fn=<SelectBackward0>)\n",
      "Epoch [2480/5000], Loss: 0.53665942\n",
      "tensor(-0.0729, grad_fn=<SelectBackward0>)\n",
      "Epoch [2490/5000], Loss: 0.53559625\n",
      "tensor(-0.0729, grad_fn=<SelectBackward0>)\n",
      "Epoch [2500/5000], Loss: 0.53453684\n",
      "tensor(-0.0729, grad_fn=<SelectBackward0>)\n",
      "Epoch [2510/5000], Loss: 0.53348130\n",
      "tensor(-0.0730, grad_fn=<SelectBackward0>)\n",
      "Epoch [2520/5000], Loss: 0.53242946\n",
      "tensor(-0.0730, grad_fn=<SelectBackward0>)\n",
      "Epoch [2530/5000], Loss: 0.53138125\n",
      "tensor(-0.0731, grad_fn=<SelectBackward0>)\n",
      "Epoch [2540/5000], Loss: 0.53033686\n",
      "tensor(-0.0731, grad_fn=<SelectBackward0>)\n",
      "Epoch [2550/5000], Loss: 0.52929616\n",
      "tensor(-0.0731, grad_fn=<SelectBackward0>)\n",
      "Epoch [2560/5000], Loss: 0.52825910\n",
      "tensor(-0.0732, grad_fn=<SelectBackward0>)\n",
      "Epoch [2570/5000], Loss: 0.52722585\n",
      "tensor(-0.0732, grad_fn=<SelectBackward0>)\n",
      "Epoch [2580/5000], Loss: 0.52619624\n",
      "tensor(-0.0733, grad_fn=<SelectBackward0>)\n",
      "Epoch [2590/5000], Loss: 0.52517015\n",
      "tensor(-0.0733, grad_fn=<SelectBackward0>)\n",
      "Epoch [2600/5000], Loss: 0.52414775\n",
      "tensor(-0.0733, grad_fn=<SelectBackward0>)\n",
      "Epoch [2610/5000], Loss: 0.52312887\n",
      "tensor(-0.0734, grad_fn=<SelectBackward0>)\n",
      "Epoch [2620/5000], Loss: 0.52211362\n",
      "tensor(-0.0734, grad_fn=<SelectBackward0>)\n",
      "Epoch [2630/5000], Loss: 0.52110207\n",
      "tensor(-0.0734, grad_fn=<SelectBackward0>)\n",
      "Epoch [2640/5000], Loss: 0.52009392\n",
      "tensor(-0.0735, grad_fn=<SelectBackward0>)\n",
      "Epoch [2650/5000], Loss: 0.51908934\n",
      "tensor(-0.0735, grad_fn=<SelectBackward0>)\n",
      "Epoch [2660/5000], Loss: 0.51808828\n",
      "tensor(-0.0735, grad_fn=<SelectBackward0>)\n",
      "Epoch [2670/5000], Loss: 0.51709080\n",
      "tensor(-0.0736, grad_fn=<SelectBackward0>)\n",
      "Epoch [2680/5000], Loss: 0.51609671\n",
      "tensor(-0.0736, grad_fn=<SelectBackward0>)\n",
      "Epoch [2690/5000], Loss: 0.51510614\n",
      "tensor(-0.0736, grad_fn=<SelectBackward0>)\n",
      "Epoch [2700/5000], Loss: 0.51411909\n",
      "tensor(-0.0737, grad_fn=<SelectBackward0>)\n",
      "Epoch [2710/5000], Loss: 0.51313543\n",
      "tensor(-0.0737, grad_fn=<SelectBackward0>)\n",
      "Epoch [2720/5000], Loss: 0.51215512\n",
      "tensor(-0.0737, grad_fn=<SelectBackward0>)\n",
      "Epoch [2730/5000], Loss: 0.51117831\n",
      "tensor(-0.0738, grad_fn=<SelectBackward0>)\n",
      "Epoch [2740/5000], Loss: 0.51020491\n",
      "tensor(-0.0738, grad_fn=<SelectBackward0>)\n",
      "Epoch [2750/5000], Loss: 0.50923491\n",
      "tensor(-0.0738, grad_fn=<SelectBackward0>)\n",
      "Epoch [2760/5000], Loss: 0.50826812\n",
      "tensor(-0.0739, grad_fn=<SelectBackward0>)\n",
      "Epoch [2770/5000], Loss: 0.50730479\n",
      "tensor(-0.0739, grad_fn=<SelectBackward0>)\n",
      "Epoch [2780/5000], Loss: 0.50634485\n",
      "tensor(-0.0739, grad_fn=<SelectBackward0>)\n",
      "Epoch [2790/5000], Loss: 0.50538808\n",
      "tensor(-0.0739, grad_fn=<SelectBackward0>)\n",
      "Epoch [2800/5000], Loss: 0.50443482\n",
      "tensor(-0.0740, grad_fn=<SelectBackward0>)\n",
      "Epoch [2810/5000], Loss: 0.50348467\n",
      "tensor(-0.0740, grad_fn=<SelectBackward0>)\n",
      "Epoch [2820/5000], Loss: 0.50253791\n",
      "tensor(-0.0740, grad_fn=<SelectBackward0>)\n",
      "Epoch [2830/5000], Loss: 0.50159436\n",
      "tensor(-0.0740, grad_fn=<SelectBackward0>)\n",
      "Epoch [2840/5000], Loss: 0.50065404\n",
      "tensor(-0.0741, grad_fn=<SelectBackward0>)\n",
      "Epoch [2850/5000], Loss: 0.49971703\n",
      "tensor(-0.0741, grad_fn=<SelectBackward0>)\n",
      "Epoch [2860/5000], Loss: 0.49878320\n",
      "tensor(-0.0741, grad_fn=<SelectBackward0>)\n",
      "Epoch [2870/5000], Loss: 0.49785256\n",
      "tensor(-0.0742, grad_fn=<SelectBackward0>)\n",
      "Epoch [2880/5000], Loss: 0.49692506\n",
      "tensor(-0.0742, grad_fn=<SelectBackward0>)\n",
      "Epoch [2890/5000], Loss: 0.49600083\n",
      "tensor(-0.0742, grad_fn=<SelectBackward0>)\n",
      "Epoch [2900/5000], Loss: 0.49507973\n",
      "tensor(-0.0742, grad_fn=<SelectBackward0>)\n",
      "Epoch [2910/5000], Loss: 0.49416178\n",
      "tensor(-0.0742, grad_fn=<SelectBackward0>)\n",
      "Epoch [2920/5000], Loss: 0.49324691\n",
      "tensor(-0.0743, grad_fn=<SelectBackward0>)\n",
      "Epoch [2930/5000], Loss: 0.49233523\n",
      "tensor(-0.0743, grad_fn=<SelectBackward0>)\n",
      "Epoch [2940/5000], Loss: 0.49142665\n",
      "tensor(-0.0743, grad_fn=<SelectBackward0>)\n",
      "Epoch [2950/5000], Loss: 0.49052113\n",
      "tensor(-0.0743, grad_fn=<SelectBackward0>)\n",
      "Epoch [2960/5000], Loss: 0.48961869\n",
      "tensor(-0.0744, grad_fn=<SelectBackward0>)\n",
      "Epoch [2970/5000], Loss: 0.48871934\n",
      "tensor(-0.0744, grad_fn=<SelectBackward0>)\n",
      "Epoch [2980/5000], Loss: 0.48782307\n",
      "tensor(-0.0744, grad_fn=<SelectBackward0>)\n",
      "Epoch [2990/5000], Loss: 0.48692977\n",
      "tensor(-0.0744, grad_fn=<SelectBackward0>)\n",
      "Epoch [3000/5000], Loss: 0.48603952\n",
      "tensor(-0.0744, grad_fn=<SelectBackward0>)\n",
      "Epoch [3010/5000], Loss: 0.48515236\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [3020/5000], Loss: 0.48426810\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [3030/5000], Loss: 0.48338684\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [3040/5000], Loss: 0.48250863\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [3050/5000], Loss: 0.48163328\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [3060/5000], Loss: 0.48076096\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [3070/5000], Loss: 0.47989151\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [3080/5000], Loss: 0.47902501\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [3090/5000], Loss: 0.47816145\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [3100/5000], Loss: 0.47730079\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [3110/5000], Loss: 0.47644293\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3120/5000], Loss: 0.47558799\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3130/5000], Loss: 0.47473598\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3140/5000], Loss: 0.47388676\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3150/5000], Loss: 0.47304040\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3160/5000], Loss: 0.47219676\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3170/5000], Loss: 0.47135603\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [3180/5000], Loss: 0.47051808\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3190/5000], Loss: 0.46968293\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3200/5000], Loss: 0.46885058\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3210/5000], Loss: 0.46802095\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3220/5000], Loss: 0.46719411\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3230/5000], Loss: 0.46636999\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3240/5000], Loss: 0.46554860\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [3250/5000], Loss: 0.46472991\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3260/5000], Loss: 0.46391392\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3270/5000], Loss: 0.46310067\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3280/5000], Loss: 0.46229005\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3290/5000], Loss: 0.46148217\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3300/5000], Loss: 0.46067688\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3310/5000], Loss: 0.45987427\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3320/5000], Loss: 0.45907432\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [3330/5000], Loss: 0.45827693\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3340/5000], Loss: 0.45748222\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3350/5000], Loss: 0.45669007\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3360/5000], Loss: 0.45590055\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3370/5000], Loss: 0.45511356\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3380/5000], Loss: 0.45432922\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3390/5000], Loss: 0.45354748\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3400/5000], Loss: 0.45276815\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3410/5000], Loss: 0.45199147\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3420/5000], Loss: 0.45121726\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3430/5000], Loss: 0.45044556\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [3440/5000], Loss: 0.44967636\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3450/5000], Loss: 0.44890976\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3460/5000], Loss: 0.44814551\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3470/5000], Loss: 0.44738385\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3480/5000], Loss: 0.44662464\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3490/5000], Loss: 0.44586784\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3500/5000], Loss: 0.44511348\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3510/5000], Loss: 0.44436160\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3520/5000], Loss: 0.44361213\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3530/5000], Loss: 0.44286507\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3540/5000], Loss: 0.44212046\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3550/5000], Loss: 0.44137818\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3560/5000], Loss: 0.44063836\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3570/5000], Loss: 0.43990085\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3580/5000], Loss: 0.43916571\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [3590/5000], Loss: 0.43843296\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3600/5000], Loss: 0.43770257\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3610/5000], Loss: 0.43697450\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3620/5000], Loss: 0.43624875\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3630/5000], Loss: 0.43552533\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3640/5000], Loss: 0.43480423\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3650/5000], Loss: 0.43408540\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3660/5000], Loss: 0.43336892\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3670/5000], Loss: 0.43265462\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3680/5000], Loss: 0.43194264\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3690/5000], Loss: 0.43123290\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3700/5000], Loss: 0.43052542\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3710/5000], Loss: 0.42982024\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3720/5000], Loss: 0.42911729\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3730/5000], Loss: 0.42841652\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3740/5000], Loss: 0.42771801\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3750/5000], Loss: 0.42702162\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3760/5000], Loss: 0.42632759\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3770/5000], Loss: 0.42563561\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3780/5000], Loss: 0.42494586\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3790/5000], Loss: 0.42425826\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3800/5000], Loss: 0.42357284\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3810/5000], Loss: 0.42288956\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3820/5000], Loss: 0.42220843\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3830/5000], Loss: 0.42152950\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3840/5000], Loss: 0.42085263\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3850/5000], Loss: 0.42017791\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3860/5000], Loss: 0.41950524\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3870/5000], Loss: 0.41883478\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3880/5000], Loss: 0.41816631\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3890/5000], Loss: 0.41749999\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3900/5000], Loss: 0.41683576\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3910/5000], Loss: 0.41617355\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3920/5000], Loss: 0.41551343\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3930/5000], Loss: 0.41485530\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3940/5000], Loss: 0.41419929\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3950/5000], Loss: 0.41354537\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3960/5000], Loss: 0.41289338\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3970/5000], Loss: 0.41224343\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3980/5000], Loss: 0.41159546\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [3990/5000], Loss: 0.41094956\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4000/5000], Loss: 0.41030562\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4010/5000], Loss: 0.40966365\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4020/5000], Loss: 0.40902373\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4030/5000], Loss: 0.40838578\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4040/5000], Loss: 0.40774974\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4050/5000], Loss: 0.40711564\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4060/5000], Loss: 0.40648353\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4070/5000], Loss: 0.40585339\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4080/5000], Loss: 0.40522519\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4090/5000], Loss: 0.40459877\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4100/5000], Loss: 0.40397435\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4110/5000], Loss: 0.40335190\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4120/5000], Loss: 0.40273136\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4130/5000], Loss: 0.40211266\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4140/5000], Loss: 0.40149581\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4150/5000], Loss: 0.40088093\n",
      "tensor(-0.0752, grad_fn=<SelectBackward0>)\n",
      "Epoch [4160/5000], Loss: 0.40026787\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4170/5000], Loss: 0.39965668\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4180/5000], Loss: 0.39904740\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4190/5000], Loss: 0.39843991\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4200/5000], Loss: 0.39783433\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4210/5000], Loss: 0.39723054\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4220/5000], Loss: 0.39662853\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4230/5000], Loss: 0.39602840\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4240/5000], Loss: 0.39543006\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4250/5000], Loss: 0.39483359\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4260/5000], Loss: 0.39423889\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4270/5000], Loss: 0.39364594\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4280/5000], Loss: 0.39305481\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4290/5000], Loss: 0.39246547\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4300/5000], Loss: 0.39187789\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4310/5000], Loss: 0.39129204\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4320/5000], Loss: 0.39070800\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4330/5000], Loss: 0.39012578\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4340/5000], Loss: 0.38954520\n",
      "tensor(-0.0751, grad_fn=<SelectBackward0>)\n",
      "Epoch [4350/5000], Loss: 0.38896638\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4360/5000], Loss: 0.38838923\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4370/5000], Loss: 0.38781393\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4380/5000], Loss: 0.38724026\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4390/5000], Loss: 0.38666835\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4400/5000], Loss: 0.38609815\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4410/5000], Loss: 0.38552961\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4420/5000], Loss: 0.38496277\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4430/5000], Loss: 0.38439766\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4440/5000], Loss: 0.38383415\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4450/5000], Loss: 0.38327241\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4460/5000], Loss: 0.38271227\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4470/5000], Loss: 0.38215381\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4480/5000], Loss: 0.38159698\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4490/5000], Loss: 0.38104180\n",
      "tensor(-0.0750, grad_fn=<SelectBackward0>)\n",
      "Epoch [4500/5000], Loss: 0.38048828\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4510/5000], Loss: 0.37993637\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4520/5000], Loss: 0.37938613\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4530/5000], Loss: 0.37883744\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4540/5000], Loss: 0.37829044\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4550/5000], Loss: 0.37774497\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4560/5000], Loss: 0.37720120\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4570/5000], Loss: 0.37665895\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4580/5000], Loss: 0.37611836\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4590/5000], Loss: 0.37557927\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4600/5000], Loss: 0.37504181\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4610/5000], Loss: 0.37450597\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4620/5000], Loss: 0.37397158\n",
      "tensor(-0.0749, grad_fn=<SelectBackward0>)\n",
      "Epoch [4630/5000], Loss: 0.37343884\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4640/5000], Loss: 0.37290764\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4650/5000], Loss: 0.37237793\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4660/5000], Loss: 0.37184986\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4670/5000], Loss: 0.37132320\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4680/5000], Loss: 0.37079820\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4690/5000], Loss: 0.37027466\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4700/5000], Loss: 0.36975265\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4710/5000], Loss: 0.36923215\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4720/5000], Loss: 0.36871308\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4730/5000], Loss: 0.36819565\n",
      "tensor(-0.0748, grad_fn=<SelectBackward0>)\n",
      "Epoch [4740/5000], Loss: 0.36767969\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4750/5000], Loss: 0.36716515\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4760/5000], Loss: 0.36665210\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4770/5000], Loss: 0.36614054\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4780/5000], Loss: 0.36563054\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4790/5000], Loss: 0.36512187\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4800/5000], Loss: 0.36461478\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4810/5000], Loss: 0.36410913\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4820/5000], Loss: 0.36360484\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4830/5000], Loss: 0.36310211\n",
      "tensor(-0.0747, grad_fn=<SelectBackward0>)\n",
      "Epoch [4840/5000], Loss: 0.36260074\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4850/5000], Loss: 0.36210078\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4860/5000], Loss: 0.36160234\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4870/5000], Loss: 0.36110526\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4880/5000], Loss: 0.36060962\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4890/5000], Loss: 0.36011544\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4900/5000], Loss: 0.35962266\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4910/5000], Loss: 0.35913125\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4920/5000], Loss: 0.35864121\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4930/5000], Loss: 0.35815254\n",
      "tensor(-0.0746, grad_fn=<SelectBackward0>)\n",
      "Epoch [4940/5000], Loss: 0.35766530\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [4950/5000], Loss: 0.35717940\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [4960/5000], Loss: 0.35669494\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [4970/5000], Loss: 0.35621184\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [4980/5000], Loss: 0.35573015\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [4990/5000], Loss: 0.35524976\n",
      "tensor(-0.0745, grad_fn=<SelectBackward0>)\n",
      "Epoch [5000/5000], Loss: 0.35477069\n",
      "activation_stack.0.weight: tensor([[-0.0123,  0.0333,  0.0400,  ..., -0.0177, -0.0052, -0.0056]])\n",
      "activation_stack.0.bias: tensor([-0.0186])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAAsTAAALEwEAmpwYAAAqvklEQVR4nO3deXhU5fn/8fedjc2wJqxhJ6CArAFBUFTUAhVxrUvdqq22aqvF2mptrfX3be231n7dF2qpS12KO1oVUUEREQgIyE7YJKyBsEPIdv/+mINNI4Qtk5PJfF7XNRdzlpm5H65kPnnOc85zzN0REZH4lRB2ASIiEi4FgYhInFMQiIjEOQWBiEicUxCIiMQ5BYGISJxTEIhEiZmdYmZLwq5D5FAUBBLzzOxyM8s2s11mtt7M3jOzwcf4nqvM7MwKtp9mZrkHWD/ZzH4I4O5T3L3LYXzWPWb2z2OpV+RYKAgkppnZaOBB4I9AM6AN8DgwKsSyqpSZJYVdg8Q2BYHELDNrANwL3OTur7v7bncvcve33f32YJ9aZvagma0LHg+aWa1gW5qZvWNm28ws38ymmFmCmT1PJFDeDnoZvzzK+v6r12BmvzKztWa208yWmNlQMxsG/Bq4JPisucG+Lc1sfFBXjpn9qMz73GNmr5rZP81sB3CHme0xsyZl9uljZnlmlnw0tUt80V8SEssGArWBNyrY5y5gANALcOAt4DfAb4HbgFwgPdh3AODufqWZnQL80N0/rIxCzawLcDPQz93XmVk7INHdl5vZH4FO7n5FmZe8DMwHWgLHAxPNbLm7fxxsHwVcDFwF1AJOBr4HPBFsvxJ42d2LKqN+qdnUI5BY1gTY7O7FFezzfeBed9/k7nnA74l8SQIUAS2AtkFPYoof2eRbLYPexDcP4GBjEyVEvrC7mlmyu69y9+UH2tHMWgODgF+5e4G7zwGeJvKlv980d3/T3UvdfS/wLHBF8PpE4DLg+SNoi8QxBYHEsi1A2iGOkbcEVpdZXh2sA7gfyAE+MLMVZnbHEX7+OndvWPYBfHagHd09B7gVuAfYZGYvm1nLA+0b1Jfv7jvL1d2qzPKacq95i0jItAfOAra7+4wjbI/EKQWBxLJpwD7gvAr2WQe0LbPcJliHu+9099vcvQNwLjDazIYG+1X6tLzu/qK7Dw7qceB/D/JZ64DGZpZaru61Zd+u3HsXAOOI9AquRL0BOQIKAolZ7r4duBt4zMzOM7O6ZpZsZsPN7M/Bbi8BvzGzdDNLC/b/J4CZnWNmnczMgO1EDt+UBq/bCHSorFrNrIuZnREMVBcAe8t9VjszSwjatQb4HLjPzGqbWQ/guv11V+A54BoioaYgkMOmIJCY5u4PAKOJDADnETlkcjPwZrDL/wDZwDzgK2B2sA4gE/gQ2EWkd/G4u08Ktt1HJEC2mdkvKqHUWsCfgM3ABqApcGew7ZXg3y1mNjt4fhnQjkjv4A3gd4cauHb3qUTCZba7r65oX5GyTDemEak5zOxj4EV3fzrsWiR2KAhEaggz6wdMBFqXG2gWqZAODYnUAGb2LJHDXLcqBORIqUcgIhLn1CMQEYlzMTfFRFpamrdr1y7sMkREYsqsWbM2u3v6gbbFXBC0a9eO7OzssMsQEYkpZnbQU4p1aEhEJM4pCERE4pyCQEQkzikIRETinIJARCTOKQhEROKcgkBEJM7FTRDMXJXP/RMWU1KqKTVERMqKmyCY8/U2Hpu0nN2FFd3eVkQk/sRNEKTWjlxEvbNAQSAiUlbUgsDMxprZJjObf5DtZmYPm1mOmc0zsz7RqgUgtXYyADsLiqL5MSIiMSeaPYJngGEVbB9O5FaBmcD1wBNRrIX01FoArN6yJ5ofIyISc6IWBO7+KZBfwS6jgOc84gugoZm1iFY9vVo3pH7tJCYs2BCtjxARiUlhjhG0InKj8f1yg3XfYmbXm1m2mWXn5eUd1YelJCXw3R4t+Pe89eTt3HdU7yEiUhPFxGCxu49x9yx3z0pPP+B02oflR6d0oLCklDGfLq/E6kREYluYQbAWaF1mOSNYFzUd0o/joj4ZPPP5Kpbn7YrmR4mIxIwwg2A8cFVw9tAAYLu7r4/2h/5y2PHUTkrknvEL0P2aRUSie/roS8A0oIuZ5ZrZdWb2YzP7cbDLu8AKIAf4G3BjtGopKz21Free1Zkpyzbz7lcaOBYRidqtKt39skNsd+CmaH1+Ra4e2JY3vszld+MXMLhTGg3qJodRhohItRATg8WVLSkxgT9d0IOtewq5771FYZcjIhKquAwCgO6tGvDDwe15eeYavlixJexyRERCE7dBAHDrmZ1p07guv379KwqKSsIuR0QkFHEdBHVSEvnD+d1ZsXk3j36cE3Y5IiKhiOsgADglM50L+rTiyU+Ws3jDjrDLERGpcnEfBAC//W5XGtRJ5vZX5lFUUhp2OSIiVUpBADSql8L/nNedr9Zu58nJmn5CROKLgiAw/MQWjOzZkoc/XsbCdTpEJCLxQ0FQxr3ndqNBnRR+8cpcHSISkbihICijUb0U/nh+dxau38Fjk3QWkYjEBwVBOWd3a875vVvx6Mc5zF+7PexyRESiTkFwAL8b2ZXG9SKHiAqLdYhIRGo2BcEBNKybwn0XnMjiDTt58MOlYZcjIhJVCoKDGHpCMy7Jas0TnyxnuuYiEpEaTEFQgbtHdqVt47qMHjeX7XuLwi5HRCQqFAQVqFcrif+7pBcbdhRw91vzwy5HRCQqFASH0LtNI24Zmslbc9bx1pyo3lJZRCQUCoLDcONpHenbthG/eWM+uVv3hF2OiEilUhAchqTEBB68pBcOjP7XXEpKddN7Eak5FASHqXXjutw7qhszVuXzxGRddSwiNYeC4Aic37sVo3q15K8TlzJjZX7Y5YiIVIqoBoGZDTOzJWaWY2Z3HGB7WzP7yMzmmdlkM8uIZj3Hysz4w/kn0rZJPX720pfk7y4MuyQRkWMWtSAws0TgMWA40BW4zMy6ltvtL8Bz7t4DuBe4L1r1VJbjaiXx6OW9yd9TyOhxcyjVeIGIxLho9gj6AznuvsLdC4GXgVHl9ukKfBw8n3SA7dVSt5YN+O13T2DykjzGTFkRdjkiIsckmkHQClhTZjk3WFfWXOCC4Pn5QKqZNSn/RmZ2vZllm1l2Xl5eVIo9UlcMaMuIE5tz/4QlzFqt8QIRiV1hDxb/AhhiZl8CQ4C1QEn5ndx9jLtnuXtWenp6Vdd4QGbGny7sQauGdfjpi1+yVeMFIhKjohkEa4HWZZYzgnXfcPd17n6Bu/cG7grWbYtiTZWqfu1kHr28N3m79vGLV+ZqvEBEYlI0g2AmkGlm7c0sBbgUGF92BzNLM7P9NdwJjI1iPVHRI6Mhv/luVz5avInHdX2BiMSgqAWBuxcDNwMTgEXAOHdfYGb3mtm5wW6nAUvMbCnQDPhDtOqJpqsGtmVUr5Y8MHEpnyytHmMYIiKHy9xj63BGVlaWZ2dnh13Gt+wpLOaCxz9n/fYC3vnpYFo3rht2SSIi3zCzWe6edaBtYQ8W1xh1U5J46sq+uDs3PD+LgqJvjXmLiFRLCoJK1LZJPR68tBcL1+/grjfmE2u9LRGJTwqCSnbG8c24ZWgmr83O5Z/Tvw67HBGRQ1IQRMEtQzM5vUs69769QBebiUi1pyCIgoQE48FLetOqYR1ueH4Wa7ftDbskEZGDUhBESYO6yTx9dRb7ikr50bPZ7CksDrskEZEDUhBEUaemqTx8eW8Wb9jBbeN05bGIVE8Kgig7vUtTfj3iBN6bv4EHP1oWdjkiIt+SFHYB8eC6we1ZsmEnD3+0jMymxzGyZ8uwSxIR+YZ6BFXAzPif87uT1bYRv3hlLvNyt4VdkojINxQEVaRWUiJPXtmXtONq8aPnslm/XWcSiUj1oCCoQmnH1eLpq7PYva+EH/xjJjsLisIuSUREQVDVTmhRn8e+34dlm3Zx04tfUlRSGnZJIhLnFAQhGNI5nT+e351Pl+bx2zc1J5GIhEtnDYXkkn5tWJO/l0cn5dC6cV1uOr1T2CWJSJxSEITotrM7s2brHu6fsISMRnUY1atV2CWJSBxSEITIzPjzRT3YsL2A21+ZR7P6tRnQoUnYZYlInNEYQchqJSUy5sosWjeuw/XPZbN4w46wSxKROKMgqAYa1E3m2Wv7Uzcliav+PoM1+XvCLklE4oiCoJrIaFSX567rz77iUq78+3Q279oXdkkiEicUBNVI52apjL2mHxt2FHD12Bm64ExEqkRUg8DMhpnZEjPLMbM7DrC9jZlNMrMvzWyemY2IZj2xoG/bRjxxRV+WbNjJ9c/NoqCoJOySRKSGi1oQmFki8BgwHOgKXGZmXcvt9htgnLv3Bi4FHo9WPbHk9C5N+cvFPZm2Ygu3vjyHEt3HQESiKJo9gv5AjruvcPdC4GVgVLl9HKgfPG8ArItiPTHlvN6tuPucrry/YAN3vfGVrj4WkaiJ5nUErYA1ZZZzgZPK7XMP8IGZ/RSoB5x5oDcys+uB6wHatGlT6YVWV9cObs/WPYU88nEOtZMT+d3IrphZ2GWJSA0T9mDxZcAz7p4BjACeN7Nv1eTuY9w9y92z0tPTq7zIMI0+qzPXDW7PM5+v4n/fX6KegYhUumj2CNYCrcssZwTryroOGAbg7tPMrDaQBmyKYl0xxcz4zXdPoKCohCc/WU7dlER+NjQz7LJEpAaJZhDMBDLNrD2RALgUuLzcPl8DQ4FnzOwEoDaQF8WaYpKZ8f9GdWdvUQl/nbiU2skJXH9qx7DLEpEaImpB4O7FZnYzMAFIBMa6+wIzuxfIdvfxwG3A38zs50QGjq9xHfs4oIQE488X9mBfcSl/fHcxtZMTuWpgu7DLEpEaIKqTzrn7u8C75dbdXeb5QmBQNGuoSZISE3jwkl4UFpdy91sLqJ2UyPf6tT70C0VEKhD2YLEcoeTEBB69vDendk7nV6/PY9zMNYd+kYhIBRQEMSgyY2lfBndK45evzePlGV+HXZKIxDAFQYyqnZzI367K4rQu6dzx+le8MH112CWJSIxSEMSw2smJPHVlX844vil3vTGf56etCrskEYlBCoIYVyspkSeu6MOZJzTlt28t4NnPV4VdkojEGAVBDVArKZHHv9+Xs7s243fjFzD2s5VhlyQiMURBUEOkJCXw2Pf7MKxbc+59ZyFPTF4edkkiEiMUBDVIcmICj1zem5E9W/K/7y/mz+8v1txEInJIUb2gTKpecnDR2XG1knh88nJ27SvmnpHdSEjQrKUicmAKghooMcH44/ndqV87iac+XcGugmL+fFEPkhLVARSRb1MQ1FBmxh3Djye1dhJ/+WApu/YV88jlvamVlBh2aSJSzehPxBrMzLj5jEzuGdmVDxZu5LpnstlTWBx2WSJSzSgI4sA1g9rzwMU9+Xz5Zr7/9HTydxeGXZKIVCMKgjhxYd8MnriiLwvW7eCiJz5nTf6esEsSkWpCQRBHvtOtOS/88CQ279rHBU98zoJ128MuSUSqAQVBnOnXrjGv/eRkkhOMS576gqk5m8MuSURCpiCIQ5nNUnntxpNp1bAO1/xjBm/NKX8raRGJJwqCONWiQR3G/Xggvds04paX5/C3T1eEXZKIhERBEMca1EnmuWv7M+LE5vzh3UXcM34BJaWakkIk3uiCsjhXOzmRRy7rQ4sGi/j7ZytZk7+Hhy7rzXG19KMhEi8Oq0dgZs8fzjqJTYkJxm/P6cr/O687k5fmcfGT01i3bW/YZYlIFTncQ0Pdyi6YWSLQ91AvMrNhZrbEzHLM7I4DbP8/M5sTPJaa2bbDrEei4MoBbfn71Vmsyd/DeY9N5atcnV4qEg8qDAIzu9PMdgI9zGxH8NgJbALeOsRrE4HHgOFAV+AyM+tadh93/7m793L3XsAjwOtH3xSpDKd1aRo5vTQxge89NY0JCzaEXZKIRFmFQeDu97l7KnC/u9cPHqnu3sTd7zzEe/cHctx9hbsXAi8DoyrY/zLgpSOqXqKiS/NU3rjpZDo3T+XH/5zFmE+X674GIjXY4R4aesfM6gGY2RVm9lcza3uI17QC1pRZzg3WfUvwXu2Bjw+y/Xozyzaz7Ly8vMMsWY5F09Ta/Ov6AYzo3oI/vruYX746j33FJWGXJSJRcLhB8ASwx8x6ArcBy4HnKrGOS4FX3f2A3zTuPsbds9w9Kz09vRI/VioSOaOoNz87oxOvzMrlkqe+YOOOgrDLEpFKdrhBUOyRYwOjgEfd/TEg9RCvWQu0LrOcEaw7kEvRYaFqKSHBGH12F574fh+WbtzJyEc+48uvt4ZdlohUosMNgp1mdidwJfBvM0sAkg/xmplAppm1N7MUIl/248vvZGbHA42AaYdftlS14Se24LWfnExKUgKXPPUFr2SvOfSLRCQmHG4QXALsA6519w1E/rq/v6IXuHsxcDMwAVgEjHP3BWZ2r5mdW2bXS4GXXaOR1d4JLeoz/ubBZLVrxO2vzuP3by+guKQ07LJE5BjZ4X7/mlkzoF+wOMPdN0WtqgpkZWV5dnZ2GB8tgeKSUv7w7iL+MXUVJ3dswqOX96FxvZSwyxKRCpjZLHfPOtC2w72y+HvADOBi4HvAdDO7qPJKlFiSlJjA70Z24/6LepC9aisjH/mMuWu2hV2WiBylwz00dBfQz92vdveriFwj8NvolSWx4OKs1rz6k4GR509O4/kvVut6A5EYdLhBkFDuUNCWI3it1GA9Mhryzk8Hc3KnJvz2zfmMHjeXPYXFYZclIkfgcL/M3zezCWZ2jZldA/wbeDd6ZUksaVQvhbFX92P0WZ15c85azntsKsvzdoVdlogcpkPNNdTJzAa5++3AU0CP4DENGFMF9UmMSEgwfjY0k+eu7U/ezn2MenQq7361PuyyROQwHKpH8CCwA8DdX3f30e4+Gngj2CbyX07JTOffPzuFTk2P48YXZnPv2wspLNYppiLV2aGCoJm7f1V+ZbCuXVQqkpjXsmEdxt0wkGtObsfYqSu58InPWbV5d9hlichBHCoIGlawrU4l1iE1TEpSAvec242nruzL1/l7+O7DU3hrzsFmGBGRMB0qCLLN7EflV5rZD4FZ0SlJapLvdGvOu7ecQteW9bnl5Tnc/orOKhKpbiq8sji4mvgNoJD/fPFnASnA+cF0E1VKVxbHpuKSUh76aBmPTsqhfVo9Hr2sD11b1g+7LJG4cdRXFrv7Rnc/Gfg9sCp4/N7dB4YRAhK7khITuO3sLrxw3UnsKijmvMen8ty0VboATaQaOOy5hqoL9Qhi35Zd+7jtlblMXpLH0OOb8qcLe5CeWivsskRqtGOea0ikMjU5rhZjr+7H3ed0ZUrOZoY9+CkTF24MuyyRuKUgkFAkJBjXDm7Pv386mGb1a/Oj57L51avz2L1PA8kiVU1BIKHKbJbKmzcN4sbTOjJu1hqGPzSFWat1BzSRqqQgkNClJCXwy2HHM+6GgZS6c/GTn/PAB0so0k1vRKqEgkCqjX7tGvPeLadwQZ8MHvk4h/Mfn8riDTvCLkukxlMQSLWSWjuZv1zckyev6MP6bQWMfOQzHvlomXoHIlGkIJBqaVj3FkwcPYRh3VvwwMSlnP/4VBatV+9AJBoUBFJtNa6XwiOX9ebJK/qwYXsB5z76GQ+rdyBS6aIaBGY2zMyWmFmOmd1xkH2+Z2YLzWyBmb0YzXokNg3r3oKJPx/CiBNb8NeJSxn16FQWrlPvQKSyRC0IzCwReAwYDnQFLjOzruX2yQTuBAa5ezfg1mjVI7GtUb0UHrq0N09d2ZdNO/dx7qOf8deJS9lXXBJ2aSIxL5o9gv5AjruvcPdC4GVgVLl9fgQ85u5bAcrdF1nkW77TrTkfjj6VkT1b8vBHyxj+0BSmr9gSdlkiMS2aQdAKWFNmOTdYV1ZnoLOZTTWzL8xsWBTrkRqiYd0U/u+SXjx7bX+KSkq5ZMwX3PHaPLbvKQq7NJGYFPZgcRKQCZwGXAb8zcwalt/JzK43s2wzy87Ly6vaCqXaGtI5nQ9uHcINQzrwyqxchv51MuPnrtOMpiJHKJpBsBZoXWY5I1hXVi4w3t2L3H0lsJRIMPwXdx/j7lnunpWenh61giX21ElJ5M7hJzD+5kG0aliHn730JT94ZiZr8veEXZpIzIhmEMwEMs2svZmlAJcC48vt8yaR3gBmlkbkUNGKKNYkNVS3lg14/cZB/G5kV2auzOfs//uUMZ8u16mmIochakHg7sXAzcAEYBEwzt0XmNm9ZnZusNsEYIuZLQQmAbe7u0b+5KgkJhg/GNSeiaOHMKhTGn98dzEjHprCtOX6kRKpiG5MIzXWhws3cs/bC8jdupeRPVty14gTaN6gdthliYRCN6aRuHRm12Z8OHoItwzNZMKCDQx9YLIOF4kcgIJAarTayYn8/KzOTPz5qQzo0OSbw0WfL98cdmki1YaCQOJC2yb1+Ps1/Xj6qiwKiku4/G/TufnF2azfvjfs0kRClxR2ASJV6cyuzRicmcaTnyzn8cnL+XDRRm44tSM3DOlA3RT9Okh8Uo9A4k7t5ERuPbMzH40ewtATmvHQR8s44y+f8PrsXEpLY+vkCZHKoCCQuNW6cV0eu7wPr/x4IE3r12L0uLmc//hUZq3OD7s0kSqlIJC4169dY968cRAPXNyTDTsKuPCJadz84mxyt+rqZIkPCgIRICHBuLBvBpN+cRo/G5rJh4s2csYDn3D/hMXsLNBkdlKzKQhEyqibksToszrz8W2nMbx7cx6btJwh909m7Gcrde8DqbEUBCIH0LJhHR66tDfjbx7E8c1TufedhQx94BPe/HKtBpSlxlEQiFSgR0ZDXvjhSTx3bX/q107m1n/N4ZxHPuPTpXma7lpqDAWByCGYGad2Tuednw7moUt7saOgiKvGzuCKv0/nq9ztYZcncswUBCKHKSHBGNWrFR/dNoS7z+nKwnU7GPnoZ9z84myW5+0KuzyRo6bZR0WO0o6CIsZ8soKxU1dSUFTCeb1bccvQTNo2qRd2aSLfUtHsowoCkWO0edc+npy8nOe/WE1JqXNR3wx+OjSTVg3rhF2ayDcUBCJVYOOOAh6flMNLM9bgOJf2a8NNp3fSPRCkWlAQiFShtdv28ujHObySvYaEBOOKk9ryk9M6kp5aK+zSJI4pCERC8PWWPTz88TJen51LSlIC3z+pLdef2oFm9dVDkKqnIBAJ0Yq8XTw6KYe35qwj0Yzv9cvgx0M6ktGobtilSRxREIhUA19v2cMTnyzn1VlrcIcL+rTixtM60S5NZxlJ9CkIRKqRddv2MubTFbw042uKSko5t2dLbjq9E5nNUsMuTWqw0G5eb2bDzGyJmeWY2R0H2H6NmeWZ2Zzg8cNo1iNSHbRsWId7zu3GlF+dzo9O6cAHCzdy9oOf8pN/ztKVyhKKqPUIzCwRWAqcBeQCM4HL3H1hmX2uAbLc/ebDfV/1CKSm2bq7kLFTV/LM56vYWVDMyR2bcMOQjpyamYaZhV2e1BBh9Qj6AznuvsLdC4GXgVFR/DyRmNSoXgq3nd2Fz+84g1+POJ4Vebu5euwMhj80hTe+zKWopDTsEqWGi2YQtALWlFnODdaVd6GZzTOzV82sdRTrEanWUmsnc/2pHfn0l6fzl4t7UurOz/81lyF/nsTTU1awa19x2CVKDRX2pHNvA+3cvQcwEXj2QDuZ2fVmlm1m2Xl5eVVaoEhVS0lK4KK+Gbx/y6mMvSaL1o3r8j//XsTJ933E/RMWs2lnQdglSg0TzTGCgcA97v6dYPlOAHe/7yD7JwL57t6govfVGIHEoy+/3sqYT1fw/oINJCckMLJnS34wqB3dW1X46yLyjYrGCJKi+LkzgUwzaw+sBS4FLi9XWAt3Xx8sngssimI9IjGrd5tGPHFFX1Zu3s0/pq7k1Vm5vDY7l/7tG3PtoPac1bUZiQkaWJajE9XrCMxsBPAgkAiMdfc/mNm9QLa7jzez+4gEQDGQD/zE3RdX9J7qEYjA9r1FjJu5hmc+X8XabXvJaFSHa05ux8VZrWlQJzns8qQa0gVlIjVUcUkpHy7ayNipq5ixMp+6KYlc1DeDa05uR4f048IuT6oRBYFIHJi/djv/mLqKt+euo7CklNO6pHPVwLYM6dxUh41EQSAST/J27uOF6at5cfrXbNq5j1YN63D5SW24pF9r0o7TVNjxSkEgEoeKSkqZuHAjz09bzbQVW0hONIZ3b8EVA9rSr10jXbUcZ8I6a0hEQpScmMCIE1sw4sQW5GzaxQvTV/PqrFzGz11Hl2apXDGgDef1bkVqbQ0uxzv1CETiyJ7CYt6eu47nv1jN/LU7qJeSyKjerbi8fxtdk1DD6dCQiPwXd2du7naen7aad+atY19xKV1b1OfS/q0Z1bMVDeqql1DTKAhE5KC27ynirblreXnGGhau30GtpASGd2/OJf3aMKBDY40l1BAKAhE5LPPXbudfM9fw5py17Cwopl2Tulyc1ZqL+2bQVPdajmkKAhE5InsLS3hv/nr+NXMN01fmk5hgnN4lne9ltea0Lk1JSQp7vko5UgoCETlqKzfvZlz2Gl6dlUvezn00qpvMuT1bckGfDHpkNNChoxihIBCRY1ZUUspnyzbz2uxcPli4kcLiUjqm1+OCPhmc37sVLRvWCbtEqYCCQEQq1fa9Rbz71Xpen53LzFVbMYOBHZpwQZ8MhnVvznG1dIlSdaMgEJGoWb1lN298uZbXZ6/l6/w91ElOZFj35pzXuxWDOjYhKVHjCdWBgkBEos7dmbV6K69/uZZ35q5jR0ExTeqlMOLEFozs2ZKsto1I0OR3oVEQiEiVKigqYfKSPN6et46PFm2koKiUlg1qc07PlpzbsyXdWtbXIHMVUxCISGh27Svmw4UbeXvuOj5ZmkdxqdMhrd43odCpqe6bUBUUBCJSLWzbU8h78zfw9tx1TFuxBXc4oUV9RvZswYjuLWiXVi/sEmssBYGIVDubdhTwzrz1jJ+7jjlrtgGRUBjRvTnDT2xOp6ap4RZYwygIRKRay926h/fnb+C9+RuYtXorAJlNj2N49+YMP7EFxzdP1ZjCMVIQiEjM2LC9gAkLNvDe/PXMWJlPqUP7tHoM696cEd1b0L2VBpqPhoJARGLS5l37+GDBRt6bv57Pl2+hpNTJaFSH73Rrzlldm5HVtpGuUzhMoQWBmQ0DHgISgafd/U8H2e9C4FWgn7tX+C2vIBCJT1t3FzJx0Ube+2o9U3O2UFhSSsO6yZzRpSlndW3GKZ3TdUVzBUIJAjNLBJYCZwG5wEzgMndfWG6/VODfQApws4JARA5l175ipizNY+LCjXy8ZBPb9hSRkpjAyZ2acFbXZpx5QjOaadrs/xJWEAwE7nH37wTLdwK4+33l9nsQmAjcDvxCQSAiR6K4pJTs1VuZuHAjExdu5Ov8PQD0zGjAmSc046xuzejSTIPNYd28vhWwpsxyLnBSucL6AK3d/d9mdvvB3sjMrgeuB2jTpk0UShWRWJWUmMCADk0Y0KEJv/nuCSzbtOubUHhg4lIemLiUjEZ1OL1LU04/Pp2BHdKok5IYdtnVSmgH1MwsAfgrcM2h9nX3McAYiPQIoluZiMQqM6Nzs1Q6N0vlptM7sWlHAR8t3sRHizby6qxcnv9iNbWSEhjYsUkkGLo0pU2TumGXHbrQDg2ZWQNgObAreElzIB84t6LDQzo0JCJHo6CohJmr8pm0OI/JSzaxYvNuADqk1/smFPq1b0StpJrZWwhrjCCJyGDxUGAtkcHiy919wUH2n4zGCESkiqzavJvJSzYxaUke01ZsobC4lLopiZzcMY3Tj09nSOd0MhrVnN5CKGME7l5sZjcDE4icPjrW3ReY2b1AtruPj9Zni4gcSru0elyT1p5rBrVnb2EJ01ZsZvKSPD5evIkPF20EIheynZKZxuBOaQzs2ITU2skhVx0duqBMRKQMd2d53m6mLMtjyrLNfLFiC3sKS0hMMHq3bsjgzDROyUyjZ0bDmLqYTVcWi4gcpcLiUmZ/vZXPlm1myrI85q3djjuk1kpiYMcmnJKZximZ6bRtUrdan6KqIBARqSRbdxfy+fItfJaTx6dLN7N2214AMhrVYVDHyCGkgR2bVLsL2hQEIiJR4O6s2rLnvw4j7SwoBiJnIw3sEAmFAR2akHZcrVBrVRCIiFSBklJn4bodTFuxmWnLtzBz1VZ27YsEQ+dmxwXBkMaADo1pWDelSmtTEIiIhKC4pJSv1m5n2ootTFu+hexVW9lbVIIZnNC8fuQwUocm9GvXmAZ1o3tGkoJARKQaKCwuZV7uNqYt38K0FVuYtXor+4pLMYMuzVI5qX1j+rVvTP92jWlayWMMCgIRkWqooKiEOWu2MXNlPjNW5TNr9Vb2FJYA0K5JXfq3b0y/do05qX0TWjeuc0xnJYU16ZyIiFSgdnLiNxPmQeRQ0oJ1O5i5Kp/pK/P5YOFGxmXnAtCsfi1+PeIERvVqVel1KAhERKqJpMQEerZuSM/WDfnhKR0oLXVy8nYxfWU+M1fm0zQ1OqekKghERKqphIT/zKZ65YC20fucqL2ziIjEBAWBiEicUxCIiMQ5BYGISJxTEIiIxDkFgYhInFMQiIjEOQWBiEici7m5hswsD1h9lC9PAzZXYjmxQG2OD2pzfDiWNrd19/QDbYi5IDgWZpZ9sEmXaiq1OT6ozfEhWm3WoSERkTinIBARiXPxFgRjwi4gBGpzfFCb40NU2hxXYwQiIvJt8dYjEBGRchQEIiJxLm6CwMyGmdkSM8sxszvCrudYmNlYM9tkZvPLrGtsZhPNbFnwb6NgvZnZw0G755lZnzKvuTrYf5mZXR1GWw6HmbU2s0lmttDMFpjZLcH6mtzm2mY2w8zmBm3+fbC+vZlND9r2LzNLCdbXCpZzgu3tyrzXncH6JWb2nZCadNjMLNHMvjSzd4LlGt1mM1tlZl+Z2Rwzyw7WVe3PtrvX+AeQCCwHOgApwFyga9h1HUN7TgX6APPLrPszcEfw/A7gf4PnI4D3AAMGANOD9Y2BFcG/jYLnjcJu20Ha2wLoEzxPBZYCXWt4mw04LnieDEwP2jIOuDRY/yTwk+D5jcCTwfNLgX8Fz7sGP++1gPbB70Fi2O07RNtHAy8C7wTLNbrNwCogrdy6Kv3ZjpceQX8gx91XuHsh8DIwKuSajpq7fwrkl1s9Cng2eP4scF6Z9c95xBdAQzNrAXwHmOju+e6+FZgIDIt68UfB3de7++zg+U5gEdCKmt1md/ddwWJy8HDgDODVYH35Nu//v3gVGGpmFqx/2d33uftKIIfI70O1ZGYZwHeBp4Nlo4a3+SCq9Gc7XoKgFbCmzHJusK4maebu64PnG4BmwfODtT0m/0+C7n9vIn8h1+g2B4dI5gCbiPxiLwe2uXtxsEvZ+r9pW7B9O9CEGGsz8CDwS6A0WG5CzW+zAx+Y2Swzuz5YV6U/27p5fQ3k7m5mNe68YDM7DngNuNXdd0T++IuoiW129xKgl5k1BN4Ajg+3ougys3OATe4+y8xOC7mcqjTY3deaWVNgopktLruxKn6246VHsBZoXWY5I1hXk2wMuogE/24K1h+s7TH1f2JmyURC4AV3fz1YXaPbvJ+7bwMmAQOJHArY/wdc2fq/aVuwvQGwhdhq8yDgXDNbReTw7RnAQ9TsNuPua4N/NxEJ/P5U8c92vATBTCAzOPsghcjA0viQa6ps44H9ZwpcDbxVZv1VwdkGA4DtQZdzAnC2mTUKzkg4O1hX7QTHff8OLHL3v5bZVJPbnB70BDCzOsBZRMZGJgEXBbuVb/P+/4uLgI89Moo4Hrg0OMOmPZAJzKiSRhwhd7/T3TPcvR2R39GP3f371OA2m1k9M0vd/5zIz+R8qvpnO+wR86p6EBltX0rkOOtdYddzjG15CVgPFBE5FngdkWOjHwHLgA+BxsG+BjwWtPsrIKvM+1xLZCAtB/hB2O2qoL2DiRxHnQfMCR4janibewBfBm2eD9wdrO9A5EstB3gFqBWsrx0s5wTbO5R5r7uC/4slwPCw23aY7T+N/5w1VGPbHLRtbvBYsP+7qap/tjXFhIhInIuXQ0MiInIQCgIRkTinIBARiXMKAhGROKcgEBGJcwoCiVtmtiv4t52ZXV7J7/3rcsufV+b7i1QmBYEItAOOKAjKXOl6MP8VBO5+8hHWJFJlFAQi8CfglGA++J8Hk73db2YzgznfbwAws9PMbIqZjQcWBuveDCYLW7B/wjAz+xNQJ3i/F4J1+3sfFrz3/GAO+kvKvPdkM3vVzBab2QtWdjIlkSjSpHMikfnef+Hu5wAEX+jb3b2fmdUCpprZB8G+fYDuHpneGOBad88PpoGYaWavufsdZnazu/c6wGddAPQCegJpwWs+Dbb1BroB64CpRObe+ayyGytSnnoEIt92NpH5XOYQme66CZH5agBmlAkBgJ+Z2VzgCyKTfmVSscHAS+5e4u4bgU+AfmXeO9fdS4lMo9GuEtoickjqEYh8mwE/dff/mrQrmBp5d7nlM4GB7r7HzCYTmf/maO0r87wE/X5KFVGPQAR2ErkF5n4TgJ8EU19jZp2DmSHLawBsDULgeCK3DtyvaP/ry5kCXBKMQ6QTue1otZwZU+KH/uIQiczwWRIc4nmGyBz47YDZwYBtHv+5VWBZ7wM/NrNFRGa5/KLMtjHAPDOb7ZGplPd7g8h9BeYSmVH1l+6+IQgSkVBo9lERkTinQ0MiInFOQSAiEucUBCIicU5BICIS5xQEIiJxTkEgIhLnFAQiInHu/wPV7SIXRpe6oAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(1999, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)\n",
    "    \n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "NeuralNetwork_model = NeuralNetwork()\n",
    "print(NeuralNetwork_model)\n",
    "optimizer = custom_optimizer_SGD(NeuralNetwork_model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "NeuralNetwork_model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = NeuralNetwork_model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    cost_history.append(loss.item())\n",
    "    \n",
    "    # Print the loss every 100 epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(outputs[1])\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in NeuralNetwork_model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "        \n",
    "# Plot the cost history\n",
    "plt.plot(cost_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost\")\n",
    "plt.title(\"Cost History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "# train_error_rate = calculate_error_rate(X_train_normalized,  y_train, w.T.detach().numpy(), b.detach().numpy())\n",
    "# print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "# if X_test is not None and y_test is not None:\n",
    "#    test_error_rate = calculate_error_rate(X_test_normalized, y_test, w.T.detach().numpy(), b.detach().numpy())\n",
    "#    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a021a09b",
   "metadata": {},
   "source": [
    "Fedearted Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientUpdate:\n",
    "    def __init__(self, model, criterion, optimizer, train_data_loader):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.train_data_loader = train_data_loader\n",
    "\n",
    "    def update_weights(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in self.train_data_loader:\n",
    "                self.optimizer.zero_grad()\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "        return self.model.state_dict()\n",
    "\n",
    "# Copy\n",
    "def send_client_weights(weights):\n",
    "    #w_all = []\n",
    "    #append(weights)\n",
    "    print(\"ok\")\n",
    "    \n",
    "    \n",
    "def aggregate_weights_on_server(client_weights_list):\n",
    "    print(\"Aggregating client weights on the server...\")\n",
    "    aggregated_weights = {}\n",
    "    # Aggregate the client weights\n",
    "    for key in client_weights_list[0].keys():\n",
    "        aggregated_weights[key] = torch.stack([weights[key] for weights in client_weights_list]).mean(dim=0)\n",
    "    print(\"Client weights aggregated successfully.\")\n",
    "    \n",
    "def federated_learning(model, criterion, optimizer, train_data, num_rounds, batch_size, num_epochs):\n",
    "    num_clients = len(train_data)\n",
    "    global_weights = model.state_dict()\n",
    "\n",
    "    for round in range(num_rounds):\n",
    "        m = min(num_clients * , 1)\n",
    "        selected_clients = torch.randperm(num_clients)[:m]\n",
    "\n",
    "        for client in selected_clients:\n",
    "            client_data = train_data[client]\n",
    "            client_loader = DataLoader(client_data, batch_size=batch_size)\n",
    "            client_update = ClientUpdate(model, criterion, optimizer, client_loader)\n",
    "            client_weights = client_update.update_weights(num_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights)\n",
    "            \n",
    "            # Collect client weights for aggregation on the server\n",
    "            client_weights_list.append(client_weights)\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = aggregate_weights_on_server(client_weights_list)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        model.load_state_dict(aggregated_weights)\n",
    "\n",
    "    return global_weights\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = 0.01\n",
    "num_rounds = 1\n",
    "num_epochs = 5000\n",
    "\n",
    "# Define the model parameters\n",
    "cost_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "model = NeuralNetwork()\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Add the client data to the train_data list\n",
    "# split data here\n",
    "\n",
    "# Run federated learning\n",
    "global_weights = federated_learning(model, criterion, optimizer, train_data, num_rounds=num_rounds, batch_size=1, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e7fb72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
