{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Optimization for Statistical Learning\" Expirement Notebook\n",
    "## Section 0 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 0.1 Data Importing and Preprocessing\n",
    "In this section, we include packages we will use in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15a8e01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages #\n",
    "# !pip install jupyter\n",
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install math\n",
    "# !pip install torch\n",
    "# !pip install xlrd\n",
    "# !pip install pandas\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93b59c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import math\n",
    "import copy\n",
    "import random\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset, random_split\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import Tensor\n",
    "from torch.optim.optimizer import (Optimizer, required, _use_grad_for_differentiable, _default_to_fused_or_foreach,\n",
    "                        _differentiable_doc, _foreach_doc, _maximize_doc)\n",
    "from typing import List, Optional\n",
    "from torch.autograd import Variable\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load npy files to restore results in the past**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#past_results = np.load('./result.npy', encoding = \"latin1\")\n",
    "#print(past_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 0.2 Global Classes, Functions and Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Spliting the DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataloader(dataloader, num_piece):\n",
    "    dataset = list(dataloader)\n",
    "    subset_size = len(dataset) // num_piece\n",
    "    remainder = len(dataset) % num_piece\n",
    "    split_subsets = []\n",
    "    start_idx = 0\n",
    "    for i in range(num_piece):\n",
    "        if i < remainder:\n",
    "            end_idx = start_idx + subset_size + 1\n",
    "        else:\n",
    "            end_idx = start_idx + subset_size\n",
    "        subset = dataset[start_idx:end_idx]\n",
    "        split_subsets.append(subset)\n",
    "        start_idx = end_idx\n",
    "    split_dataloaders = []\n",
    "    for subset in split_subsets:\n",
    "        split_dataloader = torch.utils.data.DataLoader(subset, batch_size=dataloader.batch_size, shuffle=True)\n",
    "        split_dataloaders.append(split_dataloader)\n",
    "    return split_dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Error Rate Analaysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test error rate analysis function\n",
    "def calculate_error_rate(X, y, predict):\n",
    "    num_samples = X.shape[0]\n",
    "    error_count = torch.count_nonzero(torch.round(predict) - y)\n",
    "    error_rate = error_count / num_samples\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Optimizer Class of Vanilia Gradient Descent**\n",
    "\n",
    "The name custom_optimizer_SGD is just for consistency with torch.optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class custom_optimizer_SGD(Optimizer):\n",
    "    def __init__(self, params, lr=required, weight_decay=0 ):\n",
    "        if lr is not required and lr < 0.0:\n",
    "            raise ValueError(f\"Invalid learning rate: {lr}\")\n",
    "        if weight_decay < 0.0:\n",
    "            raise ValueError(f\"Invalid weight_decay value: {weight_decay}\")\n",
    "        defaults = dict(lr=lr, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "                \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "                grad = param.grad.data\n",
    "                weight_decay = group['weight_decay']\n",
    "                lr = group['lr']\n",
    "                param.data.add_(-lr, grad)\n",
    "                if weight_decay != 0:\n",
    "                    param.data.add_(-lr * weight_decay, param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device Class and Federated Learning Algorithms**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, criterion, optimizer, X_train, y_train):\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.num_samples = self.X_train.size()[0]\n",
    "        self.num_features = self.X_train.size()[1]\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "\n",
    "    def update_weights_GDVanilia(self, num_epochs):\n",
    "        self.model.train()\n",
    "        for epoch in range(num_epochs):\n",
    "            # Update weight\n",
    "            outputs = self.model(self.X_train.float())\n",
    "            loss = self.criterion(outputs, self.y_train.float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights_GDStochastic(self, num_epochs, batch_size):\n",
    "        self.model.train()\n",
    "        num_batches = self.num_samples // batch_size\n",
    "        for epoch in range(num_epochs):\n",
    "            # Shuffle the data for each epoch\n",
    "            permutation = torch.randperm(self.num_samples)\n",
    "            X_shuffled = self.X_train[permutation]\n",
    "            y_shuffled = self.y_train[permutation]\n",
    "            for batch in range(num_batches):\n",
    "                # Select the current batch\n",
    "                start = batch * batch_size\n",
    "                end = (batch + 1) * batch_size\n",
    "                X_batch = X_shuffled[start:end]\n",
    "                y_batch = y_shuffled[start:end]\n",
    "                # Update weight\n",
    "                outputs = self.model(X_batch.float())\n",
    "                loss = self.criterion(outputs, y_batch.float())\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, input_shape, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_loader, num_epochs, self.optimizer, self.criterion, input_shape, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, dataset_loader, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], dataset_loader)\n",
    "    return client_device\n",
    "    \n",
    "\n",
    "# Define server wise functions\n",
    "def send_client_weights(server_weights, local_weights):\n",
    "    server_weights.append(local_weights)\n",
    "\n",
    "# Total weight processing functions\n",
    "def Federated_Averaging(client_weights_total):\n",
    "    total_clients = len(client_weights_total)\n",
    "    aggregate_weights = {}\n",
    "\n",
    "    # Initialize aggregate_weights with the first client's weights\n",
    "    for layer_name, layer_weights in client_weights_total[0].items():\n",
    "        aggregate_weights[layer_name] = layer_weights / total_clients\n",
    "\n",
    "    # Aggregate weights from the remaining clients\n",
    "    for client_weights in client_weights_total[1:]:\n",
    "        for layer_name, layer_weights in client_weights.items():\n",
    "            aggregate_weights[layer_name] += layer_weights / total_clients\n",
    "\n",
    "    return aggregate_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Linear Training Model\n",
    "In this section, we will focus on the dataset requires no complicated data processing, and mainly linear models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.0. Data Loading and Preprocessing\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BMI Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb81ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### BMI Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"bmi_train.csv\")\n",
    "dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "dataset = dataset.to_numpy()\n",
    "\n",
    "# Splitting off 80% of data for training, 20% for validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, [0,1,2]]\n",
    "y_train = dataset[:train_split, 3]\n",
    "X_test = dataset[train_split:, [0,1,2]]\n",
    "y_test = dataset[train_split:, 3]\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Loading prediction data\n",
    "prediction_dataset = pd.read_csv(\"bmi_validation.csv\")\n",
    "prediction_dataset.replace({'Gender': {'Female': 0, 'Male': 1}}, inplace=True) #Gender -> boolean\n",
    "X_prediction = prediction_dataset.to_numpy()\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = (X_train - X_train.min(0)) / (X_train.max(0) - X_train.min(0))\n",
    "X_test_normalized = (X_test - X_test.min(0)) / (X_test.max(0) - X_test.min(0))\n",
    "X_prediction_normalized = (X_prediction - X_prediction.min(0)) / (X_prediction.max(0) - X_prediction.min(0))\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "X_prediction_tensor = torch.from_numpy(X_prediction_normalized)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"BMI Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Epsilon Pascal Challenge Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7d3247",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Epsilon Pascal Challenge Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"epsilon_normalized.txt\", sep=' ', header=None, nrows=20000)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-1]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "#print(X_train)\n",
    "#print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train\n",
    "X_test_normalized = X_test\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(\"X_train_tensor size: \", X_train_tensor.size())\n",
    "print(\"y_train_tensor size: \", y_train_tensor.size())\n",
    "print(\"X_test_tensor size: \", X_test_tensor.size())\n",
    "print(\"y_test_tensor size: \", y_test_tensor.size())\n",
    "\n",
    "# Preset parameters\n",
    "dataset_name = \"Epsilon Datset\"\n",
    "learning_rate_preset = 0.001\n",
    "batch_size_preset = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gisette Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### gisette Dataset\n",
    "#2 class, 6000 data points, ~5000 features\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"gisette_scale\", sep=' ', header=None)\n",
    "dataset = dataset.to_numpy()\n",
    "for i in range(1, dataset.shape[1]-1):\n",
    "    dataset[:, i] = [float(value.split(':')[1]) if isinstance(value, str) else value for value in dataset[:, i]]\n",
    "dataset = dataset[:, :-2]\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "for i in range(1, dataset.shape[0]):\n",
    "    if dataset[i - 1, 0] == -1:\n",
    "        dataset[i - 1, 0] = 0\n",
    "\n",
    "#print(dataset)\n",
    "#print(dataset.shape)\n",
    "\n",
    "# Splitting off data for training and validation\n",
    "train_split = int(0.8 * len(dataset))\n",
    "X_train = dataset[:train_split, 1:].astype(np.float32)\n",
    "y_train = dataset[:train_split, 0].astype(np.float32)\n",
    "X_test = dataset[train_split:, 1:].astype(np.float32)\n",
    "y_test = dataset[train_split:, 0].astype(np.float32)\n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "denominator = X_train.max(0) - X_train.min(0)\n",
    "X_train_normalized = (X_train - X_train.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "denominator = X_test.max(0) - X_test.min(0)\n",
    "X_test_normalized = (X_test - X_test.min(0)) / np.where(denominator != 0, denominator, 1)\n",
    "\n",
    "print(\"X_train_normalized: \", X_train_normalized)\n",
    "print(\"X_test_normalized: \", X_test_normalized)\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"Gisette Datset\"\n",
    "learning_rate_preset = 0.005\n",
    "batch_size_preset = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a9bae8",
   "metadata": {},
   "source": [
    "### Section 1.1. Vanilia Gradient Descent and Stochastic Gradient Descent\n",
    "\n",
    "Initially test training dataset with custom algorithms and pytorch package. We will verify our result of our algorithms by comparing the pytorch package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1017f7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Vanilia Gradient Descent Algorithms\n",
    "def gradient_descent(X, y, learning_rate, num_epochs):\n",
    "    num_samples, num_features = X.shape\n",
    "    \n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Calculate predictions\n",
    "        y_pred = np.dot(X, w) + b\n",
    "        \n",
    "        # Calculate the difference between predictions and actual values\n",
    "        error = y_pred - y\n",
    "        \n",
    "        # Calculate the gradient\n",
    "        w_gradient = (1/num_samples) * np.dot(X.T, error)\n",
    "        b_gradient = (1/num_samples) * np.sum(error)\n",
    "        \n",
    "        # Update theta using the learning rate and gradient\n",
    "        w -= learning_rate * w_gradient\n",
    "        b -= learning_rate * b_gradient\n",
    "\n",
    "        # Record the loss\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "    \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_iterations = 100\n",
    "w, b, loss_history, error_history = gradient_descent(X_train_normalized, y_train, learning_rate, num_iterations)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Custom Algorithm for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf7d08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Stochastic Gradient Descent Algorithms\n",
    "def stochastic_gradient_descent(X, y, learning_rate, num_epochs, batch_size):\n",
    "    num_samples, num_features = X.shape\n",
    "    num_batches = num_samples // batch_size\n",
    "\n",
    "    # Initialize weights and bias\n",
    "    w = np.zeros(num_features)\n",
    "    b = 0\n",
    "    \n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data for each epoch\n",
    "        permutation = np.random.permutation(num_samples)\n",
    "        X_shuffled = X[permutation]\n",
    "        y_shuffled = y[permutation]\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            # Select the current batch\n",
    "            start = batch * batch_size\n",
    "            end = (batch + 1) * batch_size\n",
    "            X_batch = X_shuffled[start:end]\n",
    "            y_batch = y_shuffled[start:end]\n",
    "\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X_batch, w) + b\n",
    "\n",
    "            # Calculate the difference between predictions and actual values\n",
    "            error = y_pred - y_batch\n",
    "\n",
    "            # Calculate the gradients\n",
    "            w_gradient = (1 / batch_size) * np.dot(X_batch.T, error)\n",
    "            b_gradient = (1 / batch_size) * np.sum(error)\n",
    "\n",
    "            # Update weights and bias\n",
    "            w -= learning_rate * w_gradient\n",
    "            b -= learning_rate * b_gradient\n",
    "        \n",
    "        # General Output\n",
    "        y_pred = np.dot(X_train_normalized, w) + b\n",
    "        error = y_pred - y_train\n",
    "\n",
    "        # Record the loss\n",
    "        error = y_pred\n",
    "        loss = np.mean(np.square(error))\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "            \n",
    "    return w, b, loss_history, error_history\n",
    "\n",
    "# Train the model using stochastic gradient descent\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "w, b, loss_history, error_history = stochastic_gradient_descent(X_train_normalized, y_train, learning_rate, num_epochs, batch_size)\n",
    "\n",
    "# Print the learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for i, w_i in enumerate(w):\n",
    "    print(f\"w{i} =\", w_i)\n",
    "print(\"b =\", b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "y_pred = np.dot(X_train_normalized, w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_normalized,  y_train, torch.from_numpy(y_pred))\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    y_pred = np.dot(X_test_normalized, w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_normalized,  y_test, torch.from_numpy(y_pred))\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8707afbc",
   "metadata": {},
   "source": [
    "**Pytorch Package for Vanilia Gradient Descent**\n",
    "\n",
    "(It is just a Vanilia Gradient Descent... They call it SGD is confusing...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc595f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of features\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform gradient descent\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pytorch Package for Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters (weights and bias)\n",
    "w = torch.zeros(num_features, dtype=torch.float, requires_grad=True)\n",
    "# w = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "b = torch.zeros(1, dtype=torch.float, requires_grad=True)\n",
    "# b = torch.tensor([1.], requires_grad=True)\n",
    "\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "# Define the optimizer (Vanilla Gradient Descent)\n",
    "optimizer = torch.optim.SGD([w, b], lr=learning_rate, weight_decay=0)\n",
    "\n",
    "# Perform stochastic gradient descent\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = torch.matmul(X_batch.float(), w) + b\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # General Output\n",
    "    outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "\n",
    "# Print learned parameters\n",
    "print('Trained weights:', w)\n",
    "print('Trained bias:', b)\n",
    "\n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = torch.matmul(X_train_tensor.float(), w) + b\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = torch.matmul(X_test_tensor.float(), w) + b\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.2. Linear Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1daa3f70",
   "metadata": {},
   "source": [
    "**Neural Network**\n",
    "\n",
    "This neural network is simply a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom neural network class\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_features=1):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Linear(num_features, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.activation_stack(x)\n",
    "        return torch.squeeze(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16cb24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "num_class = len(torch.unique(y_train_tensor))\n",
    "print(num_samples)\n",
    "print(num_features)\n",
    "print(num_class)\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward propagation to obtain the predicted output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    \n",
    "    # Calculate the loss\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "    \n",
    "    # Backward propagation and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "    \n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear Neural Network with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Neural Network with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs and batch size\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "# Define the number of samples and features\n",
    "num_samples  = X_train_tensor.size()[0]\n",
    "num_features = X_train_tensor.size()[1]\n",
    "\n",
    "# Define the model parameters\n",
    "loss_history = []\n",
    "error_history = []\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "model = NeuralNetwork(num_features)\n",
    "print(model)\n",
    "optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "\n",
    "#for name, param in NeuralNetwork_model.named_parameters():\n",
    "#    print( name )\n",
    "#    values = torch.ones( param.shape )\n",
    "#    param.data = values\n",
    "    \n",
    "# Perform training\n",
    "model.train()\n",
    "num_batches = num_samples // batch_size\n",
    "for epoch in range(num_epochs):\n",
    "    # Shuffle the data for each epoch\n",
    "    permutation = torch.randperm(num_samples)\n",
    "    X_shuffled = X_train_tensor[permutation]\n",
    "    y_shuffled = y_train_tensor[permutation]\n",
    "    for batch in range(num_batches):\n",
    "        # Select the current batch\n",
    "        start = batch * batch_size\n",
    "        end = (batch + 1) * batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        # Forward propagation to obtain the predicted output\n",
    "        outputs = model(X_batch.float())\n",
    "    \n",
    "        # Calculate the loss\n",
    "        loss = criterion(outputs, y_batch.float())\n",
    "    \n",
    "        # Backward propagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # General Output\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "    # Record the loss\n",
    "    loss_history.append(loss.detach().numpy())\n",
    "    \n",
    "    # Record the error rate\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    error_history.append(train_error_rate)\n",
    "\n",
    "    # Print the loss every specific epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}')\n",
    "        \n",
    "# Print learned parameters\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f'{name}: {param.data}')\n",
    "        \n",
    "# Plot the training loss history\n",
    "plt.plot(loss_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate history\n",
    "plt.plot(error_history)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate History\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate train error rate\n",
    "outputs = model(X_train_tensor.float())\n",
    "train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "print(\"Train error rate:\", train_error_rate)\n",
    "    \n",
    "# Calculate test error rate if test data is provided\n",
    "if X_test is not None and y_test is not None:\n",
    "    outputs = model(X_test_tensor.float())\n",
    "    test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "    print(\"Test error rate:\", test_error_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1.3. Fedearted Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.1 Fedearted Learning Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with vanilia gradient descent method\n",
    "def FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDVanilia(local_update_epochs)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(value.numel() for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training function for Federated Learning with Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total):\n",
    "    # Define neural network model, loss criterion and optimizer\n",
    "    model = NeuralNetwork(X_train_tensor.size()[1])\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = custom_optimizer_SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Preprocess the client data\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( X_train_tensor.size(dim=0) / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "\n",
    "    # Establish client devices\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_device = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = NeuralNetwork(X_train_client[client].size()[1])\n",
    "        client_optimizer[client] = custom_optimizer_SGD(client_model[client].parameters(), lr=learning_rate)\n",
    "        client_device[client] = ClientDevice(client_model[client], criterion, client_optimizer[client], X_train_client[client], y_train_client[client])\n",
    "\n",
    "    # Cost History\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Perform training\n",
    "    global_weights = model.state_dict()\n",
    "    #print(f'Initial global weights are: {global_weights}')\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "            client_weights[client] = client_device[client].update_weights_GDStochastic(local_update_epochs, int(batch_size / num_clients))\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(value.numel() for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = Federated_Averaging(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # General Output\n",
    "        outputs = model(X_train_tensor.float())\n",
    "        loss = criterion(outputs, y_train_tensor.float())\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "        error_cost_history.append(train_error_rate)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.8f}, Error: {train_error_rate:.16f}, Culminative Send Cost: {send_cost}')\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "\n",
    "    # Plot send cost history\n",
    "    plt.plot(send_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    print(f'Total send cost: {send_cost}')\n",
    "\n",
    "    # Plot the training loss history\n",
    "    plt.plot(loss_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Training Loss Rate\")\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    plt.plot(error_cost_history)\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Error Rate\")\n",
    "    plt.title(\"Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate train error rate\n",
    "    outputs = model(X_train_tensor.float())\n",
    "    train_error_rate = calculate_error_rate(X_train_tensor,  y_train_tensor, outputs)\n",
    "    print(\"Train error rate:\", train_error_rate)\n",
    "        \n",
    "    # Calculate test error rate if test data is provided\n",
    "    if X_test is not None and y_test is not None:\n",
    "        outputs = model(X_test_tensor.float())\n",
    "        test_error_rate = calculate_error_rate(X_test_tensor,  y_test_tensor, outputs)\n",
    "        print(\"Test error rate:\", test_error_rate)\n",
    "\n",
    "    # Record the history of loss, error and send_cost\n",
    "    loss_cost_history_total.append(loss_cost_history)\n",
    "    error_cost_history_total.append(error_cost_history)\n",
    "    send_cost_history_total.append(send_cost_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Section 1.3.2 Fedearted Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baec815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [1]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_VGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_VGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'Loss_vs_Send_Cost_num_clients_{dataset_name}_VGD_per_clients.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'ErrorRate_vs_Send_Cost_num_clients_{dataset_name}_VGD_per_clients.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with number of clients of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 100\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [1,5,10,15,20]\n",
    "local_update_epochs_list = [2]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the num_clients\n",
    "local_update_epochs = local_update_epochs_list[0]\n",
    "for num_clients in num_clients_list:\n",
    "    print(f'=== The training for num_clients is {num_clients} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'Loss_VS_SendCost_num_clients_{dataset_name}_SGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate between cost history with clients\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_num_clients_{dataset_name}_SGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'Loss_vs_Send_Cost_num_clients_{dataset_name}_SGD_per_clients.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss rate between cost history with num_clients per clients\n",
    "min_cost_list = send_cost_history_total[0]\n",
    "for sublist in send_cost_history_total:\n",
    "    if len(sublist) < len(min_cost_list):\n",
    "        min_cost_list = sublist\n",
    "for i in range(num_clients_list_size):\n",
    "    plt.plot(min_cost_list, error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost per clients\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "plt.legend(num_clients_list)\n",
    "plt.savefig(f'ErrorRate_vs_Send_Cost_num_clients_{dataset_name}_SGD_per_clients.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Vanilia Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Vanilia Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 1000\n",
    "num_clients_list = [5]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDVanilia(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Vanilia Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_VGD.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Experiment with local update epochs of Stochastic Gradient Descent**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for local_update_epochs with Stochastic Gradient Descent\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 500\n",
    "batch_size = batch_size_preset\n",
    "num_clients_list = [2]\n",
    "local_update_epochs_list = [1,2,3,4,5]\n",
    "\n",
    "# Cost History Total\n",
    "num_clients_list_size = len(num_clients_list)\n",
    "local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "loss_cost_history_total = []\n",
    "error_cost_history_total = []\n",
    "send_cost_history_total = []\n",
    "\n",
    "# Compare the local_update_epochs\n",
    "num_clients = num_clients_list[0]\n",
    "for local_update_epochs in local_update_epochs_list:\n",
    "    print(f'=== The training for local_update_epochs is {local_update_epochs} ===')\n",
    "    FedLearnTrainGDStochastic(num_clients, local_update_epochs, loss_cost_history_total, error_cost_history_total, send_cost_history_total)\n",
    "\n",
    "print(f'=== The Experiment Result ===')\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "print(\"Stochastic Gradient Descent\")\n",
    "\n",
    "# Plot the training loss rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], loss_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Training Loss Rate\")\n",
    "plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.savefig(f'Loss_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')\n",
    "plt.show()\n",
    "\n",
    "# Plot the error rate between cost history with local update epochs\n",
    "for i in range(local_update_epochs_list_size):\n",
    "    plt.plot(send_cost_history_total[i], error_cost_history_total[i])\n",
    "plt.xlabel(\"Send Cost\")\n",
    "plt.ylabel(\"Error Rate\")\n",
    "plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "plt.legend(local_update_epochs_list)\n",
    "plt.savefig(f'ErrorRate_VS_SendCost_local_update_epochs_{dataset_name}_SGD.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2 Non-Linear Training Model\n",
    "Here we focus on dataset requires mainly non-linear training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.0 Data Loading and Preprocessing\n",
    "\n",
    "Here we load the data for the expriements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNIST Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MNIST Dataset\n",
    "\n",
    "# Loading training data\n",
    "dataset = pd.read_csv(\"mnist_train.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_train = dataset[:2000, 1:].astype(np.int32)\n",
    "y_train = dataset[:2000, 0].astype(np.int32)\n",
    "\n",
    "# Loading testing data\n",
    "dataset = pd.read_csv(\"mnist_test.csv\")\n",
    "dataset = dataset.to_numpy()\n",
    "X_test = dataset[:1000, 1:].astype(np.int32)\n",
    "y_test = dataset[:1000, 0].astype(np.int32)\n",
    "\n",
    "# Translation of data  \n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1).astype('float32')\n",
    "y_train = y_train.astype('float32') \n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1).astype('float32')\n",
    "y_test = y_test.astype('float32') \n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized)\n",
    "y_test_tensor = torch.from_numpy(y_test)\n",
    "print(X_train_tensor)\n",
    "print(y_train_tensor)\n",
    "print(X_test_tensor)\n",
    "print(y_test_tensor)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"MNIST Dataset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 100\n",
    "CNN_input_shape_preset = (-1, 1, 28, 28)\n",
    "\n",
    "# Number of samples, features, and classes\n",
    "num_samples_preset  = X_train_tensor.size()[0]\n",
    "num_features_preset = X_train_tensor.size()[1]\n",
    "num_classes_preset = len(torch.unique(y_train_tensor))\n",
    "\n",
    "# Translate the tensor to dataset\n",
    "MINST_train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "MINST_test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Translate to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(MINST_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(MINST_test_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train_tensor[i], cmap='gray')\n",
    "    plt.show()\n",
    "    print(y_train_tensor[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CIFAR-10 Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 59.  62.  63.]\n",
      "   [ 43.  46.  45.]\n",
      "   [ 50.  48.  43.]\n",
      "   ...\n",
      "   [158. 132. 108.]\n",
      "   [152. 125. 102.]\n",
      "   [148. 124. 103.]]\n",
      "\n",
      "  [[ 16.  20.  20.]\n",
      "   [  0.   0.   0.]\n",
      "   [ 18.   8.   0.]\n",
      "   ...\n",
      "   [123.  88.  55.]\n",
      "   [119.  83.  50.]\n",
      "   [122.  87.  57.]]\n",
      "\n",
      "  [[ 25.  24.  21.]\n",
      "   [ 16.   7.   0.]\n",
      "   [ 49.  27.   8.]\n",
      "   ...\n",
      "   [118.  84.  50.]\n",
      "   [120.  84.  50.]\n",
      "   [109.  73.  42.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[208. 170.  96.]\n",
      "   [201. 153.  34.]\n",
      "   [198. 161.  26.]\n",
      "   ...\n",
      "   [160. 133.  70.]\n",
      "   [ 56.  31.   7.]\n",
      "   [ 53.  34.  20.]]\n",
      "\n",
      "  [[180. 139.  96.]\n",
      "   [173. 123.  42.]\n",
      "   [186. 144.  30.]\n",
      "   ...\n",
      "   [184. 148.  94.]\n",
      "   [ 97.  62.  34.]\n",
      "   [ 83.  53.  34.]]\n",
      "\n",
      "  [[177. 144. 116.]\n",
      "   [168. 129.  94.]\n",
      "   [179. 142.  87.]\n",
      "   ...\n",
      "   [216. 184. 140.]\n",
      "   [151. 118.  84.]\n",
      "   [123.  92.  72.]]]\n",
      "\n",
      "\n",
      " [[[154. 177. 187.]\n",
      "   [126. 137. 136.]\n",
      "   [105. 104.  95.]\n",
      "   ...\n",
      "   [ 91.  95.  71.]\n",
      "   [ 87.  90.  71.]\n",
      "   [ 79.  81.  70.]]\n",
      "\n",
      "  [[140. 160. 169.]\n",
      "   [145. 153. 154.]\n",
      "   [125. 125. 118.]\n",
      "   ...\n",
      "   [ 96.  99.  78.]\n",
      "   [ 77.  80.  62.]\n",
      "   [ 71.  73.  61.]]\n",
      "\n",
      "  [[140. 155. 164.]\n",
      "   [139. 146. 149.]\n",
      "   [115. 115. 112.]\n",
      "   ...\n",
      "   [ 79.  82.  64.]\n",
      "   [ 68.  70.  55.]\n",
      "   [ 67.  69.  55.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[175. 167. 166.]\n",
      "   [156. 154. 160.]\n",
      "   [154. 160. 170.]\n",
      "   ...\n",
      "   [ 42.  34.  36.]\n",
      "   [ 61.  53.  57.]\n",
      "   [ 93.  83.  91.]]\n",
      "\n",
      "  [[165. 154. 128.]\n",
      "   [156. 152. 130.]\n",
      "   [159. 161. 142.]\n",
      "   ...\n",
      "   [103.  93.  96.]\n",
      "   [123. 114. 120.]\n",
      "   [131. 121. 131.]]\n",
      "\n",
      "  [[163. 148. 120.]\n",
      "   [158. 148. 122.]\n",
      "   [163. 156. 133.]\n",
      "   ...\n",
      "   [143. 133. 139.]\n",
      "   [143. 134. 142.]\n",
      "   [143. 133. 144.]]]\n",
      "\n",
      "\n",
      " [[[255. 255. 255.]\n",
      "   [253. 253. 253.]\n",
      "   [253. 253. 253.]\n",
      "   ...\n",
      "   [253. 253. 253.]\n",
      "   [253. 253. 253.]\n",
      "   [253. 253. 253.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   ...\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]\n",
      "   [255. 255. 255.]]\n",
      "\n",
      "  [[255. 255. 255.]\n",
      "   [254. 254. 254.]\n",
      "   [254. 254. 254.]\n",
      "   ...\n",
      "   [254. 254. 254.]\n",
      "   [254. 254. 254.]\n",
      "   [254. 254. 254.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[113. 120. 112.]\n",
      "   [111. 118. 111.]\n",
      "   [105. 112. 106.]\n",
      "   ...\n",
      "   [ 72.  81.  80.]\n",
      "   [ 72.  80.  79.]\n",
      "   [ 72.  80.  79.]]\n",
      "\n",
      "  [[111. 118. 110.]\n",
      "   [104. 111. 104.]\n",
      "   [ 99. 106.  98.]\n",
      "   ...\n",
      "   [ 68.  75.  73.]\n",
      "   [ 70.  76.  75.]\n",
      "   [ 78.  84.  82.]]\n",
      "\n",
      "  [[106. 113. 105.]\n",
      "   [ 99. 106.  98.]\n",
      "   [ 95. 102.  94.]\n",
      "   ...\n",
      "   [ 78.  85.  83.]\n",
      "   [ 79.  85.  83.]\n",
      "   [ 80.  86.  84.]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 35. 178. 235.]\n",
      "   [ 40. 176. 239.]\n",
      "   [ 42. 176. 241.]\n",
      "   ...\n",
      "   [ 99. 177. 219.]\n",
      "   [ 79. 147. 197.]\n",
      "   [ 89. 148. 189.]]\n",
      "\n",
      "  [[ 57. 182. 234.]\n",
      "   [ 44. 184. 250.]\n",
      "   [ 50. 183. 240.]\n",
      "   ...\n",
      "   [156. 182. 200.]\n",
      "   [141. 177. 206.]\n",
      "   [116. 149. 175.]]\n",
      "\n",
      "  [[ 98. 197. 237.]\n",
      "   [ 64. 189. 252.]\n",
      "   [ 69. 192. 245.]\n",
      "   ...\n",
      "   [188. 195. 206.]\n",
      "   [119. 135. 147.]\n",
      "   [ 61.  79.  90.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 73.  79.  77.]\n",
      "   [ 53.  63.  68.]\n",
      "   [ 54.  68.  80.]\n",
      "   ...\n",
      "   [ 17.  40.  64.]\n",
      "   [ 21.  36.  51.]\n",
      "   [ 33.  48.  49.]]\n",
      "\n",
      "  [[ 61.  68.  75.]\n",
      "   [ 55.  70.  86.]\n",
      "   [ 57.  79. 103.]\n",
      "   ...\n",
      "   [ 24.  48.  72.]\n",
      "   [ 17.  35.  53.]\n",
      "   [  7.  23.  32.]]\n",
      "\n",
      "  [[ 44.  56.  73.]\n",
      "   [ 46.  66.  88.]\n",
      "   [ 49.  77. 105.]\n",
      "   ...\n",
      "   [ 27.  52.  77.]\n",
      "   [ 21.  43.  66.]\n",
      "   [ 12.  31.  50.]]]\n",
      "\n",
      "\n",
      " [[[189. 211. 240.]\n",
      "   [186. 208. 236.]\n",
      "   [185. 207. 235.]\n",
      "   ...\n",
      "   [175. 195. 224.]\n",
      "   [172. 194. 222.]\n",
      "   [169. 194. 220.]]\n",
      "\n",
      "  [[194. 210. 239.]\n",
      "   [191. 207. 236.]\n",
      "   [190. 206. 235.]\n",
      "   ...\n",
      "   [173. 192. 220.]\n",
      "   [171. 191. 218.]\n",
      "   [167. 190. 216.]]\n",
      "\n",
      "  [[208. 219. 244.]\n",
      "   [205. 216. 240.]\n",
      "   [204. 215. 239.]\n",
      "   ...\n",
      "   [175. 191. 217.]\n",
      "   [172. 190. 216.]\n",
      "   [169. 191. 215.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[207. 199. 181.]\n",
      "   [203. 195. 175.]\n",
      "   [203. 196. 173.]\n",
      "   ...\n",
      "   [135. 132. 127.]\n",
      "   [162. 158. 150.]\n",
      "   [168. 163. 151.]]\n",
      "\n",
      "  [[198. 190. 170.]\n",
      "   [189. 181. 159.]\n",
      "   [180. 172. 147.]\n",
      "   ...\n",
      "   [178. 171. 160.]\n",
      "   [175. 169. 156.]\n",
      "   [175. 169. 154.]]\n",
      "\n",
      "  [[198. 189. 173.]\n",
      "   [189. 181. 162.]\n",
      "   [178. 170. 149.]\n",
      "   ...\n",
      "   [195. 184. 169.]\n",
      "   [196. 189. 171.]\n",
      "   [195. 190. 171.]]]\n",
      "\n",
      "\n",
      " [[[229. 229. 239.]\n",
      "   [236. 237. 247.]\n",
      "   [234. 236. 247.]\n",
      "   ...\n",
      "   [217. 219. 233.]\n",
      "   [221. 223. 234.]\n",
      "   [222. 223. 233.]]\n",
      "\n",
      "  [[222. 221. 229.]\n",
      "   [239. 239. 249.]\n",
      "   [233. 234. 246.]\n",
      "   ...\n",
      "   [223. 223. 236.]\n",
      "   [227. 228. 238.]\n",
      "   [210. 211. 220.]]\n",
      "\n",
      "  [[213. 206. 211.]\n",
      "   [234. 232. 239.]\n",
      "   [231. 233. 244.]\n",
      "   ...\n",
      "   [220. 220. 232.]\n",
      "   [220. 219. 232.]\n",
      "   [202. 203. 215.]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[150. 143. 135.]\n",
      "   [140. 135. 127.]\n",
      "   [132. 127. 120.]\n",
      "   ...\n",
      "   [224. 222. 218.]\n",
      "   [230. 228. 225.]\n",
      "   [241. 241. 238.]]\n",
      "\n",
      "  [[137. 132. 126.]\n",
      "   [130. 127. 120.]\n",
      "   [125. 121. 115.]\n",
      "   ...\n",
      "   [181. 180. 178.]\n",
      "   [202. 201. 198.]\n",
      "   [212. 211. 207.]]\n",
      "\n",
      "  [[122. 119. 114.]\n",
      "   [118. 116. 110.]\n",
      "   [120. 116. 111.]\n",
      "   ...\n",
      "   [179. 177. 173.]\n",
      "   [164. 164. 162.]\n",
      "   [163. 163. 161.]]]]\n",
      "[6. 9. 9. ... 9. 1. 1.]\n",
      "tensor([[[[0.2314, 0.2431, 0.2471],\n",
      "          [0.1686, 0.1804, 0.1765],\n",
      "          [0.1961, 0.1882, 0.1686],\n",
      "          ...,\n",
      "          [0.6196, 0.5176, 0.4235],\n",
      "          [0.5961, 0.4902, 0.4000],\n",
      "          [0.5804, 0.4863, 0.4039]],\n",
      "\n",
      "         [[0.0627, 0.0784, 0.0784],\n",
      "          [0.0000, 0.0000, 0.0000],\n",
      "          [0.0706, 0.0314, 0.0000],\n",
      "          ...,\n",
      "          [0.4824, 0.3451, 0.2157],\n",
      "          [0.4667, 0.3255, 0.1961],\n",
      "          [0.4784, 0.3412, 0.2235]],\n",
      "\n",
      "         [[0.0980, 0.0941, 0.0824],\n",
      "          [0.0627, 0.0275, 0.0000],\n",
      "          [0.1922, 0.1059, 0.0314],\n",
      "          ...,\n",
      "          [0.4627, 0.3294, 0.1961],\n",
      "          [0.4706, 0.3294, 0.1961],\n",
      "          [0.4275, 0.2863, 0.1647]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.8157, 0.6667, 0.3765],\n",
      "          [0.7882, 0.6000, 0.1333],\n",
      "          [0.7765, 0.6314, 0.1020],\n",
      "          ...,\n",
      "          [0.6275, 0.5216, 0.2745],\n",
      "          [0.2196, 0.1216, 0.0275],\n",
      "          [0.2078, 0.1333, 0.0784]],\n",
      "\n",
      "         [[0.7059, 0.5451, 0.3765],\n",
      "          [0.6784, 0.4824, 0.1647],\n",
      "          [0.7294, 0.5647, 0.1176],\n",
      "          ...,\n",
      "          [0.7216, 0.5804, 0.3686],\n",
      "          [0.3804, 0.2431, 0.1333],\n",
      "          [0.3255, 0.2078, 0.1333]],\n",
      "\n",
      "         [[0.6941, 0.5647, 0.4549],\n",
      "          [0.6588, 0.5059, 0.3686],\n",
      "          [0.7020, 0.5569, 0.3412],\n",
      "          ...,\n",
      "          [0.8471, 0.7216, 0.5490],\n",
      "          [0.5922, 0.4627, 0.3294],\n",
      "          [0.4824, 0.3608, 0.2824]]],\n",
      "\n",
      "\n",
      "        [[[0.6039, 0.6941, 0.7333],\n",
      "          [0.4941, 0.5373, 0.5333],\n",
      "          [0.4118, 0.4078, 0.3725],\n",
      "          ...,\n",
      "          [0.3569, 0.3725, 0.2784],\n",
      "          [0.3412, 0.3529, 0.2784],\n",
      "          [0.3098, 0.3176, 0.2745]],\n",
      "\n",
      "         [[0.5490, 0.6275, 0.6627],\n",
      "          [0.5686, 0.6000, 0.6039],\n",
      "          [0.4902, 0.4902, 0.4627],\n",
      "          ...,\n",
      "          [0.3765, 0.3882, 0.3059],\n",
      "          [0.3020, 0.3137, 0.2431],\n",
      "          [0.2784, 0.2863, 0.2392]],\n",
      "\n",
      "         [[0.5490, 0.6078, 0.6431],\n",
      "          [0.5451, 0.5725, 0.5843],\n",
      "          [0.4510, 0.4510, 0.4392],\n",
      "          ...,\n",
      "          [0.3098, 0.3216, 0.2510],\n",
      "          [0.2667, 0.2745, 0.2157],\n",
      "          [0.2627, 0.2706, 0.2157]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.6863, 0.6549, 0.6510],\n",
      "          [0.6118, 0.6039, 0.6275],\n",
      "          [0.6039, 0.6275, 0.6667],\n",
      "          ...,\n",
      "          [0.1647, 0.1333, 0.1412],\n",
      "          [0.2392, 0.2078, 0.2235],\n",
      "          [0.3647, 0.3255, 0.3569]],\n",
      "\n",
      "         [[0.6471, 0.6039, 0.5020],\n",
      "          [0.6118, 0.5961, 0.5098],\n",
      "          [0.6235, 0.6314, 0.5569],\n",
      "          ...,\n",
      "          [0.4039, 0.3647, 0.3765],\n",
      "          [0.4824, 0.4471, 0.4706],\n",
      "          [0.5137, 0.4745, 0.5137]],\n",
      "\n",
      "         [[0.6392, 0.5804, 0.4706],\n",
      "          [0.6196, 0.5804, 0.4784],\n",
      "          [0.6392, 0.6118, 0.5216],\n",
      "          ...,\n",
      "          [0.5608, 0.5216, 0.5451],\n",
      "          [0.5608, 0.5255, 0.5569],\n",
      "          [0.5608, 0.5216, 0.5647]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 1.0000, 1.0000],\n",
      "          [0.9922, 0.9922, 0.9922],\n",
      "          [0.9922, 0.9922, 0.9922],\n",
      "          ...,\n",
      "          [0.9922, 0.9922, 0.9922],\n",
      "          [0.9922, 0.9922, 0.9922],\n",
      "          [0.9922, 0.9922, 0.9922]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000],\n",
      "          ...,\n",
      "          [1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000],\n",
      "          [1.0000, 1.0000, 1.0000]],\n",
      "\n",
      "         [[1.0000, 1.0000, 1.0000],\n",
      "          [0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961],\n",
      "          ...,\n",
      "          [0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961],\n",
      "          [0.9961, 0.9961, 0.9961]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.4431, 0.4706, 0.4392],\n",
      "          [0.4353, 0.4627, 0.4353],\n",
      "          [0.4118, 0.4392, 0.4157],\n",
      "          ...,\n",
      "          [0.2824, 0.3176, 0.3137],\n",
      "          [0.2824, 0.3137, 0.3098],\n",
      "          [0.2824, 0.3137, 0.3098]],\n",
      "\n",
      "         [[0.4353, 0.4627, 0.4314],\n",
      "          [0.4078, 0.4353, 0.4078],\n",
      "          [0.3882, 0.4157, 0.3843],\n",
      "          ...,\n",
      "          [0.2667, 0.2941, 0.2863],\n",
      "          [0.2745, 0.2980, 0.2941],\n",
      "          [0.3059, 0.3294, 0.3216]],\n",
      "\n",
      "         [[0.4157, 0.4431, 0.4118],\n",
      "          [0.3882, 0.4157, 0.3843],\n",
      "          [0.3725, 0.4000, 0.3686],\n",
      "          ...,\n",
      "          [0.3059, 0.3333, 0.3255],\n",
      "          [0.3098, 0.3333, 0.3255],\n",
      "          [0.3137, 0.3373, 0.3294]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.6431, 0.6745, 0.7137],\n",
      "          [0.6588, 0.6824, 0.7255],\n",
      "          [0.6667, 0.6902, 0.7333],\n",
      "          ...,\n",
      "          [0.6706, 0.7373, 0.8157],\n",
      "          [0.6431, 0.7098, 0.7961],\n",
      "          [0.6275, 0.6902, 0.7843]],\n",
      "\n",
      "         [[0.6667, 0.6863, 0.7216],\n",
      "          [0.6902, 0.7020, 0.7373],\n",
      "          [0.6980, 0.7098, 0.7451],\n",
      "          ...,\n",
      "          [0.7333, 0.7765, 0.8196],\n",
      "          [0.7059, 0.7490, 0.7961],\n",
      "          [0.6784, 0.7216, 0.7804]],\n",
      "\n",
      "         [[0.6784, 0.6824, 0.7137],\n",
      "          [0.7059, 0.7020, 0.7333],\n",
      "          [0.7294, 0.7255, 0.7529],\n",
      "          ...,\n",
      "          [0.7882, 0.7882, 0.7882],\n",
      "          [0.7725, 0.7725, 0.7765],\n",
      "          [0.7490, 0.7490, 0.7608]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0863, 0.0784, 0.1647],\n",
      "          [0.1098, 0.0980, 0.2000],\n",
      "          [0.1490, 0.1373, 0.2431],\n",
      "          ...,\n",
      "          [0.5922, 0.6078, 0.6549],\n",
      "          [0.6353, 0.6471, 0.6902],\n",
      "          [0.6157, 0.6196, 0.6706]],\n",
      "\n",
      "         [[0.0667, 0.0588, 0.1451],\n",
      "          [0.0863, 0.0745, 0.1765],\n",
      "          [0.1216, 0.1098, 0.2196],\n",
      "          ...,\n",
      "          [0.5725, 0.5922, 0.6431],\n",
      "          [0.5922, 0.6118, 0.6588],\n",
      "          [0.5725, 0.5804, 0.6314]],\n",
      "\n",
      "         [[0.0549, 0.0510, 0.1255],\n",
      "          [0.0824, 0.0706, 0.1725],\n",
      "          [0.1451, 0.1333, 0.2431],\n",
      "          ...,\n",
      "          [0.5569, 0.5843, 0.6353],\n",
      "          [0.5529, 0.5725, 0.6275],\n",
      "          [0.5490, 0.5608, 0.6196]]],\n",
      "\n",
      "\n",
      "        [[[0.8078, 0.6431, 0.5569],\n",
      "          [0.6471, 0.5843, 0.5451],\n",
      "          [0.6667, 0.6275, 0.5765],\n",
      "          ...,\n",
      "          [0.9725, 0.8275, 0.6863],\n",
      "          [0.9686, 0.7882, 0.6549],\n",
      "          [1.0000, 0.8118, 0.6784]],\n",
      "\n",
      "         [[0.7961, 0.6118, 0.5333],\n",
      "          [0.5961, 0.4784, 0.4275],\n",
      "          [0.6980, 0.5843, 0.5098],\n",
      "          ...,\n",
      "          [0.9647, 0.8118, 0.6471],\n",
      "          [0.9686, 0.8118, 0.6667],\n",
      "          [1.0000, 0.8353, 0.6980]],\n",
      "\n",
      "         [[0.8235, 0.6745, 0.5922],\n",
      "          [0.7529, 0.6314, 0.5569],\n",
      "          [0.8000, 0.6706, 0.5686],\n",
      "          ...,\n",
      "          [0.9569, 0.8039, 0.6235],\n",
      "          [0.9608, 0.8235, 0.6706],\n",
      "          [0.9843, 0.8510, 0.7098]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5255, 0.5020, 0.3922],\n",
      "          [0.5451, 0.5216, 0.4157],\n",
      "          [0.5059, 0.4824, 0.3765],\n",
      "          ...,\n",
      "          [0.7412, 0.6353, 0.5529],\n",
      "          [0.8196, 0.6863, 0.6000],\n",
      "          [0.8706, 0.7216, 0.6549]],\n",
      "\n",
      "         [[0.6196, 0.5490, 0.4980],\n",
      "          [0.6196, 0.5529, 0.4980],\n",
      "          [0.6157, 0.5451, 0.4941],\n",
      "          ...,\n",
      "          [0.7059, 0.5922, 0.5529],\n",
      "          [0.7216, 0.5922, 0.5451],\n",
      "          [0.7490, 0.6039, 0.5686]],\n",
      "\n",
      "         [[0.6745, 0.5686, 0.5569],\n",
      "          [0.6627, 0.5608, 0.5529],\n",
      "          [0.6667, 0.5647, 0.5529],\n",
      "          ...,\n",
      "          [0.6784, 0.5569, 0.5569],\n",
      "          [0.6902, 0.5569, 0.5490],\n",
      "          [0.7020, 0.5647, 0.5529]]],\n",
      "\n",
      "\n",
      "        [[[0.4275, 0.6824, 0.8784],\n",
      "          [0.4314, 0.6745, 0.8706],\n",
      "          [0.4392, 0.6784, 0.8745],\n",
      "          ...,\n",
      "          [0.4196, 0.6314, 0.8314],\n",
      "          [0.4588, 0.6745, 0.8667],\n",
      "          [0.4706, 0.6706, 0.8627]],\n",
      "\n",
      "         [[0.4314, 0.6824, 0.8824],\n",
      "          [0.4353, 0.6745, 0.8745],\n",
      "          [0.4431, 0.6824, 0.8824],\n",
      "          ...,\n",
      "          [0.4235, 0.6314, 0.8196],\n",
      "          [0.4667, 0.6745, 0.8588],\n",
      "          [0.4667, 0.6627, 0.8471]],\n",
      "\n",
      "         [[0.4392, 0.6824, 0.8784],\n",
      "          [0.4392, 0.6745, 0.8667],\n",
      "          [0.4471, 0.6745, 0.8706],\n",
      "          ...,\n",
      "          [0.4392, 0.6471, 0.8431],\n",
      "          [0.4471, 0.6667, 0.8510],\n",
      "          [0.4471, 0.6510, 0.8353]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.3255, 0.4039, 0.4588],\n",
      "          [0.3216, 0.3961, 0.4510],\n",
      "          [0.3176, 0.3922, 0.4471],\n",
      "          ...,\n",
      "          [0.4510, 0.5333, 0.6157],\n",
      "          [0.4627, 0.5451, 0.6275],\n",
      "          [0.4706, 0.5529, 0.6431]],\n",
      "\n",
      "         [[0.3294, 0.4039, 0.4588],\n",
      "          [0.3255, 0.4000, 0.4549],\n",
      "          [0.3294, 0.4078, 0.4588],\n",
      "          ...,\n",
      "          [0.4588, 0.5412, 0.6275],\n",
      "          [0.4510, 0.5333, 0.6157],\n",
      "          [0.4667, 0.5490, 0.6235]],\n",
      "\n",
      "         [[0.3373, 0.4157, 0.4706],\n",
      "          [0.3294, 0.4078, 0.4588],\n",
      "          [0.3216, 0.4039, 0.4510],\n",
      "          ...,\n",
      "          [0.4627, 0.5412, 0.6392],\n",
      "          [0.4627, 0.5451, 0.6275],\n",
      "          [0.4627, 0.5529, 0.6196]]]])\n",
      "tensor([6., 9., 9., 4., 1., 1., 2., 7., 8., 3., 4., 7., 7., 2., 9., 9., 9., 3.,\n",
      "        2., 6., 4., 3., 6., 6., 2., 6., 3., 5., 4., 0., 0., 9., 1., 3., 4., 0.,\n",
      "        3., 7., 3., 3., 5., 2., 2., 7., 1., 1., 1., 2., 2., 0., 9., 5., 7., 9.,\n",
      "        2., 2., 5., 2., 4., 3., 1., 1., 8., 2., 1., 1., 4., 9., 7., 8., 5., 9.,\n",
      "        6., 7., 3., 1., 9., 0., 3., 1., 3., 5., 4., 5., 7., 7., 4., 7., 9., 4.,\n",
      "        2., 3., 8., 0., 1., 6., 1., 1., 4., 1., 8., 3., 9., 6., 6., 1., 8., 5.,\n",
      "        2., 9., 9., 8., 1., 7., 7., 0., 0., 6., 9., 1., 2., 2., 9., 2., 6., 6.,\n",
      "        1., 9., 5., 0., 4., 7., 6., 7., 1., 8., 1., 1., 2., 8., 1., 3., 3., 6.,\n",
      "        2., 4., 9., 9., 5., 4., 3., 6., 7., 4., 6., 8., 5., 5., 4., 3., 1., 8.,\n",
      "        4., 7., 6., 0., 9., 5., 1., 3., 8., 2., 7., 5., 3., 4., 1., 5., 7., 0.,\n",
      "        4., 7., 5., 5., 1., 0., 9., 6., 9., 0., 8., 7., 8., 8., 2., 5., 2., 3.,\n",
      "        5., 0., 6., 1., 9., 3., 6., 9., 1., 3., 9., 6., 6., 7., 1., 0., 9., 5.,\n",
      "        8., 5., 2., 9., 0., 8., 8., 0., 6., 9., 1., 1., 6., 3., 7., 6., 6., 0.,\n",
      "        6., 6., 1., 7., 1., 5., 8., 3., 6., 6., 8., 6., 8., 4., 6., 6., 1., 3.,\n",
      "        8., 3., 4., 1., 7., 1., 3., 8., 5., 1., 1., 4., 0., 9., 3., 7., 4., 9.,\n",
      "        9., 2., 4., 9., 9., 1., 0., 5., 9., 0., 8., 2., 1., 2., 0., 5., 6., 3.,\n",
      "        2., 7., 8., 8., 6., 0., 7., 9., 4., 5., 6., 4., 2., 1., 1., 2., 1., 5.,\n",
      "        9., 9., 0., 8., 4., 1., 1., 6., 3., 3., 9., 0., 7., 9., 7., 7., 9., 1.,\n",
      "        5., 1., 6., 6., 8., 7., 1., 3., 0., 3., 3., 2., 4., 5., 7., 5., 9., 0.,\n",
      "        3., 4., 0., 4., 4., 6., 0., 0., 6., 6., 0., 8., 1., 6., 2., 9., 2., 5.,\n",
      "        9., 6., 7., 4., 1., 8., 7., 3., 6., 9., 3., 0., 4., 0., 5., 1., 0., 3.,\n",
      "        4., 8., 5., 4., 7., 2., 3., 9., 7., 6., 7., 1., 4., 7., 0., 1., 7., 3.,\n",
      "        1., 8., 4., 4., 2., 0., 2., 2., 0., 0., 9., 0., 9., 6., 8., 2., 7., 7.,\n",
      "        4., 0., 3., 0., 8., 9., 4., 2., 7., 2., 5., 2., 5., 1., 9., 4., 8., 5.,\n",
      "        1., 7., 4., 4., 0., 6., 9., 0., 7., 8., 8., 9., 9., 3., 3., 4., 0., 4.,\n",
      "        5., 6., 6., 0., 1., 0., 8., 0., 4., 8., 8., 1., 5., 2., 6., 8., 1., 0.,\n",
      "        0., 7., 7., 5., 9., 6., 2., 8., 3., 4., 7., 3., 9., 0., 1., 2., 4., 8.,\n",
      "        1., 8., 6., 4., 4., 5., 7., 1., 3., 9., 8., 0., 1., 7.])\n",
      "tensor([[[[0.6196, 0.4392, 0.1922],\n",
      "          [0.6235, 0.4353, 0.1843],\n",
      "          [0.6471, 0.4549, 0.2000],\n",
      "          ...,\n",
      "          [0.5373, 0.3725, 0.1412],\n",
      "          [0.4941, 0.3569, 0.1412],\n",
      "          [0.4549, 0.3333, 0.1294]],\n",
      "\n",
      "         [[0.5961, 0.4392, 0.2000],\n",
      "          [0.5922, 0.4314, 0.1569],\n",
      "          [0.6235, 0.4471, 0.1765],\n",
      "          ...,\n",
      "          [0.5333, 0.3725, 0.1216],\n",
      "          [0.4902, 0.3569, 0.1255],\n",
      "          [0.4667, 0.3451, 0.1333]],\n",
      "\n",
      "         [[0.5922, 0.4314, 0.1843],\n",
      "          [0.5922, 0.4275, 0.1294],\n",
      "          [0.6196, 0.4353, 0.1412],\n",
      "          ...,\n",
      "          [0.5451, 0.3843, 0.1333],\n",
      "          [0.5098, 0.3725, 0.1333],\n",
      "          [0.4706, 0.3490, 0.1294]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2667, 0.4863, 0.6941],\n",
      "          [0.1647, 0.3922, 0.5804],\n",
      "          [0.1216, 0.3451, 0.5373],\n",
      "          ...,\n",
      "          [0.1490, 0.3804, 0.5725],\n",
      "          [0.0510, 0.2510, 0.4235],\n",
      "          [0.1569, 0.3333, 0.4980]],\n",
      "\n",
      "         [[0.2392, 0.4549, 0.6588],\n",
      "          [0.1922, 0.4000, 0.5804],\n",
      "          [0.1373, 0.3333, 0.5176],\n",
      "          ...,\n",
      "          [0.1020, 0.3216, 0.5098],\n",
      "          [0.1137, 0.3216, 0.4941],\n",
      "          [0.0784, 0.2510, 0.4196]],\n",
      "\n",
      "         [[0.2118, 0.4196, 0.6275],\n",
      "          [0.2196, 0.4118, 0.5843],\n",
      "          [0.1765, 0.3490, 0.5176],\n",
      "          ...,\n",
      "          [0.0941, 0.3020, 0.4863],\n",
      "          [0.1333, 0.3294, 0.5059],\n",
      "          [0.0824, 0.2627, 0.4314]]],\n",
      "\n",
      "\n",
      "        [[[0.9216, 0.9216, 0.9216],\n",
      "          [0.9059, 0.9059, 0.9059],\n",
      "          [0.9098, 0.9098, 0.9098],\n",
      "          ...,\n",
      "          [0.9137, 0.9137, 0.9137],\n",
      "          [0.9137, 0.9137, 0.9137],\n",
      "          [0.9098, 0.9098, 0.9098]],\n",
      "\n",
      "         [[0.9333, 0.9333, 0.9333],\n",
      "          [0.9216, 0.9216, 0.9216],\n",
      "          [0.9216, 0.9216, 0.9216],\n",
      "          ...,\n",
      "          [0.9255, 0.9255, 0.9255],\n",
      "          [0.9255, 0.9255, 0.9255],\n",
      "          [0.9216, 0.9216, 0.9216]],\n",
      "\n",
      "         [[0.9294, 0.9294, 0.9294],\n",
      "          [0.9176, 0.9176, 0.9176],\n",
      "          [0.9176, 0.9176, 0.9176],\n",
      "          ...,\n",
      "          [0.9216, 0.9216, 0.9216],\n",
      "          [0.9216, 0.9216, 0.9216],\n",
      "          [0.9176, 0.9176, 0.9176]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.3412, 0.3882, 0.3490],\n",
      "          [0.1686, 0.2000, 0.1451],\n",
      "          [0.0745, 0.0902, 0.0431],\n",
      "          ...,\n",
      "          [0.6627, 0.7216, 0.7020],\n",
      "          [0.7137, 0.7725, 0.7569],\n",
      "          [0.7373, 0.7922, 0.7882]],\n",
      "\n",
      "         [[0.3216, 0.3765, 0.3216],\n",
      "          [0.1804, 0.2235, 0.1412],\n",
      "          [0.1412, 0.1725, 0.0863],\n",
      "          ...,\n",
      "          [0.6824, 0.7412, 0.7176],\n",
      "          [0.7255, 0.7843, 0.7686],\n",
      "          [0.7333, 0.7922, 0.7843]],\n",
      "\n",
      "         [[0.3333, 0.3961, 0.3255],\n",
      "          [0.2431, 0.2941, 0.1882],\n",
      "          [0.2275, 0.2627, 0.1490],\n",
      "          ...,\n",
      "          [0.6588, 0.7176, 0.6980],\n",
      "          [0.7059, 0.7647, 0.7490],\n",
      "          [0.7294, 0.7843, 0.7804]]],\n",
      "\n",
      "\n",
      "        [[[0.6196, 0.7451, 0.8706],\n",
      "          [0.6196, 0.7333, 0.8549],\n",
      "          [0.5451, 0.6510, 0.7608],\n",
      "          ...,\n",
      "          [0.8941, 0.9059, 0.9176],\n",
      "          [0.9294, 0.9373, 0.9529],\n",
      "          [0.9333, 0.9451, 0.9647]],\n",
      "\n",
      "         [[0.6667, 0.7843, 0.8980],\n",
      "          [0.6745, 0.7804, 0.8863],\n",
      "          [0.5922, 0.6902, 0.7882],\n",
      "          ...,\n",
      "          [0.9098, 0.9098, 0.9255],\n",
      "          [0.9647, 0.9647, 0.9804],\n",
      "          [0.9647, 0.9686, 0.9843]],\n",
      "\n",
      "         [[0.6824, 0.7882, 0.8824],\n",
      "          [0.6902, 0.7843, 0.8706],\n",
      "          [0.6157, 0.7020, 0.7804],\n",
      "          ...,\n",
      "          [0.9020, 0.8980, 0.9098],\n",
      "          [0.9804, 0.9765, 0.9843],\n",
      "          [0.9608, 0.9569, 0.9686]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1216, 0.1569, 0.1765],\n",
      "          [0.1176, 0.1529, 0.1725],\n",
      "          [0.1020, 0.1373, 0.1569],\n",
      "          ...,\n",
      "          [0.1451, 0.1569, 0.1804],\n",
      "          [0.0353, 0.0510, 0.0549],\n",
      "          [0.0157, 0.0275, 0.0196]],\n",
      "\n",
      "         [[0.0902, 0.1333, 0.1529],\n",
      "          [0.1059, 0.1490, 0.1686],\n",
      "          [0.0980, 0.1412, 0.1608],\n",
      "          ...,\n",
      "          [0.0745, 0.0784, 0.0941],\n",
      "          [0.0157, 0.0235, 0.0118],\n",
      "          [0.0196, 0.0275, 0.0118]],\n",
      "\n",
      "         [[0.1098, 0.1608, 0.1843],\n",
      "          [0.1176, 0.1686, 0.1961],\n",
      "          [0.1255, 0.1765, 0.2039],\n",
      "          ...,\n",
      "          [0.0196, 0.0235, 0.0314],\n",
      "          [0.0157, 0.0196, 0.0118],\n",
      "          [0.0275, 0.0314, 0.0275]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0.2824, 0.3686, 0.6863],\n",
      "          [0.2824, 0.3647, 0.6824],\n",
      "          [0.2863, 0.3608, 0.6745],\n",
      "          ...,\n",
      "          [0.2471, 0.3294, 0.6314],\n",
      "          [0.2627, 0.3294, 0.6431],\n",
      "          [0.2667, 0.3333, 0.6549]],\n",
      "\n",
      "         [[0.2902, 0.3765, 0.6902],\n",
      "          [0.2902, 0.3765, 0.6863],\n",
      "          [0.2902, 0.3725, 0.6824],\n",
      "          ...,\n",
      "          [0.3020, 0.3647, 0.6392],\n",
      "          [0.2706, 0.3412, 0.6588],\n",
      "          [0.2706, 0.3412, 0.6627]],\n",
      "\n",
      "         [[0.3020, 0.3922, 0.7059],\n",
      "          [0.3059, 0.3922, 0.7020],\n",
      "          [0.3059, 0.3882, 0.7098],\n",
      "          ...,\n",
      "          [0.3922, 0.4431, 0.6235],\n",
      "          [0.2902, 0.3569, 0.6863],\n",
      "          [0.2824, 0.3569, 0.6824]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.2941, 0.3490, 0.4549],\n",
      "          [0.3098, 0.3647, 0.4667],\n",
      "          [0.3137, 0.3647, 0.4627],\n",
      "          ...,\n",
      "          [0.0745, 0.0706, 0.0902],\n",
      "          [0.1020, 0.0980, 0.1294],\n",
      "          [0.1216, 0.1176, 0.1529]],\n",
      "\n",
      "         [[0.2784, 0.3373, 0.4431],\n",
      "          [0.2902, 0.3529, 0.4510],\n",
      "          [0.3333, 0.3882, 0.4863],\n",
      "          ...,\n",
      "          [0.0941, 0.0902, 0.1176],\n",
      "          [0.1059, 0.1059, 0.1451],\n",
      "          [0.1176, 0.1176, 0.1490]],\n",
      "\n",
      "         [[0.2706, 0.3294, 0.4353],\n",
      "          [0.2980, 0.3608, 0.4627],\n",
      "          [0.3059, 0.3608, 0.4667],\n",
      "          ...,\n",
      "          [0.0863, 0.0863, 0.1137],\n",
      "          [0.1020, 0.1020, 0.1373],\n",
      "          [0.1294, 0.1294, 0.1569]]],\n",
      "\n",
      "\n",
      "        [[[0.4549, 0.4353, 0.3569],\n",
      "          [0.3451, 0.3529, 0.2627],\n",
      "          [0.3569, 0.3569, 0.2902],\n",
      "          ...,\n",
      "          [0.3059, 0.3059, 0.2157],\n",
      "          [0.3216, 0.3255, 0.2314],\n",
      "          [0.3765, 0.3529, 0.2745]],\n",
      "\n",
      "         [[0.4314, 0.4706, 0.3686],\n",
      "          [0.2275, 0.3255, 0.2039],\n",
      "          [0.2314, 0.3216, 0.2275],\n",
      "          ...,\n",
      "          [0.1373, 0.2157, 0.0745],\n",
      "          [0.1255, 0.2078, 0.0549],\n",
      "          [0.1882, 0.2118, 0.1059]],\n",
      "\n",
      "         [[0.3922, 0.4667, 0.3529],\n",
      "          [0.1451, 0.2824, 0.1490],\n",
      "          [0.1529, 0.2667, 0.1529],\n",
      "          ...,\n",
      "          [0.0667, 0.1765, 0.0431],\n",
      "          [0.0627, 0.1647, 0.0275],\n",
      "          [0.1412, 0.1765, 0.0784]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.5255, 0.5725, 0.4000],\n",
      "          [0.4510, 0.5490, 0.2902],\n",
      "          [0.4824, 0.5647, 0.3176],\n",
      "          ...,\n",
      "          [0.4627, 0.5098, 0.3098],\n",
      "          [0.4824, 0.5333, 0.3373],\n",
      "          [0.4118, 0.4078, 0.2824]],\n",
      "\n",
      "         [[0.5216, 0.5804, 0.4196],\n",
      "          [0.4314, 0.5412, 0.2980],\n",
      "          [0.4667, 0.5490, 0.3176],\n",
      "          ...,\n",
      "          [0.4784, 0.5176, 0.3294],\n",
      "          [0.4706, 0.5098, 0.3216],\n",
      "          [0.3843, 0.3804, 0.2510]],\n",
      "\n",
      "         [[0.5608, 0.5765, 0.4549],\n",
      "          [0.4902, 0.5412, 0.3647],\n",
      "          [0.5059, 0.5451, 0.3725],\n",
      "          ...,\n",
      "          [0.5412, 0.5255, 0.3961],\n",
      "          [0.5608, 0.5333, 0.4039],\n",
      "          [0.5098, 0.4667, 0.3686]]],\n",
      "\n",
      "\n",
      "        [[[0.4118, 0.4667, 0.2392],\n",
      "          [0.4039, 0.4667, 0.2275],\n",
      "          [0.4118, 0.4745, 0.2314],\n",
      "          ...,\n",
      "          [0.4353, 0.4980, 0.2588],\n",
      "          [0.4392, 0.5020, 0.2588],\n",
      "          [0.4980, 0.5608, 0.3098]],\n",
      "\n",
      "         [[0.4392, 0.4980, 0.2667],\n",
      "          [0.4667, 0.5294, 0.2863],\n",
      "          [0.4588, 0.5216, 0.2784],\n",
      "          ...,\n",
      "          [0.4314, 0.4902, 0.2549],\n",
      "          [0.4275, 0.4902, 0.2510],\n",
      "          [0.4510, 0.5137, 0.2667]],\n",
      "\n",
      "         [[0.4824, 0.5412, 0.3098],\n",
      "          [0.4902, 0.5529, 0.3098],\n",
      "          [0.4314, 0.4941, 0.2510],\n",
      "          ...,\n",
      "          [0.4353, 0.4941, 0.2667],\n",
      "          [0.4549, 0.5176, 0.2824],\n",
      "          [0.4431, 0.5020, 0.2627]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.4627, 0.5059, 0.2667],\n",
      "          [0.4667, 0.5137, 0.2745],\n",
      "          [0.4784, 0.5216, 0.2902],\n",
      "          ...,\n",
      "          [0.2824, 0.3059, 0.1882],\n",
      "          [0.2941, 0.3137, 0.2039],\n",
      "          [0.2824, 0.3020, 0.1961]],\n",
      "\n",
      "         [[0.5098, 0.5569, 0.2941],\n",
      "          [0.5216, 0.5647, 0.3216],\n",
      "          [0.4706, 0.5137, 0.2863],\n",
      "          ...,\n",
      "          [0.3647, 0.3882, 0.2353],\n",
      "          [0.4157, 0.4392, 0.2941],\n",
      "          [0.4667, 0.4863, 0.3490]],\n",
      "\n",
      "         [[0.4941, 0.5490, 0.2824],\n",
      "          [0.5137, 0.5686, 0.3176],\n",
      "          [0.4863, 0.5373, 0.3059],\n",
      "          ...,\n",
      "          [0.4000, 0.4314, 0.2549],\n",
      "          [0.4471, 0.4745, 0.3059],\n",
      "          [0.3843, 0.4118, 0.2510]]]])\n",
      "tensor([3., 8., 8., 0., 6., 6., 1., 6., 3., 1., 0., 9., 5., 7., 9., 8., 5., 7.,\n",
      "        8., 6., 7., 0., 4., 9., 5., 2., 4., 0., 9., 6., 6., 5., 4., 5., 9., 2.,\n",
      "        4., 1., 9., 5., 4., 6., 5., 6., 0., 9., 3., 9., 7., 6.])\n",
      "torch.Size([500, 32, 32, 3])\n",
      "torch.Size([500])\n",
      "torch.Size([50, 32, 32, 3])\n",
      "torch.Size([50])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict\n",
    "\n",
    "def load_cifar10_data(directory):\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    test_images = []\n",
    "    test_labels = []\n",
    "    \n",
    "    # Load data batches\n",
    "    for i in range(1, 6):\n",
    "        train_batch_file = os.path.join(directory, 'data_batch_{}'.format(i))\n",
    "        train_data_dict = unpickle(train_batch_file)\n",
    "        train_batch_images = train_data_dict[b'data']\n",
    "        train_batch_labels = train_data_dict[b'labels']\n",
    "        \n",
    "        # Reshape the image data\n",
    "        train_batch_images = train_batch_images.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n",
    "        \n",
    "        train_images.append(train_batch_images)\n",
    "        train_labels.extend(train_batch_labels)\n",
    "    \n",
    "    # Load test batch\n",
    "    test_batch_file = os.path.join(directory, 'test_batch')\n",
    "    test_data_dict = unpickle(test_batch_file)\n",
    "    test_batch_images = test_data_dict[b'data']\n",
    "    test_batch_labels = test_data_dict[b'labels']\n",
    "    \n",
    "    # Reshape the test image data\n",
    "    test_batch_images = test_batch_images.reshape((-1, 3, 32, 32)).transpose((0, 2, 3, 1))\n",
    "\n",
    "    test_images.append(test_batch_images)\n",
    "    test_labels.extend(test_batch_labels)\n",
    "    \n",
    "    train_images = np.concatenate(train_images, axis=0)\n",
    "    train_labels = np.array(train_labels)\n",
    "    test_images = np.concatenate(test_images, axis=0)\n",
    "    test_labels = np.array(test_labels)\n",
    "    \n",
    "    return train_images, train_labels, test_images, test_labels\n",
    "\n",
    "# Specify the path to the 'cifar-10-batches-py' directory\n",
    "data_directory = 'cifar-10-batches-py'\n",
    "\n",
    "# Load CIFAR-10 data\n",
    "train_images, train_labels, test_images, test_labels = load_cifar10_data(data_directory)\n",
    "\n",
    "# Input to X and y\n",
    "X_train = train_images.astype(np.int32)\n",
    "y_train = train_labels.astype(np.int32)\n",
    "X_test = test_images.astype(np.int32)\n",
    "y_test = test_labels.astype(np.int32)\n",
    "\n",
    "# Translation of data\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32') \n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32') \n",
    "print(X_train)\n",
    "print(y_train)\n",
    "\n",
    "# Normalize data set\n",
    "X_train_normalized = X_train / 255.0\n",
    "X_test_normalized = X_test / 255.0\n",
    "\n",
    "# Turn data to tensor\n",
    "X_train_tensor = torch.from_numpy(X_train_normalized[:500])\n",
    "y_train_tensor = torch.from_numpy(y_train[:500])\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized[:50])\n",
    "y_test_tensor = torch.from_numpy(y_test[:50])\n",
    "print(X_train_tensor)\n",
    "print(y_train_tensor)\n",
    "print(X_test_tensor)\n",
    "print(y_test_tensor)\n",
    "print(X_train_tensor.size())\n",
    "print(y_train_tensor.size())\n",
    "print(X_test_tensor.size())\n",
    "print(y_test_tensor.size())\n",
    "\n",
    "# Learning Rate and Batch size\n",
    "dataset_name = \"CIFAR-10 Datset\"\n",
    "learning_rate_preset = 0.01\n",
    "batch_size_preset = 100\n",
    "CNN_input_shape_preset = (-1, 3, 32, 32)\n",
    "\n",
    "# Number of samples, features, and classes\n",
    "num_samples_preset  = X_train_tensor.size()[0]\n",
    "num_features_preset = X_train_tensor.size()[1]\n",
    "num_classes_preset = len(torch.unique(y_train_tensor))\n",
    "\n",
    "# Translate the tensor to dataset\n",
    "CIFAR_10_train_dataset = torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor)\n",
    "CIFAR_10_test_dataset = torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Translate to DataLoader\n",
    "train_loader = torch.utils.data.DataLoader(CIFAR_10_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "test_loader = torch.utils.data.DataLoader(CIFAR_10_test_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.imshow(X_train_tensor[i])\n",
    "    plt.show()\n",
    "    print(y_train_tensor[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.1 Non-Linear Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convolutional Neural Network (CNN)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(32 * 7 * 7, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return torch.squeeze(x)\n",
    "    \n",
    "class ConvolutionalNeuralNetwork_CIFAR10(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.activation_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 64 x 16 x 16\n",
    "            nn.BatchNorm2d(64),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 128 x 8 x 8\n",
    "            nn.BatchNorm2d(128),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2), # output: 256 x 4 x 4\n",
    "            nn.BatchNorm2d(256),\n",
    "\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(256*4*4, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation_stack(x)\n",
    "        return torch.squeeze(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Client Device for CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom class for each client so they can update separately\n",
    "class ClientDevice:\n",
    "    def __init__(self, model, optimizer, criterion, train_dataloader, test_dataloader=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.test_dataloader = test_dataloader\n",
    "\n",
    "    def load_global_weights(self, global_weights):\n",
    "        self.model.load_state_dict(global_weights)\n",
    "\n",
    "    def get_local_weights(self):\n",
    "        return self.model.state_dict()\n",
    "    \n",
    "    def update_weights(self, num_epochs, iterate_func):\n",
    "        self.model.train()\n",
    "        loss_history, error_history = iterate_func(self.model, self.train_dataloader, num_epochs, self.optimizer, self.criterion, show_history=False, training=True)\n",
    "        return self.model.state_dict(), loss_history, error_history\n",
    "\n",
    "# Establish client devices\n",
    "def establish_client_devices(num_clients, train_dataloader_list, test_dataloader_list, model_list, optimizer_list, criterion_list):\n",
    "    # Establish client devices\n",
    "    client_device = [None] * num_clients\n",
    "    client_model = [None] * num_clients\n",
    "    client_optimizer = [None] * num_clients\n",
    "    client_criterion = [None] * num_clients\n",
    "    client_weights = [None] * num_clients\n",
    "    for client in range(num_clients):\n",
    "        client_model[client] = model_list[client]\n",
    "        client_optimizer[client] = optimizer_list[client]\n",
    "        client_criterion[client] = criterion_list[client]\n",
    "        client_weights[client] = client_model[client].state_dict()\n",
    "        client_device[client] = ClientDevice(client_model[client], client_optimizer[client], client_criterion[client], train_dataloader_list[client], test_dataloader_list[client])\n",
    "    return client_device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN Training and Testing Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_CNN_model(model, dataset_loader, num_epochs, optimizer, criterion, show_history=True, training=True):\n",
    "    loss_history = []\n",
    "    error_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        single_iteration_count = 0.00\n",
    "        batch_iteration_count = 0.00\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "        for i, (images, labels) in enumerate(dataset_loader):\n",
    "            # Define variables\n",
    "            X_batch = Variable(images.view(CNN_input_shape))\n",
    "            y_batch = Variable(labels)\n",
    "\n",
    "            # Forward propagation to obtain the predicted output\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            # Calculate the loss\n",
    "            loss = criterion(outputs, y_batch.long())\n",
    "            \n",
    "            # Backward propagation and optimization\n",
    "            if training is True:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Record the iteration used\n",
    "            single_iteration_count += 1\n",
    "            batch_iteration_count += len(y_batch)\n",
    "\n",
    "            # Record the loss\n",
    "            loss_count += loss.data.detach().numpy()\n",
    "\n",
    "            # Record the error rate\n",
    "            predicted = torch.max(outputs.data, 1)[1]\n",
    "            error_count += (predicted != y_batch).float().sum()\n",
    "        \n",
    "        # Summarize the loss rate\n",
    "        loss_rate = loss_count / float(single_iteration_count)\n",
    "        loss_history.append(loss_rate)\n",
    "\n",
    "        # Summarize the error rate\n",
    "        train_error_rate = error_count / float(batch_iteration_count)\n",
    "        error_history.append(train_error_rate)\n",
    "\n",
    "        # Print the summarized loss and error every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 10 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_rate.item():.8f}, Average Error: {train_error_rate:.16f}')\n",
    "\n",
    "    return loss_history, error_history\n",
    "\n",
    "def fit_CNN_model(model, train_loader, test_loader, num_epochs, optimizer, criterion, show_history=True):\n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "       print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history = iterate_CNN_model(model, train_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history = iterate_CNN_model(model, test_loader, num_epochs, optimizer, criterion, show_history=show_history, training=True)\n",
    "    \n",
    "    # Experiment Result\n",
    "    print(\"!-- CNN Model Result --!\")\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "            \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train the CNN model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "# Define the learning rate and number of epochs\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 10\n",
    "\n",
    "# Define neural network model, loss criterion and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "if dataset_name == \"CIFAR-10 Datset\":\n",
    "    model = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "else:\n",
    "    model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "print(model)\n",
    "\n",
    "# Perform training\n",
    "start = timeit.default_timer()\n",
    "fit_CNN_model(model, train_loader, None, num_epochs, optimizer, criterion, show_history=True)\n",
    "stop = timeit.default_timer()\n",
    "train_time = stop - start\n",
    "print(\"Time: \", train_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2.2 Federated Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Federated Learning Training with stochastic gradient descent method\n",
    "def iterate_FedLearn_model(model, dataset_loader, num_epochs, optimizer, criterion, num_clients, local_update_epochs, client_device, iterate_func=iterate_CNN_model, aggregate_func=Federated_Averaging, show_history=True, training = True):\n",
    "    # Initialize cost history for recording\n",
    "    loss_cost_history = []\n",
    "    error_cost_history = []\n",
    "    send_cost_history = []\n",
    "    time_history = []\n",
    "    send_cost = 0\n",
    "\n",
    "    # Initialize global weights\n",
    "    client_weights = [None] * num_clients\n",
    "    global_weights = model.state_dict()\n",
    "    \n",
    "    start_time = timeit.default_timer()\n",
    "    # Iteration\n",
    "    for epoch in range(num_epochs):\n",
    "        client_weights_total = []\n",
    "        loss_count = 0.00\n",
    "        error_count = 0.00\n",
    "\n",
    "        # Clients local update\n",
    "        for client in range(num_clients):\n",
    "            # Transmit the global weight to clients\n",
    "            client_device[client].load_global_weights(global_weights)\n",
    "\n",
    "            # Local update\n",
    "            client_weights[client], local_loss_history, local_error_history = client_device[client].update_weights(local_update_epochs, iterate_func)\n",
    "            local_loss = sum(local_loss_history) / len(local_loss_history)\n",
    "            local_error = sum(local_error_history) / len(local_error_history)\n",
    "\n",
    "            # Send client weights to the server\n",
    "            send_client_weights(client_weights_total, client_weights[client])\n",
    "            client_weights_size = sum(value.numel() for value in client_weights[client].values())\n",
    "            send_cost = send_cost + client_weights_size\n",
    "\n",
    "            # Send client loss and error to the server (we ignore the cost involved here)\n",
    "            loss_count += local_loss\n",
    "            error_count += local_error\n",
    "\n",
    "        # Aggregate client weights on the server\n",
    "        aggregated_weights = aggregate_func(client_weights_total)\n",
    "\n",
    "        # Update global weights with aggregated weights\n",
    "        global_weights.update(aggregated_weights)\n",
    "        model.load_state_dict(global_weights)\n",
    "\n",
    "        # Summarize loss and error\n",
    "        loss_average = loss_count / num_clients\n",
    "        train_error_rate_average = error_count / num_clients\n",
    "\n",
    "        # Record the loss\n",
    "        loss_cost_history.append(loss_average.item())\n",
    "\n",
    "        # Record the error rate\n",
    "        error_cost_history.append(train_error_rate_average)\n",
    "\n",
    "        # Record the send cost\n",
    "        send_cost_history.append(send_cost)\n",
    "\n",
    "        # Record the time history\n",
    "        current_time = timeit.default_timer()\n",
    "        time_history.append(current_time - start_time)\n",
    "\n",
    "        # Print the loss every specific epochs\n",
    "        if show_history is True and ((epoch + 1) % 1 == 0 or epoch == 0):\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {loss_average.item():.8f}, Average Error: {train_error_rate_average:.16f}, Culminative Send Cost: {send_cost}')\n",
    "    stop_time = timeit.default_timer()\n",
    "    used_time = stop_time - start_time\n",
    "    print(\"Time for all iteration: \", used_time)\n",
    "    return loss_cost_history, error_cost_history, send_cost_history, time_history\n",
    "\n",
    "def fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True):\n",
    "    # Preprocess the client data\n",
    "    #client_dataloader_list = split_dataloader(train_loader, num_clients)\n",
    "\n",
    "    # This is the alternative for preprocessing, will be changed in the future\n",
    "\n",
    "    ### Alternative ###\n",
    "    client_dataloader_list = [] * num_clients\n",
    "    X_train_client = [None] * num_clients\n",
    "    y_train_client = [None] * num_clients\n",
    "    client_row = math.floor( num_samples_preset / num_clients )\n",
    "    for client in range(num_clients):\n",
    "        X_train_client[client] = X_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        y_train_client[client] = y_train_tensor[(client)*client_row : (client+1)*client_row]\n",
    "        \n",
    "        # Translate the tensor to dataset\n",
    "        client_train_dataset = torch.utils.data.TensorDataset(X_train_client[client], y_train_client[client])\n",
    "\n",
    "        # Translate to DataLoader\n",
    "        client_loader = torch.utils.data.DataLoader(client_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "        client_dataloader_list.append(client_loader)\n",
    "    ### Alternative End ###\n",
    "\n",
    "    ### MINST Split ###\n",
    "    #client_dataloader_list = [] * num_clients\n",
    "    #X_train_client = [[]] * num_clients\n",
    "    #y_train_client = [[]] * num_clients\n",
    "    #client_row = math.floor( num_samples_preset / num_clients )\n",
    "    #for client in range(num_clients):\n",
    "    #    client_labels = torch.where(y_train_tensor == client)[0]\n",
    "    #    X_train_client[client] = X_train_tensor[client_labels]\n",
    "    #    y_train_client[client] = y_train_tensor[client_labels]\n",
    "\n",
    "        # Translate the tensor to dataset\n",
    "    #    client_train_dataset = torch.utils.data.TensorDataset(X_train_client[client], y_train_client[client])\n",
    "\n",
    "        # Translate to DataLoader\n",
    "    #    client_loader = torch.utils.data.DataLoader(client_train_dataset, batch_size = batch_size_preset, shuffle = True)\n",
    "    #    client_dataloader_list.append(client_loader)\n",
    "\n",
    "        # Size\n",
    "        #print(X_train_client[client])\n",
    "        #print(y_train_client[client])\n",
    "        #print(X_train_client[client].size())\n",
    "        #print(y_train_client[client].size())\n",
    "    ### MINST Split End ###\n",
    "\n",
    "    # Define the client model, criterion, optimizer, its dataset\n",
    "    client_model_list = [None] * num_clients\n",
    "    client_optimizer_list = [None] * num_clients\n",
    "    client_criterion_list = [None] * num_clients\n",
    "    client_setup_func(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list)\n",
    "    \n",
    "    # Establish client devices\n",
    "    client_device = establish_client_devices(num_clients=num_clients, \n",
    "                                             train_dataloader_list=client_dataloader_list, \n",
    "                                             test_dataloader_list= [None] * num_clients,\n",
    "                                             model_list=client_model_list, \n",
    "                                             optimizer_list=client_optimizer_list,\n",
    "                                             criterion_list=client_criterion_list)\n",
    "\n",
    "    # Perform iteration\n",
    "    train_loss_history = []\n",
    "    train_error_history = []\n",
    "    train_send_cost_history = []\n",
    "    train_time_history = []\n",
    "    test_loss_history = []\n",
    "    test_error_history = []\n",
    "    test_send_cost_history = []\n",
    "    test_time_history = []\n",
    "\n",
    "    # Variance Anaylsis\n",
    "    train_loss_var_history = [None] * num_clients\n",
    "    train_error_var_history = [None] * num_clients\n",
    "    test_loss_var_history = [None] * num_clients\n",
    "    test_error_var_history = [None] * num_clients\n",
    "    train_loss_var_history_uphalf = [None] * num_clients\n",
    "    train_error_var_history_uphalf = [None] * num_clients\n",
    "    test_loss_var_history_uphalf = [None] * num_clients\n",
    "    test_error_var_history_uphalf = [None] * num_clients\n",
    "    \n",
    "    # Model becomes \"Train Mode\"\n",
    "    model.train()\n",
    "    if test_loader is not None:\n",
    "        print(\"!-- Training Session --!\")\n",
    "    train_loss_history, train_error_history, train_send_cost_history, train_time_history = iterate_FedLearn_model(model=model, dataset_loader=train_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = True)\n",
    "\n",
    "    if test_loader is not None:\n",
    "        # Model becomes \"Eval Mode\"\n",
    "        model.eval()\n",
    "        print(\"!-- Testing Session --!\")\n",
    "        test_loss_history, test_error_history, test_send_cost_history, test_time_history = iterate_FedLearn_model(model=model, dataset_loader=test_loader, \n",
    "                                                                                         num_epochs=num_epochs, optimizer=optimizer, criterion=criterion, \n",
    "                                                                                         num_clients=num_clients, local_update_epochs=local_update_epochs, client_device=client_device,\n",
    "                                                                                         iterate_func=iterate_func, aggregate_func=aggregate_func,\n",
    "                                                                                         show_history=show_history, training = False)\n",
    "\n",
    "    # Print learned parameters\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:\n",
    "            print(f'{name}: {param.data}')\n",
    "    \n",
    "    # Plot send cost history\n",
    "    if test_loader is not None:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        print(f'Total send cost in testing: {sum(test_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history, label='Training Send Cost')\n",
    "        plt.plot(test_send_cost_history, label='Testing Send Cost')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        print(f'Total send cost in training: {sum(train_send_cost_history)}')\n",
    "        plt.plot(train_send_cost_history)\n",
    "    plt.plot(train_send_cost_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Send Cost\")\n",
    "    plt.title(\"Send Cost History\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the training and testing loss history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_loss_history, label='Training Loss')\n",
    "        plt.plot(test_loss_history, label='Testing Loss')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_loss_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Training Loss Rate\")\n",
    "    plt.title(\"Average Training Loss History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the error rate history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_error_history, label='Training Error Rate')\n",
    "        plt.plot(test_error_history, label='Testing Error Rate')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_error_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Average Error Rate\")\n",
    "    plt.title(\"Average Error Rate History\")\n",
    "    plt.show()\n",
    "\n",
    "    # Variance Anaylsis\n",
    "    train_loss_var_history[client] = np.var(train_loss_history)\n",
    "    print(f'Train Loss Variance: {train_loss_var_history[client]}')\n",
    "    test_loss_var_history[client] = np.var(test_loss_history)\n",
    "    print(f'Test Loss Variance: {test_loss_var_history[client]}')\n",
    "\n",
    "    train_loss_var_history_uphalf[client] = np.var(train_loss_history[int(len(train_loss_history) / 2):])\n",
    "    print(f'Train Loss Variance For Half: {train_loss_var_history_uphalf[client]}')\n",
    "    test_loss_var_history_uphalf[client] = np.var(test_loss_history[int(len(test_loss_history) / 2):])\n",
    "    print(f'Test Loss Variance For Half: {test_loss_var_history_uphalf[client]}')\n",
    "\n",
    "    train_error_var_history[client] = np.var(train_error_history)\n",
    "    print(f'Train error Variance: {train_error_var_history[client]}')\n",
    "    test_error_var_history[client] = np.var(test_error_history)\n",
    "    print(f'Test error Variance: {test_error_var_history[client]}')\n",
    "\n",
    "    train_error_var_history_uphalf[client] = np.var(train_error_history[int(len(train_error_history) / 2):])\n",
    "    print(f'Train error Variance For Half: {train_loss_var_history_uphalf[client]}')\n",
    "    test_error_var_history_uphalf[client] = np.var(test_error_history[int(len(test_error_history) / 2):])\n",
    "    print(f'Test error Variance For Half: {test_loss_var_history_uphalf[client]}')\n",
    "\n",
    "    # Plot the train time history\n",
    "    if test_loader is not None:\n",
    "        plt.plot(train_time_history, label='Training Time History')\n",
    "        plt.plot(test_time_history, label='Testing Time History')\n",
    "        plt.legend()\n",
    "    else:\n",
    "        plt.plot(train_time_history)\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Time used\")\n",
    "    plt.title(\"Iteration Time History\")\n",
    "    plt.show()\n",
    "\n",
    "    return train_loss_history, train_error_history, train_send_cost_history, train_time_history, test_loss_history, test_error_history, test_send_cost_history, test_time_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Federated Learning Expriment Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_FedLearn_model(global_model_class_setup_func, train_loader, test_loader, num_epochs_list, num_clients_list, local_update_epochs_list, client_setup_func, iterate_func, aggregate_func, compareClients=False):\n",
    "    # Cost History Total\n",
    "    num_clients_list_size = len(num_clients_list)\n",
    "    local_update_epochs_list_size = len(local_update_epochs_list)\n",
    "    train_loss_history_total = []\n",
    "    train_error_history_total = []\n",
    "    train_send_cost_history_total = []\n",
    "    train_time_history_total = []\n",
    "    test_loss_history_total = []\n",
    "    test_error_history_total = []\n",
    "    test_send_cost_history_total = []\n",
    "    test_time_history_total = []\n",
    "    experiment_type = \"test\"\n",
    "\n",
    "    if compareClients is True:\n",
    "        iteration_list = num_clients_list\n",
    "        experiment_type = \"num_clients\"\n",
    "    else:\n",
    "        iteration_list = local_update_epochs_list\n",
    "        experiment_type = \"local_update_epochs\"\n",
    "\n",
    "    for n in range(len(iteration_list)):\n",
    "        if compareClients is True:\n",
    "            print(f'=== The training for num_clients is {num_clients_list[n]} ===')\n",
    "            num_epochs = num_epochs_list[n]\n",
    "            num_clients = num_clients_list[n]\n",
    "            local_update_epochs = local_update_epochs_list[0]\n",
    "        else:\n",
    "            print(f'=== The training for local_update_epochs is {local_update_epochs_list[n]} ===')\n",
    "            num_epochs = num_epochs_list[n]\n",
    "            num_clients = num_clients_list[0]\n",
    "            local_update_epochs = local_update_epochs_list[n]\n",
    "\n",
    "        # Define global neural network model, loss criterion and optimizer\n",
    "        model, criterion, optimizer = global_model_class_setup_func()\n",
    "        print(model)\n",
    "\n",
    "        # Perform training\n",
    "        train_loss_history, train_error_history, train_send_cost_history, train_time_history, test_loss_history, test_error_history, test_send_cost_history, test_time_history = fit_FedLearn_model(model, train_loader, test_loader, num_epochs, num_clients, client_setup_func, local_update_epochs, optimizer, criterion, iterate_func, aggregate_func, show_history=True)\n",
    "\n",
    "        # Record the history of loss, error and send_cost\n",
    "        train_loss_history_total.append(train_loss_history)\n",
    "        train_error_history_total.append(train_error_history)\n",
    "        train_send_cost_history_total.append(train_send_cost_history)\n",
    "        train_time_history_total.append(train_time_history)\n",
    "        test_loss_history_total.append(test_loss_history)\n",
    "        test_error_history_total.append(test_error_history)\n",
    "        test_send_cost_history_total.append(test_send_cost_history)\n",
    "        test_time_history_total.append(test_time_history)\n",
    "\n",
    "        # Save the npy result for client (train_loss, train_send_cost, train_time, test_error)\n",
    "        #filename = \"{}_{}_{}_{}epoch_result\".format(dataset_name, experiment_type, iteration_list[n-1], num_epochs)\n",
    "        #with open('filename', 'wb') as f:\n",
    "        #    np.save(f, train_loss_history)\n",
    "        #    np.save(f, train_error_history)\n",
    "        #    np.save(f, train_send_cost_history)\n",
    "        #    np.save(f, train_time_history)\n",
    "        #    np.save(f, test_loss_history)\n",
    "        #    np.save(f, test_error_history)\n",
    "        #    np.save(f, test_send_cost_history)\n",
    "        #    np.save(f, test_time_history)\n",
    "    \n",
    "    # Save the npy result (train_loss_history_total)\n",
    "    total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "    with open(total_filename, 'wb') as f:\n",
    "        np.savez(f, train_loss = train_loss_history_total, train_error = train_error_history_total, train_send_cost = train_send_cost_history_total, train_time = train_time_history_total, test_loss = test_loss_history_total, test_error = test_error_history_total, test_send_cost = test_send_cost_history, test_time = test_time_history_total)\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "        #np.save(f, )\n",
    "    \n",
    "    print(f'=== The Experiment Result ===')\n",
    "    # Show Dataset Name\n",
    "    print(f'Current Dataset: {dataset_name}')\n",
    "\n",
    "    # Training Result\n",
    "    print(f'!-- Training Result --!')\n",
    "    if compareClients is True:\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = train_send_cost_history_total[0]\n",
    "        for sublist in train_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_per_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = train_send_cost_history_total[0]\n",
    "        for sublist in train_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_per_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'!-- x-axis as global epochs --!')\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_loss_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_{dataset_name}_num_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(train_error_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_{dataset_name}_num_clients_training.png')\n",
    "        plt.show()\n",
    "\n",
    "    else:\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_local_update_epochs_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_send_cost_history_total[i], train_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_local_update_epochs_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'!-- x-axis as global epochs --!')\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_loss_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'Loss_epochs_{dataset_name}_local_update_epochs_training.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(train_error_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'ErrorRate_epochs_{dataset_name}_local_update_epochs_training.png')\n",
    "        plt.show()\n",
    "\n",
    "    # Testing Result\n",
    "    print(f'!-- Testing Result --!')\n",
    "    if compareClients is True:\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Testing Loss Rate\")\n",
    "        plt.title(\"Testing Loss Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = test_send_cost_history_total[0]\n",
    "        for sublist in test_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Testing Loss Rate\")\n",
    "        plt.title(\"Testing Loss Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_num_clients_per_clients_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the training loss rate between cost history with num_clients per clients\n",
    "        min_cost_list = test_send_cost_history_total[0]\n",
    "        for sublist in test_send_cost_history_total:\n",
    "            if len(sublist) < len(min_cost_list):\n",
    "                min_cost_list = sublist\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(min_cost_list, test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost per clients\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost per clients (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_num_clients_per_clients_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'!-- x-axis as global epochs --!')\n",
    "        # Plot the training loss rate between cost history with num_clients\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_loss_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Testing Loss Rate\")\n",
    "        plt.title(\"Testing Loss Rate (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'Loss_{dataset_name}_num_clients_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(num_clients_list_size):\n",
    "            plt.plot(test_error_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate (with different number of clients)\")\n",
    "        plt.legend(num_clients_list)\n",
    "        plt.savefig(f'ErrorRate_{dataset_name}_num_clients_testing.png')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_loss_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'Loss_vs_Send_Cost_{dataset_name}_local_update_epochs_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_send_cost_history_total[i], test_error_history_total[i])\n",
    "        plt.xlabel(\"Send Cost\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate vs Send Cost (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'ErrorRate_vs_Send_Cost_{dataset_name}_local_update_epochs_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        print(f'!-- x-axis as global epochs --!')\n",
    "        # Plot the training loss rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_loss_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Training Loss Rate\")\n",
    "        plt.title(\"Training Loss Rate (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'Loss_epochs_{dataset_name}_local_update_epochs_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "        # Plot the error rate between cost history with local_update_epochs\n",
    "        for i in range(local_update_epochs_list_size):\n",
    "            plt.plot(test_error_history_total[i])\n",
    "        plt.xlabel(\"Epochs\")\n",
    "        plt.ylabel(\"Error Rate\")\n",
    "        plt.title(\"Error Rate (with different local update epochs)\")\n",
    "        plt.legend(local_update_epochs_list)\n",
    "        plt.savefig(f'ErrorRate_epochs_{dataset_name}_local_update_epochs_testing.png')\n",
    "        plt.show()\n",
    "\n",
    "    # Compare training and testing result\n",
    "    \n",
    "\n",
    "    # Save the training result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_global_model_func_CNN():\n",
    "    if dataset_name == \"CIFAR-10 Datset\":\n",
    "        model = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "    else:\n",
    "        model = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay = 0)\n",
    "    return model, criterion, optimizer\n",
    "\n",
    "def simple_client_setup_func_CNN(num_clients, client_model_list, client_optimizer_list, client_criterion_list, client_dataloader_list):\n",
    "    for client in range(num_clients):\n",
    "        if dataset_name == \"CIFAR-10 Datset\":\n",
    "            client_model_list[client] = ConvolutionalNeuralNetwork_CIFAR10(num_classes_preset)\n",
    "        else:\n",
    "            client_model_list[client] = ConvolutionalNeuralNetwork(num_classes_preset)\n",
    "        client_criterion_list[client] = nn.CrossEntropyLoss()\n",
    "        client_optimizer_list[client] = torch.optim.SGD(client_model_list[client].parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiment for num_clients with CNN model\n",
    "\n",
    "# Define the model parameters\n",
    "learning_rate = learning_rate_preset\n",
    "num_epochs = 5\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "num_epochs_list = [50, 5]\n",
    "num_clients_list = [1,2,5]\n",
    "local_update_epochs_list = [2]\n",
    "\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "experiment_type = \"num_clients\"\n",
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "\n",
    "# Experiment\n",
    "experiment_FedLearn_model(simple_global_model_func_CNN, train_loader, test_loader, num_epochs_list, num_clients_list, local_update_epochs_list, simple_client_setup_func_CNN, iterate_CNN_model, Federated_Averaging, compareClients=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expirement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Dataset: CIFAR-10 Datset\n",
      "=== The training for local_update_epochs is 1 ===\n",
      "ConvolutionalNeuralNetwork_CIFAR10(\n",
      "  (activation_stack): Sequential(\n",
      "    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU()\n",
      "    (8): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU()\n",
      "    (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU()\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU()\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (18): Flatten(start_dim=1, end_dim=-1)\n",
      "    (19): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (20): ReLU()\n",
      "    (21): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (22): ReLU()\n",
      "    (23): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "!-- Training Session --!\n"
     ]
    }
   ],
   "source": [
    "### Experiment for local_update_epochs with CNN model\n",
    "\n",
    "# Define the model parameters\n",
    "learning_rate = 0.2\n",
    "batch_size = batch_size_preset\n",
    "\n",
    "num_epochs_list = [1,1]\n",
    "num_clients_list = [1]\n",
    "local_update_epochs_list = [1,2]\n",
    "num_epochs = num_epochs_list[0]\n",
    "\n",
    "CNN_input_shape = CNN_input_shape_preset\n",
    "\n",
    "# Show Dataset Name\n",
    "print(f'Current Dataset: {dataset_name}')\n",
    "experiment_type = \"local_update_epochs\"\n",
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs)\n",
    "\n",
    "# Experiment\n",
    "experiment_FedLearn_model(simple_global_model_func_CNN, train_loader, test_loader, num_epochs_list, num_clients_list, local_update_epochs_list, simple_client_setup_func_CNN, iterate_CNN_model, Federated_Averaging, compareClients=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_filename = \"{}_{}_{}epoch_result\".format(dataset_name, experiment_type, num_epochs) #update name of result .npz file\n",
    "result = np.load(total_filename) #load data into 'result'\n",
    "print('filename: ', total_filename)\n",
    "print(result.files) #show attributes inside 'result'\n",
    "\n",
    "data_train_loss = result['train_loss']    #np.load(total_filename['train_loss'])\n",
    "data_train_error = result['train_error']    #np.load(total_filename['train_error'])\n",
    "data_train_send_cost = result['train_send_cost']    #np.load(total_filename['train_send_cost'])\n",
    "data_train_time = result['train_time']    #np.load(total_filename['train_time'])\n",
    "data_test_loss = result['test_loss']    #np.load(total_filename['test_loss'])\n",
    "data_test_error = result['test_error']    #np.load(total_filename['test_error'])\n",
    "data_test_send_cost = result['test_send_cost']    #np.load(total_filename['test_send_cost'])\n",
    "data_test_time = result['test_time']    #np.load(total_filename['test_time'])\n",
    "\n",
    "print(\"=======TRAIN RESULT=======\")\n",
    "print(\"train loss: \", data_train_loss)\n",
    "print(\"train error: \", data_train_error)\n",
    "print(\"train send cost: \", data_train_send_cost)\n",
    "print(\"train time: \", data_train_time)\n",
    "print(\"=======TEST RESULT=======\")\n",
    "print(\"test loss: \", data_test_loss)\n",
    "print(\"test error: \", data_test_error)\n",
    "print(\"test send cost: \", data_test_send_cost)\n",
    "print(\"test time: \", data_test_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
